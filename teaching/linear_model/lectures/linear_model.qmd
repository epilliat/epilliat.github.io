---
title: "Linear Regression Model"
format: 
  html:
    incremental: true
    callout-icon: false
    theme: [default]
smaller: true
css: ../../../styles.css
#filters:
  #- parse-latex
---

AI was used to assist with the formatting and writing of the proofs on this page.

# Estimation of $\hat \beta$

## Formula

By definition of the orthogonal projection, the vector $Y - X\hat{\beta}$ is orthogonal to any vector in the space $[X]$, in particular to each of the vectors $X^{(j)}$. Thus, for $j = 1, \ldots, p$, the scalar product between $Y - X\hat{\beta}$ and $X^{(j)}$ is zero, i.e.

$$\left(X^{(j)}\right)^T(Y - X\hat{\beta}) = 0.$$

Since this relation is true for all $j = 1, \ldots, p$, we have

$$X^T(Y - X\hat{\beta}) = 0,$$

where this time $0$ denotes the zero vector of size $p$. This implies $X^T X\hat{\beta} = X^T Y$. Finally, the hypothesis $\text{rg}(X) = p$ guarantees that the matrix $X^T X$ is invertible, and we obtain the result 

$$\hat{\beta} = (X^T X)^{-1}X^T Y.$$

## Expectation

We compute:

$$\mathbb{E}(\hat{\beta}) = \mathbb{E}((X^T X)^{-1}X^T Y) = (X^T X)^{-1}X^T \mathbb{E}(Y)$$

since $(X^T X)^{-1}X^T$ is a deterministic matrix and expectation is linear. Furthermore

$$\mathbb{E}(Y) = \mathbb{E}(X\beta + \varepsilon) = X\beta + \mathbb{E}(\varepsilon) = X\beta$$

since $\mathbb{E}(\varepsilon) = 0$. We indeed obtain $\mathbb{E}(\hat{\beta}) = (X^T X)^{-1}X^T X\beta = \beta$.


## Variance

For the variance, let us recall that for any deterministic matrix $A$ and for any random vector $Z$, $\mathbb{V}(AZ) = A\mathbb{V}(Z)A^T$. We thus have

$$\mathbb{V}(\hat{\beta}) = \mathbb{V}((X^T X)^{-1}X^T Y) = (X^T X)^{-1}X^T \mathbb{V}(Y)((X^T X)^{-1}X^T)^T.$$

Now $\mathbb{V}(Y) = \mathbb{V}(X\beta + \varepsilon) = \mathbb{V}(\varepsilon) = \sigma^2 I_n$ and $((X^T X)^{-1}X^T)^T = (X^T)^T ((X^T X)^{-1})^T = X(X^T X)^{-1}$, since the matrix $X^T X$ (and therefore also $(X^T X)^{-1}$) is symmetric.

We therefore arrive at

$$\mathbb{V}(\hat{\beta}) = (X^T X)^{-1}X^T \sigma^2 I_n X(X^T X)^{-1} = \sigma^2(X^T X)^{-1}X^T X(X^T X)^{-1} = \sigma^2(X^T X)^{-1}.$$


### Convergence of $\hat \beta$

Concerning convergence in quadratic mean, let us recall that the mean squared error, defined by $\text{MSE}(\hat{\beta}) = \mathbb{E}(\|\hat{\beta} - \beta\|^2)$, decomposes into "variance + bias$^2$" which translates in the vectorial case to

$$\text{MSE}(\hat{\beta}) = \mathbb{V}(\hat{\beta}) + (\mathbb{E}(\hat{\beta}) - \beta)^T(\mathbb{E}(\hat{\beta}) - \beta).$$

Since $\mathbb{E}(\hat{\beta}) = \beta$, we have $\text{MSE}(\hat{\beta}) = \mathbb{V}(\hat{\beta}) = \sigma^2(X^T X)^{-1}$. This quantity tends to $0$ when $n \to +\infty$ as soon as $(X^T X)^{-1}$ tends to $0$ when $n \to +\infty$.

## Gauss Markov

$\newcommand{\VS}{\quad \mathrm{VS} \quad}$
$\newcommand{\and}{\quad \mathrm{and} \quad}$
$\newcommand{\E}{\mathbb E}$
$\newcommand{\P}{\mathbb P}$
$\newcommand{\Var}{\mathbb V}$

::: {.callout-note}

Under the same assumptions, if $\tilde \beta$ is another [linear and unbiased estimator]{style="background-color: yellow;"} then $$\mathbb V(\hat \beta) \preceq \mathbb V(\tilde \beta),$$

where $A\preceq B$  means that $B-A$ is a symmetric positive semidefinite matrix

:::


With the previous notations, since $\tilde{\beta} = MY$ is assumed to be unbiased, we have $\mathbb{E}(\tilde{\beta}) = \beta$ for all $\beta$, that is $M\mathbb{E}(Y) = \beta$, in other words $MX\beta = \beta$ for all $\beta$. This last relation means that $MX = I_p$, which is the only matrix satisfying this equality for all $\beta$.

There are several ways to prove the theorem. One of them consists in starting from the calculation of the covariance matrix between $\tilde{\beta}$ and $\hat{\beta}$:

$$\text{Cov}(\tilde{\beta}, \hat{\beta}) = \mathbb{E}((\tilde{\beta} - \mathbb{E}(\tilde{\beta}))(\hat{\beta} - \mathbb{E}(\hat{\beta}))^T).$$

Now $\tilde{\beta} - \mathbb{E}(\tilde{\beta}) = MY - \beta = M(X\beta + \varepsilon) - \beta = M\varepsilon$ since $MX\beta = \beta$. Similarly $\hat{\beta} - \mathbb{E}(\hat{\beta}) = (X^TX)^{-1}X^T\varepsilon$ (this is the case $M = (X^TX)^{-1}X^T$). We therefore obtain

$$\text{Cov}(\tilde{\beta}, \hat{\beta}) = \mathbb{E}(M\varepsilon\varepsilon^T((X^TX)^{-1}X^T)^T) = M\mathbb{E}(\varepsilon\varepsilon^T)X(X^TX)^{-1}.$$

Now since $\varepsilon$ is centered, $\mathbb{E}(\varepsilon\varepsilon^T) = \mathbb{V}(\varepsilon) = \sigma^2I_n$, and since $MX = I_p$ we conclude

$$\text{Cov}(\tilde{\beta}, \hat{\beta}) = \sigma^2(X^TX)^{-1}.$$

This means that $\text{Cov}(\tilde{\beta}, \hat{\beta}) = \mathbb{V}(\hat{\beta})$. We wish at this stage to use the Cauchy-Schwarz inequality, but since this last equality concerns matrices, we must first reduce to scalars. Let $c$ be any vector of dimension $p$. From the previous equality we deduce $c^T\text{Cov}(\tilde{\beta}, \hat{\beta})c = c^T\mathbb{V}(\hat{\beta})c$, that is $\text{Cov}(c^T\tilde{\beta}, c^T\hat{\beta}) = \mathbb{V}(c^T\hat{\beta})$. This equality concerns scalars. We now apply the Cauchy-Schwarz inequality:

$$\mathbb{V}(c^T\hat{\beta}) = \text{Cov}(c^T\tilde{\beta}, c^T\hat{\beta}) \leq \sqrt{\mathbb{V}(c^T\tilde{\beta})\mathbb{V}(c^T\hat{\beta})}.$$

By simplifying both sides by $\sqrt{\mathbb{V}(c^T\hat{\beta})}$, we arrive at

$$\sqrt{\mathbb{V}(c^T\hat{\beta})} \leq \sqrt{\mathbb{V}(c^T\tilde{\beta})},$$

and therefore $\mathbb{V}(c^T\hat{\beta}) \leq \mathbb{V}(c^T\tilde{\beta})$. Since this inequality is valid for any vector $c$, this means that $\mathbb{V}(\tilde{\beta}) - \mathbb{V}(\hat{\beta})$ is positive semi-definite, which is what we wanted to show.





# Maximum Likelihood Estimators for Linear Regression

## Maximum Likelihood Estimator

::: {.callout-note}
## MLE
Let $\hat \beta_{MLE}$ and $\hat \sigma_{MLE}^2$ be the MLE of $\beta$ and $\sigma^2$, respectively.


- $\hat{\beta}_{MLE} = \hat{\beta}$ et $\hat{\sigma}^2_{MLE} = \frac{SCR}{n} = \frac{n-p}{n} \hat{\sigma}^2$.

- $\hat{\beta} \sim N(\beta, \sigma^2(X^TX)^{-1})$.

- $\frac{n-p}{\sigma^2} \hat{\sigma}^2 = \frac{n}{\sigma^2} \hat{\sigma}^2_{MLE} \sim \chi^2(n - p)$.

- $\hat{\beta}$ and $\hat{\sigma}^2$ are independent

:::

## Proof {#proof-mle}

### Setup
Model: $Y = X\beta + \varepsilon$ where $\varepsilon \sim N(0, \sigma^2 I)$

This means: $Y \sim N(X\beta, \sigma^2 I)$

### Likelihood Function

For $n$ observations, the likelihood function is:

$$L(\beta, \sigma^2) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2} (Y - X\beta)^T(Y - X\beta)\right)$$

### Log-Likelihood Function

$$\ell(\beta, \sigma^2) = \log L(\beta, \sigma^2) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (Y - X\beta)^T(Y - X\beta)$$

$$\ell(\beta, \sigma^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} (Y - X\beta)^T(Y - X\beta)$$

### Finding MLE for $\beta$

Taking the partial derivative with respect to $\beta$:

$$\frac{\partial \ell}{\partial \beta} = \frac{1}{\sigma^2} X^T(Y - X\beta)$$

Setting equal to zero:
$$X^T(Y - X\hat{\beta}_{MLE}) = 0$$

$$X^TY - X^TX\hat{\beta}_{MLE} = 0$$

$$X^TX\hat{\beta}_{MLE} = X^Ty$$

Therefore: 
$$\hat{\beta}_{MLE} = (X^TX)^{-1}X^Ty$$

### Finding MLE for $\sigma^2$

Taking the partial derivative with respect to $\sigma^2$:

$$\frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} (Y - X\beta)^T(Y - X\beta)$$

Setting equal to zero:
$$-\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} (Y - X\hat{\beta}_{MLE})^T(Y - X\hat{\beta}_{MLE}) = 0$$

Multiplying by $2\sigma^4$:
$$-n\sigma^2 + (y - X\hat{\beta}_{MLE})^T(y - X\hat{\beta}_{MLE}) = 0$$

Therefore: 
$$\hat{\sigma}^2 = \frac{1}{n} (y - X\hat{\beta}_{MLE})^T(y - X\hat{\beta}_{MLE}) = \frac{SSR}{n}$$

### Verification (Second-Order Conditions)

The Hessian matrix has:

$$\frac{\partial^2 \ell}{\partial \beta \partial \beta'} = -\frac{1}{\sigma^2} X^TX$$

This is negative definite (assuming $X^TX$ is invertible), confirming $\hat{\beta}$ is a maximum.

$$\frac{\partial^2 \ell}{\partial (\sigma^2)^2} = \frac{n}{2\sigma^4} - \frac{1}{\sigma^6} (y - X\beta)^T(y - X\beta)$$

At the MLE: $\frac{\partial^2 \ell}{\partial (\sigma^2)^2}\bigg|_{\hat{\sigma}^2} = \frac{n}{2\sigma^4} - \frac{n}{\sigma^4} = -\frac{n}{2\sigma^4} < 0$

This confirms $\hat{\sigma}^2$ is a maximum.

### Key Properties

- Consistency: Both estimators are consistent
- Bias: $\hat{\beta}$ is unbiased, but $\hat{\sigma}^2$ is biased (divides by $n$ instead of $n-k$)
- Efficiency: Under normality, these MLEs achieve the Cramér-Rao lower bound
- Relationship to OLS: $\hat{\beta}_{MLE} = \hat{\beta}_{OLS}$ under normality assumption




# $\hat \beta$ is an Efficient Estimator in the Gaussian Model {#efficient-beta}

::: {.callout-note}
## Theorem
In the Gaussian Model, $\hat \beta$ is an [efficient]{style="background-color: yellow;"} estimator of $\hat \beta$. This means that 
$$
\Var(\hat \beta) \preceq \Var(\tilde \beta)\; ,
$$
for [any]{style="background-color: yellow;"} estimator $\tilde \beta$

:::

## Setup
Consider the linear regression model:
$$Y = X\beta + \varepsilon, \quad \varepsilon \sim N(0, \sigma^2 I)$$

We want to prove that $\hat \beta = (X^TX)^{-1}X^TY$ is efficient.

## Definition of Efficiency
An unbiased estimator is efficient if it achieves the Cramér-Rao lower bound:
$$\text{Var}(\hat \beta) = [I(\beta)]^{-1}$$
where $I(\beta)$ is the Fisher Information Matrix.
This comes from the [Cramér-Rao lower bound](../notes/cramer-rao.qmd) 

## Step 1: Fisher Information Matrix

The log-likelihood function is:
$$\ell(\beta, \sigma^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}(Y - X\beta)^T(Y - X\beta)$$

First derivative with respect to $\beta$:
$$\frac{\partial \ell}{\partial \beta} = \frac{1}{\sigma^2}X^T(Y - X\beta)$$

Second derivative:
$$\frac{\partial^2 \ell}{\partial \beta \partial \beta^T} = -\frac{1}{\sigma^2}X^TX$$

Fisher Information Matrix for $\beta$:
$$I(\beta) = -\mathbb{E}\left[\frac{\partial^2 \ell}{\partial \beta \partial \beta^T}\right] = \frac{1}{\sigma^2}X^TX$$

Cramér-Rao lower bound:
$$[I(\beta)]^{-1} = \sigma^2(X^TX)^{-1}$$

## Step 2: Variance of $\hat \beta$

$$\hat \beta = (X^TX)^{-1}X^TY = (X^TX)^{-1}X^T(X\beta + \varepsilon) = \beta + (X^TX)^{-1}X^T\varepsilon$$

Since $\varepsilon \sim N(0, \sigma^2 I)$:
$$\text{Var}(\hat \beta) = \text{Var}((X^TX)^{-1}X^T\varepsilon)$$

$$= (X^TX)^{-1}X^T \cdot \text{Var}(\varepsilon) \cdot X(X^TX)^{-1}$$

$$= (X^TX)^{-1}X^T \cdot \sigma^2 I \cdot X(X^TX)^{-1}$$

$$= \sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}$$

$$= \sigma^2(X^TX)^{-1}$$

## Step 3: Verification of Efficiency

We have shown:
- Cramér-Rao bound: $[I(\beta)]^{-1} = \sigma^2(X^TX)^{-1}$
- Variance of $\hat \beta$: $\text{Var}(\hat \beta) = \sigma^2(X^TX)^{-1}$

Since:
$$\text{Var}(\hat \beta) = [I(\beta)]^{-1}$$

The estimator $\hat \beta$ achieves the Cramér-Rao lower bound.

## Conclusion

Therefore, $\hat \beta = (X^TX)^{-1}X^TY$ is an efficient estimator of $\beta$ in the Gaussian linear regression model.

## Additional Notes

- This efficiency holds specifically under the normality assumption
- $\hat \beta$ is also the Best Linear Unbiased Estimator (BLUE) by the Gauss-Markov theorem
- Under normality, $\hat \beta$ is the Best Unbiased Estimator (BUE) among all estimators, not just linear ones
