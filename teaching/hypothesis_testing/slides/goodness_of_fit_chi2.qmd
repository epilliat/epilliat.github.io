---
title: "Goodness of Fit Tests"
format: 
  revealjs:
    incremental: true
    callout-icon: false
    theme: [default, custom.scss]
    html-math-method: katex
smaller: true
css: style.css
#filters:
  #- parse-latex
---


## Multinomials 

::::{.columns}

:::{.column}
::: {.fragment}
:::{.callout-note}
## Binomial Distribution
- Draw $n$ balls, **blue or red**.
- $p_1, (1-p_1)$: proportion of blues/red
- $X$, $Y$: counts of blues/red
- $X \sim \mathrm{Bin}(n,p_1)$
- $\mathbb P((X,Y) = (k_1,k_2)) = \binom{n}{k_1}p_1^k(1-p_1)^{k_2}$ \
  if $k_1 +k_2 = n$
:::
:::
:::

:::{.column}
::: {.fragment}
:::{.callout-note}
## Multinomial Distribution
- Draw $n$ balls, **$m$ potential colors**.
- $(p_1, \dots, p_m)$: proportions of each color: $\sum_{i=1}^m p_i = 1$
- $X_1, \dots, X_m$: count of each color
- $(X_1, \dots, X_m) \sim \mathrm{Mult}(n,(p_1, \dots, p_m))$
- $\mathbb P((X_1, \dots, X_m)=(k_1, \dots, k_m)) = \frac{n!}{k_1!\dots k_m!}p_1^{k_1} \dots p_m^{k_m}$\
  if $k_1 + \dots + k_m = n$
:::
:::
:::

::::

- $\frac{n!}{k_1!\dots k_m!} = \binom{n}{k_1,\dots, k_m}$ is a **multinomial** coefficient
- In this course: $n \gg m$.
- $m \gg n$ corresponds to a high-dimensional setting.



# $\chi^2$ Goodness of Fit

## $\chi^2$ Goodness of Fit Test

- We observe $(X_1, \dots, X_m) \sim \mathrm{Mult}(n, q)$
  This corresponds to $n$ counts: $X_1 + \dots + X_m = n$
- $q = (q_1, \dots, q_m)$ corresponds to probabilities of getting color $1, \dots, m$
- Let $p = (p_1, \dots, p_m)$ be a known vector s.t. $p_1 + \dots + p_m = 1$.
- $H_0:~ q = p ~~~\text{or}~~~ H_1: q \neq p \; .$

:::{.columns}


:::{.column}
::: {.fragment}
::: {.callout-note}
## $\chi^2$ Goodness of Fit Test (Adéquation)

- Chi-Squared Test Statistic:
$$\psi(X) = \sum_{i=1}^m\frac{(X_i-n_i)^2}{n_i} \; ,$$
where $n_i = np_i = \mathbb E[X_i]$ is the expected number of counts for color $i.$
- When $np_i = n_i$ are large, under $H_0$, we approximate the distribution of $\psi(X)$ as
$\psi(X) \sim \chi^2(m-1)$
- Reject if $\psi(X) > t_{1- \alpha}$ (right-tail of $\chi^2(m-1)$)
:::
:::
:::

:::{.column}
:::{.fragment}
![](images/chisq3.svg)
:::
:::

:::

## Example: Bag of Sweets

::: {.callout}
- We observe a **bag of sweets** containing $n=100$ sweets of $m=3$ different colors: **red**, **green**, and **yellow**. 
- Manufacturer: $p_1= 40\%$ red, $p_2=35\%$ green, and $p_3=25\%$ yellow.
- $H_0: q=p$ (manufacturer's claim is correct)
- $H_1: q\neq p$ (manufacturer's claim is incorrect)
:::


:::{.columns}
:::{.column}

:::{.fragment style="font-size:50%;"}
|Color|Observed Counts|
|---|---|
|Red|$X_1=50$|
|Green|$X_2=30$|
|Yellow|$X_3=20$|
:::
:::
:::{.column}
:::{.fragment style="font-size:50%;"}
|Expected Counts|
|---|
|$n_1=40$|
|$n_2=35$|
|$n_3=25$|
:::
:::
::: 

::: {.columns}

::: {.column}

::: {.callout .fragment}
- $\psi(X) = \sum_{i=1}^m\frac{(X_i-n_i)^2}{n_i} \approx 2.5 +0.71+1 \approx 4.21$
- $\mathrm{cdf}(\chi^2(2), 4.21) \approx 0.878$ ($p_{value} \approx 0.222$)
- Conclusion: do not reject $H_0$
:::

:::
::: {.column}

::: {.fragment}
![](images/chi-2-sweets.svg)

:::
::: 
::: 




# Comparison to a Theoretical Disrtribution


## Histograms

::::{.columns}

:::{.column}
:::{.callout-note .fragment}

## histogram 
- We observe $(X_1, \dots, X_n) \in \mathbb R^n$\
- $\mathrm{counts}(I) = \sum \mathbf 1\{X_i \in I\} \in \{1, \dots, n\}\; .$
- $\mathrm{freq}(I) = \mathrm{counts}(I)/n$
- $\mathrm{hist}(a,b,k) = (\mathrm{counts}(I_1), \dots,\mathrm{counts}(I_k))$
- where $I_l = \big[a + (l-1)\tfrac{b-a}{k},a + l\tfrac{b-a}{k}\big)$
:::

:::{.callout-warning icon="true" .fragment}
## Normalization
 Can be normalized in **counts** (default), **frequency**, or **density** (area under the curve = 1)
:::
:::

:::{.column}
:::{.fragment}
![](images/histogram.svg)
:::
:::

::::

::::{.columns}
:::{.column}

:::{.callout-note .fragment}

## Law of Large Number, Monte Carlo (Informal)
  - Assume that $(X_1, \dots, X_n)$ are iid of distrib $P$, and that $a$,$b$, $k$ are fixed
  - The histogram $\mathrm{hist}(a,b,k)$ converges to the histogram of the density $P$
:::
:::

:::{.column}

:::{.fragment}
![](images/histogram_discrete_continuous.svg){width=60%}
:::

:::

::::


## $\chi^2$ Goodness of Fit to a given distribution

::: {.callout}
- We observe $(X_1, \dots, X_n) \in \mathbb R^n$, iid with **unknown** distrib $P$
- $H_0$: $P = P_0$, where $P_0$ is **known**
- $H_1$: $P \neq P_0$
- Idea: Under $H_0$, the counts in the intervals 
$[a_0, a_1), [a_1, a_2), \dots, [a_{k-1}, a_k)$
follow a **multinomial** distribution $\mathrm{Mult}(n, (p_1, \dots, p_{k}))$ where
$p_1 = P_0([a_0, a_1)), \dots, p_{k} = P_0([a_{k-1}, a_k))$
:::


:::{.callout-note .fragment}
## Reduction to Chi-Squared Statistic Test
1. Count the number of data $(c_1, \dots, c_k)$ falling in $I_1, \dots, I_k$
2. Compare the counts the theoretical $nP_0(I_1) \dots, nP_0(I_k)$ with a chi-squared statistic:
$$ \sum_{j=1}^k \frac{(c_j - nP_0(I_j))^2}{nP_0(I_1)}$$
3. Decide using an $\alpha$-quantile of a $\chi^2(k-1)$ distribution

:::

## Example: Goodness of Fit to a Poisson distribution

$H_0$: $X_i$ iid $\mathcal P(2)$

```julia
X = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]
```

::: {.fragment}

| | $0$ | $1$ | $2$ | $\geq 3$ | Total |
|---| --- | --- | --- | -------- | --- |
| Counts | 16 | 8 | 3 | 3 | 30 |
| Theoretical Counts | 4.06 | 8.1 | 8.1 | 9.7 | |
:::

- To get $9.7$, we compute ```(1-cdf(Poisson(2),2))*30```
- chi square stat $\gtrsim \frac{(16-4)^2}{4} = 36$
- ```(1-cdf(Chisq(3),36))``` is very small: Reject


## Comparison with QQ-Plots

::: {.callout}
- We observe $(X_1, \dots, X_n)$ of **unknown** CDF $F$
- $H_0$: $F = F_0$ where $F_0$ is **known**
- $H_1$: $F \neq F_0$
- We write $X_{(1)} \leq \dots \leq X_{(n)}$ for the ordered data
- **empirical $\frac{k}{n}$-quantile**: $X_{(k)}$
- **$\frac{k}{n}$-quantile**: $x$ such that $F(x) = \frac{k}{n}$

:::

:::{.columns}

:::{.column}
:::{.callout-note .fragment}
## QQ-Plot

- Represent the empirical quantiles in function of the theoretical quantiles.
- Compare the scatter plot with $y=x$
:::
:::
:::{.column}
:::{.fragment}
![](images/qqplot_NNTN.svg)
:::
:::
:::

## Kolmogorov-Smirnov Test

::: {.callout}

::: {.columns}
::: {.column width=50%}
- We observe $(X_1, \dots, X_n)$ of **unknown** CDF $F$
- $H_0$: $F = F_0$ where $F_0$ is **known**
- $H_1$: $F \neq F_0$
- We write $X_{(1)} \leq \dots \leq X_{(n)}$ for the ordered data
- **empirical $\frac{k}{n}$-quantile**: $X_{(k)}$
- **$\frac{k}{n}$-quantile**: $x$ such that $F(x) = \frac{k}{n}$ 
- **Empirical CDF**: $\hat F(x) = \frac{1}{n}\sum_{i=1}^n \mathbf 1\{X_i \leq x\}$
- Idea: Max distance between empirical and true CDF 
:::

::: {.column}
::: {.r-stack}

![](images/empirical_cdf.svg){.fragment}

![](images/ks_illustration.svg){.fragment}

:::
:::
::: 

:::

:::: {.callout-note}
## Kolmogorov-Smirnov Test

- $\psi(X) = \sup_{x}|\hat F(x) - F_0(x)|$
- Approx: $\mathbb P_0(\psi(X) >c/\sqrt{n}) \to 2\sum_{r=1}^{+\infty}(-1)^{r-1}\exp(-2c^2r^2)$ when $n \to +\infty$
- In practice, use Julia, Python or R
::::