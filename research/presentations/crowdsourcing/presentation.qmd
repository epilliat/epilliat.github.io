---
title: "Recovering Labels from Crowdsourced Data"
bibliography: biblio.bib
csl: apa.csl
citation-location: margin
suppress-bibliography: true
subtitle: "An Optimal and Polynomial Time Method"
format: 
  revealjs:
    incremental: true
    slide-number: true
    callout-icon: false
smaller: false
css: "styles.css"
#filters:
  #- parse-latex
---


## Introduction


::: {.notes}

Thank you for having me here today.

For context, much of this work was conducted during my postdoctoral position at ENS Lyon, where  was able to build upon my earlier PhD research in crowdsourcing.

So, let's start with a scenario. Picture a group of workers, and each one is given binary classification tasks. 

For each task, they need to make yes-or-no decisions.

There are many situations where this happens. Think of image classification, text moderation, or sentiment analysis
:::

. . .

[Workers]{style="background-color: yellow;"} are given [binary tasks]{style="background-color: yellow;"} to which they have to give a response: YES or NO

Examples

- **Image Classification**: "Does this image contain a dog?"
- **Text Moderation**: "Is this comment toxic or offensive?"
- **Sentiment Analysis**: "Does this review express positive sentiment?"

## Questions

::: {.notes}

Three natural questions emerge

- what are the actual true labels?
- Can we compare workers and identify who performs better or worse?
- How well do workers perform on a given task?

I will touch on all these issues during my talk, but my primary focus will be on the central question of recovering the true labels.


:::
. . .

[3 questions]{style="background-color: yellow;"}

- [recover the true label]{style="background-color: lightblue;"}?
- rank the workers?
- estimate their abilities?

. . .


**Main Quetion**: How can we accurately [recover the labels]{style="background-color: lightblue;"}?

## This Talk


::: {.notes}

I will break down my presentation in 3 parts. 

I'll start by introducing isotonic model.

Next, I'll review two existing methods. 

In the final part, I'll go over an optimal and computationnally feasible method for this problem.

:::

1. Introducing the non-parametric [isotonic model]{style="background-color: yellow;"} and main result
2. Presenting two already [existing algorithms]{style="background-color: yellow;"}
3. Iterative Spectral Voting (ISV) algo and [key insights]{style="background-color: yellow;"}

# The Isotonic Model

## Illustration 

::: {.notes}

Let me start with an illustration.

Consider binary tasks where labels are either $-1$ or $+1$. There is a vector $x^*$ of true labels that we do not know. 

We observe a matrix $Y$ where each entry $(i,k)$ represents the response of worker $i$ to task $k$.

We can have missing values. When a worker doesn't respond to a particular task, we simply put $0$ in that matrix position. 

We write $\lambda$ for the rate of partial observations .

:::

- Two binary tasks with labels in $\{\color{green}{-1},\color{blue}{+1}\}$
- **Unknown** vector $x^*$ of labels with $d=9$ tasks
$x^*= (\color{blue}{+1},\color{blue}{+1},\color{green}{-1},\color{blue}{+1},\color{green}{-1},\color{blue}{+1},\color{blue}{+1},\color{blue}{+1},\color{blue}{+1})$

::: {.r-stack}

::: {.fragment .fade-in-then-out}

::: {.center}
$$
Y=\left(\begin{array}{ccccccccc}
\color{green}{-1} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} & \color{blue}{+1} & \color{green}{-1} & \color{green}{-1} & \color{blue}{+1} & \color{blue}{+1} \\
\color{blue}{+1} & \color{blue}{+1} & \color{blue}{+1} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} \\
\color{blue}{+1} & \color{green}{-1} & \color{green}{-1} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} & \color{blue}{+1} \\
\color{green}{-1} & \color{blue}{+1} & \color{blue}{+1} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} & \color{blue}{+1} & \color{blue}{+1} & \color{blue}{+1}
\end{array}\right)
$$
:::
:::

::: {.fragment}
::: {.center}
$$
Y=\left(\begin{array}{ccccccccc}
\color{red}{0} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} & \color{red}{0} & \color{green}{-1} & \color{green}{-1} & \color{blue}{+1} & \color{blue}{+1} \\
\color{red}{0} & \color{blue}{+1} & \color{red}{0} & \color{blue}{+1} & \color{green}{-1} & \color{red}{0} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} \\
\color{blue}{+1} & \color{green}{-1} & \color{green}{-1} & \color{blue}{+1} & \color{green}{-1} & \color{blue}{+1} & \color{red}{0} & \color{blue}{+1} & \color{blue}{+1} \\
\color{green}{-1} & \color{blue}{+1} & \color{blue}{+1} & \color{red}{0} & \color{green}{-1} & \color{blue}{+1} & \color{red}{0} & \color{red}{0} & \color{red}{0}
\end{array}\right)
$$
:::
:::

:::
- Rate of observations: $\color{red}{\lambda= 0.72}$

<!--
## Observation Model


::: {.notes}

Given a worker $i$ and a task $k$, we observe the response $Y_{ik}$, and we assume this observation model:

- $x^*_k$ is the unknown label of task $k$
- $M_{ik} \in [0,1]$ represents the unknown ability of worker $i$ on task $k$. If $M_{ik}=1$, it means that worker $i$ is very good at task $k$.
- $E_{ik}$ are independent subgaussian noise
- $B_{ik}$ is a $0$-$1$ coefficient that indicates whether worker $i$ considers task $k$ or not

:::

. . .

Given workers $i\in\{1, \dots, n\}$ and tasks $k\in\{1, \dots, d\}$

. . .

We observe $Y_{ik} \in \{-1,0,1\}$
  
::: {.square-def}
$$
Y_{ik} = B_{ik}(M_{ik}x_k^* + E_{ik}) \enspace 
$$
:::

1. $x_k^*\in \{-1,1\}$ [true response]{style="background-color: yellow;"} of task $k$ 
2. $M_{ik} \in [0,1]$ represents the **unknown** [ability of worker $i$ to task $k$]{style="background-color: yellow;"}
3. $E_{ik}$ are independent and [$1$-subGaussian noise]{style="background-color: yellow;"}
4. $B_{ik}$ are iid Bernoulli $\lambda \in [0,1]$. [$=1$ if $(i,k)$ is observed]{style="background-color: yellow;"}

-->

## Observation Model

::: {.notes}

We assume that the matrix of responses follows this model.

- $x^*$ is the vector of **unknown** labels
- $E$ is a matrix of independent sub-gaussian **noise**
- $M \in [0,1]^{n \times d}$ is the **unknown ability** matrix. If $M_{ik}=1$, it means that worker $i$ is very good at question $k$, and $M_{ik}=0$ means that they answer randomly. 
- This odot is a Hadamard Product so you can think of $B$ as a Bernoulli mask matrix.

:::
. . .

We observe

::: {.square-def}
$$Y = B\odot(M\mathrm{diag}(x^*) + E) \in \mathbb R^{n \times d}$$
:::

where $\odot$ is the Hadamard product.



- $x^*\in \{-1, 1\}^d$ is the vector of **unknown** labels
- $E$ is a matrix of independent sub-gaussian **noise**
- $M \in [0,1]^{n \times d}$ is the **unknown ability** matrix.
- $B$ is a Bernoulli $\mathcal B(\lambda)$ "mask" matrix, modelling **partial observations**

<!--
## Bernoulli Observation Model

::: {.notes}

In this talk, I'll assume a model based on Bernoulli random variables, as introduced by Shah et al. in 2020.

- Each worker has proba $(1+M_{ik})/2$ of being correct on task $k$. $M_{ik}=1$ means that worker $i$ knows perfectly the answer and $M_{ik}=0$ means that they answer randomly.
- We observe each response with probability $\lambda$

:::

. . .

::: {.square-def}
$$
\begin{aligned}\label{eq:bernoulli_model}
    Y_{ik} = \begin{cases}
        x^*_k \text{ with proba } \lambda\left(\frac{1+M_{ik}}{2}\right)
        \\
        -x^*_k \text{ with proba } \lambda\left(\frac{1-M_{ik}}{2}\right)\\
        0 \text{ with proba } 1-\lambda
        \end{cases}
\end{aligned}
$$
:::

- $\frac{1+M_{ik}}{2}$ is the proba that $i$ answers **correctly** to task $k$.
- $\lambda \in[0,1]$ is the probability of observing worker/task pair.
- $Y = B\odot (M\mathrm{diag}(x^*) + E)$ where $E$ is has indep. subgaussian entries and $B_{ik} \sim \mathcal B(\lambda)$
-->







## Shape Constraints

::: {.notes}
Until now, I have'nt assumed anything on the ability matrix $M$ except that it has coef. in $[0,1]$.

From now on, we also assume that $M$ is isotonic up to a permutation $\pi^*$ of its rows, that is, 

it has increasing columns, up to an unknown permutation $\pi^*$

It means that, on average, a worker $i$ is either uniformly better or uniformly worse than another worker $j$ 
:::

- $M \in [0,1]^{n \times d}$ represents the **ability** matrix of worker/task pairs
- We assume that [$M$ is isotonic up to a permutation $\pi^*$]{style="background-color: yellow;"} , that is

:::{.r-stack}
:::{.fragment .fade-in-then-out}
$$
M_{\pi^*}=\left(\begin{array}{ccccccccc}
\color{#000099}{0.9} & \color{#000099}{0.8} & \color{#000099}{0.9} & \color{#000099}{1\phantom{.0}} \\
\color{#4B0082}{0.8} & \color{#4B0082}{0.7} & \color{#4B0082}{0.9} & \color{#4B0082}{0.8} \\
\color{#990066}{0.6} & \color{#990066}{0.7} & \color{#990066}{0.7} & \color{#990066}{0.6} \\
\color{#CC0000}{0.5} & \color{#CC0000}{0.7} & \color{#CC0000}{0.5} & \color{#CC0000}{0.6}
\end{array}\right)
$$
:::

:::{.fragment}
$$
M_{\phantom{{\pi^*}}}=\left(\begin{array}{ccccccccc}
\color{#990066}{0.6} & \color{#990066}{0.7} & \color{#990066}{0.7} & \color{#990066}{0.6} \\
\color{#CC0000}{0.5} & \color{#CC0000}{0.7} & \color{#CC0000}{0.5} & \color{#CC0000}{0.6} \\
\color{#000099}{0.9} & \color{#000099}{0.8} & \color{#000099}{0.9} & \color{#000099}{1\phantom{.0}} \\
\color{#4B0082}{0.8} & \color{#4B0082}{0.7} & \color{#4B0082}{0.9} & \color{#4B0082}{0.8}
\end{array}\right)
$$
:::
:::


<!--
## Shape Constraints

::: {.notes}
More formally, the isotonicity constraint can be written as this set of inequalities.

It means that the rows of $M$ are uniformly increasing. 

For any two workers $i$ and $j$, either $i$ or $j$ is uniformly better than the other one.
:::

- For all $k = 1, \dots, d$,

. . .

::: {.square-def}
$$
M_{\pi^*(1), k}\geq \dots \geq M_{\pi^*(n),k}
$$
:::

- It means that the [rows are uniformly increasing]{style="background-color: yellow;"}
- A worker $i$ is better **on average** than $j$, if it is better on **all tasks** on average
-->
<!--
## Illustration when $x^*$ is Known

::: {.notes}
This is an isotonic matrix, and this is a shuffled isotonic matrix. 

If $x^*$ was known, we would observe this bottom right picture.
:::

:::{.r-stack}

![](IllustrationsCrowdsourcing/imshow22_n150_d150_scale0.2.png){.fragment width=60% fig-align="center"}


:::

-->


## Square Norm Loss VS Hamming Loss

::: {.notes} 

A usual loss for an estimator $\hat x$ is the Hamming loss. It simply consists in summing up all the mistakes we've made.

I do not consider Hamming Loss Here. Instead, we restrict $M$ to incorrectly labeled task, and take the square frobenius norm. 

- If workers are bad ($M$ close to $0$), square norm loss is small but Hamming Loss can be of order $d$
- Hence, a very good feature of the square norm loss is that it evaluates the quality of the estimator $\hat x$ instead of the performance of the workers!
:::

::: {.columns}
::: {.column}
::: {.fragment}

::: {.square-def}
**Hamming Loss**:
$$ \sum_{k=1}^d \mathbf 1\{\hat x_k \neq x_k^*\}$$
:::
:::


:::

::: {.column}
::: {.fragment}
::: {.square-def}
**Square Norm Loss**:
$$ \|M \mathrm{diag}(x^* \neq \hat x)\|_F^2$$
:::


:::

:::
::: 

- If [workers are bad]{style="background-color: orange;"} ($M \sim 0$), $M$ is small but [Hamming Loss is large]{style="background-color: orange;"} ($\sim d$)
- [Square norm loss]{style="background-color: lightgreen;"} evaluates the [quality of $\hat x$]{style="background-color: lightgreen;"} rather than performances of workers



## Crowdsourcing Problems


::: {.notes}
In addition to recovering labels, we can define the two other problems I mentioned earlier: 

ranking the workers and estimating their abilities. Each objective corresponds to a similar square norm loss.

:::

:::{.callout-note style="font-size:120%"}
## Setting
$$ Y = B\odot(M \mathrm{diag}(x^*) + E)$$
**Shape constraint**: $\exists \pi^*$ s.t. $M_{\pi^*} \in [0,1]^{n \times d}$ is **isotonic**
:::



::: {.columns}
::: {.column}

:::{.fragment}
### Objectives
:::

::: {.fragment}
**Recovering** the labels ($x^*$)
:::
::: {.fragment}
**Ranking** the workers ($\pi^*$)
:::
::: {.fragment}
**Estimating** their abilities ($M$)
:::
:::
::: {.column}

:::{.fragment}
### Losses
:::

:::{.fragment}
$\phantom{\color{purple}{\mathbb E}}\|M \mathrm{diag}(x^* \neq \hat x)\|_F^2$
:::
:::{.fragment}
$\phantom{\color{purple}{\mathbb E}}\|M_{\pi^*} - M_{\hat \pi}\|_F^2$\
:::
:::{.fragment}
$\phantom{\color{purple}{\mathbb E}}\|M - \hat M\|_F^2$
:::
:::
:::




## Crowdsourcing Problems

:::{.nonincremental}
:::{.callout-note style="font-size:120%"}
## Setting
$$ Y = B\odot(M \mathrm{diag}(x^*) + E)$$
**Shape constraint**: $\exists \pi^*$ s.t. $M_{\pi^*} \in [0,1]^{n \times d}$ is **isotonic**
:::



::: {.columns}
::: {.column}

### Objectives


**Recovering** the labels ($x^*$)\

**Ranking** the workers ($\pi^*$)\

**Estimating** their abilities ($M$)
:::

::: {.column}

### Risks


$\color{purple}{\mathbb E}[\|M \mathrm{diag}(x^* \neq \hat x)\|_F^2]$\

$\color{purple}{\mathbb E}[\|M_{\pi^*} - M_{\hat \pi}\|_F^2]$\

$\color{purple}{\mathbb E}[\|M - \hat M\|_F^2]$
:::
:::

:::



<!--
## Other Parametric Models

- [DS: $M_{ik} = q_i$]{style="background-color: yellow;"} [@dawid1979maximum], [@shah2020permutation]
    - The **abilities** are independent of the tasks
- [BTL: $M_{ik}=\phi(a_i-b_k)$]{style="background-color: yellow;"} [@bradley1952rank]
    - $a_i$: abilities of the workers
    - $b_i$: difficulties of the tasks

:::{.fragment}
These [parametric]{style="background-color: orange;"} models often [fail to fit data well]{style="background-color: orange;"}
:::

## Non-Parametric Models

- [**known labels** $x^*$]{style="background-color: lightgreen;"}, [$M_{\pi^*\eta^*}$ is bi-isotonic]{style="background-color: lightblue;"} [@mao2020towards] [@liu2020better]
- [**known labels** $x^*$]{style="background-color: lightgreen;"}, [$M_{\pi^*}$ is isotonic]{style="background-color: lightblue;"} [@flammarion2019optimal] [@pilliat2024optimal]
- [**unknown labels** $x^*$]{style="background-color: lightgreen;"}, [$M_{\pi^*\eta^*}$ is bi-isotonic]{style="background-color: lightblue;"} [@shah2020permutation]
- [**unknown labels** $x^*$]{style="background-color: lightgreen;"}, [$M_{\pi^*}$ is isotonic]{style="background-color: lightblue;"} (this paper)



## Risks Recap

\

::: {.columns}
::: {.column}

### Objectives

\

**Recovering** the labels \

**Ranking** the workers \

**Estimating** their abilities
:::

::: {.column}


### Risks

\

$\color{purple}{\mathbb E}[\|M \mathrm{diag}(x^* \neq \hat x)\|_F^2]$\

$\color{purple}{\mathbb E}[\|M_{\pi^*} - M_{\hat \pi}\|_F^2]$\

$\color{purple}{\mathbb E}[\|M - \hat M\|_F^2]$
:::
:::

-->

## MiniMax Risk for Recovering Labels

::: {.notes}
This brings us to the minimax risk for recovering labels. 

It is defined as the risk of the best estimator $\hat x$ in the worst case.
:::

. . .

::: {.square-def}
$$\mathcal R^*_{\mathrm{reco}}(n,d,\lambda)=\min_{\hat x}\max_{M,\pi^*, x^*}{\mathbb E}[\|M \mathrm{diag}(x^* \neq \hat x)\|_F^2]$$

:::


- minimize on all estimator $\hat x$
- maximize square norm loss on all $M$, $\pi^*$, $x^*$ such that $M_{\pi^*}$ is **isotonic**
<!--- Similarly, we can define [minimax ranking risk and estimation risks]{style="background-color: yellow;"} for $\pi^*$ and $M$-->

<!--

## Minimax Risks

:::{.callout-note .fragment}
## Recovering labels
$$\mathcal R^*_{\mathrm{reco}}(n,d,\lambda)=\min_{\hat x}\max_{M,\pi^*, x^*}{\mathbb E}[\|M \mathrm{diag}(x^* \neq \hat x)\|_F^2]$$
:::

:::{.callout-note .fragment}
## Ranking workers
$$\mathcal R^*_{\mathrm{rk}}(n,d,\lambda)=\min_{\hat \pi}\max_{M,\pi^*, x^*}{\mathbb E}[\|M_{\pi^*} - M_{\hat \pi}\|_F^2]$$
:::

:::{.callout-note .fragment}
## Estimating abilities
$$\mathcal R^*_{\mathrm{est}}(n,d,\lambda)=\min_{\hat M}\max_{M,\pi^*, x^*}{\mathbb E}[\|M- \hat M\|_F^2]$$
:::
-->

<!--
## Short Story {style="font-size:90%"}

::: {.notes}
To make a long story short, Shah and co. first considered the more constrained bi-isotonic model, where $M$ has not only increasing columns, but also increasing rows. Unfortunately, their optimal method cannot be computed in polynomial time.

Subsequently, assuming that the labels are known, there has been considerable effort to improve the state of the art on ranking and estimation.

In this work, we address the problem of recovering the labels in polynomial time and how we can recover past results on ranking and estimation

:::



. . .

[[@shah2020permutation]]{style="background-color: lightgreen;"}: recovering $x^*$ optimally but [not in poly. time]{style="background-color: orange;"}, ([$M_{\pi^*\eta^*}$ **bi-isotonic**]{style="background-color: lightgreen;"}).

. . .

[[@mao2020towards],[@liu2020better]]{style="background-color: lightgreen;"}, [@pilliat2024optimal]{style="background-color: lightblue;"}: ranking $\pi^*$ and estimating abilities $M$: improve state of the art poly. time when [$x^*$ **known**,]{style="background-color: orange;"}([$M_{\pi^*\eta^*}$ **bi-isotonic**]{style="background-color: lightgreen;"} or [$M_{\pi^*}$ **isotonic**]{style="background-color: lightblue;"})

. . .

[**This work**]{style="background-color: lightblue;"}: recovering $x^*$ [in poly. time]{style="background-color: yellow;"} and recovering (most of the) past results on ranking and estimation ([$M_{\pi^*}$ **isotonic**]{style="background-color: lightblue;"})
-->


## Main Results

::: {.notes}
The main result I want to present, is that there is no computational-statistical gap here.

There is a polynomial-time methods which nearly achieves the minimax risk for recovering the labels.


Moreover, the minimax risk is of order $d/\lambda$
:::


:::{.callout-note style="font-size=120%" .fragment}
## Theorem

If $\tfrac{1}{\lambda} \leq n \leq d$, there exists a [poly. time method $\hat x$ achieving $\mathcal R^*_{\mathrm{reco}}$]{style="background-color: yellow;"}  up to polylog factors, i.e. for all [$M\in[0,1]^{n\times d}$]{style="background-color: lightblue;"}, $\pi^*$ and [$x^* \in \{-1,1\}^d$]{style="background-color: lightblue;"},
$$
\max_{M,\pi^*, x^*}{\mathbb E}[\|M \mathrm{diag}(x^* \neq \hat x)\|_F^2] \lesssim \mathcal R^*_{\mathrm{reco}}(n,d,\lambda) \enspace .
$$

::: {.fragment}

Moreover, up to polylogs,

::: {.square-def}
$$
\mathcal R^*_{\mathrm{reco}}(n,d,\lambda) \asymp \frac{d}{\lambda} \enspace .
$$
:::


:::

:::

# Already Existing Methods
::: {.notes}
Let me now shortly present two existing methods
:::


## Majority Voting

::: {.notes}
Majority voting is the simplest method you can think of. Sum each column of $Y$ and take the sign to estimate each label.

Unfortunately, it does not achieve the optimal $d/\lambda$ rate. So we have this additional $\sqrt{n}$ compared to the minimax risk.

:::


. . .

::: {.square-def}
$\hat x^{(maj)}_k = \mathrm{sign} \left( \sum_{i=1}^n Y_{ik} \right)$
:::



. . .

[Maximum risk]{style="background-color: yellow;"} of $\hat x^{(maj)}$:

::: {.square-def}
$$\max_{M,\pi^*, x^*}\mathbb E\|M\mathrm{diag}(\hat x^{maj} \neq x^*)\|_F^2 \asymp \tfrac{d \sqrt{n}}{\lambda}$$
:::


<!--
. . .

In the worst case ($\lambda=1$): $M \asymp \frac{1}{\sqrt{n}}(\mathbf 1_{n\times d})$, [$\hat x^{(maj)}$ is not much better than random labelling]{style="background-color: orange;"}
 and $\|M\mathrm{diag}(\hat x \neq x^*)\|_F^2 \asymp d\sqrt{n}$
-->


<!--

## Least square (conjectured NP Hard)

. . .

Minimize 

::: {.square-def}
$$\|Y- \lambda M' \mathrm{diag}(x)\|_F^2$$
:::

- over all $x \in \{-1, 1\}^d$
- and $M'$ **isotonic**, up to a permutation $\pi^*$

. . .

The set of [isotonic matrices is convex]{style="background-color: lightgreen;"}...\

. . .

But [not isotonic matrices up to a permutation $\pi^*$]{style="background-color: orange;"}

. . .

It is minimax optimal [@shah2020permutation]

-->

## OBI-WAN [@shah2020permutation]


::: {.notes}
In the Obi-Wan Method from Shah and co. The main idea is to use that $(M\mathrm{diag}(x^*))(M\mathrm{diag}(x^*))^T$ does not depend on $x^*$.

and to exploit this fact by computing the leading left eigen vector of $Y$

:::
. . .

**Idea**: Population term [$(M\mathrm{diag}(x^*))(M\mathrm{diag}(x^*))^T= MM^T$]{style="background-color: lightblue;"} is independent of $x^*$

. . .

**PCA** Step: 

::: {.square-def}
$$\hat v = \underset{\|v\|=1}{\mathrm{argmax}}\|v^T Y\|^2$$
:::

## Result on Obi-Wan Method

::: {.notes}
It turns out that the Obi-Wan method is minimax optimal in a rank $1$ model, it achieves the rate $d/\lambda$.

However, it is not minimax optimal in the isotonic model. It only achieves a rate that is comparable to majority voting in the worst case.
:::

. . .

::: {.callout-note style="font-size: 100%;"}
## Theorem 1 [@shah2020permutation]

In submodel where [$\mathrm{rk}(M)= 1$]{style="background-color: yellow;"}, $\hat x^{(\mathrm{Obi-Wan})}$ [achieves minimax risk up]{style="background-color: yellow;"} to polylogs:
$$\mathbb E\|M\mathrm{diag}(\hat x^{\text{Obi-Wan}} \neq x^*)\|_F^2 \lesssim \frac{d}{\lambda} \quad \textbf{(minimax)}$$

:::

. . .

::: {.callout-note style="font-size: 100%;"}
## Theorem 2 [@shah2020permutation]

In the model where $M_{\pi^*\eta^*}$ is [bi-isotonic]{style="background-color: yellow;"} up to polylogs:
$$\mathbb E\|M\mathrm{diag}(\hat x^{\text{Obi-Wan}} \neq x^*)\|_F^2 \lesssim \frac{d\sqrt{n}}{\lambda} \quad \textbf{(not minimax)}$$

:::


# Iterative Spectral Voting

::: {.notes}
To achieve the $d/\lambda$ rate in the isotonic model, let me now introduce iterative spectral voting

The core of the approach consists in iterating two steps: PCA and weighted voting.

:::
<!-- 
## Subsampling

. . .

Let $T \geq 1$. We generate $T$ samples $(Y^{(1)}, \dots, Y^{(T)})$ from $Y$.

. . .

[Put $Y_{ik}$ uniformly at random]{style="background-color: yellow;"} into one of the $Y^{(s)}$. 

- $Y_{ik}^{(s)}= 0$ for all $s$ except one, which is $Y_{ik}$
- The $(Y^{(s)})$'s are [not independent]{style="background-color: orange;"}!
- Technical trick: condition on the sampling scheme

-->


## PCA Step

::: {.notes}
First, we compute the leading eigenvector as Shah et al. did.

The insight is that since $M$ is isotonic, then we can prove that $M$ is close to low rank, so it makes sense to look for leading eigen vectors
:::

. . .

::: {.square-def}
$$\hat v = \underset{\|v\|=1}{\mathrm{argmax}}\|v^T Y\|^2$$
:::

. . .

[Main idea]{style="background-color: lightgreen;"}: Since $M$ is **isotonic**, $M$ is [close to low rank]{style="background-color: yellow;"}

::: {.square-tip}
$$\|MM^T\|_{\mathrm{op}} \gtrsim \|M\|_F^2$$
:::

## Voting Step

::: {.notes}
Without going into too much details, the idea is then to do define weights and perform weighted votes.
:::

. . .

Define the [weighted vote]{style="background-color: yellow;"} vector (on second sample)

::: {.square-def}
$$\hat w = \hat v^T Y$$
:::

. . .

Define the estimated label as

::: {.square-def}
$$\hat x_k^{(1)} = \mathrm{sign}(\hat w_k)\mathbf{1}\bigg\{|\hat w_k| \gg \sqrt{\sum_{i=1}^n \hat v_i B_{ik}}\bigg\}$$
:::

## Iterate these two Steps

::: {.notes}
We then iterate these PCA and voting steps on labels that are left uncertain,

until we get a final estimator.
:::

. . .

[Keep certain labels]{style="background-color: yellow;"}: if $\hat x^{(t)}_k\neq 0$, set $\hat x^{(t+1)}_k= \hat x^{(t)}_k$.

. . .

Restrict columns $k$ of $Y$ to [uncertain labels]{style="background-color: yellow;"} $\hat x^{(t)}_k=0$

. . .


. . .

[Repeat PCA Step + Voting Step]{style="background-color: yellow;"} on $Y \mathrm{diag}(\hat x^{(t)} \neq 0)$ a polylogarithmic number of times.

. . .


Output last estimator $\hat x^{(T)}$

$\newcommand{\and}{\quad \mathrm{and} \quad}$

<!--

## Proof Idea


. . .

Let [$M(t) = M\mathrm{diag}(\hat x^{(t-1)} = 0)$]{style="background-color: yellow;"} 

. . .

While [$M(t) \gg d/\lambda$]{style="background-color: yellow;"}, we prove that

:::{style="font-size: 90%;"}
::: {.square-def}
$$\|\tilde v^TM(t)\|_2^2 \gtrsim \|M\|_F^2 \and \|\tilde v^TM(t+1)\|_2^2 \lesssim d/\lambda$$
:::
:::

. . .

By Pythagoeran Theorem, we have

:::{style="font-size: 90%;"}
::: {.square-def}
$$\|M(t)\|_F^2 - \|M(t+1)\|_F^2 \geq \|\tilde v M(t)\|_2^2 - \|\tilde v^TM(t+1)\|_2^2$$
:::
:::

. . .

This leads to [exponential decay of $\|M(t)\|_F^2$]{style="background-color: yellow;"} until $M(t) \leq d/\lambda$

-->

<!--

# Simulations

## Synthetic Data

:::: {.columns}


::: {.column width="70%"}

:::{.fragment fragment-index=1}
![](generated_matrices.svg){width=80%}
:::
:::{.fragment fragment-index=3}
![](monte_carlo_N100_nd1000_T10.png)
:::

:::


::: {.column width="30%"}

\ 

::: {.fragment fragment-index=2}
**Black**: $M_{ik}=0$\
**Blue**: $M_{ik} = h$
:::

:::

::::

-->

## Ranking and Estimating Abilities

::: {.notes}
This approach allows us to recover some of the true labels.

Then, we can do ranking and ability estimation!

To do that, we first restrict matrix of responses $Y$ to the labels for which we have a good guess

Then, we use previous work in the litterature to estimate $\pi^*$ and $M$


:::

1. Use ISV to [recover some labels $\hat x$]{style="background-color: yellow;"}
2. [Restrict]{style="background-color: yellow;"} to columns $k$ such that $\hat x_k \in \{-1, 1\}$
3. Use methods from @pilliat2024optimal to [estimate $\pi^*$ and $M$]{style="background-color: yellow;"}

## Conclusion

::: {.notes}

Let me wrap up my talk in three key points.

- First, there are three interconnected problems in crowdsourcing: recovering labels, ranking workers and estimating their abilities.
- Second, we use the isotonic model because it's very flexible for tackling these problems.
- Most importantly, there is no computational statistical gap.

:::

1. [Three connected problems]{style="background-color: yellow;"}: recovering labels, ranking workers and estimating their abilities
2. [Non parametric isotonic model]{style="background-color: yellow;"} very flexible in crowdsourcing problems
3. [No computational-statistical gap]{style="background-color: yellow;"} recovering labels, ranking or estimating ability

<!--
## Main Insights

1. Spectral method because [$\|M\mathrm{diag}(x^*)(M\mathrm{diag}(x^*))^T\|_{\mathrm{op}}=\|MM^T\|_{\mathrm{op}}$]{style="background-color: lightblue;"} does not depend on $x^*$
2. Iterate to reduce remaining square norm loss [$\|M\mathrm{diag}(x^* \neq \hat x^{(t)})\|_F^2$]{style="background-color: lightblue;"}
3. Because [$\|M\|^2_{\mathrm{op}}\gtrsim\|M\|_F^2$]{style="background-color: lightblue;"}


## What's Next?

. . .


::: {style="text-align: center;"}
Can we [do better]{style="background-color: yellow;"} if we are allowed to select worker task pairs $(i,k)$ based on [past information]{style="background-color: yellow;"}?
:::

-->