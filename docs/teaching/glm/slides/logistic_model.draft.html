<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.22">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Logistic Model – Emmanuel Pilliat</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-7b4406b7675125bc2ba204020e191172.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-338eaa8bf8d7f1eb506da55f2b711d05.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Emmanuel Pilliat</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Presentation</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/epilliat"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://fr.linkedin.com/in/emmanuel-pilliat-18ab2b180"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#glm-models-for-binary-variables" id="toc-glm-models-for-binary-variables" class="nav-link active" data-scroll-target="#glm-models-for-binary-variables">GLM Models for Binary Variables</a>
  <ul class="collapse">
  <li><a href="#context" id="toc-context" class="nav-link" data-scroll-target="#context">Context</a></li>
  <li><a href="#example-coronary-heart-disease" id="toc-example-coronary-heart-disease" class="nav-link" data-scroll-target="#example-coronary-heart-disease">Example: Coronary Heart Disease</a></li>
  <li><a href="#example-simple-idea" id="toc-example-simple-idea" class="nav-link" data-scroll-target="#example-simple-idea">Example: Simple Idea</a></li>
  <li><a href="#example-glm-approach" id="toc-example-glm-approach" class="nav-link" data-scroll-target="#example-glm-approach">Example: GLM Approach</a></li>
  <li><a href="#example-the-logit-model" id="toc-example-the-logit-model" class="nav-link" data-scroll-target="#example-the-logit-model">Example: The Logit Model</a></li>
  <li><a href="#example-the-probit-model" id="toc-example-the-probit-model" class="nav-link" data-scroll-target="#example-the-probit-model">Example: The Probit Model</a></li>
  <li><a href="#example-model-cloglog" id="toc-example-model-cloglog" class="nav-link" data-scroll-target="#example-model-cloglog">Example: Model cloglog</a></li>
  <li><a href="#example-comparison-of-the-three-models" id="toc-example-comparison-of-the-three-models" class="nav-link" data-scroll-target="#example-comparison-of-the-three-models">Example: Comparison of the Three Models</a></li>
  <li><a href="#which-link-function-to-choose-for-binary-case" id="toc-which-link-function-to-choose-for-binary-case" class="nav-link" data-scroll-target="#which-link-function-to-choose-for-binary-case">Which Link Function to Choose for Binary Case?</a></li>
  <li><a href="#details-on-the-probit-model" id="toc-details-on-the-probit-model" class="nav-link" data-scroll-target="#details-on-the-probit-model">Details on the Probit Model</a></li>
  <li><a href="#examples-in-probit-model" id="toc-examples-in-probit-model" class="nav-link" data-scroll-target="#examples-in-probit-model">Examples in Probit Model</a></li>
  <li><a href="#probit-vs-logit-current-trends" id="toc-probit-vs-logit-current-trends" class="nav-link" data-scroll-target="#probit-vs-logit-current-trends">Probit vs Logit: Current Trends</a></li>
  <li><a href="#remarks-on-cloglog-model" id="toc-remarks-on-cloglog-model" class="nav-link" data-scroll-target="#remarks-on-cloglog-model">Remarks on cloglog Model</a></li>
  <li><a href="#details-on-the-logit-model" id="toc-details-on-the-logit-model" class="nav-link" data-scroll-target="#details-on-the-logit-model">Details on the Logit Model</a></li>
  <li><a href="#theoretical-motivation-of-logit" id="toc-theoretical-motivation-of-logit" class="nav-link" data-scroll-target="#theoretical-motivation-of-logit">Theoretical Motivation of Logit</a></li>
  <li><a href="#summary-for-binary-variables" id="toc-summary-for-binary-variables" class="nav-link" data-scroll-target="#summary-for-binary-variables">Summary for Binary Variables:</a></li>
  <li><a href="#outline" id="toc-outline" class="nav-link" data-scroll-target="#outline">Outline</a></li>
  </ul></li>
  <li><a href="#model-interpretation" id="toc-model-interpretation" class="nav-link" data-scroll-target="#model-interpretation">Model Interpretation</a>
  <ul class="collapse">
  <li><a href="#interpretation-of-the-logistic-model" id="toc-interpretation-of-the-logistic-model" class="nav-link" data-scroll-target="#interpretation-of-the-logistic-model">Interpretation of the Logistic Model</a></li>
  <li><a href="#shape-of-logit-function" id="toc-shape-of-logit-function" class="nav-link" data-scroll-target="#shape-of-logit-function">Shape of logit function</a></li>
  <li><a href="#example-bmi-study-french-imc" id="toc-example-bmi-study-french-imc" class="nav-link" data-scroll-target="#example-bmi-study-french-imc">Example: BMI Study (French: IMC)</a></li>
  <li><a href="#model-definition" id="toc-model-definition" class="nav-link" data-scroll-target="#model-definition">Model Definition</a></li>
  <li><a href="#model-results" id="toc-model-results" class="nav-link" data-scroll-target="#model-results">Model Results</a></li>
  <li><a href="#model-results-with-age2" id="toc-model-results-with-age2" class="nav-link" data-scroll-target="#model-results-with-age2">Model Results with <span class="math inline">\(AGE^2\)</span></a></li>
  <li><a href="#odds" id="toc-odds" class="nav-link" data-scroll-target="#odds">Odds</a></li>
  <li><a href="#odds-given-xx" id="toc-odds-given-xx" class="nav-link" data-scroll-target="#odds-given-xx">Odds given <span class="math inline">\(X=x\)</span></a></li>
  <li><a href="#odds-ratio" id="toc-odds-ratio" class="nav-link" data-scroll-target="#odds-ratio">Odds Ratio</a></li>
  <li><a href="#link-with-proba.-ratio" id="toc-link-with-proba.-ratio" class="nav-link" data-scroll-target="#link-with-proba.-ratio">Link with Proba. Ratio</a></li>
  <li><a href="#other-property-of-odds-ratio" id="toc-other-property-of-odds-ratio" class="nav-link" data-scroll-target="#other-property-of-odds-ratio">Other Property of Odds Ratio</a></li>
  <li><a href="#examples-using-odds-ratios" id="toc-examples-using-odds-ratios" class="nav-link" data-scroll-target="#examples-using-odds-ratios">Examples Using Odds Ratios</a></li>
  <li><a href="#odds-ratio-in-logistic-regression" id="toc-odds-ratio-in-logistic-regression" class="nav-link" data-scroll-target="#odds-ratio-in-logistic-regression">Odds Ratio in Logistic Regression</a></li>
  <li><a href="#key-summary-statement" id="toc-key-summary-statement" class="nav-link" data-scroll-target="#key-summary-statement">Key Summary Statement</a></li>
  <li><a href="#example-1-intense-sports-activity" id="toc-example-1-intense-sports-activity" class="nav-link" data-scroll-target="#example-1-intense-sports-activity">Example 1: Intense Sports Activity</a></li>
  <li><a href="#example-2-diastolic-pressure" id="toc-example-2-diastolic-pressure" class="nav-link" data-scroll-target="#example-2-diastolic-pressure">Example 2: Diastolic Pressure</a></li>
  </ul></li>
  <li><a href="#estimation-of-the-parameters" id="toc-estimation-of-the-parameters" class="nav-link" data-scroll-target="#estimation-of-the-parameters">EStimation of the Parameters</a>
  <ul class="collapse">
  <li><a href="#the-framework" id="toc-the-framework" class="nav-link" data-scroll-target="#the-framework">The Framework</a></li>
  <li><a href="#the-logistic-model" id="toc-the-logistic-model" class="nav-link" data-scroll-target="#the-logistic-model">The Logistic Model</a></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation">Parameter Estimation</a></li>
  <li><a href="#likelihood-calculation" id="toc-likelihood-calculation" class="nav-link" data-scroll-target="#likelihood-calculation">Likelihood Calculation</a></li>
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood">Likelihood</a></li>
  <li><a href="#log-likelihood" id="toc-log-likelihood" class="nav-link" data-scroll-target="#log-likelihood">Log-Likelihood</a></li>
  <li><a href="#mle-calculation" id="toc-mle-calculation" class="nav-link" data-scroll-target="#mle-calculation">MLE Calculation</a></li>
  <li><a href="#remarks" id="toc-remarks" class="nav-link" data-scroll-target="#remarks">Remarks</a></li>
  <li><a href="#mle-uniqueness" id="toc-mle-uniqueness" class="nav-link" data-scroll-target="#mle-uniqueness">MLE Uniqueness</a></li>
  <li><a href="#proof-of-mle-uniqueness" id="toc-proof-of-mle-uniqueness" class="nav-link" data-scroll-target="#proof-of-mle-uniqueness">Proof of MLE Uniqueness</a></li>
  <li><a href="#hessian-properties" id="toc-hessian-properties" class="nav-link" data-scroll-target="#hessian-properties">Hessian Properties</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#about-mle-existence" id="toc-about-mle-existence" class="nav-link" data-scroll-target="#about-mle-existence">About MLE Existence</a></li>
  <li><a href="#non-overlap-situation" id="toc-non-overlap-situation" class="nav-link" data-scroll-target="#non-overlap-situation">Non-Overlap Situation</a></li>
  <li><a href="#illustration-of-non-overlap-situation" id="toc-illustration-of-non-overlap-situation" class="nav-link" data-scroll-target="#illustration-of-non-overlap-situation">Illustration of Non-Overlap Situation</a></li>
  <li><a href="#non-overlap-and-existence" id="toc-non-overlap-and-existence" class="nav-link" data-scroll-target="#non-overlap-and-existence">Non-Overlap and Existence</a></li>
  <li><a href="#beyond-the-dead-zone" id="toc-beyond-the-dead-zone" class="nav-link" data-scroll-target="#beyond-the-dead-zone">Beyond the Dead Zone</a></li>
  <li><a href="#existence-and-uniqueness-of-mle" id="toc-existence-and-uniqueness-of-mle" class="nav-link" data-scroll-target="#existence-and-uniqueness-of-mle">Existence and Uniqueness of MLE</a></li>
  <li><a href="#fisher-information-recall" id="toc-fisher-information-recall" class="nav-link" data-scroll-target="#fisher-information-recall">Fisher Information (Recall)</a></li>
  <li><a href="#asymptotic-efficiency" id="toc-asymptotic-efficiency" class="nav-link" data-scroll-target="#asymptotic-efficiency">Asymptotic Efficiency</a></li>
  <li><a href="#comments" id="toc-comments" class="nav-link" data-scroll-target="#comments">Comments</a></li>
  <li><a href="#comparison-with-linear-regression" id="toc-comparison-with-linear-regression" class="nav-link" data-scroll-target="#comparison-with-linear-regression">Comparison with Linear Regression</a></li>
  <li><a href="#comparison-with-linear-regression-1" id="toc-comparison-with-linear-regression-1" class="nav-link" data-scroll-target="#comparison-with-linear-regression-1">Comparison with Linear Regression</a></li>
  </ul></li>
  <li><a href="#tests-and-confidence-intervals" id="toc-tests-and-confidence-intervals" class="nav-link" data-scroll-target="#tests-and-confidence-intervals">Tests and Confidence Intervals</a>
  <ul class="collapse">
  <li><a href="#asymptotic-framework" id="toc-asymptotic-framework" class="nav-link" data-scroll-target="#asymptotic-framework">Asymptotic Framework</a></li>
  <li><a href="#computation-of-j_nbeta" id="toc-computation-of-j_nbeta" class="nav-link" data-scroll-target="#computation-of-j_nbeta">Computation of <span class="math inline">\(J_n(\beta)\)</span></a></li>
  <li><a href="#equivalent-form" id="toc-equivalent-form" class="nav-link" data-scroll-target="#equivalent-form">Equivalent Form</a></li>
  <li><a href="#estimation" id="toc-estimation" class="nav-link" data-scroll-target="#estimation">Estimation</a></li>
  <li><a href="#estimated-variance-of-hat-beta_j" id="toc-estimated-variance-of-hat-beta_j" class="nav-link" data-scroll-target="#estimated-variance-of-hat-beta_j">Estimated Variance of <span class="math inline">\(\hat \beta_j\)</span></a></li>
  <li><a href="#confidence-interval" id="toc-confidence-interval" class="nav-link" data-scroll-target="#confidence-interval">Confidence Interval</a></li>
  <li><a href="#significance-test-for-one-coefficient" id="toc-significance-test-for-one-coefficient" class="nav-link" data-scroll-target="#significance-test-for-one-coefficient">Significance Test for One Coefficient</a></li>
  <li><a href="#p-value" id="toc-p-value" class="nav-link" data-scroll-target="#p-value">P-value</a></li>
  <li><a href="#example-in-r" id="toc-example-in-r" class="nav-link" data-scroll-target="#example-in-r">Example in R</a></li>
  <li><a href="#estimation-of-an-odds-ratio" id="toc-estimation-of-an-odds-ratio" class="nav-link" data-scroll-target="#estimation-of-an-odds-ratio">Estimation of an Odds-Ratio</a></li>
  <li><a href="#important-example" id="toc-important-example" class="nav-link" data-scroll-target="#important-example">Important Example</a></li>
  <li><a href="#asymptotic-ci-for-an-odds-ratio" id="toc-asymptotic-ci-for-an-odds-ratio" class="nav-link" data-scroll-target="#asymptotic-ci-for-an-odds-ratio">Asymptotic CI for an Odds-Ratio</a></li>
  <li><a href="#significance-test-for-an-odds-ratio" id="toc-significance-test-for-an-odds-ratio" class="nav-link" data-scroll-target="#significance-test-for-an-odds-ratio">Significance Test for an Odds-Ratio</a></li>
  <li><a href="#two-sided-test" id="toc-two-sided-test" class="nav-link" data-scroll-target="#two-sided-test">Two-Sided Test</a></li>
  <li><a href="#one-sided-tests" id="toc-one-sided-tests" class="nav-link" data-scroll-target="#one-sided-tests">One-Sided Tests</a></li>
  </ul></li>
  <li><a href="#deviance" id="toc-deviance" class="nav-link" data-scroll-target="#deviance">Deviance</a>
  <ul class="collapse">
  <li><a href="#saturated-model" id="toc-saturated-model" class="nav-link" data-scroll-target="#saturated-model">Saturated Model</a></li>
  <li><a href="#saturated-estimator" id="toc-saturated-estimator" class="nav-link" data-scroll-target="#saturated-estimator">Saturated Estimator</a></li>
  <li><a href="#remark" id="toc-remark" class="nav-link" data-scroll-target="#remark">Remark</a></li>
  <li><a href="#remarks-1" id="toc-remarks-1" class="nav-link" data-scroll-target="#remarks-1">Remarks</a></li>
  <li><a href="#likelihood-of-saturated-estimator" id="toc-likelihood-of-saturated-estimator" class="nav-link" data-scroll-target="#likelihood-of-saturated-estimator">Likelihood of Saturated Estimator</a></li>
  <li><a href="#case-1-distinct-observations" id="toc-case-1-distinct-observations" class="nav-link" data-scroll-target="#case-1-distinct-observations">Case 1: Distinct Observations</a></li>
  <li><a href="#case-2-non-distinct-observations" id="toc-case-2-non-distinct-observations" class="nav-link" data-scroll-target="#case-2-non-distinct-observations">Case 2: Non-Distinct Observations</a></li>
  <li><a href="#deviance-1" id="toc-deviance-1" class="nav-link" data-scroll-target="#deviance-1">Deviance</a></li>
  <li><a href="#role-of-deviance-and-computation-in-r" id="toc-role-of-deviance-and-computation-in-r" class="nav-link" data-scroll-target="#role-of-deviance-and-computation-in-r">Role of Deviance and Computation in R</a></li>
  </ul></li>
  <li><a href="#testing" id="toc-testing" class="nav-link" data-scroll-target="#testing">Testing</a>
  <ul class="collapse">
  <li><a href="#linear-constraint-test" id="toc-linear-constraint-test" class="nav-link" data-scroll-target="#linear-constraint-test">Linear Constraint Test</a></li>
  <li><a href="#available-procedures" id="toc-available-procedures" class="nav-link" data-scroll-target="#available-procedures">Available Procedures</a></li>
  <li><a href="#the-deviance-test-or-likelihood-ratio-test" id="toc-the-deviance-test-or-likelihood-ratio-test" class="nav-link" data-scroll-target="#the-deviance-test-or-likelihood-ratio-test">The Deviance Test (or Likelihood Ratio Test)</a></li>
  <li><a href="#deviance-test-statistic" id="toc-deviance-test-statistic" class="nav-link" data-scroll-target="#deviance-test-statistic">Deviance Test Statistic</a></li>
  <li><a href="#rejection-region-and-p-value" id="toc-rejection-region-and-p-value" class="nav-link" data-scroll-target="#rejection-region-and-p-value">Rejection Region and P-value</a></li>
  <li><a href="#special-case-1-significance-test" id="toc-special-case-1-significance-test" class="nav-link" data-scroll-target="#special-case-1-significance-test">Special Case 1: Significance Test</a></li>
  <li><a href="#test-statistic" id="toc-test-statistic" class="nav-link" data-scroll-target="#test-statistic">Test Statistic</a></li>
  <li><a href="#special-case-2-nested-models" id="toc-special-case-2-nested-models" class="nav-link" data-scroll-target="#special-case-2-nested-models">Special Case 2: Nested Models</a></li>
  <li><a href="#aic-and-bic-criteria" id="toc-aic-and-bic-criteria" class="nav-link" data-scroll-target="#aic-and-bic-criteria">AIC and BIC Criteria</a></li>
  <li><a href="#aic-and-bic-criteria-1" id="toc-aic-and-bic-criteria-1" class="nav-link" data-scroll-target="#aic-and-bic-criteria-1">AIC and BIC Criteria</a></li>
  <li><a href="#example-obesity-study" id="toc-example-obesity-study" class="nav-link" data-scroll-target="#example-obesity-study">Example: Obesity Study</a></li>
  <li><a href="#example-significance-test-of-marital" id="toc-example-significance-test-of-marital" class="nav-link" data-scroll-target="#example-significance-test-of-marital">Example: Significance Test of MARITAL</a></li>
  <li><a href="#example-significance-test-of-age2" id="toc-example-significance-test-of-age2" class="nav-link" data-scroll-target="#example-significance-test-of-age2">Example: Significance Test of AGE<span class="math inline">\(~^2\)</span></a></li>
  </ul></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a>
  <ul class="collapse">
  <li><a href="#setting-and-objective" id="toc-setting-and-objective" class="nav-link" data-scroll-target="#setting-and-objective">Setting and Objective</a></li>
  <li><a href="#recall-in-the-logit-model" id="toc-recall-in-the-logit-model" class="nav-link" data-scroll-target="#recall-in-the-logit-model">Recall in the Logit Model</a></li>
  <li><a href="#outline-for-prediction" id="toc-outline-for-prediction" class="nav-link" data-scroll-target="#outline-for-prediction">Outline for Prediction</a></li>
  <li><a href="#ci-asymptotic-distribution-of-p_betax" id="toc-ci-asymptotic-distribution-of-p_betax" class="nav-link" data-scroll-target="#ci-asymptotic-distribution-of-p_betax">CI: Asymptotic Distribution of <span class="math inline">\(p_\beta(x)\)</span></a></li>
  <li><a href="#ci-for-linear-predictor" id="toc-ci-for-linear-predictor" class="nav-link" data-scroll-target="#ci-for-linear-predictor">CI for Linear Predictor</a></li>
  <li><a href="#ci-for-probability-p_betax" id="toc-ci-for-probability-p_betax" class="nav-link" data-scroll-target="#ci-for-probability-p_betax">CI for Probability <span class="math inline">\(p_{\beta}(x)\)</span></a></li>
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification">Classification</a></li>
  <li><a href="#evaluation-of-classification-quality" id="toc-evaluation-of-classification-quality" class="nav-link" data-scroll-target="#evaluation-of-classification-quality">Evaluation of Classification Quality</a></li>
  <li><a href="#evaluation-of-classification-quality-1" id="toc-evaluation-of-classification-quality-1" class="nav-link" data-scroll-target="#evaluation-of-classification-quality-1">Evaluation of Classification Quality</a></li>
  <li><a href="#other-indicators" id="toc-other-indicators" class="nav-link" data-scroll-target="#other-indicators">Other Indicators</a></li>
  <li><a href="#choice-of-threshold-s" id="toc-choice-of-threshold-s" class="nav-link" data-scroll-target="#choice-of-threshold-s">Choice of Threshold <span class="math inline">\(s\)</span></a></li>
  <li><a href="#choosing-the-score" id="toc-choosing-the-score" class="nav-link" data-scroll-target="#choosing-the-score">Choosing the score</a></li>
  <li><a href="#roc-curve" id="toc-roc-curve" class="nav-link" data-scroll-target="#roc-curve">ROC Curve</a></li>
  <li><a href="#roc-curve-illustration" id="toc-roc-curve-illustration" class="nav-link" data-scroll-target="#roc-curve-illustration">ROC Curve Illustration</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="logistic_model.draft.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Logistic Model</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><span class="math inline">\(\newcommand{\VS}{\quad \mathrm{VS} \quad}\)</span> <span class="math inline">\(\newcommand{\and}{\quad \mathrm{and} \quad}\)</span> <span class="math inline">\(\newcommand{\E}{\mathbb E}\)</span> <span class="math inline">\(\newcommand{\P}{\mathbb P}\)</span> <span class="math inline">\(\newcommand{\Var}{\mathbb V}\)</span> <span class="math inline">\(\newcommand{\Cov}{\mathrm{Cov}}\)</span> <span class="math inline">\(\newcommand{\1}{\mathbf 1}\)</span></p>
<section id="glm-models-for-binary-variables" class="level1">
<h1>GLM Models for Binary Variables</h1>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<div class="notes">
<p>Let’s now focus specifically on the binary case and work through it in detail.</p>
<p>Y is a binary variable—Y_k is in {0,1}.</p>
<p>X equals (X^(1) through X^(p)) are our p explanatory variables or regressors.</p>
<p>Y given X equals x follows a Bernoulli distribution with parameter p(x), which equals the probability that Y equals 1 given X equals x.</p>
<p>We model p(x) as g-inverse of x-transpose-beta.</p>
<p>Where g-inverse is a strictly increasing function with values in [0,1]. This ensures our probabilities are valid.</p>
<p>We’ll begin by discussing how to choose g-inverse—or equivalently, the link function g.</p>
</div>
<p><span class="math inline">\(Y\)</span> is a binary variable <span style="background-color: lightblue;"><span class="math inline">\(Y_k \in \{0,1\}\)</span></span></p>
<p>. . .</p>
<p><span class="math inline">\(X = (X^{(1)}, \ldots, X^{(p)})\)</span> are <span class="math inline">\(p\)</span> regressors</p>
<p>. . .</p>
<p><span class="math inline">\(Y|X = x\)</span> follows a <span style="background-color: yellow;">Bernoulli distribution</span> with parameter <span style="background-color: lightblue;"><span class="math inline">\(p(x) = P(Y = 1|X = x)\)</span></span>. Model:</p>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[p(x) = g^{-1}(x^T\beta)\]</span></p>
</div>
<p>where <span class="math inline">\(g^{-1}\)</span> is a <span style="background-color: yellow;">strictly increasing function</span> with values in <span class="math inline">\([0, 1]\)</span></p>
<p>. . .</p>
<p><strong>Approach</strong>: We begin by discussing the choice of <span class="math inline">\(g^{-1}\)</span> (or <span class="math inline">\(g\)</span>)</p>
</section>
<section id="example-coronary-heart-disease" class="level2">
<h2 class="anchored" data-anchor-id="example-coronary-heart-disease">Example: Coronary Heart Disease</h2>
<div class="notes">
<p>Let’s work through a concrete example to see how this works in practice.</p>
<p>We have data on the presence of coronary heart disease—chd—as a function of age. So Y equals chd, taking values 0 or 1, and X equals age.</p>
<p>We want to estimate p(x)— the probability that a given individual with characteristics x suffers from coronary heart desease</p>
<p>Here’s a scatter plot of the raw data. Each point is either 0 or 1 in functino of age. As you can see, it’s hard to discern a pattern directly.</p>
</div>
<p>. . .</p>
<p><strong>Data Description</strong>: Presence of chd as a function of age <span class="math inline">\(Y = \text{chd} \in \{0,1\}\)</span>, <span class="math inline">\(X = \text{age}\)</span></p>
<p>. . .</p>
<p>We want to estimate <span class="math inline">\(p(x) = \E(Y|X = x) = \P(\text{chd} = 1|X = x)\)</span> for all <span class="math inline">\(x\)</span></p>
<div style="text-align: center;">
<p><img src="../images/logit1.png" class="img-fluid" style="width:50.0%"></p>
</div>
</section>
<section id="example-simple-idea" class="level2">
<h2 class="anchored" data-anchor-id="example-simple-idea">Example: Simple Idea</h2>
<div class="notes">
<p>A simple first approach to understand the relationship.</p>
<p>We can group the x values by age class, then calculate the proportion of chd equals 1 in each class containing x.</p>
<p>Now we see something clearer! The proportion increases with age. But this approach is crude and doesn’t give us a smooth model.</p>
</div>
<ul>
<li>Group the <span class="math inline">\(x\)</span> values by age class</li>
<li>Calculate the proportion of <span class="math inline">\(chd = 1\)</span> in the class containing <span class="math inline">\(x\)</span></li>
</ul>
<p>. . .</p>
<div style="text-align: center;">
<p><img src="../images/logit2.png" class="img-fluid" style="width:50.0%"></p>
</div>
</section>
<section id="example-glm-approach" class="level2">
<h2 class="anchored" data-anchor-id="example-glm-approach">Example: GLM Approach</h2>
<div class="notes">
<p>The GLM approach gives us a smoother model.</p>
<p>We want to model p(x)—the probability that chd equals 1 given X equals x—as g-inverse of (beta_0 plus beta_1 times x).</p>
<p>This is a simple model with just an intercept and a slope. The age effect is captured by beta_1.</p>
<p>We need g-inverse to have values in [0,1] to ensure valid probabilities.</p>
</div>
<p>. . .</p>
<p><strong>Objective</strong>: We want to model <span class="math inline">\(p(x) = P(\text{chd} = 1|X = x)\)</span> by: <span class="math display">\[p(x) = g^{-1}(\beta_0 + \beta_1 x)\]</span></p>
<p>. . .</p>
<p><strong>Constraint</strong>: We need <span class="math inline">\(g^{-1}\)</span> to have values in <span class="math inline">\([0, 1]\)</span></p>
</section>
<section id="example-the-logit-model" class="level2">
<h2 class="anchored" data-anchor-id="example-the-logit-model">Example: The Logit Model</h2>
<div class="notes">
<p>Let’s try the logit link—the most common choice.</p>
<p>g-inverse(t) equals e^t over (1 plus e^t), which means g(t) equals the natural log of t over (1 minus t)—the logit function.</p>
<p>Here are the results of g-inverse of (beta-hat_0 plus beta-hat_1 times x), obtained by maximum likelihood estimation. Notice how smooth the curve is, and how it respects the bounds—staying between 0 and 1.</p>
</div>
<p>. . .</p>
<div style="font-size: 60%;">
<div class="square-def">
<p><span class="math display">\[g^{-1}(t) = \frac{e^t}{1 + e^t} \quad \text{i.e.} \quad g(t) = \ln\left(\frac{t}{1-t}\right) = \text{logit}(t)\]</span></p>
</div>
</div>
<p>. . .</p>
<div class="columns">
<div class="column">
<div style="text-align: center;">
<p><img src="../images/logit3.png" class="img-fluid" style="width:100.0%"></p>
</div>
</div><div class="column">
<p><br>
<br>
Results of <span class="math inline">\(g^{-1}(\hat \beta_0 + \hat \beta_1 x)\)</span> obtained by MLE</p>
</div>
</div>
</section>
<section id="example-the-probit-model" class="level2">
<h2 class="anchored" data-anchor-id="example-the-probit-model">Example: The Probit Model</h2>
<div class="notes">
<p>What if we use a different link function?</p>
<p>If Phi is the CDF of a standard normal distribution, we take g-inverse(t) equals Phi(t).</p>
<p>Here are the results using the probit model with g-inverse of (beta-hat_0 plus beta-hat_1 times x), again obtained by MLE. Very similar to the logit model!</p>
</div>
<p>. . .</p>
<p>If <span class="math inline">\(\Phi\)</span> is the CDF of a <span class="math inline">\(\mathcal{N}(0, 1)\)</span> distribution, we take <span style="background-color: lightblue;"><span class="math inline">\(g^{-1}(t) = \Phi(t)\)</span></span></p>
<div class="columns">
<div class="column">
<div style="text-align: center;">
<p><img src="../images/probit.png" class="img-fluid" style="width:100.0%"></p>
</div>
</div><div class="column">
<p><br>
<br>
Results of <span class="math inline">\(g^{-1}(\hat \beta_0 + \hat \beta_1 x)\)</span> obtained by MLE</p>
</div>
</div>
</section>
<section id="example-model-cloglog" class="level2">
<h2 class="anchored" data-anchor-id="example-model-cloglog">Example: Model cloglog</h2>
<div class="notes">
<p>Let’s try one more: the complementary log-log model.</p>
<p>g-inverse(t) equals 1 minus e to the negative e^t.</p>
<p>Here are the results. Notice this curve is not symmetric—it rises differently on the left versus the right compared to logit and probit.</p>
</div>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[g^{-1}(t) = 1 - e^{-e^t}\]</span></p>
</div>
<p>. . .</p>
<div class="columns">
<div class="column">
<div style="text-align: center;">
<p><img src="../images/cloglog.png" class="img-fluid" style="width:100.0%"></p>
</div>
</div><div class="column">
<p><br>
<br>
Results of <span class="math inline">\(g^{-1}(\hat \beta_0 + \hat \beta_1 x)\)</span> obtained by MLE</p>
</div>
</div>
</section>
<section id="example-comparison-of-the-three-models" class="level2">
<h2 class="anchored" data-anchor-id="example-comparison-of-the-three-models">Example: Comparison of the Three Models</h2>
<div class="notes">
<p>Let’s compare all three models side by side.</p>
<p>Logit and probit give approximately the same result—the curves are nearly identical.</p>
<p>The cloglog differs slightly and is not symmetric. It approaches 1 much faster than it appraoches 0</p>
<p>Here they are overlaid. For this data, the choice doesn’t matter much, but in principle there are differences.</p>
</div>
<p>. . .</p>
<p>logit and probit give <span style="background-color: yellow;">approximately the same result</span> cloglog differs slightly and is not “symmetric”</p>
<div style="text-align: center;">
<p><img src="../images/logitprobitloglog.png" class="img-fluid" style="width:50.0%"></p>
</div>
</section>
<section id="which-link-function-to-choose-for-binary-case" class="level2">
<h2 class="anchored" data-anchor-id="which-link-function-to-choose-for-binary-case">Which Link Function to Choose for Binary Case?</h2>
<div class="notes">
<p>By default, we favor the logit model. This is the main choice for most applications.</p>
<p>However, there are exceptions. We might choose something else - such as the probit model, complementary log-log, or log-log - when there’s a good reason to do so.</p>
<p>In the next few slides, we’ll return to the three usual choices and justify why logit is typically preferred.</p>
</div>
<p>. . .</p>
<p><strong>Question</strong>: Which link function to choose in practice when <span class="math inline">\(Y\)</span> is binary?</p>
<p>. . .</p>
<p><strong>Default choice</strong>: By default, we favor the <strong>logit model</strong></p>
<p>. . .</p>
<p><strong>Exceptions</strong>: Unless there is a good reason to choose something else (probit model, complementary log-log, or log-log)</p>
<p>. . .</p>
<p><strong>Next steps</strong>: We return to the 3 usual choices to justify this preference</p>
</section>
<section id="details-on-the-probit-model" class="level2">
<h2 class="anchored" data-anchor-id="details-on-the-probit-model">Details on the Probit Model</h2>
<div class="notes">
<p>The probit model has a specific theoretical justification. It’s appropriate when the binary variable Y given X equals x comes from thresholding a Gaussian latent variable Z of x.</p>
<p>Mathematically, Y given X equals x is the indicator that Z of x is greater than or equal to some threshold tau.</p>
<p>The key assumption is that Z of x follows a normal distribution with mean x transpose beta and variance sigma squared.</p>
<p>Let’s see what this implies for probabilities. If Phi is the cumulative distribution function of a standard normal distribution - that’s a normal with mean zero and variance one - then the probability that Y equals 1 given X equals x can be expressed as the probability that Z of x is at least tau. This equals Phi of the quantity x transpose beta minus tau, all divided by sigma.</p>
<p>This formulation shows how the probit model naturally arises from a latent variable framework.</p>
</div>
<p>. . .</p>
<p>The probit model is justified when the binary variable <span class="math inline">\(Y|X = x\)</span> comes from thresholding a Gaussian latent variable <span class="math inline">\(Z(x)\)</span>:</p>
<div class="square-def">
<p><span class="math display">\[(Y|X = x) = \mathbf{1}_{Z(x) \geq \tau}\]</span></p>
</div>
<p>where <span style="background-color: lightblue;"><span class="math inline">\(Z(x) \sim \mathcal{N}(x^T\beta, \sigma^2)\)</span></span></p>
<p>. . .</p>
<p>If <span class="math inline">\(\Phi\)</span> is the CDF of a <span class="math inline">\(\mathcal N(0,1)\)</span> <span class="math display">\[P(Y = 1|X = x) = P(Z(x) \geq \tau) = \Phi\left(\frac{x^T\beta - \tau}{\sigma}\right)\]</span></p>
</section>
<section id="examples-in-probit-model" class="level2">
<h2 class="anchored" data-anchor-id="examples-in-probit-model">Examples in Probit Model</h2>
<div class="notes">
<p>Let me give you two concrete examples where the probit model makes intuitive sense.</p>
<p>First example: Y represents a purchase decision - whether someone buys a product or not. Here, Z of x quantifies the utility or satisfaction the person would get from the good. When this utility exceeds a threshold, they make the purchase.</p>
<p>Second example: Y is a declared psychological state, such as whether someone reports being happy or depressed. In this case, Z of x represents a latent, unobserved measure of personal satisfaction. The binary outcome we observe is just whether this latent satisfaction crosses a certain threshold.</p>
<p>In both cases, the idea of an underlying continuous variable being thresholded to create a binary outcome is very natural.</p>
</div>
<ul>
<li><span class="math inline">\(Y\)</span> represents a purchase decision, and <span class="math inline">\(Z(x)\)</span> quantifies the utility of the good</li>
<li><span class="math inline">\(Y\)</span> is a declared psychological state (happiness, depression) and <span class="math inline">\(Z(x)\)</span> is a latent, unobserved measure of personal satisfaction</li>
</ul>
</section>
<section id="probit-vs-logit-current-trends" class="level2">
<h2 class="anchored" data-anchor-id="probit-vs-logit-current-trends">Probit vs Logit: Current Trends</h2>
<div class="notes">
<p>The probit model remains relatively popular among econometricians. There’s a tradition of using it in economics research.</p>
<p>However, the general trend is that it’s increasingly being replaced by the logistic model across many fields.</p>
<p>Why is this happening? The logit model has many advantages that probit does not have. Let me highlight two key ones:</p>
<p>First, interpretation of results. The logit model allows us to work with odds ratios, which are very intuitive to interpret.</p>
<p>Second, explicit formulas. The logit model has closed-form expressions that make computation and interpretation easier.</p>
<p>From a theoretical perspective, there’s also good justification for this trend. The cumulative distribution function of the probit model is actually quite close to the CDF of the logit model, so we’re not losing much by switching.</p>
</div>
<p>. . .</p>
<p><strong>Econometricians</strong>: The <span style="background-color: yellow;">probit model</span> remains relatively popular among <span style="background-color: yellow;">econometricians</span>…</p>
<p>. . .</p>
<p><strong>General trend</strong>: but it tends to be <span style="background-color: yellow;">replaced by the logistic model</span></p>
<p>. . .</p>
<p><strong>Advantages of logit</strong>: The <span style="background-color: yellow;">logit model has many advantages</span> that probit does not have:</p>
<ul>
<li>Interpretation of results</li>
<li>Explicit formulas</li>
</ul>
<p>. . .</p>
<p><strong>Theoretical justification</strong>: CDF of probit close to CDF of logit</p>
</section>
<section id="remarks-on-cloglog-model" class="level2">
<h2 class="anchored" data-anchor-id="remarks-on-cloglog-model">Remarks on cloglog Model</h2>
<div class="notes">
<p>Now let’s turn to the complementary log-log, or cloglog, model.</p>
<p>For cloglog, the link function g of t is the natural log of negative log of one minus t. The inverse link is therefore g inverse of t equals one minus e to the negative e to the t.</p>
<p>An important property of this link function is that it’s not symmetric. Specifically, g of t does not equal negative g of one minus t. What this means in practice is that p of x approaches zero slowly, but approaches one very rapidly.</p>
<p>If the opposite behavior is true in your data - if probabilities approach one slowly but zero rapidly - you should use the log-log model instead, where g of t equals negative natural log of negative natural log of t.</p>
<p>The cloglog model is particularly useful in survival models, such as Cox model where this kind of loglog appears naturally.</p>
</div>
<p>. . .</p>
<p>The modeling approach is <span class="math inline">\(p(x) = g^{-1}(x^T\beta)\)</span> with</p>
<div class="square-def">
<p><span class="math display">\[g(t) = \ln(-\ln(1-t)) \quad \text{i.e.} \quad g^{-1}(t) = 1 - e^{-e^t}\]</span></p>
</div>
<p>. . .</p>
<p>Not symmetric in the sense that <span style="background-color: lightblue;"><span class="math inline">\(g(t) \neq -g(1-t)\)</span></span>.</p>
<p><span class="math inline">\(p(x)\)</span> approaches <span class="math inline">\(0\)</span> slowly but <span class="math inline">\(1\)</span> very rapidly</p>
<p>. . .</p>
<p>If the opposite is true: take <span class="math inline">\(g(t) = -\ln(-\ln(t))\)</span> (loglog model)</p>
<p>. . .</p>
<p>Useful in <span style="background-color: yellow;">survival models</span> (e.g.&nbsp;Cox)</p>
</section>
<section id="details-on-the-logit-model" class="level2">
<h2 class="anchored" data-anchor-id="details-on-the-logit-model">Details on the Logit Model</h2>
<div class="notes">
<p>Let me give you three compelling reasons why the logit model is our preferred choice for binary regression.</p>
<p>First, it provides a highly valued interpretation tool: odds ratios. These allow us to communicate results in an intuitive way that’s meaningful across many fields, from medicine to social sciences to marketing.</p>
<p>Second, it’s more practical from a theoretical point of view. The mathematical properties make it easier to work with, and we have good theoretical foundations for why it arises naturally.</p>
<p>Third, the logit model is the natural model in many situations. As we’ll see in the next slide, it emerges automatically under common distributional assumptions.</p>
</div>
<ol type="1">
<li><p>Highly valued interpretation tool: odds-ratios.</p></li>
<li><p>More “practical” from a theoretical point of view.</p></li>
<li><p>Natural model in many situations.</p></li>
</ol>
</section>
<section id="theoretical-motivation-of-logit" class="level2">
<h2 class="anchored" data-anchor-id="theoretical-motivation-of-logit">Theoretical Motivation of Logit</h2>
<div class="notes">
<p>Now let’s see a beautiful theoretical result that explains why logit is so natural.</p>
<p>We will show in exercises that there’s an important connection between the logit model and Gaussian distributions.</p>
<p>Here’s the setup: If the two groups of individuals associated with Y equals 0 and Y equals 1 have a Gaussian distribution of X with different means - that is, for m_0 not equal to m_1, we have X given Y equals 0 follows a normal distribution with mean m_0 and covariance Sigma, and X given Y equals 1 follows a normal distribution with mean m_1 and the same covariance Sigma - then the probability that Y equals 1 given X equals x automatically follows a logistic model.</p>
<p>This is a powerful result! It tells us that whenever we have two normally distributed groups that differ in their means but share the same covariance structure, the logit model emerges naturally.</p>
<p>And there’s more. The previous result remains true for any exponential family instead normal distributions. So this property of the logit model is quite general - it’s not limited to just Gaussian distributions.</p>
<p>This theoretical foundation helps explain why the logit model appears so frequently in practice.</p>
</div>
<p>. . .</p>
<p>We will show in exercises that:</p>
<p><span style="background-color: orange;">If</span> the two groups of individuals associated with <span class="math inline">\(Y = 0\)</span> and <span class="math inline">\(Y = 1\)</span> have a Gaussian distribution of <span class="math inline">\(X\)</span> with different means, i.e.&nbsp;for <span class="math inline">\(m_0 \neq m_1\)</span>,</p>
<div class="square-def">
<p><span class="math inline">\(X|(Y = 0) \sim \mathcal N(m_0, \Sigma) \and X|(Y = 1) \sim \mathcal N(m_1, \Sigma)\)</span></p>
</div>
<p><span style="background-color: orange;">then</span> <span class="math inline">\(\mathbb P(Y = 1|X = x)\)</span> follows a logistic model.</p>
<p>. . .</p>
<p>The previous result remains true for any distribution from the exponential family instead of <span class="math inline">\(\mathcal N\)</span>.</p>
</section>
<section id="summary-for-binary-variables" class="level2">
<h2 class="anchored" data-anchor-id="summary-for-binary-variables">Summary for Binary Variables:</h2>
<div class="notes">
<p>Let’s summarize our discussion about modeling binary variables.</p>
<p>If Y is a binary variable, then Y given X equals x follows a Bernoulli distribution with parameter p of x.</p>
<p>In a GLM model for Y, we set p of x equals g inverse of x transpose beta, where g is the link function. Now, which link function should we choose?</p>
<p>By default, we use the logit function, which is the most natural choice for all the reasons we’ve discussed: odds ratio interpretation, theoretical justification, and mathematical convenience.</p>
<p>Possibly, we might use probit if we have good reasons to justify it - for example, if there’s a clear latent variable story. But keep in mind that the results will typically be very similar to logit, so unless you have a compelling reason, logit is simpler.</p>
<p>We might use cloglog, or its mirror image loglog, if we have good reasons to justify it. The main reasons would be: first, strong asymmetry in p of x - probabilities that approach zero and one at very different rates - and second, a connection with a Cox model when dealing with discretized survival data.</p>
<p>In the following sections, we will focus on the logit model, since it’s the default choice and the most widely applicable.</p>
<p>This completes our journey through link function selection for binary outcomes. The key takeaway is: start with logit, and only consider alternatives when you have a specific, justifiable reason to do so.</p>
</div>
<p>. . .</p>
<p>If <span class="math inline">\(Y\)</span> is a binary variable, <span style="background-color: lightblue;"><span class="math inline">\((Y|X = x) \sim \mathcal B(p(x))\)</span></span>.</p>
<p>In a GLM model for <span class="math inline">\(Y\)</span>, we set <span class="math inline">\(p(x) = g^{-1}(x^T\beta)\)</span> where <span class="math inline">\(g\)</span> is:</p>
<ul>
<li><p><span style="background-color: yellow;">by default the logit</span> function, which is the most natural;</p></li>
<li><p><span style="background-color: yellow;">possibly probit</span> if we have good reasons to justify it (but the results will be similar to logit);</p></li>
<li><p><span style="background-color: yellow;">cloglog (or loglog)</span> if we have good reasons to justify it (<span style="background-color: yellow;">strong asymmetry of <span class="math inline">\(p(x)\)</span></span>, connection with a Cox model).</p></li>
</ul>
<p>. . .</p>
<p>In the following, we will <span style="background-color: yellow;">focus on the logit model</span>.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<div class="notes">
<p>Now that we’ve established why the logit model is our preferred choice, let’s see how to work with it in practice.</p>
<p>We’ll cover four essential aspects: First, how to interpret the model and understand what the coefficients tell us. Second, how to estimate beta from a dataset. Third, how to evaluate the quality of our estimation. And fourth, how to exploit the model to make predictions and perform classification.</p>
<p>Let’s begin with model interpretation.</p>
</div>
<ul>
<li>interpret the model,</li>
<li>estimate <span class="math inline">\(\beta\)</span> from a dataset,</li>
<li>evaluate the quality of estimation,</li>
<li>exploit it to make predictions/classification.</li>
</ul>
</section>
</section>
<section id="model-interpretation" class="level1">
<h1>Model Interpretation</h1>
<section id="interpretation-of-the-logistic-model" class="level2">
<h2 class="anchored" data-anchor-id="interpretation-of-the-logistic-model">Interpretation of the Logistic Model</h2>
<div class="notes">
<p>Let’s start with the mathematical form of the logistic model.</p>
<p>If x is a vector with p components - x superscript 1 through x superscript p - then our model is:</p>
<p>p of x equals logit inverse of x transpose beta, which equals e to the x transpose beta, divided by 1 plus e to the x transpose beta.</p>
<p>What does this tell us about how each variable affects the probability?</p>
<p>First key point: p of x is increasing with x^{(j)} if beta j is positive, and decreasing otherwise. So the sign of the coefficient tells us the direction of the effect.</p>
<p>Second key point: The larger the absolute value of beta j, the stronger the discriminatory power of regressor X j. In other words, when the coefficient has a large magnitude, a small variation in x j can cause a large variation in p of x.</p>
</div>
<p>If <span class="math inline">\(x=(x^{(1)}, \dots, x^{(p)}) \in \mathbb R^{p \times 1}\)</span></p>
<div class="square-def">
<p><span class="math display">\[p(x) = \text{logit}^{-1}(x^T\beta) = \frac{e^{x^T\beta}}{1 + e^{x^T\beta}}\]</span></p>
</div>
<ul>
<li><p><span class="math inline">\(x^{(j)} \to p(x)\)</span> is increasing if <span class="math inline">\(\beta_j &gt; 0\)</span>, decreasing otherwise.</p></li>
<li><p>The larger <span class="math inline">\(|\beta_j|\)</span> is, the stronger the discriminatory power of regressor <span class="math inline">\(X^{(j)}\)</span> (a small variation in <span class="math inline">\(x^{(j)}\)</span> can cause a large variation in <span class="math inline">\(p(x)\)</span>).</p></li>
</ul>
</section>
<section id="shape-of-logit-function" class="level2">
<h2 class="anchored" data-anchor-id="shape-of-logit-function">Shape of logit function</h2>
<div class="notes">
<p>Let me show you what this relationship looks like visually.</p>
<p>The shape of the function that maps x superscript j to p of x has this characteristic S-curve, or sigmoid shape. You can see how the probability transitions smoothly from near zero to near one as the predictor increases. The steepness of this S-curve in the middle region is determined by the magnitude of beta j - larger coefficients create steeper transitions.</p>
<p>This S-shaped relationship is one of the appealing features of the logistic model - it naturally constrains probabilities to lie between zero and one, and it has this smooth, interpretable transition region.</p>
</div>
<p>. . .</p>
<div style="text-align: center;">
<p><img src="../images/shape_logit.png" class="img-fluid" style="width:40.0%"></p>
</div>
<p>shape of <span class="math inline">\(x^{(j)} \to p(x)\)</span></p>
</section>
<section id="example-bmi-study-french-imc" class="level2">
<h2 class="anchored" data-anchor-id="example-bmi-study-french-imc">Example: BMI Study (French: IMC)</h2>
<div class="notes">
<p>Let’s look at a concrete example to make this more tangible. This is a BMI study with French data - BMI is called IMC in French.</p>
<p>For each of 5,300 patients, we observe several variables:</p>
<p>Our outcome variable Y is binary: it equals 1 if BMI is greater than 35, and 0 otherwise. So we’re trying to predict obesity.</p>
<p>Our predictor variables include: AGE in years, DBP which is diastolic blood pressure - that’s the low pressure measurement, SEXE indicating male or female, ACTIV which equals 1 if the person engages in intense sports activity and 0 otherwise, WALK which equals 1 if they walk or cycle to work and 0 otherwise, and MARITAL which is marital status with 6 categories: married, widowed, divorced, separated, single, or cohabiting.</p>
</div>
<p>For each of the 5300 patients, we observe:</p>
<ul>
<li><span style="background-color: yellow;"><span class="math inline">\(Y\)</span>: 1 if BMI &gt; 35</span>, 0 otherwise</li>
<li>AGE</li>
<li>DBP: low pressure (diastolic)</li>
<li>SEXE: male or female</li>
<li>ACTIV: 1 if intense sports activity, 0 otherwise</li>
<li>WALK: 1 if walking or cycling to work, 0 otherwise</li>
<li>MARITAL: marital status (6 categories: married, widowed, divorced, separated, single or cohabiting)</li>
</ul>
</section>
<section id="model-definition" class="level2">
<h2 class="anchored" data-anchor-id="model-definition">Model Definition</h2>
<div class="notes">
<p>Our goal is to model the probability that Y equals 1 given X, where X groups all the predictor variables we just mentioned, excluding Y of course.</p>
<p>In R, implementing this is straightforward. We use the glm function - that’s generalized linear model - with the family equals binomial option. This tells R we’re doing logistic regression.</p>
<p>The syntax would be: glm of Y tilde AGE plus DBP plus SEXE plus ACTIV plus WALK plus MARITAL, with family equals binomial.</p>
<p>The tilde notation means “is modeled by” and the plus signs indicate we’re including all these variables as predictors.</p>
</div>
<p>. . .</p>
<p>We seek to model <span class="math inline">\(P(Y = 1|X)\)</span> where <span class="math inline">\(X\)</span> groups the previous variables (excluding <span class="math inline">\(Y\)</span>).</p>
<p>. . .</p>
<p>In R, we use the <code>glm</code> function with the <code>family=binomial</code> option.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glm</span>(Y <span class="sc">~</span> AGE <span class="sc">+</span> DBP <span class="sc">+</span> SEXE <span class="sc">+</span> ACTIV <span class="sc">+</span> WALK <span class="sc">+</span> MARITAL, <span class="at">family=</span>binomial)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="model-results" class="level2">
<h2 class="anchored" data-anchor-id="model-results">Model Results</h2>
<div class="notes">
<p>Now let’s look at what the model produces.</p>
<p>The output gives us estimated coefficients, as in linear regression</p>
<p>Looking at the significant variables - those marked with asterisks - we see several interesting patterns:</p>
<p>DBP, diastolic blood pressure, has a positive coefficient and is highly significant. Higher blood pressure is associated with higher probability of obesity.</p>
<p>Being female - SEXE FEMME - has a positive significant coefficient, indicating women in this sample have higher probability of high BMI.</p>
<p>WALK1 and ACTIV1 both have negative coefficients and are highly significant. This makes intuitive sense: people who walk or cycle to work, and people who engage in intense physical activity, have lower probability of high BMI.</p>
<p>AGE appears not to be significant in this simple linear form.</p>
<p>Looking at the marital status variables - MARITAL 2 through 6 - none of them are statistically significant. This suggests we want to remove the MARITAL variable from the model to simplify it.</p>
</div>
<p>. . .</p>
<div style="font-size: 50%;">
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 18%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Estimate</th>
<th>Std. Error</th>
<th>z value</th>
<th>Pr(&gt;</th>
<th>z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>-2.810240</td>
<td>0.294316</td>
<td>-9.548</td>
<td>&lt; 2e-16</td>
<td>***</td>
</tr>
<tr class="even">
<td>AGE</td>
<td>-0.004407</td>
<td>0.002717</td>
<td>-1.622</td>
<td>0.105</td>
<td></td>
</tr>
<tr class="odd">
<td>DBP</td>
<td>0.017581</td>
<td>0.003283</td>
<td>5.356</td>
<td>8.53e-08</td>
<td>***</td>
</tr>
<tr class="even">
<td>SEXEFEMME</td>
<td>0.544916</td>
<td>0.081261</td>
<td>6.706</td>
<td>2.00e-11</td>
<td>***</td>
</tr>
<tr class="odd">
<td>WALK1</td>
<td>-0.409344</td>
<td>0.095972</td>
<td>-4.265</td>
<td>2.00e-05</td>
<td>***</td>
</tr>
<tr class="even">
<td>ACTIV1</td>
<td>-0.789734</td>
<td>0.126653</td>
<td>-6.235</td>
<td>4.51e-10</td>
<td>***</td>
</tr>
<tr class="odd">
<td>MARITAL2</td>
<td>0.070132</td>
<td>0.149638</td>
<td>0.469</td>
<td>0.639</td>
<td></td>
</tr>
<tr class="even">
<td>MARITAL3</td>
<td>-0.071318</td>
<td>0.127510</td>
<td>-0.559</td>
<td>0.576</td>
<td></td>
</tr>
<tr class="odd">
<td>MARITAL4</td>
<td>0.188228</td>
<td>0.206598</td>
<td>0.911</td>
<td>0.362</td>
<td></td>
</tr>
<tr class="even">
<td>MARITAL5</td>
<td>0.070613</td>
<td>0.115928</td>
<td>0.609</td>
<td>0.542</td>
<td></td>
</tr>
<tr class="odd">
<td>MARITAL6</td>
<td>-0.150165</td>
<td>0.157687</td>
<td>-0.952</td>
<td>0.341</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>. . .</p>
<p>The interpretation is similar to that of a linear regression model.</p>
<p>. . .</p>
<p>We want to remove the MARITAL variable from the model.</p>
</section>
<section id="model-results-with-age2" class="level2">
<h2 class="anchored" data-anchor-id="model-results-with-age2">Model Results with <span class="math inline">\(AGE^2\)</span></h2>
<div class="notes">
<p>Perhaps AGE has a non-linear effect. Let’s try adding AGE squared to the model.</p>
<p>In R, we write: glm of Y tilde AGE plus I of AGE squared</p>
<p>Now look at what happens.</p>
<p>Both AGE and AGE squared are now highly significant! The positive coefficient on AGE and negative coefficient on AGE squared suggests an inverted U-shape relationship: the probability of high BMI increases with age initially, then decreases at older ages.</p>
<p>All our other key variables remain significant with similar interpretations.</p>
<p>For someone for which WALK1 equals 0 and ACTIV1 equals 0 - so they don’t walk to work and don’t do intense sports - the probability that Y equals 1 given their age and blood pressure is: logit inverse of negative 3.95 plus 0.064 times AGE minus 0.00068 times AGE squared plus 0.0122 times DBP.</p>
<p>Now, for someone for which WALK1 equals 0 and ACTIV1 equals 1 - they do engage in intense sports activity - we get the same formula but we subtract 0.657. That’s the coefficient of ACTIV1 in red. This person has a lower probability of high BMI, all else being equal, and we can quantify exactly how much lower.</p>
<p>This demonstrates how we can use the fitted model to compute predicted probabilities for individuals with different covariate profiles.</p>
</div>
<p>. . .</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glm</span>(Y <span class="sc">~</span> AGE <span class="sc">+</span> <span class="fu">I</span>(AGE<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> DBP <span class="sc">+</span> SEXE <span class="sc">+</span> WALK <span class="sc">+</span> ACTIV, <span class="at">family=</span>binomial)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Gives</p>
<p>. . .</p>
<div style="font-size: 50%;">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 17%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Estimate</th>
<th>Std. Error</th>
<th>z value</th>
<th>Pr(&gt;</th>
<th>z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>-3.9564361</td>
<td>0.3155529</td>
<td>-12.538</td>
<td>&lt; 2e-16</td>
<td>***</td>
</tr>
<tr class="even">
<td>AGE</td>
<td>0.0640837</td>
<td>0.0123960</td>
<td>5.170</td>
<td>2.34e-07</td>
<td>***</td>
</tr>
<tr class="odd">
<td>I(AGE^2)</td>
<td>-0.0006758</td>
<td>0.0001260</td>
<td>-5.364</td>
<td>8.14e-08</td>
<td>***</td>
</tr>
<tr class="even">
<td>DBP</td>
<td>0.0121546</td>
<td>0.0033775</td>
<td>3.599</td>
<td>0.00032</td>
<td>***</td>
</tr>
<tr class="odd">
<td>SEXEFEMME</td>
<td>0.5155651</td>
<td>0.0776229</td>
<td>6.642</td>
<td>3.10e-11</td>
<td>***</td>
</tr>
<tr class="even">
<td>WALK1</td>
<td>-0.4042257</td>
<td>0.0913195</td>
<td>-4.426</td>
<td>9.58e-06</td>
<td>***</td>
</tr>
<tr class="odd">
<td>ACTIV1</td>
<td>-0.6573558</td>
<td>0.1150226</td>
<td>-5.715</td>
<td>1.10e-08</td>
<td>***</td>
</tr>
</tbody>
</table>
</div>
<p>For someone for which WALK1=0 and <span style="background-color: yellow;">ACTIV1=0</span>:</p>
<div style="font-size: 50%;">
<p><span class="math inline">\(P(Y = 1|\text{AGE}, \text{DBP}) = \text{logit}^{-1}(-3.95 + 0.064 \times \text{AGE} - 0.00068 \times \text{AGE}^2 + 0.0122 \times \text{DBP})\)</span></p>
</div>
<p>. . .</p>
<p>For someone for which WALK1=0 and <span style="background-color: yellow;">ACTIV1=1</span>:</p>
<div style="font-size: 50%;">
<p><span class="math inline">\(P(Y = 1|\text{AGE}, \text{DBP}) = \text{logit}^{-1}(-3.95 + 0.064 \times \text{AGE} - 0.00068 \times \text{AGE}^2 + 0.0122 \times \text{DBP} \color{red}{ - 0.657})\)</span></p>
</div>
</section>
<section id="odds" class="level2">
<h2 class="anchored" data-anchor-id="odds">Odds</h2>
<div class="notes">
<p>Let’s look at how odds relate to probability. When we know the probability pp of an event, its odds are simply p/(1−p)p/(1−p). For example, if the probability of winning is 0.75, then the odds are 0.75/0.25=30.75/0.25=3. That means the event is three times as likely to happen than not to happen.</p>
<p>​</p>
<p>In betting, when you hear “odds 3 to 1,” it means for every 3 people who bet on event A, there’s 1 person betting against it. That also means, if you pick a random bettor, there’s a 3 out of 4 chance they chose A, giving you a probability p=3/4p=3/4 and for not-A, 1/41/4.</p>
</div>
<div class="square-def">
<p><span class="math display">\[\text{odds} = \frac{p}{1-p}\]</span></p>
</div>
<p>. . .</p>
<p><strong>Betting interpretation</strong> for example, <span class="math inline">\(3\)</span> to <span class="math inline">\(1\)</span> means that for <span class="math inline">\(3\)</span> people betting on <span class="math inline">\(A\)</span>, <span class="math inline">\(1\)</span> person bets on <span class="math inline">\(B\)</span>.</p>
<p>. . .</p>
<p>So a randomly chosen bettor has a probability of <span class="math inline">\(p=3/4\)</span> of betting on <span class="math inline">\(A\)</span> and <span class="math inline">\(1-p=1/4\)</span> on betting on <span class="math inline">\(B\)</span></p>
</section>
<section id="odds-given-xx" class="level2">
<h2 class="anchored" data-anchor-id="odds-given-xx">Odds given <span class="math inline">\(X=x\)</span></h2>
<div class="notes">
<p>​</p>
<p>Just like before, if we consider the probability of an event given some condition—say, the odds of Y=1Y=1 given X=xX=x—we use p(x)p(x) to denote the probability. The odds in this case are odds(x)=p(x)1−p(x)odds(x)=1−p(x)p(x), where p(x)=P(Y=1∣X=x)p(x)=P(Y=1∣X=x) If <span class="math inline">\(p\)</span> is the probability of an event <span class="math inline">\(A\)</span>, then its odds are:</p>
</div>
<p>. . .</p>
<p>Similarly, the odds of obtaining <span class="math inline">\(Y = 1\)</span> given <span class="math inline">\(X = x\)</span> is:</p>
<div class="square-def">
<p><span class="math display">\[\text{odds}(x) = \frac{p(x)}{1 - p(x)}\]</span></p>
</div>
<p>. . .</p>
<p>where <span class="math inline">\(p(x) = P(Y = 1|X = x)\)</span>.</p>
</section>
<section id="odds-ratio" class="level2">
<h2 class="anchored" data-anchor-id="odds-ratio">Odds Ratio</h2>
<div class="notes">
<p>Now let’s talk about the odds ratio, which compares odds between two different individuals or groups.</p>
<p>If we have two people with characteristics x1x1 and x2x2, the odds ratio tells us how the odds compare between them. It’s calculated as OR(x1,x2)=odds(x1)odds(x2)OR(x1,x2)=odds(x2)odds(x1), which expands to that double fraction you see on the slide.</p>
<p>Here’s something really important though: don’t confuse odds ratio with probability ratio. They’re not the same thing.</p>
<p>The odds ratio is a ratio of odds, not probabilities. So if someone has odds of 3 and another person has odds of 1.5, the odds ratio is 2. But that doesn’t mean one person has twice the probability of the other.</p>
<p>There’s only one exception to watch out for: when both probabilities are very small—say, less than 0.1. In that case, 1−p(x1)1−p(x1) is approximately 1, and 1−p(x2)1−p(x2) is also approximately 1. So the denominators basically cancel out, and the odds ratio becomes pretty close to the probability ratio. But this only works for rare events.</p>
<p>Otherwise, always remember: odds ratios and probability ratios are different beasts, so don’t treat them the same way.</p>
</div>
<p>. . .</p>
<p>If two individuals have characteristics <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> respectively, we call the odds ratio between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>:</p>
<p><span class="math display">\[OR(x_1, x_2) = \frac{\text{odds}(x_1)}{\text{odds}(x_2)} = \frac{\frac{p(x_1)}{1-p(x_1)}}{\frac{p(x_2)}{1-p(x_2)}}\]</span></p>
<p>. . .</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><span style="background-color: orange;"><strong>DO NOT CONFUSE ODDS RATIO WITH PROBABILITY RATIO</strong></span></p>
<p>Only possible exception: if <span class="math inline">\(p(x_1)\)</span> and <span class="math inline">\(p(x_2)\)</span> are very small because then <span class="math inline">\(1 - p(x_1) \approx 1\)</span> and <span class="math inline">\(1 - p(x_2) \approx 1\)</span>, so that <span class="math inline">\(OR(x_1, x_2) \approx p(x_1)/p(x_2)\)</span></p>
</div>
</div>
</section>
<section id="link-with-proba.-ratio" class="level2">
<h2 class="anchored" data-anchor-id="link-with-proba.-ratio">Link with Proba. Ratio</h2>
<div class="notes">

</div>
<p>. . .</p>
<p>However, it remains that:</p>
<p><span class="math display">\[\begin{aligned}
OR(x_1, x_2) &gt; 1 &amp;\Leftrightarrow \frac{p(x_1)}{p(x_2)} &gt; 1\\
OR(x_1, x_2) &lt; 1 &amp;\Leftrightarrow \frac{p(x_1)}{p(x_2)} &lt; 1\\
OR(x_1, x_2) = 1 &amp;\Leftrightarrow \frac{p(x_1)}{p(x_2)} = 1
\end{aligned}\]</span></p>
</section>
<section id="other-property-of-odds-ratio" class="level2">
<h2 class="anchored" data-anchor-id="other-property-of-odds-ratio">Other Property of Odds Ratio</h2>
<p>. . .</p>
<p><span class="math inline">\(OR(x_1, x_2)\)</span> accentuates the differences compared to <span class="math inline">\(p(x_1)/p(x_2)\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
OR(x_1, x_2) &gt; \frac{p(x_1)}{p(x_2)} &amp;\text{ when } OR(x_1, x_2) &gt; 1\\
OR(x_1, x_2) &lt; \frac{p(x_1)}{p(x_2)} &amp;\text{ when } OR(x_1, x_2) &lt; 1
\end{aligned}\]</span></p>
</section>
<section id="examples-using-odds-ratios" class="level2">
<h2 class="anchored" data-anchor-id="examples-using-odds-ratios">Examples Using Odds Ratios</h2>
<p>. . .</p>
<p>A logistic regression is most often used to compare the <span style="background-color: lightgreen;">behavior of two individuals</span> with respect to <span style="background-color: lightblue;">the variable of interest</span>.</p>
<p>. . .</p>
<p>Examples:</p>
<ul>
<li><span style="background-color: lightgreen;">probability of purchase</span> depending on whether or not one has been the <span style="background-color: lightblue;">subject of a personalized promotion</span>;</li>
<li>for a given vehicle, <span style="background-color: lightgreen;">probability of experiencing a breakdown</span> <span style="background-color: lightblue;">according to age</span>;</li>
<li><span style="background-color: lightgreen;">probability of recovery</span> according to the <span style="background-color: lightblue;">treatment used</span>;</li>
</ul>
</section>
<section id="odds-ratio-in-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="odds-ratio-in-logistic-regression">Odds Ratio in Logistic Regression</h2>
<p>. . .</p>
<p>It holds that <span style="background-color: lightblue;"><span class="math inline">\(\text{odds}(x) = \frac{p(x)}{1 - p(x)} = \exp(x^T \beta)\)</span></span></p>
<p>Hence,</p>
<div class="square-def">
<p><span class="math inline">\(OR(x_1, x_2) = \frac{\text{odds}(x_1)}{\text{odds}(x_2)} = \exp((x_1 - x_2)^T \beta)\)</span></p>
</div>
<p>. . .</p>
<p>If the two individuals differ only by regressor <span class="math inline">\(j\)</span>, then</p>
<p><span class="math inline">\(OR(x_1, x_2) = \exp(\beta_j (x_1^{(j)} - x_2^{(j)}))\)</span></p>
<p>. . .</p>
<p>If regressor <span class="math inline">\(j\)</span> is binary (<span class="math inline">\(x_1^{(j)} = 1\)</span> while <span class="math inline">\(x_2^{(j)} = 0\)</span>):</p>
<p><span class="math inline">\(OR(x_1, x_2) = \exp(\beta_j)\)</span></p>
</section>
<section id="key-summary-statement" class="level2">
<h2 class="anchored" data-anchor-id="key-summary-statement">Key Summary Statement</h2>
<p>. . .</p>
<p>In a logistic regression model, <span class="math inline">\(\beta_j\)</span> is interpreted as the logarithm of the odds-ratio between two individuals differing by a quantity of <span class="math inline">\(1\)</span> on regressor <span class="math inline">\(j\)</span>, all else being equal.</p>
<p>. . .</p>
<p><strong>In brief:</strong> <span style="background-color: yellow;"><span class="math inline">\(\exp(\beta_j) = OR(x^{(j)} + 1, x^{(j)})\)</span></span></p>
<p>If regressor <span class="math inline">\(j\)</span> is binary (absence or presence of a certain characteristic):</p>
<p>. . .</p>
<p><span class="math inline">\(\exp(\beta_j)\)</span> is simply the OR between the <span style="background-color: yellow;">presence or absence</span> of this characteristic, all else being equal.</p>
</section>
<section id="example-1-intense-sports-activity" class="level2">
<h2 class="anchored" data-anchor-id="example-1-intense-sports-activity">Example 1: Intense Sports Activity</h2>
<p>. . .</p>
<div style="font-size: 50%;">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 17%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Estimate</th>
<th>Std. Error</th>
<th>z value</th>
<th>Pr(&gt;</th>
<th>z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>-3.9564361</td>
<td>0.3155529</td>
<td>-12.538</td>
<td>&lt; 2e-16</td>
<td>***</td>
</tr>
<tr class="even">
<td>AGE</td>
<td>0.0640837</td>
<td>0.0123960</td>
<td>5.170</td>
<td>2.34e-07</td>
<td>***</td>
</tr>
<tr class="odd">
<td>I(AGE^2)</td>
<td>-0.0006758</td>
<td>0.0001260</td>
<td>-5.364</td>
<td>8.14e-08</td>
<td>***</td>
</tr>
<tr class="even">
<td>DBP</td>
<td>0.0121546</td>
<td>0.0033775</td>
<td>3.599</td>
<td>0.00032</td>
<td>***</td>
</tr>
<tr class="odd">
<td>SEXEFEMME</td>
<td>0.5155651</td>
<td>0.0776229</td>
<td>6.642</td>
<td>3.10e-11</td>
<td>***</td>
</tr>
<tr class="even">
<td>WALK1</td>
<td>-0.4042257</td>
<td>0.0913195</td>
<td>-4.426</td>
<td>9.58e-06</td>
<td>***</td>
</tr>
<tr class="odd">
<td>ACTIV1</td>
<td>-0.6573558</td>
<td>0.1150226</td>
<td>-5.715</td>
<td>1.10e-08</td>
<td>***</td>
</tr>
</tbody>
</table>
</div>
<p>. . .</p>
<p>The Odds Ratio corresponding to practicing or not practicing intense sports activity is, all else being equal:</p>
<p>. . .</p>
<p><span class="math inline">\(\exp(-0.657) \approx 0.52\)</span></p>
<p>. . .</p>
<p>The odds of obesity occurrence therefore decrease by half for individuals practicing intense sports activity.</p>
<p><strong>(The odds, not the probability!)</strong></p>
</section>
<section id="example-2-diastolic-pressure" class="level2">
<h2 class="anchored" data-anchor-id="example-2-diastolic-pressure">Example 2: Diastolic Pressure</h2>
<div style="font-size: 50%;">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 17%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Estimate</th>
<th>Std. Error</th>
<th>z value</th>
<th>Pr(&gt;</th>
<th>z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>-3.9564361</td>
<td>0.3155529</td>
<td>-12.538</td>
<td>&lt; 2e-16</td>
<td>***</td>
</tr>
<tr class="even">
<td>AGE</td>
<td>0.0640837</td>
<td>0.0123960</td>
<td>5.170</td>
<td>2.34e-07</td>
<td>***</td>
</tr>
<tr class="odd">
<td>I(AGE^2)</td>
<td>-0.0006758</td>
<td>0.0001260</td>
<td>-5.364</td>
<td>8.14e-08</td>
<td>***</td>
</tr>
<tr class="even">
<td>DBP</td>
<td>0.0121546</td>
<td>0.0033775</td>
<td>3.599</td>
<td>0.00032</td>
<td>***</td>
</tr>
<tr class="odd">
<td>SEXEFEMME</td>
<td>0.5155651</td>
<td>0.0776229</td>
<td>6.642</td>
<td>3.10e-11</td>
<td>***</td>
</tr>
<tr class="even">
<td>WALK1</td>
<td>-0.4042257</td>
<td>0.0913195</td>
<td>-4.426</td>
<td>9.58e-06</td>
<td>***</td>
</tr>
<tr class="odd">
<td>ACTIV1</td>
<td>-0.6573558</td>
<td>0.1150226</td>
<td>-5.715</td>
<td>1.10e-08</td>
<td>***</td>
</tr>
</tbody>
</table>
</div>
<p>The OR for a diastolic pressure difference of <span class="math inline">\(+20\)</span> is:</p>
<p>. . .</p>
<p><span class="math inline">\(\exp(0.0121546 \times 20) \approx 1.28\)</span></p>
<p>. . .</p>
<p>The odds of obesity occurrence therefore increase by <span class="math inline">\(28\%\)</span>.</p>
</section>
</section>
<section id="estimation-of-the-parameters" class="level1">
<h1>EStimation of the Parameters</h1>
<section id="the-framework" class="level2">
<h2 class="anchored" data-anchor-id="the-framework">The Framework</h2>
<p>. . .</p>
<p>We observe <span class="math inline">\(n\)</span> i.i.d. realizations <span class="math inline">\((Y_i, X_i)\)</span> where <span class="math inline">\(Y_i \in \{0, 1\}\)</span> and <span class="math inline">\(X_i \in \mathbb{R}^p\)</span>.</p>
<p>. . .</p>
<p>We denote <span class="math inline">\(p(x_i) = P(Y_i = 1|X_i = x_i)\)</span>.</p>
</section>
<section id="the-logistic-model" class="level2">
<h2 class="anchored" data-anchor-id="the-logistic-model">The Logistic Model</h2>
<p>. . .</p>
<p>We assume the logistic model: for all <span class="math inline">\(i\)</span>,</p>
<div class="square-def">
<p><span class="math display">\[p(x_i) = \text{logit}^{-1}(x_i^T \beta) = \frac{e^{x_i^T \beta}}{1 + e^{x_i^T \beta}}\]</span></p>
</div>
<p>. . .</p>
<p>where <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_p)^T\)</span> and <span class="math inline">\(x_i^T \beta = \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\)</span>.</p>
</section>
<section id="parameter-estimation" class="level2">
<h2 class="anchored" data-anchor-id="parameter-estimation">Parameter Estimation</h2>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[p(x_i) = \text{logit}^{-1}(x_i^T \beta) = \frac{e^{x_i^T \beta}}{1 + e^{x_i^T \beta}}\]</span></p>
</div>
<p>We will estimate <span class="math inline">\(\beta\)</span> by <span style="background-color: yellow;">maximizing the likelihood</span>.</p>
<p>. . .</p>
<p>We will denote <span style="background-color: lightblue;"><span class="math inline">\(p_\beta(x_i)\)</span></span> to emphasize the dependence of <span style="background-color: lightblue;"><span class="math inline">\(p(x_i)\)</span></span> on <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="likelihood-calculation" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-calculation">Likelihood Calculation</h2>
<p>. . .</p>
<p>For all <span class="math inline">\(i\)</span>, <span class="math inline">\(Y_i|(X_i = x_i)\)</span> follows the distribution <span class="math inline">\(B(p_\beta(x_i))\)</span>. Therefore</p>
<div style="font-size: 80%;">
<div class="square-def">
<p><span class="math display">\[P(Y_i = y_i|X_i = x_i) = p_\beta(x_i)^{y_i}(1 - p_\beta(x_i))^{1-y_i}\]</span></p>
</div>
</div>
<p>. . .</p>
<p>for all <span class="math inline">\(y_i \in \{0, 1\}\)</span>.</p>
</section>
<section id="likelihood" class="level2">
<h2 class="anchored" data-anchor-id="likelihood">Likelihood</h2>
<p>. . .</p>
<p>By independence, we obtain the likelihood</p>
<div class="square-def">
<p><span class="math display">\[\ell(\beta, y_1, \ldots, y_n, x_1, \ldots, x_n) = \prod_{i=1}^n p_\beta(x_i)^{y_i}(1 - p_\beta(x_i))^{1-y_i}\]</span></p>
</div>
</section>
<section id="log-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="log-likelihood">Log-Likelihood</h2>
<p>. . .</p>
<p>Taking the log and replacing <span class="math inline">\(p(x_i)\)</span> by its expression, we obtain the log-likelihood:</p>
<div style="font-size: 80%;">
<div class="square-def">
<p><span class="math display">\[\begin{aligned}
L(\beta, y_1, \ldots, y_n, x_1, \ldots, x_n) &amp;= \ln(\ell) \\
&amp;=\sum_{i=1}^n \left[y_i x_i^T \beta - \ln(1 + e^{x_i^T \beta})\right]
\end{aligned}\]</span></p>
</div>
</div>
</section>
<section id="mle-calculation" class="level2">
<h2 class="anchored" data-anchor-id="mle-calculation">MLE Calculation</h2>
<p>. . .</p>
<p>The MLE <span class="math inline">\(\hat{\beta}\)</span>, if it exists, cancels the gradient of <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(\beta\)</span>. This gradient equals</p>
<div class="square-def">
<p><span class="math display">\[\frac{\partial L}{\partial \beta} = \sum_{i=1}^n x_i \left(y_i - \frac{e^{x_i^T \beta}}{1 + e^{x_i^T \beta}}\right)\]</span></p>
</div>
<p>We therefore need to solve a system of <span class="math inline">\(p\)</span> equations with <span class="math inline">\(p\)</span> unknowns.</p>
<p>. . .</p>
<p>But the solution is not explicit: we resort to <span style="background-color: yellow;">numerical methods</span> (Newto-Raphso algo)</p>
</section>
<section id="remarks" class="level2">
<h2 class="anchored" data-anchor-id="remarks">Remarks</h2>
<p>. . .</p>
<p>This is a classic situation when using advanced statistical models: we often <span style="background-color: yellow;">resort to optimization algorithms</span>.</p>
<p>. . .</p>
<p>Does the solution exist? Is it unique?</p>
</section>
<section id="mle-uniqueness" class="level2">
<h2 class="anchored" data-anchor-id="mle-uniqueness">MLE Uniqueness</h2>
<p>. . .</p>
<p>Let <span class="math inline">\(X\)</span> be the design matrix <span class="math inline">\((X^{(1)}, \dots, X^{(p)}) \in \mathbb R^{n \times p}\)</span></p>
<p>. . .</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(\text{rank}(X) = p\)</span>, then the MLE, if it exists, is unique.</p>
</div>
</div>
</section>
<section id="proof-of-mle-uniqueness" class="level2">
<h2 class="anchored" data-anchor-id="proof-of-mle-uniqueness">Proof of MLE Uniqueness</h2>
<p>. . .</p>
<p>It suffices to show that <span class="math inline">\(L\)</span> is strictly concave in <span class="math inline">\(\beta\)</span>.</p>
<p>. . .</p>
<p>Hessian Matrix of <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[\frac{\partial^2 L}{\partial \beta^2} = -\sum_{i=1}^n p_\beta(x_i)(1 - p_\beta(x_i)) x_i x_i^T\]</span></p>
</section>
<section id="hessian-properties" class="level2">
<h2 class="anchored" data-anchor-id="hessian-properties">Hessian Properties</h2>
<p>. . .</p>
<p>It is negative semi-definite. Moreover, for all <span class="math inline">\(u \in \mathbb{R}^p\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
u^T \frac{\partial^2 L}{\partial \beta^2} u = 0 &amp;\Leftrightarrow u^T x_i x_i^T u = 0 \text{ for all } i\\
&amp;\Leftrightarrow u^T x_i = 0 \text{ for all } i\\
&amp;\Leftrightarrow Xu = 0\\
&amp;\Leftrightarrow u = 0
\end{aligned}\]</span></p>
<p>since <span class="math inline">\(\text{rank}(X) = p\)</span>.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>. . .</p>
<p>Thus, for all <span class="math inline">\(u \neq 0\)</span>,</p>
<div class="square-def">
<p><span class="math display">\[u^T \frac{\partial^2 L}{\partial \beta^2} u &lt; 0\]</span></p>
</div>
<p>The Hessian matrix is negative definite and therefore <span class="math inline">\(L\)</span> is strictly concave,</p>
<p>The MLE is unique</p>
</section>
<section id="about-mle-existence" class="level2">
<h2 class="anchored" data-anchor-id="about-mle-existence">About MLE Existence</h2>
<p>. . .</p>
<p>Although <span class="math inline">\(L\)</span> is strictly concave, its <span style="background-color: orange;">maximum can occur at infinity</span> (think of the <span class="math inline">\(\ln\)</span> function), in which case <span class="math inline">\(\hat{\beta}\)</span> does not exist.</p>
<p>. . .</p>
<p>This occurs if there is <span style="background-color: yellow;">non-overlap</span>, i.e., separation by a hyperplane of the <span class="math inline">\(x_i\)</span> for which <span class="math inline">\(y_i = 0\)</span> and those for which <span class="math inline">\(y_i = 1\)</span>.</p>
</section>
<section id="non-overlap-situation" class="level2">
<h2 class="anchored" data-anchor-id="non-overlap-situation">Non-Overlap Situation</h2>
<p>. . .</p>
<p>Mathematically, there is non-overlap if there exists <span class="math inline">\(\alpha \in \mathbb{R}^p\)</span> such that</p>
<p><span class="math display">\[\begin{cases}
\text{for all } i \text{ such that } y_i = 0, &amp; \alpha^T x_i \geq 0 \\
\text{for all } i \text{ such that } y_i = 1, &amp; \alpha^T x_i \leq 0
\end{cases}\]</span></p>
</section>
<section id="illustration-of-non-overlap-situation" class="level2">
<h2 class="anchored" data-anchor-id="illustration-of-non-overlap-situation">Illustration of Non-Overlap Situation</h2>
<p><img src="../images/non_recouvrement.png" class="img-fluid"></p>
</section>
<section id="non-overlap-and-existence" class="level2">
<h2 class="anchored" data-anchor-id="non-overlap-and-existence">Non-Overlap and Existence</h2>
<p>. . .</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition (admitted)
</div>
</div>
<div class="callout-body-container callout-body">
<p>In case of non-overlap, the estimator <span style="background-color: orange;"><span class="math inline">\(\hat{\beta}\)</span> does not exist</span>, in the sense that <span class="math inline">\(L(\beta)\)</span> is maximal when <span class="math inline">\(\|\beta\| \to \infty\)</span> (in one or several directions).</p>
</div>
</div>
<p>. . .</p>
<p>For all <span class="math inline">\(x\)</span>, <span style="background-color: lightblue;"><span class="math inline">\(\hat{p}(x) = \in \{0,1\}\)</span></span>, depending on the position of <span class="math inline">\(x\)</span> relative to the separating hyperplane.</p>
<p>. . .</p>
<p>Nevertheless, there is a “dead zone” in the middle of the <span class="math inline">\(2\)</span> point clouds, because the <span style="background-color: orange;">separating hyperplane is not necessarily unique</span>.</p>
</section>
<section id="beyond-the-dead-zone" class="level2">
<h2 class="anchored" data-anchor-id="beyond-the-dead-zone">Beyond the Dead Zone</h2>
<p>. . .</p>
<p>Beyond this dead zone, classification is very simple (<span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>).</p>
<p>. . .</p>
<p>But no interpretation of the model is possible (the OR are worth <span class="math inline">\(0\)</span> or <span class="math inline">\(+\infty\)</span>).</p>
</section>
<section id="existence-and-uniqueness-of-mle" class="level2">
<h2 class="anchored" data-anchor-id="existence-and-uniqueness-of-mle">Existence and Uniqueness of MLE</h2>
<p>. . .</p>
<p>We say <span style="background-color: yellow;">there is overlap</span> when no hyperplane can separate the red points from the blue points.</p>
<p>. . .</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition (admitted)
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(\text{rank}(X) = p\)</span> and <span style="background-color: yellow;">there is overlap</span>, then the MLE exists and is unique.</p>
</div>
</div>
<p>Under these conditions, we <span style="background-color: yellow;">can therefore search for the MLE</span> using the Newton-Raphson algorithm.</p>
<ol type="1">
<li><p>the maximum exists,</p></li>
<li><p>the function to optimize is strictly concave and there is therefore no local maximum, only a global maximum.</p></li>
</ol>
</section>
<section id="fisher-information-recall" class="level2">
<h2 class="anchored" data-anchor-id="fisher-information-recall">Fisher Information (Recall)</h2>
<p>. . .</p>
<p>Let <span class="math inline">\(X\)</span> be the design matrix (whose rows are the vectors <span class="math inline">\(x_i\)</span>).</p>
<p>. . .</p>
<p>Let <span class="math inline">\(J_n(\beta)\)</span> be the Fisher information matrix:</p>
<div class="square-def">
<p><span class="math display">\[J_n(\beta) = -E\left[\frac{\partial^2 L}{\partial \beta^2}(\beta) \right]\]</span></p>
</div>
</section>
<section id="asymptotic-efficiency" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-efficiency">Asymptotic Efficiency</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition (admitted)
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the logistic regression model, <span style="background-color: yellow;">if</span></p>
<ol type="1">
<li>the distribution of the regressors <span class="math inline">\((X_1, \ldots, X_p)\)</span> has compact support,</li>
<li><span class="math inline">\(\text{rank}(X) = p\)</span>,</li>
<li>the smallest eigenvalue of <span class="math inline">\(X^T X\)</span> tends to infinity with <span class="math inline">\(n\)</span>,</li>
</ol>
<p><span style="background-color: yellow;">then</span></p>
<ol type="1">
<li><p>the maximum likelihood estimator <span class="math inline">\(\hat{\beta}\)</span> is consistent;</p></li>
<li><p><span class="math inline">\(J_n(\beta)^{1/2}(\hat{\beta} - \beta) \xrightarrow{L} N(0, I_p)\)</span></p></li>
</ol>
<p>where <span class="math inline">\(I_p\)</span> is the identity matrix of size <span class="math inline">\(p\)</span>.</p>
</div>
</div>
</section>
<section id="comments" class="level2">
<h2 class="anchored" data-anchor-id="comments">Comments</h2>
<p>. . .</p>
<p>Under these conditions, the MLE therefore exists for sufficiently large <span class="math inline">\(n\)</span>. In fact, there is necessarily overlap when <span class="math inline">\(n\)</span> is large.</p>
<p>. . .</p>
<p>It is asymptotically efficient (= minimal asymptotic variance)</p>
<p>. . .</p>
<p>The Fisher information matrix <span class="math inline">\(J_n(\beta)\)</span> can be calculated</p>
<p>. . .</p>
<p>We will be able to rely on asymptotic normality to perform tests and construct confidence intervals</p>
</section>
<section id="comparison-with-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-linear-regression">Comparison with Linear Regression</h2>
<p>. . .</p>
<p>The formula for <span class="math inline">\(\hat{\beta}\)</span> is <span style="background-color: lightgreen;">explicit</span>: <span class="math inline">\(\hat{\beta} = (X^T X)^{-1} X^T Y\)</span>;</p>
<p>. . .</p>
<p>Its expectation and variance are <span style="background-color: lightgreen;">explicit</span>;</p>
<p>. . .</p>
<p>In the Gaussian model (<span class="math inline">\(Y|X\)</span> Gaussian), the distribution of <span class="math inline">\(\hat{\beta}\)</span> is <span style="background-color: lightgreen;">explicit</span>, which allows constructing exact tests (Student, Fisher).</p>
<p>. . .</p>
<p>If the model is not Gaussian, these tests are valid asymptotically.</p>
</section>
<section id="comparison-with-linear-regression-1" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-linear-regression-1">Comparison with Linear Regression</h2>
<p>. . .</p>
<p><span style="background-color: orange;">No explicit</span> formula for <span class="math inline">\(\hat{\beta}\)</span>, the solution is obtained numerically;</p>
<p>. . .</p>
<p>We know neither the bias nor the variance of <span class="math inline">\(\hat{\beta}\)</span>;</p>
<p>. . .</p>
<p>The distribution of <span class="math inline">\(Y|X\)</span> is simple (a Bernoulli), but we don’t know the distribution of <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p>. . .</p>
<p>We only know its asymptotic distribution.</p>
<p>. . .</p>
<p>We’ll do <span style="background-color: orange;">asymptotic tests</span>!</p>
</section>
</section>
<section id="tests-and-confidence-intervals" class="level1">
<h1>Tests and Confidence Intervals</h1>
<section id="asymptotic-framework" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-framework">Asymptotic Framework</h2>
<p>. . .</p>
<p>Under “good conditions”,</p>
<div class="square-def">
<p><span class="math display">\[J_n(\beta)^{1/2}(\hat{\beta} - \beta) \xrightarrow{L} N(0, I_p)\]</span></p>
</div>
<p>where <span class="math inline">\(J_n(\beta)\)</span> is the Fisher information matrix.</p>
<p>. . .</p>
<p>To build asymptotic tests, we need to understand <span class="math inline">\(J_n(\beta)\)</span> and be able to estimate it.</p>
</section>
<section id="computation-of-j_nbeta" class="level2">
<h2 class="anchored" data-anchor-id="computation-of-j_nbeta">Computation of <span class="math inline">\(J_n(\beta)\)</span></h2>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[J_n(\beta) = -E\left[\frac{\partial^2 L}{\partial \beta^2}(\beta) \bigg| X\right]\]</span></p>
</div>
<p>where <span class="math inline">\(L\)</span> is the log-likelihood of the model.</p>
<p>. . .</p>
<p>From the proof of existence of MLE,</p>
<div class="square-def">
<p><span class="math display">\[J_n(\beta) = \sum_{i=1}^n p_\beta(x_i)(1 - p_\beta(x_i)) x_i x_i^T\]</span></p>
</div>
</section>
<section id="equivalent-form" class="level2">
<h2 class="anchored" data-anchor-id="equivalent-form">Equivalent Form</h2>
<p>. . .</p>
<p>We can write equivalently</p>
<div class="square-def">
<p><span class="math display">\[J_n(\beta) = X^T W_\beta X\]</span></p>
</div>
<p>where <span class="math inline">\(X\)</span> is the design matrix and <span class="math inline">\(W_\beta\)</span> is the diagonal matrix</p>
<p>. . .</p>
<div style="font-size: 60%;">
<div class="square-def">
<p><span class="math display">\[W_\beta = \begin{pmatrix}
p_\beta(x_1)(1 - p_\beta(x_1)) &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; p_\beta(x_2)(1 - p_\beta(x_2)) &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; p_\beta(x_n)(1 - p_\beta(x_n))
\end{pmatrix}\]</span></p>
</div>
</div>
</section>
<section id="estimation" class="level2">
<h2 class="anchored" data-anchor-id="estimation">Estimation</h2>
<p>. . .</p>
<p>To estimate <span class="math inline">\(J_n(\beta)\)</span>, we simply replace <span class="math inline">\(\beta\)</span> by the MLE <span class="math inline">\(\hat{\beta}\)</span></p>
<p>. . .</p>
<p>Under the same regularity assumptions, we can show that</p>
<div class="square-def">
<p><span class="math display">\[J_n(\hat{\beta})^{1/2}(\hat{\beta} - \beta) \xrightarrow{L} N(0, I_p)\]</span></p>
</div>
</section>
<section id="estimated-variance-of-hat-beta_j" class="level2">
<h2 class="anchored" data-anchor-id="estimated-variance-of-hat-beta_j">Estimated Variance of <span class="math inline">\(\hat \beta_j\)</span></h2>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[J_n(\hat{\beta})^{1/2}(\hat{\beta} - \beta) \xrightarrow{L} N(0, I_p)\]</span></p>
</div>
<p>. . .</p>
<p>Denoting <span style="background-color: lightblue;"><span class="math inline">\(\hat{\sigma}_j^2\)</span></span> the <span class="math inline">\(j\)</span>-th diagonal element of <span style="background-color: lightblue;"><span class="math inline">\(J_n(\hat{\beta})^{-1}\)</span></span>, we obtain (admitted):</p>
<div class="square-def">
<p><span class="math display">\[\frac{\hat{\beta}_j - \beta_j}{\hat{\sigma}_j} \xrightarrow{L} N(0, 1)\]</span></p>
</div>
</section>
<section id="confidence-interval" class="level2">
<h2 class="anchored" data-anchor-id="confidence-interval">Confidence Interval</h2>
<p>. . .</p>
<p>We deduce a confidence interval for <span class="math inline">\(\beta_j\)</span>, at asymptotic confidence level <span class="math inline">\(1 - \alpha\)</span>:</p>
<div class="square-def">
<p><span class="math display">\[\text{CI}_{1-\alpha}(\beta_j) = \left[\hat{\beta}_j - q_{1-\alpha/2}\hat{\sigma}_j \,;\, \hat{\beta}_j + q_{1-\alpha/2}\hat{\sigma}_j\right]\]</span></p>
</div>
<p>where <span class="math inline">\(q(1 - \alpha/2)\)</span> is the quantile of order <span class="math inline">\(1 - \alpha/2\)</span> of the <span class="math inline">\(N(0, 1)\)</span> distribution.</p>
<p>. . .</p>
<p>We verify that we have <span style="background-color: lightblue;"><span class="math inline">\(\P(\beta_j \in \text{CI}_{1-\alpha}(\beta_j)) \to 1 - \alpha\)</span></span>.</p>
</section>
<section id="significance-test-for-one-coefficient" class="level2">
<h2 class="anchored" data-anchor-id="significance-test-for-one-coefficient">Significance Test for One Coefficient</h2>
<p>. . .</p>
<p>We want to test <span style="background-color: yellow;"><span class="math inline">\(H_0: \beta_j = 0\)</span> against <span class="math inline">\(H_1: \beta_j \neq 0\)</span></span>.</p>
<p>Under <span class="math inline">\(H_0\)</span> we know that <span style="background-color: lightblue;"><span class="math inline">\(\frac{\hat{\beta}_j}{\hat{\sigma}_j} \xrightarrow{L} N(0, 1)\)</span></span></p>
<p>We deduce a critical region at asymptotic level <span class="math inline">\(\alpha\)</span>:</p>
<div class="square-def">
<p><span class="math display">\[\mathcal R_\alpha = \left\{\frac{|\hat{\beta}_j|}{\hat{\sigma}_j} &gt; q_{1-\alpha/2}\right\}\]</span></p>
</div>
<p>. . .</p>
<p>Indeed <span class="math inline">\(P_{H_0}(\mathcal R_\alpha) \to \alpha\)</span>.</p>
<p>This test is called the Wald test. (As any other test that relies on asymptotic normality)</p>
</section>
<section id="p-value" class="level2">
<h2 class="anchored" data-anchor-id="p-value">P-value</h2>
<p>. . .</p>
<p>Denoting <span class="math inline">\(\Phi\)</span> the cdf of the <span class="math inline">\(\mathcal N(0, 1)\)</span> distribution, the p-value of the test equals</p>
<div class="square-def">
<p><span class="math display">\[p\text{-value} = 2\left(1 - \Phi\left(\frac{|\hat{\beta}_j|}{\hat{\sigma}_j}\right)\right)\]</span></p>
</div>
</section>
<section id="example-in-r" class="level2">
<h2 class="anchored" data-anchor-id="example-in-r">Example in R</h2>
<p>. . .</p>
<div style="font-size: 50%;">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 12%">
<col style="width: 17%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Estimate</th>
<th>Std. Error</th>
<th>z value</th>
<th>Pr(&gt;</th>
<th>z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>-3.9564361</td>
<td>0.3155529</td>
<td>-12.538</td>
<td>&lt; 2e-16</td>
<td>***</td>
</tr>
<tr class="even">
<td>AGE</td>
<td>0.0640837</td>
<td>0.0123960</td>
<td>5.170</td>
<td>2.34e-07</td>
<td>***</td>
</tr>
<tr class="odd">
<td>I(AGE^2)</td>
<td>-0.0006758</td>
<td>0.0001260</td>
<td>-5.364</td>
<td>8.14e-08</td>
<td>***</td>
</tr>
<tr class="even">
<td>DBP</td>
<td>0.0121546</td>
<td>0.0033775</td>
<td>3.599</td>
<td>0.00032</td>
<td>***</td>
</tr>
<tr class="odd">
<td>SEXEFEMME</td>
<td>0.5155651</td>
<td>0.0776229</td>
<td>6.642</td>
<td>3.10e-11</td>
<td>***</td>
</tr>
<tr class="even">
<td>WALK1</td>
<td>-0.4042257</td>
<td>0.0913195</td>
<td>-4.426</td>
<td>9.58e-06</td>
<td>***</td>
</tr>
<tr class="odd">
<td>ACTIV1</td>
<td>-0.6573558</td>
<td>0.1150226</td>
<td>-5.715</td>
<td>1.10e-08</td>
<td>***</td>
</tr>
</tbody>
</table>
</div>
<p>. . .</p>
<p>Each columns corresponds resp. to</p>
<div class="nonincremental">
<ul>
<li><span class="math inline">\(\hat \beta_j\)</span></li>
<li><span class="math inline">\(\hat \sigma_j\)</span></li>
<li><span class="math inline">\(\hat \beta_j/\hat \sigma_j\)</span> (z-score).</li>
</ul>
</div>
</section>
<section id="estimation-of-an-odds-ratio" class="level2">
<h2 class="anchored" data-anchor-id="estimation-of-an-odds-ratio">Estimation of an Odds-Ratio</h2>
<p>. . .</p>
<p>We consider two individuals <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> who differ only by their regressor <span class="math inline">\(j\)</span>. Then,</p>
<div class="square-def">
<p><span class="math display">\[OR(x_1^{(j)}, x_2^{(j)}) = e^{\beta_j(x_1^{(j)} - x_2^{(j)})}\]</span></p>
</div>
<p>. . .</p>
<p>Do we have <span class="math inline">\(OR(x_1^{(j)}, x_2^{(j)})\approx 1\)</span>?</p>
<p>. . .</p>
<p>The estimation of <span class="math inline">\(OR(x_1^{(j)}, x_2^{(j)})\)</span> is simply</p>
<div class="square-def">
<p><span class="math display">\[\widehat{OR}(x_1^{(j)}, x_2^{(j)}) = e^{\hat{\beta}_j(x_1^{(j)} - x_2^{(j)})}\]</span></p>
</div>
</section>
<section id="important-example" class="level2">
<h2 class="anchored" data-anchor-id="important-example">Important Example</h2>
<p>. . .</p>
<p>If regressor <span class="math inline">\(j\)</span> is binary with <span class="math inline">\(x_1^{(j)} = 1\)</span> and <span class="math inline">\(x_2^{(j)} = 0\)</span>, then</p>
<div class="square-def">
<p><span class="math display">\[\widehat{OR}(x_1^{(j)}, x_2^{(j)}) = e^{\hat{\beta}_j}\]</span></p>
</div>
</section>
<section id="asymptotic-ci-for-an-odds-ratio" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-ci-for-an-odds-ratio">Asymptotic CI for an Odds-Ratio</h2>
<p>. . .</p>
<p>We have seen that an asymptotic CI at confidence level <span class="math inline">\(1 - \alpha\)</span> for <span class="math inline">\({\beta}_j\)</span> is</p>
<div class="square-def">
<p><span class="math display">\[\text{CI}_{1-\alpha}(\beta_j) = \left[\hat{\beta}_j - q_{1-\alpha/2}\hat{\sigma}_j \,;\, \hat{\beta}_j + q_{1-\alpha/2}\hat{\sigma}_j\right]= [l,r]\]</span></p>
</div>
<p>. . .</p>
<p>If <span class="math inline">\(x_1^{(j)} &gt; x_2^{(j)}\)</span>, an asymptotic CI for <span class="math inline">\(OR(x_1^{(j)}, x_2^{(j)})= e^{\beta_j(x^{(j)}_1 - x^{(j)}_2)}\)</span> at level <span class="math inline">\(1 - \alpha\)</span> is</p>
<div class="square-def">
<p><span class="math display">\[\text{CI}_{1-\alpha}(OR(x_1^{(j)}, x_2^{(j)})) = \left[e^{l(x_1^{(j)} - x_2^{(j)})}, e^{r(x_1^{(j)} - x_2^{(j)})}\right]\]</span></p>
</div>
</section>
<section id="significance-test-for-an-odds-ratio" class="level2">
<h2 class="anchored" data-anchor-id="significance-test-for-an-odds-ratio">Significance Test for an Odds-Ratio</h2>
<p>. . .</p>
<p>We generally want to compare <span class="math inline">\(OR(x_1^{(j)}, x_2^{(j)})\)</span> to <span class="math inline">\(1\)</span>.</p>
<div class="square-def">
<p><span class="math display">\[OR(x_1^{(j)}, x_2^{(j)}) = 1 \Leftrightarrow e^{\beta_j(x_1^{(j)} - x_2^{(j)})} = 1 \Leftrightarrow \beta_j = 0\]</span></p>
</div>
</section>
<section id="two-sided-test" class="level2">
<h2 class="anchored" data-anchor-id="two-sided-test">Two-Sided Test</h2>
<p>. . .</p>
<div class="square-def">
<p><span class="math inline">\(H_0: OR(x_1^{(j)}, x_2^{(j)}) = 1\)</span> VS <span class="math inline">\(H_1: OR(x_1^{(j)}, x_2^{(j)}) \neq 1\)</span></p>
</div>
<p>. . .</p>
<p>amounts to testing <span style="background-color: lightblue;"><span class="math inline">\(H_0: \beta_j = 0\)</span> against <span class="math inline">\(H_1: \beta_j \neq 0\)</span></span>. The Rejection region at level <span class="math inline">\(\alpha\)</span> is</p>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[\mathcal R_\alpha = \left\{\frac{|\hat{\beta}_j|}{\hat{\sigma}_j} &gt; q_{1-\alpha/2}\right\}\]</span></p>
</div>
</section>
<section id="one-sided-tests" class="level2">
<h2 class="anchored" data-anchor-id="one-sided-tests">One-Sided Tests</h2>
<p>. . .</p>
<p>Nevertheless, for ORs, we <span style="background-color: yellow;">often prefer one-sided tests</span>.</p>
<p>. . .</p>
<div class="square-def">
<p><span class="math inline">\(H_0: OR(x_1^{(j)}, x_2^{(j)}) = 1\)</span> VS <span class="math inline">\(H_1: OR(x_1^{(j)}, x_2^{(j)}) &gt; 1\)</span></p>
</div>
<p>. . .</p>
<p>If <span class="math inline">\(x_1^{(j)} &gt; x_2^{(j)}\)</span>. Since <span style="background-color: lightblue;"><span class="math inline">\(OR(x_1^{(j)}, x_2^{(j)}) \geq 1 \Leftrightarrow \beta_j \geq 0\)</span></span> rejection region at level <span class="math inline">\(\alpha\)</span> is</p>
<div class="square-def">
<p><span class="math display">\[\mathcal R_{\alpha}=\left\{\frac{\hat{\beta}_j}{\hat{\sigma}_j} &gt; q_{1-\alpha}\right\}\]</span></p>
</div>
</section>
</section>
<section id="deviance" class="level1">
<h1>Deviance</h1>
<section id="saturated-model" class="level2">
<h2 class="anchored" data-anchor-id="saturated-model">Saturated Model</h2>
<p>. . .</p>
<p>Suppose we have <span class="math inline">\(n\)</span> observations <span class="math inline">\((Y_1, \dots, Y_n)\)</span> and <span class="math inline">\((X_1, \dots, X_n)\)</span> (categorical)</p>
<p>. . .</p>
<p>Here <span class="math inline">\(X_k\)</span> can represent the vector <span class="math inline">\(X_k = (X^{(1)}_k, \dots, X^{(p)}_k)^T\)</span>.</p>
<p>. . .</p>
<p>Assume that indivudal are iid, with <span style="background-color: lightblue;"><span class="math inline">\(\mathbb P(Y=1|X_k=x) = p(x)\)</span></span>.</p>
<p>. . .</p>
<p>How do we estimate <span class="math inline">\(p(x)\)</span>?</p>
</section>
<section id="saturated-estimator" class="level2">
<h2 class="anchored" data-anchor-id="saturated-estimator">Saturated Estimator</h2>
<p>. . .</p>
<p>Suppose we have <span class="math inline">\(n\)</span> observations <span class="math inline">\((Y_1, \dots, Y_n)\)</span> and <span class="math inline">\((X_1, \dots, X_n)\)</span></p>
<p>. . .</p>
<p><span style="background-color: lightblue;"><span class="math inline">\(n(x) = |\{k:~ X_k = x\}|\)</span></span> (number of indiv. <span class="math inline">\(k\)</span> s.t. <span class="math inline">\(X_k=x\)</span>)</p>
<p>. . .</p>
<p><span style="background-color: lightblue;"><span class="math inline">\(n_1(x) = |\{k:~ X_k = x ~~\text{and}~~Y_k=1\}|\)</span></span></p>
<p>. . .</p>
<p>The saturated model is one that estimates <span class="math inline">\(p(x)\)</span>, for an observed <span class="math inline">\(x\)</span>, by</p>
<div class="square-def">
<p><span class="math display">\[\hat{p}_{\text{sat}}(x) = \frac{n_1(x)}{n(x)}\]</span></p>
</div>
</section>
<section id="remark" class="level2">
<h2 class="anchored" data-anchor-id="remark">Remark</h2>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[\hat{p}_{\text{sat}}(x) = \frac{n_1(x)}{n(x)}\]</span></p>
</div>
<p><span style="background-color: yellow;">If all observations are distinct</span>, i.e., each observed <span class="math inline">\(x\)</span> is only for a single individual, then for an observed <span class="math inline">\(x\)</span>:</p>
<p>. . .</p>
<p><span class="math inline">\(n(x) = 1\)</span>, <span class="math inline">\(n_1(x) \in \{0,1\}\)</span>, and <span class="math inline">\(\hat{p}_{\text{sat}}(x) = 0\)</span> or <span class="math inline">\(1\)</span>.</p>
</section>
<section id="remarks-1" class="level2">
<h2 class="anchored" data-anchor-id="remarks-1">Remarks</h2>
<p>. . .</p>
<p>The saturated model is the <span style="background-color: yellow;">simplest model</span> to imagine.</p>
<p>. . .</p>
<p>It fits the data perfectly.</p>
<p>. . .</p>
<p>However, it has <span style="background-color: orange;">no explanatory power</span> (effect of regressors on <span class="math inline">\(Y\)</span>?).</p>
<p>. . .</p>
<p>And it says nothing about <span class="math inline">\(p(x)\)</span> if <span class="math inline">\(x\)</span> is not observed.</p>
<p>. . .</p>
<p>It will serves as a <span style="background-color: yellow;">reference</span> for fit</p>
</section>
<section id="likelihood-of-saturated-estimator" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-of-saturated-estimator">Likelihood of Saturated Estimator</h2>
<p>. . .</p>
<p>For the saturated model with probabilities <span class="math inline">\(p(x)\)</span>, the Log-likelihood is:</p>
<div style="font-size: 80%;">
<div class="square-def">
<p><span class="math display">\[L(y_1, \ldots, y_n|x_1, \ldots, x_n) = \sum_{i=1}^n y_i \ln(p(x_i)) + (1 - y_i) \ln(1 - p(x_i))\]</span></p>
</div>
</div>
<p>. . .</p>
<p>The saturated model minimizes this likelihood, and we denote</p>
<div style="font-size: 80%;">
<div class="square-def">
<p><span class="math display">\[L_{\text{sat}} = \sum_{i=1}^n y_i \ln(\hat p_{\text{sat}}(x_i)) + (1 - y_i) \ln(1 - \hat p_{\text{sat}}(x_i))\]</span></p>
</div>
</div>
</section>
<section id="case-1-distinct-observations" class="level2">
<h2 class="anchored" data-anchor-id="case-1-distinct-observations">Case 1: Distinct Observations</h2>
<p>. . .</p>
<p>If all observations <span class="math inline">\(x_i\)</span> are distinct, we have <span class="math inline">\(\hat{p}_{\text{sat}}(x_i) = y_i\)</span> with <span class="math inline">\(y_i \in \{0, 1\}\)</span>. We thus have</p>
<div class="square-def">
<p><span class="math display">\[L_{\text{sat}} = 0\]</span></p>
</div>
<p>. . .</p>
<p>The saturated estimator has highest possible log-likelihood: it <span style="background-color: orange;">fits the data perfectly</span> (too well).</p>
</section>
<section id="case-2-non-distinct-observations" class="level2">
<h2 class="anchored" data-anchor-id="case-2-non-distinct-observations">Case 2: Non-Distinct Observations</h2>
<p>. . .</p>
<p>If the observations <span style="background-color: yellow;"><span class="math inline">\(x_i\)</span> are not distinct</span>, we obtain</p>
<div style="font-size: 80%;">
<div class="square-def">
<p><span class="math display">\[L_{\text{sat}} = \sum_x \left[n_1(x) \ln\left(\frac{n_1(x)}{n(x)}\right) + (n(x) - n_1(x)) \ln\left(1 - \frac{n_1(x)}{n(x)}\right)\right]\]</span></p>
</div>
</div>
<p>where the sum runs over the set of values <span class="math inline">\(x\)</span> taken by the <span class="math inline">\(x_i\)</span>.</p>
</section>
<section id="deviance-1" class="level2">
<h2 class="anchored" data-anchor-id="deviance-1">Deviance</h2>
<p>. . .</p>
<p>The deviance of a model measures how much this model <span style="background-color: yellow;">deviates from the saturated model</span> (the ideal model in terms of likelihood).</p>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[D = 2(L_{\text{sat}} - L_{\text{mod}})\]</span></p>
</div>
<p>where <span class="math inline">\(L_{\text{mod}}\)</span> denotes the log-likelihood for the model parameters.</p>
<p>. . .</p>
<p>We always have <span style="background-color: lightblue;"><span class="math inline">\(D \geq 0\)</span></span>.</p>
<p>. . .</p>
<p>If all observations are distinct, <span class="math inline">\(L_{\text{sat}} = 0\)</span> therefore <span style="background-color: lightblue;"><span class="math inline">\(D = -2L_{\text{mod}}\)</span></span></p>
</section>
<section id="role-of-deviance-and-computation-in-r" class="level2">
<h2 class="anchored" data-anchor-id="role-of-deviance-and-computation-in-r">Role of Deviance and Computation in R</h2>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[D = 2(L_{\text{sat}} - L_{\text{mod}})\]</span></p>
</div>
<p>. . .</p>
<p>Deviance plays the role of the SSR of a linear model: <span style="background-color: yellow;">the higher the deviance, the less well the model is fitted</span> to the data.</p>
<p>. . .</p>
<p><span style="background-color: yellow;">In R</span>, The returned deviance is <span style="background-color: yellow;"><span class="math inline">\(-2L_{\text{mod}}\)</span></span>: the term <span class="math inline">\(L_{\text{sat}}\)</span> is therefore omitted.</p>
</section>
</section>
<section id="testing" class="level1">
<h1>Testing</h1>
<section id="linear-constraint-test" class="level2">
<h2 class="anchored" data-anchor-id="linear-constraint-test">Linear Constraint Test</h2>
<p>. . .</p>
<p>As in linear regression, we would like to test <span style="background-color: lightblue;"><span class="math inline">\(H_0: R\beta = 0\)</span> VS <span class="math inline">\(H_1: R\beta \neq 0\)</span></span></p>
<p>where <span class="math inline">\(R\)</span> is a constraint matrix of size <span style="background-color: lightblue;"><span class="math inline">\((q, p)\)</span></span> of full rank.</p>
<p>. . .</p>
<p>For recall, depending on the choice of <span class="math inline">\(R\)</span> this allows:</p>
<ul>
<li><p>testing the minimum: is there at least one relevant regressor?</p></li>
<li><p>comparing nested models</p></li>
<li><p>examining the collective significance of a family of regressors</p></li>
</ul>
</section>
<section id="available-procedures" class="level2">
<h2 class="anchored" data-anchor-id="available-procedures">Available Procedures</h2>
<p>. . .</p>
<p>In GLM, several test procedures address the problem.</p>
<p><strong>The Wald test</strong>, based on the asymptotic normality of <span class="math inline">\(\hat{\beta}\)</span>, which generalizes the one seen for testing <span class="math inline">\(\beta_j = 0\)</span> against <span class="math inline">\(\beta_j \neq 0\)</span>.</p>
<p>. . .</p>
<p><span style="background-color: yellow;"><strong>The likelihood ratio test</strong></span>, called in this context the <span style="background-color: yellow;">deviance test</span>.</p>
<p>. . .</p>
<p><strong>The score test</strong>, based on the behavior of the gradient of the log-likelihood at the critical point.</p>
<p>. . .</p>
<p><strong>The most used</strong> is the <span style="background-color: yellow;">deviance test</span>.</p>
</section>
<section id="the-deviance-test-or-likelihood-ratio-test" class="level2">
<h2 class="anchored" data-anchor-id="the-deviance-test-or-likelihood-ratio-test">The Deviance Test (or Likelihood Ratio Test)</h2>
<p>. . .</p>
<p>To test <span style="background-color: lightblue;"><span class="math inline">\(H_0: R\beta = 0\)</span> against <span class="math inline">\(H_1: R\beta \neq 0\)</span></span>, the principle of the test is as follows:</p>
<p>. . .</p>
<p>We calculate the MLE in each model to obtain <span class="math inline">\(\hat{\beta}\)</span> in the complete model and <span style="background-color: yellow;"><span class="math inline">\(\hat{\beta}_{H_0}\)</span> in the constrained model</span>.</p>
<p>. . .</p>
<p><strong>Logic</strong>: If <span class="math inline">\(H_0\)</span> is true, the constrained model should be as “likely” as the complete model, so <span style="background-color: yellow;"><span class="math inline">\(L(\hat{\beta})\)</span> and <span class="math inline">\(L(\hat{\beta}_{H_0})\)</span> should be similar</span>.</p>
</section>
<section id="deviance-test-statistic" class="level2">
<h2 class="anchored" data-anchor-id="deviance-test-statistic">Deviance Test Statistic</h2>
<p>. . .</p>
<p>The test statistic is the <span style="background-color: yellow;">difference of deviances</span>:</p>
<div class="square-def">
<p><span class="math display">\[D_{H_0} - D_{H_1} = 2\left(L(\hat{\beta}) - L(\hat{\beta}_{H_0})\right)\]</span></p>
</div>
<p>. . .</p>
<p>Under <span class="math inline">\(H_0\)</span>, denoting <span class="math inline">\(q\)</span> the number of constraints, we have the convergence (admitted):</p>
<div class="square-def">
<p><span class="math display">\[D_{H_0} - D_{H_1} = 2\left(L(\hat{\beta}) - L(\hat{\beta}_{H_0})\right) \xrightarrow{L} \chi^2_q\]</span></p>
</div>
</section>
<section id="rejection-region-and-p-value" class="level2">
<h2 class="anchored" data-anchor-id="rejection-region-and-p-value">Rejection Region and P-value</h2>
<p>. . .</p>
<p>The asymp. rejection region at asymptotic level <span class="math inline">\(\alpha\)</span> is therefore</p>
<div class="square-def">
<p><span class="math display">\[\mathcal R_\alpha = \{D_{H_0} - D_{H_1} &gt; \chi^2_{q,1-\alpha}\}\]</span></p>
</div>
<p>where <span class="math inline">\(\chi^2_{q,1-\alpha}\)</span>: quantile <span class="math inline">\(1 - \alpha\)</span> of a <span class="math inline">\(\chi^2_q\)</span> distribution.</p>
<p>. . .</p>
<p>The p-value equals</p>
<div class="square-def">
<p><span class="math display">\[p\text{-value} = 1 - F(D_{H_0} - D_{H_1})\]</span></p>
</div>
<p>where <span class="math inline">\(F\)</span> is the cdf of a <span class="math inline">\(\chi^2_q\)</span> distribution.</p>
</section>
<section id="special-case-1-significance-test" class="level2">
<h2 class="anchored" data-anchor-id="special-case-1-significance-test">Special Case 1: Significance Test</h2>
<p>. . .</p>
<p>We want to test if a model (having a constant) is significant</p>
<p>. . .</p>
<p>We therefore test <span class="math inline">\(H_0\)</span>: all its <span style="background-color: yellow;">coefficients are zero except the constant</span>. This corresponds to the special case</p>
<div class="square-def">
<p><span class="math inline">\(R = [0 | I_{p-1}]\)</span>.</p>
</div>
<p>. . .</p>
<p>We <span style="background-color: yellow;">compare the deviance of the model to the null deviance <span class="math inline">\(D_0\)</span></span>, corresponding to a model that contains only the constant.</p>
</section>
<section id="test-statistic" class="level2">
<h2 class="anchored" data-anchor-id="test-statistic">Test Statistic</h2>
<p>. . .</p>
<p>The test statistic is <span class="math inline">\(D_0 - D\)</span>. Under <span class="math inline">\(H_0\)</span>, when <span class="math inline">\(n \to \infty\)</span>:</p>
<div class="square-def">
<p><span class="math display">\[D_0 - D \sim \chi^2_{p-1}\]</span></p>
</div>
<p>. . .</p>
<p>The model is therefore significant (relative to the null model) if the sample is in the critical region of asymptotic level <span class="math inline">\(\alpha\)</span>:</p>
<div class="square-def">
<p><span class="math display">\[\mathcal R_\alpha = \{D_0 - D &gt; \chi^2_{p-1,1-\alpha/2}\}\]</span></p>
</div>
</section>
<section id="special-case-2-nested-models" class="level2">
<h2 class="anchored" data-anchor-id="special-case-2-nested-models">Special Case 2: Nested Models</h2>
<p>. . .</p>
<p>Suppose that model <span style="background-color: yellow;"><span class="math inline">\(1\)</span> (with deviance <span class="math inline">\(D_1\)</span>) is a sub-model of model <span class="math inline">\(2\)</span></span> (with deviance <span class="math inline">\(D_2\)</span>)</p>
<p>. . .</p>
<p>Model <span class="math inline">\(1\)</span> is therefore obtained from model <span class="math inline">\(2\)</span>, with parameter <span class="math inline">\(\beta\)</span>, via a constraint of the type <span class="math inline">\(R\beta = 0\)</span> where <span style="background-color: lightblue;"><span class="math inline">\(R\)</span> is a <span class="math inline">\((q, p)\)</span> matrix</span>.</p>
<p>. . .</p>
<p>Under <span class="math inline">\(H_0: R\beta = 0\)</span>, we have asymptotically <span style="background-color: lightblue;"><span class="math inline">\(D_1 - D_2 \sim \chi^2_q\)</span></span></p>
<p>. . .</p>
<p>Hence the asymptotic test: <span style="background-color: lightblue;"></span>.</p>
<div class="square-def">
<p><span class="math inline">\(\mathcal R_\alpha = \{D_1 - D_2 &gt; \chi^2_{q,1 - \alpha}\}\)</span></p>
</div>
</section>
<section id="aic-and-bic-criteria" class="level2">
<h2 class="anchored" data-anchor-id="aic-and-bic-criteria">AIC and BIC Criteria</h2>
<p>. . .</p>
<p>The AIC and BIC criteria are defined similarly to linear regression, i.e.</p>
<div class="square-def">
<p><span class="math inline">\(\text{AIC} = -2L_{\text{mod}} + 2p\)</span></p>
</div>
<div class="square-def">
<p><span class="math inline">\(\text{BIC} = -2L_{\text{mod}} + \ln(n)p\)</span></p>
</div>
<p>where <span class="math inline">\(L_{\text{mod}}\)</span> is the log-likelihood of the estimated model.</p>
</section>
<section id="aic-and-bic-criteria-1" class="level2">
<h2 class="anchored" data-anchor-id="aic-and-bic-criteria-1">AIC and BIC Criteria</h2>
<p>. . .</p>
<p>If we ignore saturated likelihood and set <span class="math inline">\(L_{\text{sat}}=0\)</span>,</p>
<div class="square-def">
<p><span class="math inline">\(\text{AIC} = D + 2p\)</span></p>
</div>
<div class="square-def">
<p><span class="math inline">\(\text{BIC} = D + \ln(n)p\)</span></p>
</div>
<p><strong>In practice</strong>, we choose the model having the minimal AIC or BIC</p>
<p>. . .</p>
<p>As in linear regression, we can use automatic selection procedures (backward, forward, etc).</p>
</section>
<section id="example-obesity-study" class="level2">
<h2 class="anchored" data-anchor-id="example-obesity-study">Example: Obesity Study</h2>
<p>. . .</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model<span class="ot">=</span><span class="fu">glm</span>(Y<span class="sc">~</span>AGE<span class="sc">+</span>DBP<span class="sc">+</span>SEXE<span class="sc">+</span>ACTIV<span class="sc">+</span>WALK<span class="sc">+</span>MARITAL, <span class="at">family=</span>binomial)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>. . .</p>
<div style="font-size: 60%;">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Statistic</th>
<th>Value</th>
<th>Degrees of Freedom</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Null deviance</td>
<td><span class="math inline">\(4610.8\)</span></td>
<td>on <span class="math inline">\(5300\)</span></td>
</tr>
<tr class="even">
<td>Residual deviance</td>
<td><span class="math inline">\(4459.5\)</span></td>
<td>on <span class="math inline">\(5290\)</span></td>
</tr>
<tr class="odd">
<td>AIC</td>
<td><span class="math inline">\(4481.5\)</span></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>. . .</p>
<p>The model deviance is therefore <span class="math inline">\(D = 4459.5\)</span>.</p>
<p><strong>Significance Test</strong>: We compare <span class="math inline">\(D\)</span> to the null deviance <span class="math inline">\(D_0 = 4610.8\)</span>: <span style="background-color: lightblue;"><span class="math inline">\(D_0 - D = 151.3\)</span></span>. The p-value of the test equals <span style="background-color: lightblue;"><span class="math inline">\(1 - \chi^2_{10}(151.3) \approx 0\)</span></span> where <span class="math inline">\(\chi^2_{10}\)</span> is the cdf of a <span class="math inline">\(\chi^2_{10}\)</span>.</p>
<p>. . .</p>
<p>The model is significant.</p>
</section>
<section id="example-significance-test-of-marital" class="level2">
<h2 class="anchored" data-anchor-id="example-significance-test-of-marital">Example: Significance Test of MARITAL</h2>
<p>. . .</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>model<span class="ot">=</span><span class="fu">glm</span>(Y<span class="sc">~</span>AGE<span class="sc">+</span>DBP<span class="sc">+</span>SEXE<span class="sc">+</span>ACTIV<span class="sc">+</span>WALK, <span class="at">family=</span>binomial) <span class="co"># We want to test if `MARITAL` is significant</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>. . .</p>
<div style="font-size: 60%;">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Statistic</th>
<th>Value</th>
<th>Degrees of Freedom</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Null deviance</td>
<td><span class="math inline">\(4610.8\)</span></td>
<td>on <span class="math inline">\(5300\)</span></td>
</tr>
<tr class="even">
<td>Residual deviance</td>
<td><span class="math inline">\(4462.7\)</span></td>
<td>on <span class="math inline">\(5295\)</span></td>
</tr>
<tr class="odd">
<td>AIC</td>
<td><span class="math inline">\(4474.7\)</span></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>. . .</p>
<p>The deviance is now <span class="math inline">\(D_2 = 4462.7\)</span>. To compare with the previous model, we calculate: <span style="background-color: lightblue;"><span class="math inline">\(D_2 - D = 3.2\)</span></span>.</p>
<p>. . .</p>
<p>The p-value of the test equals <span style="background-color: lightblue;"><span class="math inline">\(1 - F_5(3.2) \approx 0.67\)</span></span>, where <span class="math inline">\(F_5\)</span>: cdf of a <span class="math inline">\(\chi^2_5\)</span>.</p>
<p>We therefore <span style="background-color: yellow;">accept <span class="math inline">\(H_0\)</span></span>: the coefficients related to <span style="background-color: yellow;">MARITAL are zero</span>. (Also confirmed with AIC)</p>
</section>
<section id="example-significance-test-of-age2" class="level2">
<h2 class="anchored" data-anchor-id="example-significance-test-of-age2">Example: Significance Test of AGE<span class="math inline">\(~^2\)</span></h2>
<p>. . .</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model<span class="ot">=</span><span class="fu">glm</span>(Y∼AGE<span class="sc">+</span><span class="fu">I</span>(AGE2 )<span class="sc">+</span>DBP<span class="sc">+</span>SEXE<span class="sc">+</span>WALK<span class="sc">+</span>ACTIV, <span class="at">family=</span>binomial) <span class="co"># We add AGE^2</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div style="font-size: 60%;">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Statistic</th>
<th>Value</th>
<th>Degrees of Freedom</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Null deviance</td>
<td><span class="math inline">\(4610.8\)</span></td>
<td>on <span class="math inline">\(5300\)</span></td>
</tr>
<tr class="even">
<td>Residual deviance</td>
<td><span class="math inline">\(4439.5\)</span></td>
<td>on <span class="math inline">\(5294\)</span></td>
</tr>
<tr class="odd">
<td>AIC</td>
<td><span class="math inline">\(4453.5\)</span></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>. . .</p>
<p>The deviance test with the previous model has p-value <span style="background-color: lightblue;"><span class="math inline">\(1 - F_1(4462.7 - 4439.5) = 1 - F_1(23.2) \approx 10^{-6}\)</span></span></p>
<p>. . .</p>
<p>This model is <span style="background-color: yellow;">therefore preferable</span>, (confirmed with AIC).</p>
<p>. . .</p>
<p>However, we <span style="background-color: orange;">cannot compare this model with the first</span> one by deviance test because they are <span style="background-color: orange;">not nested</span>.</p>
<p>. . .</p>
<p>We <span style="background-color: lightgreen;">can however compare their AIC</span>: this model is preferable.</p>
</section>
</section>
<section id="prediction" class="level1">
<h1>Prediction</h1>
<section id="setting-and-objective" class="level2">
<h2 class="anchored" data-anchor-id="setting-and-objective">Setting and Objective</h2>
<p>. . .</p>
<p>Suppose we are interested in a <span style="background-color: yellow;">new individual</span> for whom</p>
<ul>
<li>we know their characteristics <span class="math inline">\(x \in \mathbb{R}^p\)</span>,</li>
<li>we do not know their <span class="math inline">\(Y\)</span>.</li>
</ul>
<p>. . .</p>
<p>We want to <span style="background-color: yellow;">predict <span class="math inline">\(Y\)</span></span> for this <span style="background-color: yellow;">new individual</span>.</p>
</section>
<section id="recall-in-the-logit-model" class="level2">
<h2 class="anchored" data-anchor-id="recall-in-the-logit-model">Recall in the Logit Model</h2>
<p>. . .</p>
<p>If we have <span style="background-color: yellow;">fitted a logistic regression</span> model, we can estimate</p>
<div class="square-def">
<p><span class="math display">\[p_\beta(x) = P(Y = 1|X = x)\]</span></p>
</div>
<p>by</p>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[p_{\hat{\beta}}(x) = \text{logit}^{-1}(x^T \hat{\beta}) = \frac{e^{x^T \hat{\beta}}}{1 + e^{x^T \hat{\beta}}}\]</span></p>
</div>
</section>
<section id="outline-for-prediction" class="level2">
<h2 class="anchored" data-anchor-id="outline-for-prediction">Outline for Prediction</h2>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[p_\beta(x) = P(Y = 1|X = x)\]</span></p>
</div>
<p>We will see:</p>
<ol type="1">
<li><p>how to construct a <span style="background-color: yellow;">confidence interval around the estimation <span class="math inline">\(p_{\hat{\beta}}(x)\)</span></span>;</p></li>
<li><p>how to exploit this estimation to <span style="background-color: yellow;">classify the new individual</span> in category <span class="math inline">\(Y = 0\)</span> or <span class="math inline">\(Y = 1\)</span>.</p></li>
</ol>
</section>
<section id="ci-asymptotic-distribution-of-p_betax" class="level2">
<h2 class="anchored" data-anchor-id="ci-asymptotic-distribution-of-p_betax">CI: Asymptotic Distribution of <span class="math inline">\(p_\beta(x)\)</span></h2>
<p>. . .</p>
<p>We know that when <span class="math inline">\(n \to \infty\)</span>:</p>
<div class="square-def">
<p><span class="math display">\[\hat{\beta} \sim N(\beta, (X^T W_{\hat{\beta}} X)^{-1})\]</span></p>
</div>
<ul>
<li><span class="math inline">\(X=(X^{(1)}, \dots, X^{(p)})\)</span> is the <span class="math inline">\(n \times p\)</span> <span style="background-color: yellow;">design matrix</span></li>
<li><span class="math inline">\(W_{\hat{\beta}}\)</span> is the <span class="math inline">\(n \times n\)</span> <span style="background-color: yellow;">diagonal matrix</span> with coefs <span style="background-color: lightblue;"><span class="math inline">\(p_{\hat \beta}(x_i)(1-p_{\hat \beta}(x_i))\)</span></span></li>
</ul>
</section>
<section id="ci-for-linear-predictor" class="level2">
<h2 class="anchored" data-anchor-id="ci-for-linear-predictor">CI for Linear Predictor</h2>
<p>. . .</p>
<p>We deduce that when <span class="math inline">\(n \to +\infty\)</span>, <span style="background-color: lightblue;"><span class="math inline">\(x^T \hat{\beta} \sim N(x^T \beta, x^T (X^T W_{\hat{\beta}} X)^{-1} x)\)</span></span>, and the asymptotic CI of <span class="math inline">\(x^T\beta\)</span>:</p>
<p>. . .</p>
<div class="square-def">
<p><span class="math display">\[\text{CI}_{1-\alpha}(x^T \beta) = \left[x^T \hat{\beta} \pm q_{1-\alpha/2} \sqrt{x^T (X^T W_{\hat{\beta}} X)^{-1} x}\right]\]</span></p>
</div>
</section>
<section id="ci-for-probability-p_betax" class="level2">
<h2 class="anchored" data-anchor-id="ci-for-probability-p_betax">CI for Probability <span class="math inline">\(p_{\beta}(x)\)</span></h2>
<p>. . .</p>
<p>Since <span style="background-color: lightblue;"><span class="math inline">\(p_{\hat{\beta}}(x) = \text{logit}^{-1}(x^T \hat{\beta})\)</span></span>, we have therefore by application of the increasing function <span class="math inline">\(\text{logit}^{-1}\)</span>, the CI at asymptotic level <span class="math inline">\(1 - \alpha\)</span>:</p>
<p>. . .</p>
<div style="font-size: 80%;">
<div class="square-def">
<p><span class="math display">\[\text{CI}_{1-\alpha}(p_\beta(x)) = \left[\text{logit}^{-1}\left(x^T \hat{\beta} \pm q_{1-\alpha/2} \sqrt{x^T (X^T W_{\hat{\beta}} X)^{-1} x}\right)\right]\]</span></p>
</div>
</div>
</section>
<section id="classification" class="level2">
<h2 class="anchored" data-anchor-id="classification">Classification</h2>
<p>. . .</p>
<p>We have estimated <span class="math inline">\(p_\beta(x) = P(Y = 1|X = x)\)</span> by <span class="math inline">\(p_{\hat{\beta}}(x)\)</span>.</p>
<p>. . .</p>
<p>For a <span style="background-color: yellow;">threshold to choose <span class="math inline">\(s \in [0, 1]\)</span></span>, we use the rule:</p>
<div class="square-def">
<p><span class="math display">\[\begin{cases}
\text{if } p_{\hat{\beta}}(x) &gt; s, &amp; \hat{Y} = 1 \\
\text{if } p_{\hat{\beta}}(x) &lt; s, &amp; \hat{Y} = 0
\end{cases}\]</span></p>
</div>
<p>. . .</p>
<p>The “natural” choice of threshold is <span class="math inline">\(s = 0.5\)</span> but <span style="background-color: yellow;">this choice can be optimized</span>.</p>
</section>
<section id="evaluation-of-classification-quality" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-of-classification-quality">Evaluation of Classification Quality</h2>
<p>. . .</p>
<p>We proceed by <span style="background-color: yellow;">cross-validation</span>:</p>
<p>. . .</p>
<p>Using a <span style="background-color: yellow;">train sample</span>, predict <span class="math inline">\(Y\)</span> on a <span style="background-color: yellow;">test sample</span> and form the <span style="background-color: yellow;">confusion matrix</span>.</p>
<p>. . .</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(Y = 0\)</span></th>
<th><span class="math inline">\(Y = 1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat{Y} = 0\)</span></td>
<td>TN</td>
<td>FN</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y} = 1\)</span></td>
<td>FP</td>
<td>TP</td>
</tr>
</tbody>
</table>
<p><strong>Reading</strong>: T: true, F:False, N: Negative, P: Positive.</p>
<p>. . .</p>
<p><strong>FP</strong>: false positives: number of individuals who were classified <span style="background-color: yellow;">positive</span> who were actually <span style="background-color: orange;">negative</span></p>
</section>
<section id="evaluation-of-classification-quality-1" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-of-classification-quality-1">Evaluation of Classification Quality</h2>
<p>. . .</p>
<p>The ideal is to have a confusion matrix that is <span style="background-color: yellow;">as diagonal as possible</span>.</p>
<p>. . .</p>
<p>We generally seek to maximize the <span style="background-color: yellow;">following indicators</span>:</p>
<p>. . .</p>
<div class="columns">
<div class="column">
<p>The <strong>sensitivity</strong> (or recall, or true positive rate) estimates <span style="background-color: lightblue;"><span class="math inline">\(\P(\hat{Y} = 1|Y = 1)\)</span></span> by</p>
<div class="square-def">
<p><span class="math display">\[\frac{\text{TP}}{\text{TP} + \text{FN}}\]</span></p>
</div>
</div><div class="column">
<p>The <strong>specificity</strong> (or selectivity, or true negative rate) estimates <span style="background-color: lightblue;"><span class="math inline">\(\P(\hat{Y} = 0|Y = 0)\)</span></span></p>
<div class="square-def">
<p><span class="math display">\[\frac{\text{TN}}{\text{TN} + \text{FP}}\]</span></p>
</div>
</div>
</div>
<p>. . .</p>
</section>
<section id="other-indicators" class="level2">
<h2 class="anchored" data-anchor-id="other-indicators">Other Indicators</h2>
<p>. . .</p>
<p>The <strong>precision</strong> (or positive predictive value) estimates <span class="math inline">\(\P(Y = 1|\hat{Y} = 1)\)</span> by</p>
<div class="square-def">
<p><span class="math inline">\(\frac{\text{TP}}{\text{TP} + \text{FP}}\)</span></p>
</div>
<p>. . .</p>
<p>The <strong><span class="math inline">\(F\)</span>-score</strong> is the harmonic mean between sensitivity and precision:</p>
<div class="square-def">
<p><span class="math display">\[F_1 = 2 \frac{\text{precision} \times \text{sensitivity}}{\text{precision} + \text{sensitivity}}\]</span></p>
</div>
</section>
<section id="choice-of-threshold-s" class="level2">
<h2 class="anchored" data-anchor-id="choice-of-threshold-s">Choice of Threshold <span class="math inline">\(s\)</span></h2>
<p>. . .</p>
<p>For each threshold <span class="math inline">\(s\)</span>, from a test sample:</p>
<ul>
<li>we can form the confusion matrix</li>
<li>calculate scores (sensitivity, <span class="math inline">\(F\)</span>-score, etc.)</li>
</ul>
<p>. . .</p>
<p>We finally choose the <span style="background-color: yellow;">optimal threshold <span class="math inline">\(s\)</span></span>, according to the <span style="background-color: yellow;">chosen score</span>.</p>
</section>
<section id="choosing-the-score" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-score">Choosing the score</h2>
<ul>
<li>It depends on the context of the study</li>
<li>It can be much more serious to wrongly predict <span class="math inline">\(\hat{Y} = 0\)</span> than <span class="math inline">\(\hat{Y} = 1\)</span></li>
</ul>
<p>. . .</p>
<p><span class="math inline">\(\hat Y=1\)</span> (treatment) while the patient is <span style="background-color: yellow;">not ill</span> (<span class="math inline">\(Y = 0\)</span>)</p>
<p>. . .</p>
<p><span class="math inline">\(\hat Y=0\)</span> (no treatment) while the patient <span style="background-color: yellow;">has a serious illness</span> (<span class="math inline">\(Y=1\)</span>)</p>
</section>
<section id="roc-curve" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve">ROC Curve</h2>
<p>. . .</p>
<p>We can also plot the ROC curve (TP rate as a function of FP rate for <span class="math inline">\(s \in [0, 1]\)</span>):</p>
<div style="font-size: 80%;">
<div class="square-def">
<p><span class="math inline">\(ROC:~~\mathrm{sensitiv.} = \frac{TP}{TP+FN} = F\left(\frac{FP}{FP+TN}\right) = F(1-\mathrm{specific.})\)</span></p>
</div>
</div>
<p>. . .</p>
<p>The <span style="background-color: yellow;">AUC (area under the curve)</span> is a quality indicator of the model (<span class="math inline">\(0 \leq \text{AUC} \leq 1\)</span>).</p>
<p>. . .</p>
<p>Or equivalently, the <span style="background-color: yellow;">Gini index: <span class="math inline">\(2 \times \text{AUC} - 1\)</span></span>.</p>
<p>. . .</p>
<p><strong>Use</strong>: compare <span class="math inline">\(2\)</span> models by <span style="background-color: yellow;">plotting the 2 ROC curves</span>.</p>
</section>
<section id="roc-curve-illustration" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-illustration">ROC Curve Illustration</h2>
<p>. . .</p>
<div style="text-align: center;">
<p><img src="../images/roc.png" class="img-fluid"></p>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>