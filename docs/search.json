[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentation",
    "section": "",
    "text": "I am an assistant professor in statistics at ENSAI (Rennes) working on topics related to machine learning and high-dimensional statistics. My PhD centered on two areas in modern statistics: change-point detection and ranking problems.\ncontact: firstname.lastname@ensai.fr"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Presentation",
    "section": "Publications",
    "text": "Publications\n\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal rates for ranking a permuted isotonic matrix in polynomial time (2024) SODA\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal permutation estimation in crowd-sourcing problems (2023) Annals of Statistics. [arxiv], [presentation],[poster]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal multiple change-point detection for high-dimensional data (2023) EJS [arxiv]"
  },
  {
    "objectID": "index.html#vitae",
    "href": "index.html#vitae",
    "title": "Presentation",
    "section": "Vitae",
    "text": "Vitae\n\nResearch\n\nSep 2024 - Present: assistant professor in statistics at ENSAI\n2024: Postdoctoral researcher at ENS Lyon\nFeb 2020 - Apr 2022 and Oct 2022 - Dec 2023: PhD Student at Université de Montpellier and INRAE\n2019: Research Project at Ecole Normale Supérieure Paris Saclay with Vianney Perchet on Optimal Order Selection for an Online Reward Maximization problem\nApr 2019: Research Project at OVGU Magdeburg with Alexandra Carpentier on Signal Detection and Change-Point Detection\n2018: Research Internship at the University of Cambridge on Gaussian Free Field\n\nExperience\n\nApr 2022 - Oct 2022: Quantitative Intern at QRT\n\nEducation\n\n2016-2020: Student at Ecole Normale Supérieure de Lyon\n2013-2016: Preparatory School of Mathematics and Physics (CPGE MPSI/MP*) in Strasbourg\n\nTeaching\n\n2020-2022: Teaching assistant at Université de Montpellier."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#multinomials",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#multinomials",
    "title": "Multi-dimensional Tests",
    "section": "",
    "text": "Binomial Distribution\n\n\n\n\nDraw \\(n\\) balls, blue or red.\n\\(p_1, (1-p_1)\\): proportion of blues/red\n\\(X\\), \\(Y\\): counts of blues/red\n\\(X \\sim \\mathrm{Bin}(n,p_1)\\)\n\\(\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\\)\nif \\(k_1 +k_2 = n\\)\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Distribution\n\n\n\n\nDraw \\(n\\) balls, \\(m\\) potential colors.\n\\((p_1, \\dots, p_m)\\): proportions of each color: \\(\\sum_{i=1}^m p_i = 1\\)\n\\(X_1, \\dots, X_m\\): count of each color\n\\((X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\\)\n\\(\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\\)\nif \\(k_1 + \\dots + k_m = n\\)\n\n\n\n\n\n\n\n\\(\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m}\\) is a multinomial coefficient\nIn this course: \\(n \\gg m\\).\n\\(m \\gg n\\) corresponds to a high-dimensional setting."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#goodness-of-fit-test",
    "title": "Multi-dimensional Tests",
    "section": "Goodness of Fit Test",
    "text": "Goodness of Fit Test\n\nWe observe \\((X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q)\\) This corresponds to \\(n\\) counts: \\(X_1 + \\dots + X_m = n\\)\n\\(q = (q_1, \\dots, q_m)\\) corresponds to probabilities of getting color \\(1, \\dots, m\\)\nLet \\(p = (p_1, \\dots, p_m)\\) be a known vector s.t. \\(p_1 + \\dots + p_m = 1\\).\n\\(H_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\\)\n\n\n\n\n\n\n\n\n\n\nGoodness of Fit Test (Adéquation)\n\n\n\n\nChi-Squared Test Statistic: \\[\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; ,\\] where \\(n_i = np_i = \\mathbb E[X_i]\\) is the expected number of counts for color \\(i.\\)\nWhen \\(np_i = n_i\\) are large, under \\(H_0\\), we approximate the distribution of \\(\\psi(X)\\) as \\(\\psi(X) \\sim \\chi^2(m-1)\\)\nReject if \\(\\psi(X) &gt; t_{1- \\alpha}\\) (right-tail of \\(\\chi^2(m-1)\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#example-bag-of-sweets",
    "title": "Multi-dimensional Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\nWe observe a bag of sweets containing \\(n=100\\) sweets of \\(m=3\\) different colors: red, green, and yellow.\nManufacturer: \\(p_1= 40\\%\\) red, \\(p_2=35\\%\\) green, and \\(p_3=25\\%\\) yellow.\n\\(H_0: q=p\\) (manufacturer’s claim is correct)\n\\(H_1: q\\neq p\\) (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\nColor\nObserved Counts\n\n\n\n\nRed\n\\(X_1=50\\)\n\n\nGreen\n\\(X_2=30\\)\n\n\nYellow\n\\(X_3=20\\)\n\n\n\n\n\n\n\n\n\nExpected Counts\n\n\n\n\n\\(n_1=40\\)\n\n\n\\(n_2=35\\)\n\n\n\\(n_3=25\\)\n\n\n\n\n\n\n\n\\(\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\\)\n\\(\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878\\) (\\(p_{value} \\approx 0.222\\))\nConclusion: do not reject \\(H_0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#homogeneity-test",
    "title": "Multi-dimensional Tests",
    "section": "Homogeneity Test",
    "text": "Homogeneity Test\n\n\\(d\\) different bags, each containing \\(m\\) colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#example-soft-drink-preferences",
    "title": "Multi-dimensional Tests",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\)\n\n\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n\n\n\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#histograms",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#histograms",
    "title": "Multi-dimensional Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\n\n\nhistogram\n\n\n\n\nWe observe \\((X_1, \\dots, X_n) \\in \\mathbb R^n\\)\n\n\\(\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\\)\n\\(\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\\)\n\\(\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\\)\nwhere \\(I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\\)\n\n\n\n\n\n\n\n\n\nNormalization\n\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Number, Monte Carlo (Informal)\n\n\n\n\nAssume that \\((X_1, \\dots, X_n)\\) are iid of distrib \\(P\\), and that \\(a\\),\\(b\\), \\(k\\) are fixed\nThe histogram \\(\\mathrm{hist}(a,b,k)\\) converges to the histogram of the density \\(P\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#goodness-of-fit-to-a-given-distribution",
    "title": "Multi-dimensional Tests",
    "section": "Goodness of Fit to a given distribution",
    "text": "Goodness of Fit to a given distribution\n\nWe observe \\((X_1, \\dots, X_n) \\in \\mathbb R^n\\), iid with unknown distrib \\(P\\)\n\\(H_0\\): \\(P = P_0\\), where \\(P_0\\) is known\n\\(H_1\\): \\(P \\neq P_0\\)\nIdea: Under \\(H_0\\), the counts in the intervals \\([a_0, a_1), [a_1, a_2), \\dots, [a_{k-1}, a_k)\\) follow a multinomial distribution \\(\\mathrm{Mult}(n, (p_1, \\dots, p_{k}))\\) where \\(p_1 = P_0([a_0, a_1)), \\dots, p_{k} = P_0([a_{k-1}, a_k))\\)\n\n\n\n\n\n\n\nReduction to Chi-Squared Statistic Test\n\n\n\n\nCount the number of data \\((c_1, \\dots, c_k)\\) falling in \\(I_1, \\dots, I_k\\)\nCompare the counts the theoretical \\(nP_0(I_1) \\dots, nP_0(I_k)\\) with a chi-squared statistic: \\[ \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_1)}\\]\nDecide using an \\(\\alpha\\)-quantile of a \\(\\chi^2(k-1)\\) distribution"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Multi-dimensional Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\n\\(H_0\\): \\(X_i\\) iid \\(\\mathcal P(2)\\)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n\\(0\\)\n\\(1\\)\n\\(2\\) \n\\(\\geq 3\\)\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\nTo get \\(9.7\\), we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\(\\gtrsim \\frac{(16-4)^2}{4} = 36\\)\n(1-cdf(Chisq(3),36)) is very small: Reject"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html#comparison-with-qq-plots",
    "title": "Multi-dimensional Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\nWe observe \\((X_1, \\dots, X_n)\\) of unknown CDF \\(F\\)\n\\(H_0\\): \\(F = F_0\\) where \\(F_0\\) is known\n\\(H_1\\): \\(F \\neq F_0\\)\nWe write \\(X_{(1)} \\leq \\dots \\leq X_{(n)}\\) for the ordered data\nempirical \\(\\frac{k}{n}\\)-quantile: \\(X_{(k)}\\)\n\\(\\frac{k}{n}\\)-quantile: \\(x\\) such that \\(F(x) = \\frac{k}{n}\\)\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with \\(y=x\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Known variance",
    "text": "Testing Mean with Known variance\n\n\\(X = (X_1, \\dots, X_n)\\), iid with distribution \\(\\mathcal N(\\mu, \\sigma)\\).\n\n\n\n\n\n\n\nTest Problems\n\n\n\\[\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(One-Sided Right)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(One-Sided Left)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(Two-Sided)}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTest Statistic: \\[ \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\\]\n\\(\\psi(X) \\sim \\mathcal N(0,1)\\)\n\n\n\n\n\n\nTests\n\n\n\\[\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(One-Sided Right)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( One-Sided Left)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(Two-Sided)}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "Why 0.05 and 1.96 ?",
    "text": "Why 0.05 and 1.96 ?\n\n\n\n\n\nFisher’s Quote\n\n\nThe value for which \\(p=0.05\\), or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Unknown Variance",
    "text": "Testing Mean with Unknown Variance\n\nMultiple VS Multiple Test Problem: \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n\\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\) no longer test statistic.\nIdea: replace \\(\\sigma\\) by its estimator \\[ \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\\]\nThis gives \\[\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "Chi-Square and Student Distributions",
    "text": "Chi-Square and Student Distributions\n\n\n\n\n\n\nChi-Squared Distribution \\(\\chi^2(k)\\)\n\n\n\n\\(k\\): degree of freedom\nDistrib of \\(\\sum_{i=1}^k Z_i^2\\)\nwhere the \\(Z_i\\)’s are iid \\(\\mathcal N(0,1)\\).\n\\(\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\\)\n\\(\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\n\nStudent Distribution \\(\\mathcal T(k)\\)\n\n\n\n\\(k\\): degree of freedom\nDistrib of \\(\\tfrac{Z}{\\sqrt{U/k}}\\)\n\\(Z\\), \\(U\\) are independent and follow resp. \\(\\mathcal N(0,1)\\) and a \\(\\chi^2(k)\\)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\nAssume \\(X_i\\) are iid \\(\\mathcal N(\\mu_0, \\sigma)\\).\n\nThe Test Statistic \\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma}\\) pivotal (indep. of \\(\\sigma\\)).\nIt follows a Student Distribution \\(\\mathcal T(n-1)\\).\n\n\n\n\n\nProof idea: \\(\\overline X \\cdot (1, \\dots, 1)\\) and \\((X_1 - \\overline X, \\dots, X_n - \\overline X)\\) are orthogonal in \\(\\mathbb R^n\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "Student T-Test",
    "text": "Student T-Test\n\nMultiple VS Multiple Test Problem \\(X=(X_1, \\dots, X_n)\\): \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n(Student) T-Test Statistic: \\[\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\) are known, \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nTest Problem: \\(H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\\)\nIdea: Normalize \\(\\overline X - \\overline Y\\): \\[\n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\\]\nTwo-Sided Test for Testing Variance: \\[\nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\\]\n\\(t_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of a Gaussian Distrb"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nVariance Testing Problem: \\[\nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\\]\nF-Test Statistic of the Variances (ANOVA) \\[\n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher Distribution \\(\\mathcal F(k_1,k_2)\\)\n\n\n\n\\((k_1, k_2)\\): degrees of freedom\nDistribution of \\(\\frac{U_1/k_1}{U_2/k_2}\\)\nWhere \\(U_1\\), \\(U_2\\) are indep. and follow \\(\\chi^2(k_1)\\), \\(\\chi^2(k_2)\\). wiki\n\\(\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right)\\) when \\(k_1,k_2 \\to +\\infty\\)\nExample: \\(\\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1)\\) if \\(Z_i \\sim \\mathcal N(0,1)\\)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2}\\) is independent of \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1\\), \\(\\sigma_2\\). It is pivotal\nIt follow distribution \\(\\mathcal F(n_1-1, n_2-1)\\)\n\n\n\n\n\n\n\n\nTwo-Sided Test: \\[ \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown, but we know that \\(\\sigma_1=\\sigma_2\\)\nEquality of Mean Testing Problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}\\).\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\\(\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\\)\nNormalize \\(\\overline X - \\overline Y\\):\n\n\\[\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\\]\n\n\\(\\psi(X,Y)\\) is pivotal because \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nEquality of Mean Testing Problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}\\).\n\n\n\n\n\nStudent Welch Test Statistic\n\n\n\\[\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\\]\n\n\\(\\psi(X,Y)\\) is not pivotal\nGaussian approximation: \\(\\psi(X,Y) \\approx \\mathcal N(0,1)\\) when \\(n_1, n_2 \\to \\infty\\)\nBetter approximation: Student Welch"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \\]\nEquality when \\(X_i\\)’s are \\(\\mathcal N(\\mu, \\sigma)\\)\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\n\n\nExample: Binomials\n\n\n\nIf \\(p \\in (0,1)\\)\n\\(\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1)\\) when \\(n \\to \\infty\\)\n\\(n\\) should be \\(\\gg \\frac{1}{p}\\)\n\n\n\n\n\n\n\n\nGood Approx for (\\(n=100\\), \\(p=0.2\\))\n\n\n\n\nBad Approx for (\\(n=100\\), \\(p=0.01\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe \\(X \\sim Bin(n_1, p_1)\\) and \\(Y \\sim Bin(n_2, p_2)\\).\n\\(n_1\\), \\(n_2\\) are known but \\(p_1\\), \\(p_2\\) are unknown in \\((0,1)\\)\n\\(H_0\\): \\(p_1 = p_2\\) or \\(H_1\\): \\(p_1 \\neq p_2\\)\n\n\n\n\n\nTest Statistic\n\n\n\\[ \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\\]\n\n\\(\\hat p_1 = X/n_1\\), \\(\\hat p_2 = X/n_2\\)\n\\(\\hat p = \\frac{X+Y}{n_1+n_2}\\)\nIf \\(np_1, np_2 \\gg 1\\): \\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nWe reject if \\(|\\psi(X,Y)| \\geq t_{1-\\alpha/2}\\) (gaussian quantile)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion Asked: “Should we raise taxes on cigarettes to pay for a healthcare reform ?”\n\\(p_1\\), \\(p_2\\): proportion of non-smokers or smokers willing to raise taxes\n\\(H_0\\): \\(p_1=p_1\\) or \\(H_1\\): \\(p_1 &lt; p_2\\)\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\(\\hat p_1 \\approx 0.58\\), \\(\\hat p_2 \\approx 0.21\\).\n\\(\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\\)\n\\(\\mathbb P(\\psi(X,Y) &gt; 8.99)\\) = 1-cdf(Normal(0,1), 8.99)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/glossary.html",
    "href": "teaching/hypothesis_testing/slides/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "General\n\n\n\nEnglish\n\nincreasing function\nnondecreasing function\ninterval \\([a,b)\\)\n\n\n\n\nFrench\n\nfonction strictement croissante\nfonction croissante\nintervalle \\([a,b[\\)\n\n\n\n\n\n\nProbability/Statistics\n\n\n\nEnglish\n\nexpectation\nlikelihood\ndistribution tail\nCDF (Cumul. Distrib. Func.)\nPDF (Proba. Density Func.)\n\n\n\n\nFrench\n\nespérance\nvraissemblance\nqueue de distribution\nFonction de Répartition\nDensité d’une distribution\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nEnglish\n\none-sided test\ntwo-sided test\nchi-squared goodness of fit test\nchi-squared homogeneity test\n\n\n\n\nFrench\n\ntest unilatéral\ntest bilatéral\ntest d’adéquation du Khi-deux\ntest d’homogénéité du Khi-deux\n\n\n\n\n\n\nProgramming Languages\n\n\n\n\nJulia\nusing Distributions\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\ncdf(Binomial(n, p), x)\nquantile(Binomial(n,p), q)\ncdf(Normal(0,1), x)\nquantile(Normal(0,1), q)\n\ncdf(Chisq(n), x) # Chi-squared\ncdf(TDist(n), x) # Student\n\nn1,n2 = 5,10\ncdf(FDist(n1, n2), x) # Fisher\n\nlmbda = 2\ncdf(Gamma(n, lmbda), x) # Gamma\ncdf(Poisson(lmbda), x) # Poisson\n\n\n\nPython\nfrom scipy.stats import *\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\nbinom.cdf(x, n, p)\nbinom.ppf(q, n, p) # Quantile\nnorm.cdf(x)\nnorm.ppf(q)\n\nchi2.cdf(x, n)\nt.cdf(x, n) # Student\n\nn1,n2 = 5,10\nf.cdf(x, n1, n2) # Fisher\n\nlmbda = 2\ngamma.cdf(x,n,lmbda) # Gamma\npoisson.cdf(x, lmbda) # Poisson\n\n\n\nR\n\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\npbinom(x, n, p)\nqbinom(q, n, p)\npnorm(x)\nqnorm(q)\n\npchisq(x, n)\npt(x, n)\n\nn1,n2 = 5,10\npf(x, n1,n2)\n\nlmbda = 2\npgamma(x,n,lmbda) \nppois(x, lmbda)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "General Principles",
    "text": "General Principles\n\nFix an Objective. Test whether Bob has diabetes\nDesign an Experiment. Measure glucose level\nDefine Hypotheses\n\nNull Hypothesis: \\(H_0\\): A priori, Bob has no diabetes\nAlternative Hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a Decision Rule: Function of the glucose level\nCollect Data: Do the measure of glucose level\nApply the Decision Rule: Reject \\(H_0\\) or not\nDraw a Conclusion: Should Bob follow a treatment or make other tests ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "Good and Bad Decisions",
    "text": "Good and Bad Decisions\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-a-dice",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-a-dice",
    "title": "Hypothesis Testing",
    "section": "Fairness of a Dice",
    "text": "Fairness of a Dice\n\nObjective: Test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): The probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): The probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision Rule: The probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: The dice falls \\(10\\) times on \\(6\\)\nDecision: The probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "title": "Hypothesis Testing",
    "section": "Medical Test",
    "text": "Medical Test\n\nObjective: Test if a Fetus has Down syndrome\nExperiment: Measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): Nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): Nucal translucency is large\n\n\n\n\n\nDecision Rule: reject if \\(P_0(X \\geq x_{obs}) \\leq =0.05\\)\nCollect Data: \\(x_{obs}=3.02\\)\nMake a Decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: Estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: Decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]\nQuestion Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\nTest"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple\n\n\nTest Model\n\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: Decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\nMultiple VS Multiple"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nparametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nnon-parametric: Otherwise\n\n\nExample of Multiple VS Multiple Parametric Problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)\n\n\n\nSimple VS Multiple Non-Parametric Problems"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\n\nDecision Rule\n\n\n\nA Decision Rule or Test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\\(T(x) = 0\\) (or \\(1\\)) for all \\(x\\) is the trivial decision rule. Question: Decision Rule\n\n\n\n\n\n\n\n\n\n\n\n\nTest Statistic\n\n\n\na Test Statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter. Question: Test Statistic"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nFor a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\;\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "title": "Hypothesis Testing",
    "section": "Recall of Proba",
    "text": "Recall of Proba\nConsider a probability measure \\(P\\) on \\(\\mathbb R\\).\n\nCDF (Cumulative Distribution Function): \\[x \\to P(~(-\\infty,x]~) = \\mathbb P(X \\leq x) ~~~~\\text{(if $X \\sim P$ under $\\mathbb P$)}\\]\n\n\n\nContinous Measures\n\ndensity wrp to Lebesgue: \\(dP(x) = p(x)dx\\)\nPDF (Proba Density Function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha} = \\alpha)\\)\n\n\n\nDiscrete Measures\n\ndensity wrp to Counting Measure: \\(P(X=x) = p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nExponential \\(\\mathrm{Exp}(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-2",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-2",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{x^{k-1}e^{-x/\\theta}}{\\Gamma(k)\\theta^k}\\]\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\tfrac{1}{\\theta}\\)\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\nNumber of tics before time \\(1\\) of an atomic clock of rate \\(\\lambda\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "title": "Hypothesis Testing",
    "section": "Simple VS Simple Problem",
    "text": "Simple VS Simple Problem\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\nFor a given test \\(T\\) we define:\n\nlevel of \\(T\\) = (type-1 error): \\(\\alpha = P(T(X)=1)\\)\npower of \\(T\\) = 1 - (type-2 error): \\(\\beta = Q(T(X)=1) = 1-Q(T(X)=0)\\)\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta \\geq \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) ! But \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\n\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\) ?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\n\n\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta &gt; \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) !\nBut \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson’s Theorem",
    "text": "Neyman Pearson’s Theorem\n\n\n\n\n\n\n\n\n\n\nNeyman Pearson’s Theorem\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\n\n\n\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nWhere, if \\(X\\sim P\\), \\[ P(T^*(X)=1)=\\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\nEquivalent to Log-Likelihood Ratio Test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "title": "Hypothesis Testing",
    "section": "Example with Gaussians",
    "text": "Example with Gaussians\n\n\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\n\n\n\nLog-Likelihood Ratio Test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\nDistrib of \\(\\overline X\\) ?\n\\(t_{\\alpha}\\) ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "title": "Hypothesis Testing",
    "section": "Generalization: Exponential Families",
    "text": "Generalization: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood Ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\nLikelihood Ratio Test: (Q: Select Exp. Families) \\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "title": "Hypothesis Testing",
    "section": "Example: Radioactive Source",
    "text": "Example: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): Reject if \\(N \\geq 19\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests",
    "text": "Multiple-Multiple Tests\n\n\n\n\\(H_0 = \\{ \\mathcal P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\n\n\n\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-Sided Tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(One-Sided Right: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(One-Sided Left: uniilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-Sided Tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(Simple/Multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( Multiple/Multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-Sided test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (one-sided right test)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\nConsider \\(\\Theta_0\\) not singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\n\nPivotal Test Statistic\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\n\n\n\n\n\n\npvalue under \\(H_0\\)\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(\\mathrm{pvalue}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\n\n\n\n\n\nIn practice: Reject if \\(\\mathrm{pvalue}(X) &lt; \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\) under \\(H_0\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/hypothesis_testing.html",
    "href": "teaching/hypothesis_testing/hypothesis_testing.html",
    "title": "Introduction to Hypothesis Testing",
    "section": "",
    "text": "Testing Models\nCommon Tests on Gaussian Populations\nMulti-dimensional Tests\nWilcoxon-Dependency Test\n\nSee the Glossary for English/French translations",
    "crumbs": [
      "Presentation",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/hypothesis_testing.html#slides",
    "href": "teaching/hypothesis_testing/hypothesis_testing.html#slides",
    "title": "Introduction to Hypothesis Testing",
    "section": "",
    "text": "Testing Models\nCommon Tests on Gaussian Populations\nMulti-dimensional Tests\nWilcoxon-Dependency Test\n\nSee the Glossary for English/French translations",
    "crumbs": [
      "Presentation",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/wilcoxon.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/slides/wilcoxon.html#symetric-random-variable",
    "title": "Chi-Squared Tests",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)\n\n\n\n\n\n\n\n\nSymetrization\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t) &=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy = \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/wilcoxon.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/slides/wilcoxon.html#dependency-problem-for-paired-data",
    "title": "Chi-Squared Tests",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The distribution of \\((X_i - Y_i)\\) is symetric\n\\(H_1:\\) The distribution of \\((X_i - Y_i)\\) is not symetric\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\)).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/wilcoxon.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/slides/wilcoxon.html#wilcoxons-signed-rank-test",
    "title": "Chi-Squared Tests",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]\n\n\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\nUnder \\(H_0\\),\n\nSigns of the \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/wilcoxon.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/slides/wilcoxon.html#wilcoxons-signed-rank-test-1",
    "title": "Chi-Squared Tests",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\nProperties on the Signed Ranks\n\n\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\n\n\n\nThis approximation fits well the exact distribution. Monte-Carlo simulation:\n\n\n\n\n\n\n\n\nTo generate a \\(W_-\\) under \\(H_0\\) in Julia:\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "title": "Hypothesis Testing",
    "section": "Dice Biased Toward \\(6\\)",
    "text": "Dice Biased Toward \\(6\\)\n\nObjective: Test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): The probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): The probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision Rule: The probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: The dice falls \\(10\\) times on \\(6\\)\nDecision: The probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/chi-squared-stats.html",
    "href": "teaching/hypothesis_testing/slides/chi-squared-stats.html",
    "title": "Multi-dimensional Tests",
    "section": "",
    "text": "Binomial Distribution\n\n\n\n\nDraw \\(n\\) balls, blue or red.\n\\(p_1, (1-p_1)\\): proportion of blues/red\n\\(X\\), \\(Y\\): counts of blues/red\n\\(X \\sim \\mathrm{Bin}(n,p_1)\\)\n\\(\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\\)\nif \\(k_1 +k_2 = n\\)\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Distribution\n\n\n\n\nDraw \\(n\\) balls, \\(m\\) potential colors.\n\\((p_1, \\dots, p_m)\\): proportions of each color: \\(\\sum_{i=1}^m p_i = 1\\)\n\\(X_1, \\dots, X_m\\): count of each color\n\\((X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\\)\n\\(\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\\)\nif \\(k_1 + \\dots + k_m = n\\)\n\n\n\n\n\n\n\n\\(\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m}\\) is a multinomial coefficient\nIn this course: \\(n \\gg m\\).\n\\(m \\gg n\\) corresponds to a high-dimensional setting."
  }
]