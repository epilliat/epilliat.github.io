[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentation",
    "section": "",
    "text": "I am an assistant professor in statistics at ENSAI (Rennes) working on topics related to machine learning and high-dimensional statistics. My PhD centered on two areas in modern statistics: change-point detection and ranking problems.\nI am interested in GPU parallel computing. I’m currently developing a Julia library focused on optimizing fundamental functions like mapreduce (for operations such as sum) and accumulate (for operations like prefix sum). You can explore my experimental work at my GitHub repository Luma for more technical details.\ncontact: firstname.lastname@ensai.fr"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Presentation",
    "section": "Publications",
    "text": "Publications\n\nE. Pilliat, Recovering Labels from Crowdsourced Data: An Optimal and Polynomial-Time Method (2025) COLT [presentation]\nA. Garivier, E. Pilliat, On Sparsity and Sub-Gaussianity in the Johnson-Lindenstrauss Lemma (2024) [arxiv]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal rates for ranking a permuted isotonic matrix in polynomial time (2024) SODA [presentation]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal permutation estimation in crowd-sourcing problems (2023) Annals of Statistics. [arxiv], [presentation],[poster]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal multiple change-point detection for high-dimensional data (2023) EJS [arxiv]"
  },
  {
    "objectID": "index.html#vitae",
    "href": "index.html#vitae",
    "title": "Presentation",
    "section": "Vitae",
    "text": "Vitae\n\nResearch\n\nSep 2024 - Present: assistant professor in statistics at ENSAI\n2024: Postdoctoral researcher at ENS Lyon\nFeb 2020 - Apr 2022 and Oct 2022 - Dec 2023: PhD Student at Université de Montpellier and INRAE\n2019: Research Project at Ecole Normale Supérieure Paris Saclay with Vianney Perchet on Optimal Order Selection for an Online Reward Maximization problem\nApr 2019: Research Project at OVGU Magdeburg with Alexandra Carpentier on Signal Detection and Change-Point Detection\n2018: Research Internship at the University of Cambridge on Gaussian Free Field\n\nExperience\n\nApr 2022 - Oct 2022: Quantitative Intern at QRT\n\nEducation\n\n2016-2020: Student at Ecole Normale Supérieure de Lyon\n2013-2016: Preparatory School of Mathematics and Physics (CPGE MPSI/MP*) in Strasbourg\n\nTeaching\n\n2020-2022: Teaching assistant at Université de Montpellier."
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#introduction",
    "href": "research/presentations/crowdsourcing/presentation.html#introduction",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Introduction",
    "text": "Introduction\n\nThank you for having me here today.\nFor context, much of this work was conducted during my postdoctoral position at ENS Lyon, where was able to build upon my earlier PhD research in crowdsourcing.\nSo, let’s start with a scenario. Picture a group of workers, and each one is given binary classification tasks.\nFor each task, they need to make yes-or-no decisions.\nThere are many situations where this happens. Think of image classification, text moderation, or sentiment analysis\n\n\nWorkers are given binary tasks to which they have to give a response: YES or NO\nExamples\n\nImage Classification: “Does this image contain a dog?”\nText Moderation: “Is this comment toxic or offensive?”\nSentiment Analysis: “Does this review express positive sentiment?”"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#questions",
    "href": "research/presentations/crowdsourcing/presentation.html#questions",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Questions",
    "text": "Questions\n\nThree natural questions emerge\n\nwhat are the actual true labels?\nCan we compare workers and identify who performs better or worse?\nHow well do workers perform on a given task?\n\nI will touch on all these issues during my talk, but my primary focus will be on the central question of recovering the true labels.\n\n\n3 questions\n\nrecover the true label?\nrank the workers?\nestimate their abilities?\n\n\n\nMain Quetion: How can we accurately recover the labels?"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#this-talk",
    "href": "research/presentations/crowdsourcing/presentation.html#this-talk",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "This Talk",
    "text": "This Talk\n\nI will break down my presentation in 3 parts.\nI’ll start by introducing isotonic model.\nNext, I’ll review two existing methods.\nIn the final part, I’ll go over an optimal and computationnally feasible method for this problem.\n\n\nIntroducing the non-parametric isotonic model and main result\nPresenting two already existing algorithms\nIterative Spectral Voting (ISV) algo and key insights"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#illustration",
    "href": "research/presentations/crowdsourcing/presentation.html#illustration",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration",
    "text": "Illustration\n\nLet me start with an illustration.\nConsider binary tasks where labels are either \\(-1\\) or \\(+1\\). There is a vector \\(x^*\\) of true labels that we do not know.\nWe observe a matrix \\(Y\\) where each entry \\((i,k)\\) represents the response of worker \\(i\\) to task \\(k\\).\nWe can have missing values. When a worker doesn’t respond to a particular task, we simply put \\(0\\) in that matrix position.\nWe write \\(\\lambda\\) for the rate of partial observations .\n\n\nTwo binary tasks with labels in \\(\\{\\color{green}{-1},\\color{blue}{+1}\\}\\)\nUnknown vector \\(x^*\\) of labels with \\(d=9\\) tasks \\(x^*= (\\color{blue}{+1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1})\\)\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1}\n\\end{array}\\right)\n\\]\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{red}{0} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0}\n\\end{array}\\right)\n\\]\n\n\n\n\nRate of observations: \\(\\color{red}{\\lambda= 0.72}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#observation-model",
    "href": "research/presentations/crowdsourcing/presentation.html#observation-model",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nWe assume that the matrix of responses follows this model.\n\n\\(x^*\\) is the vector of unknown labels\n\\(E\\) is a matrix of independent sub-gaussian noise\n\\(M \\in [0,1]^{n \\times d}\\) is the unknown ability matrix. If \\(M_{ik}=1\\), it means that worker \\(i\\) is very good at question \\(k\\), and \\(M_{ik}=0\\) means that they answer randomly.\nThis odot is a Hadamard Product so you can think of \\(B\\) as a Bernoulli mask matrix.\n\n\n\nWe observe\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\nwhere \\(\\odot\\) is the Hadamard product.\n\n\\(x^*\\in \\{-1, 1\\}^d\\) is the vector of unknown labels\n\\(E\\) is a matrix of independent sub-gaussian noise\n\\(M \\in [0,1]^{n \\times d}\\) is the unknown ability matrix.\n\\(B\\) is a Bernoulli \\(\\mathcal B(\\lambda)\\) “mask” matrix, modelling partial observations"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#shape-constraints",
    "href": "research/presentations/crowdsourcing/presentation.html#shape-constraints",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nUntil now, I have’nt assumed anything on the ability matrix \\(M\\) except that it has coef. in \\([0,1]\\).\nFrom now on, we also assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) of its rows, that is,\nit has increasing columns, up to an unknown permutation \\(\\pi^*\\)\nIt means that, on average, a worker \\(i\\) is either uniformly better or uniformly worse than another worker \\(j\\)\n\n\n\\(M \\in [0,1]^{n \\times d}\\) represents the ability matrix of worker/task pairs\nWe assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) , that is\n\n\n\n\\[\nM_{\\pi^*}=\\left(\\begin{array}{ccccccccc}\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8} \\\\\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6}\n\\end{array}\\right)\n\\]\n\n\n\\[\nM_{\\phantom{{\\pi^*}}}=\\left(\\begin{array}{ccccccccc}\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6} \\\\\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "href": "research/presentations/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Square Norm Loss VS Hamming Loss",
    "text": "Square Norm Loss VS Hamming Loss\n\nA usual loss for an estimator \\(\\hat x\\) is the Hamming loss. It simply consists in summing up all the mistakes we’ve made.\nI do not consider Hamming Loss Here. Instead, we restrict \\(M\\) to incorrectly labeled task, and take the square frobenius norm.\n\nIf workers are bad (\\(M\\) close to \\(0\\)), square norm loss is small but Hamming Loss can be of order \\(d\\)\nHence, a very good feature of the square norm loss is that it evaluates the quality of the estimator \\(\\hat x\\) instead of the performance of the workers!\n\n\n\n\n\n\nHamming Loss: \\[ \\sum_{k=1}^d \\mathbf 1\\{\\hat x_k \\neq x_k^*\\}\\]\n\n\n\n\n\nSquare Norm Loss: \\[ \\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\]\n\n\n\n\nIf workers are bad (\\(M \\sim 0\\)), \\(M\\) is small but Hamming Loss is large (\\(\\sim d\\))\nSquare norm loss evaluates the quality of \\(\\hat x\\) rather than performances of workers"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#crowdsourcing-problems",
    "href": "research/presentations/crowdsourcing/presentation.html#crowdsourcing-problems",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\nIn addition to recovering labels, we can define the two other problems I mentioned earlier:\nranking the workers and estimating their abilities. Each objective corresponds to a similar square norm loss.\n\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\n\nObjectives\n\n\nRecovering the labels (\\(x^*\\))\n\n\nRanking the workers (\\(\\pi^*\\))\n\n\nEstimating their abilities (\\(M\\))\n\n\n\nLosses\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\)\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2\\)\n\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M - \\hat M\\|_F^2\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "href": "research/presentations/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\nObjectives\nRecovering the labels (\\(x^*\\))\n\nRanking the workers (\\(\\pi^*\\))\n\nEstimating their abilities (\\(M\\))\n\nRisks\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#minimax-risk-for-recovering-labels",
    "href": "research/presentations/crowdsourcing/presentation.html#minimax-risk-for-recovering-labels",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "MiniMax Risk for Recovering Labels",
    "text": "MiniMax Risk for Recovering Labels\n\nThis brings us to the minimax risk for recovering labels.\nIt is defined as the risk of the best estimator \\(\\hat x\\) in the worst case.\n\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\n\nminimize on all estimator \\(\\hat x\\)\nmaximize square norm loss on all \\(M\\), \\(\\pi^*\\), \\(x^*\\) such that \\(M_{\\pi^*}\\) is isotonic"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#main-results",
    "href": "research/presentations/crowdsourcing/presentation.html#main-results",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Results",
    "text": "Main Results\n\nThe main result I want to present, is that there is no computational-statistical gap here.\nThere is a polynomial-time methods which nearly achieves the minimax risk for recovering the labels.\nMoreover, the minimax risk is of order \\(d/\\lambda\\)\n\n\n\n\n\nTheorem\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), there exists a poly. time method \\(\\hat x\\) achieving \\(\\mathcal R^*_{\\mathrm{reco}}\\) up to polylog factors, i.e. for all \\(M\\in[0,1]^{n\\times d}\\), \\(\\pi^*\\) and \\(x^* \\in \\{-1,1\\}^d\\), \\[\n\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2] \\lesssim \\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\enspace .\n\\]\n\nMoreover, up to polylogs,\n\n\\[\n\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\asymp \\frac{d}{\\lambda} \\enspace .\n\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#majority-voting",
    "href": "research/presentations/crowdsourcing/presentation.html#majority-voting",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Majority Voting",
    "text": "Majority Voting\n\nMajority voting is the simplest method you can think of. Sum each column of \\(Y\\) and take the sign to estimate each label.\nUnfortunately, it does not achieve the optimal \\(d/\\lambda\\) rate. So we have this additional \\(\\sqrt{n}\\) compared to the minimax risk.\n\n\n\n\\(\\hat x^{(maj)}_k = \\mathrm{sign} \\left( \\sum_{i=1}^n Y_{ik} \\right)\\)\n\n\n\nMaximum risk of \\(\\hat x^{(maj)}\\):\n\n\\[\\max_{M,\\pi^*, x^*}\\mathbb E\\|M\\mathrm{diag}(\\hat x^{maj} \\neq x^*)\\|_F^2 \\asymp \\tfrac{d \\sqrt{n}}{\\lambda}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "href": "research/presentations/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "OBI-WAN (Shah et al., 2020)",
    "text": "OBI-WAN (Shah et al., 2020)\n\nIn the Obi-Wan Method from Shah and co. The main idea is to use that \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T\\) does not depend on \\(x^*\\).\nand to exploit this fact by computing the leading left eigen vector of \\(Y\\)\n\n\nIdea: Population term \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T= MM^T\\) is independent of \\(x^*\\)\n\n\nPCA Step:\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "href": "research/presentations/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Result on Obi-Wan Method",
    "text": "Result on Obi-Wan Method\n\nIt turns out that the Obi-Wan method is minimax optimal in a rank \\(1\\) model, it achieves the rate \\(d/\\lambda\\).\nHowever, it is not minimax optimal in the isotonic model. It only achieves a rate that is comparable to majority voting in the worst case.\n\n\n\n\n\n\nTheorem 1 (Shah et al., 2020)\n\n\nIn submodel where \\(\\mathrm{rk}(M)= 1\\), \\(\\hat x^{(\\mathrm{Obi-Wan})}\\) achieves minimax risk up to polylogs: \\[\\mathbb E\\|M\\mathrm{diag}(\\hat x^{\\text{Obi-Wan}} \\neq x^*)\\|_F^2 \\lesssim \\frac{d}{\\lambda} \\quad \\textbf{(minimax)}\\]\n\n\n\n\n\n\n\n\n\n\nTheorem 2 (Shah et al., 2020)\n\n\nIn the model where \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic up to polylogs: \\[\\mathbb E\\|M\\mathrm{diag}(\\hat x^{\\text{Obi-Wan}} \\neq x^*)\\|_F^2 \\lesssim \\frac{d\\sqrt{n}}{\\lambda} \\quad \\textbf{(not minimax)}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#pca-step",
    "href": "research/presentations/crowdsourcing/presentation.html#pca-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "PCA Step",
    "text": "PCA Step\n\nFirst, we compute the leading eigenvector as Shah et al. did.\nThe insight is that since \\(M\\) is isotonic, then we can prove that \\(M\\) is close to low rank, so it makes sense to look for leading eigen vectors\n\n\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]\n\n\n\nMain idea: Since \\(M\\) is isotonic, \\(M\\) is close to low rank\n\n\\[\\|MM^T\\|_{\\mathrm{op}} \\gtrsim \\|M\\|_F^2\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#voting-step",
    "href": "research/presentations/crowdsourcing/presentation.html#voting-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Voting Step",
    "text": "Voting Step\n\nWithout going into too much details, the idea is then to do define weights and perform weighted votes.\n\n\nDefine the weighted vote vector (on second sample)\n\n\\[\\hat w = \\hat v^T Y\\]\n\n\n\nDefine the estimated label as\n\n\\[\\hat x_k^{(1)} = \\mathrm{sign}(\\hat w_k)\\mathbf{1}\\bigg\\{|\\hat w_k| \\gg \\sqrt{\\sum_{i=1}^n \\hat v_i B_{ik}}\\bigg\\}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#iterate-these-two-steps",
    "href": "research/presentations/crowdsourcing/presentation.html#iterate-these-two-steps",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Iterate these two Steps",
    "text": "Iterate these two Steps\n\nWe then iterate these PCA and voting steps on labels that are left uncertain,\nuntil we get a final estimator.\n\n\nKeep certain labels: if \\(\\hat x^{(t)}_k\\neq 0\\), set \\(\\hat x^{(t+1)}_k= \\hat x^{(t)}_k\\).\n\n\nRestrict columns \\(k\\) of \\(Y\\) to uncertain labels \\(\\hat x^{(t)}_k=0\\)\n\n\nRepeat PCA Step + Voting Step on \\(Y \\mathrm{diag}(\\hat x^{(t)} \\neq 0)\\) a polylogarithmic number of times.\n\n\nOutput last estimator \\(\\hat x^{(T)}\\)\n\\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#ranking-and-estimating-abilities",
    "href": "research/presentations/crowdsourcing/presentation.html#ranking-and-estimating-abilities",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Ranking and Estimating Abilities",
    "text": "Ranking and Estimating Abilities\n\nThis approach allows us to recover some of the true labels.\nThen, we can do ranking and ability estimation!\nTo do that, we first restrict matrix of responses \\(Y\\) to the labels for which we have a good guess\nThen, we use previous work in the litterature to estimate \\(\\pi^*\\) and \\(M\\)\n\n\nUse ISV to recover some labels \\(\\hat x\\)\nRestrict to columns \\(k\\) such that \\(\\hat x_k \\in \\{-1, 1\\}\\)\nUse methods from Pilliat et al. (2024) to estimate \\(\\pi^*\\) and \\(M\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#conclusion",
    "href": "research/presentations/crowdsourcing/presentation.html#conclusion",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nLet me wrap up my talk in three key points.\n\nFirst, there are three interconnected problems in crowdsourcing: recovering labels, ranking workers and estimating their abilities.\nSecond, we use the isotonic model because it’s very flexible for tackling these problems.\nMost importantly, there is no computational statistical gap.\n\n\n\nThree connected problems: recovering labels, ranking workers and estimating their abilities\nNon parametric isotonic model very flexible in crowdsourcing problems\nNo computational-statistical gap recovering labels, ranking or estimating ability"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I currently am a teaching three topics in statistics at ENSAI Rennes: Hypothesis Testing, Linear and Generalized Linear Model and Times Series (TD 2024).\n\nIntroduction to Hypothesis Testing\nSee the Glossary for English/French translations. See also the note recalling some notion on Density and Likelihood.\n\nSlidesLecturesExercisesAnnalsPluto Notebooks\n\n\n\nTesting Models\nClassical Tests on Gaussian Populations\nGoodness of Fit and Homogeneity Tests\nTests for Dependency\n\n\n\n\nTesting Models\n\nClassical Tests on Gaussian Populations\nGoodness of Fit and Homogeneity Tests\nTests for Dependency\n\n\n\n\nTD1\nTD2\nTD3\nTD4\nTP\n\n\n\n\nExam 2025\nCorrection 2025\n\n\n\n\nProba Basics\nApproximation of Distributions\nIllustration of pvalue\n\n\n\n\n\n\nLinear and Generalized Linear Model\nThis course is offered to second-year ENSAI students in collaboration with Frédéric Lavancier. Most resources below are adapted from Frédéric Lavancier’s teaching materials\n\nSlidesExercises\n\n\n\nLinear Model\n\nIntroduction\nDefinition of the Linear Model\nInference\nValidation\nModel Selection\nANOVA/ANCOVA\n\n\n\nGeneralized Linear Model\n\nIntroduction to GLM\nLogistic Model\nModels for Categorical Data\nCounting Models\n\n\n\n\n\nTD1\nTD2-5\nTD6-8"
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html",
    "href": "teaching/linear_model/lectures/estimation.html",
    "title": "Linear Regression Model",
    "section": "",
    "text": "By definition of the orthogonal projection, the vector \\(Y - X\\hat{\\beta}\\) is orthogonal to any vector in the space \\([X]\\), in particular to each of the vectors \\(X^{(j)}\\). Thus, for \\(j = 1, \\ldots, p\\), the scalar product between \\(Y - X\\hat{\\beta}\\) and \\(X^{(j)}\\) is zero, i.e.\n\\[\\left(X^{(j)}\\right)^T(Y - X\\hat{\\beta}) = 0.\\]\nSince this relation is true for all \\(j = 1, \\ldots, p\\), we have\n\\[X^T(Y - X\\hat{\\beta}) = 0,\\]\n\n\n\n\n\n\nOther possible idea\n\n\n\nWe get the same equation if we instead use that the gradient of \\(\\|Y-X\\beta\\|_2^2\\) must be \\(0\\) at \\(\\beta=\\hat \\beta\\).\n\n\nwhere this time \\(0\\) denotes the zero vector of size \\(p\\). This implies \\(X^T X\\hat{\\beta} = X^T Y\\). Finally, the hypothesis \\(\\text{rg}(X) = p\\) guarantees that the matrix \\(X^T X\\) is invertible, and we obtain the result\n\n\\[\\hat{\\beta} = (X^T X)^{-1}X^T Y \\enspace .\\]\n\n\n\n\nWe compute:\n\\[\\mathbb{E}(\\hat{\\beta}) = \\mathbb{E}((X^T X)^{-1}X^T Y) = (X^T X)^{-1}X^T \\mathbb{E}(Y)\\]\nsince \\((X^T X)^{-1}X^T\\) is a deterministic matrix and expectation is linear. Furthermore\n\\[\\mathbb{E}(Y) = \\mathbb{E}(X\\beta + \\varepsilon) = X\\beta + \\mathbb{E}(\\varepsilon) = X\\beta\\]\nsince \\(\\mathbb{E}(\\varepsilon) = 0\\). We indeed obtain\n\n\\[\\mathbb{E}(\\hat{\\beta}) = (X^T X)^{-1}X^T X\\beta = \\beta \\enspace .\\]\n\n\n\n\nFor the variance, let us recall that for any deterministic matrix \\(A\\) and for any random vector \\(Z\\), \\(\\mathbb{V}(AZ) = A\\mathbb{V}(Z)A^T\\). We thus have\n\\[\\mathbb{V}(\\hat{\\beta}) = \\mathbb{V}((X^T X)^{-1}X^T Y) = (X^T X)^{-1}X^T \\mathbb{V}(Y)((X^T X)^{-1}X^T)^T.\\]\nNow \\(\\mathbb{V}(Y) = \\mathbb{V}(X\\beta + \\varepsilon) = \\mathbb{V}(\\varepsilon) = \\sigma^2 I_n\\) and \\(((X^T X)^{-1}X^T)^T = (X^T)^T ((X^T X)^{-1})^T = X(X^T X)^{-1}\\), since the matrix \\(X^T X\\) (and therefore also \\((X^T X)^{-1}\\)) is symmetric.\nWe therefore arrive at\n\\[\\mathbb{V}(\\hat{\\beta}) = (X^T X)^{-1}X^T \\sigma^2 I_n X(X^T X)^{-1} = \\sigma^2(X^T X)^{-1}X^T X(X^T X)^{-1} = \\sigma^2(X^T X)^{-1}.\\]\nHence the result:\n\n\\[\\mathbb{V}(\\hat{\\beta}) = \\sigma^2(X^T X)^{-1}.\\]\n\n\n\nFor the vector estimator \\(\\hat{\\beta}\\), the mean squared error is a scalar defined by: \\[\\text{MSE}(\\hat{\\beta}) = \\mathbb{E}[\\|\\hat{\\beta} - \\beta\\|^2]\\]\nThe MSE decomposes into variance plus squared bias. For a vector estimator: \\[\\text{MSE}(\\hat{\\beta}) = \\mathbb{E}[\\|\\hat{\\beta} - \\beta\\|^2] = \\mathbb{E}[\\text{Tr}((\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T)]= \\text{Tr}(\\mathbb{E}[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T])=\\sigma^2 \\text{Tr}((X^TX)^{-1})\\]\nHence, we have convergence as soon as \\(\\text{Tr}((X^TX)^{-1})\\) goes to \\(0\\) as \\(n \\to \\infty\\).\n\n\n\n\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\) \\(\\newcommand{\\E}{\\mathbb E}\\) \\(\\newcommand{\\P}{\\mathbb P}\\) \\(\\newcommand{\\Var}{\\mathbb V}\\)\n\n\n\n\n\n\nNote\n\n\n\nUnder the same assumptions, if \\(\\tilde \\beta\\) is another linear and unbiased estimator then \\[\\mathbb V(\\hat \\beta) \\preceq \\mathbb V(\\tilde \\beta),\\]\nwhere \\(A\\preceq B\\) means that \\(B-A\\) is a symmetric positive semidefinite matrix\n\n\nSince \\(\\tilde \\beta\\) is a linear estimator, then there exists a known matrix \\(M\\) such that \\(\\tilde \\beta = MY\\). Since it is also assumed to be unbiased, we have \\(\\mathbb{E}(\\tilde{\\beta}) = \\beta\\) for all \\(\\beta\\), that is \\(M\\mathbb{E}(Y) = \\beta\\), in other words \\(MX\\beta = \\beta\\) for all \\(\\beta\\). This last relation means that \\(MX = I_p\\), which is the only matrix satisfying this equality for all \\(\\beta\\).\nThere are several ways to prove the theorem. One of them consists in starting from the calculation of the covariance matrix between \\(\\tilde{\\beta}\\) and \\(\\hat{\\beta}\\):\n\\[\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta}) = \\mathbb{E}((\\tilde{\\beta} - \\mathbb{E}(\\tilde{\\beta}))(\\hat{\\beta} - \\mathbb{E}(\\hat{\\beta}))^T).\\]\nNow \\(\\tilde{\\beta} - \\mathbb{E}(\\tilde{\\beta}) = MY - \\beta = M(X\\beta + \\varepsilon) - \\beta = M\\varepsilon\\) since \\(MX\\beta = \\beta\\). Similarly \\(\\hat{\\beta} - \\mathbb{E}(\\hat{\\beta}) = (X^TX)^{-1}X^T\\varepsilon\\) (this is the case \\(M = (X^TX)^{-1}X^T\\)). We therefore obtain\n\\[\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta}) = \\mathbb{E}(M\\varepsilon\\varepsilon^T((X^TX)^{-1}X^T)^T) = M\\mathbb{E}(\\varepsilon\\varepsilon^T)X(X^TX)^{-1}.\\]\nNow since \\(\\varepsilon\\) is centered, \\(\\mathbb{E}(\\varepsilon\\varepsilon^T) = \\mathbb{V}(\\varepsilon) = \\sigma^2I_n\\), and since \\(MX = I_p\\) we conclude\n\\[\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta}) = \\sigma^2(X^TX)^{-1}.\\]\nThis means that \\(\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta}) = \\mathbb{V}(\\hat{\\beta})\\). We wish at this stage to use the Cauchy-Schwarz inequality, but since this last equality concerns matrices, we must first reduce to scalars. Let \\(c\\) be any vector of dimension \\(p\\). From the previous equality we deduce \\(c^T\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta})c = c^T\\mathbb{V}(\\hat{\\beta})c\\), that is \\(\\text{Cov}(c^T\\tilde{\\beta}, c^T\\hat{\\beta}) = \\mathbb{V}(c^T\\hat{\\beta})\\). This equality concerns scalars. We now apply the Cauchy-Schwarz inequality:\n\\[\\mathbb{V}(c^T\\hat{\\beta}) = \\text{Cov}(c^T\\tilde{\\beta}, c^T\\hat{\\beta}) \\leq \\sqrt{\\mathbb{V}(c^T\\tilde{\\beta})\\mathbb{V}(c^T\\hat{\\beta})}.\\]\nBy simplifying both sides by \\(\\sqrt{\\mathbb{V}(c^T\\hat{\\beta})}\\), we arrive at\n\\[\\sqrt{\\mathbb{V}(c^T\\hat{\\beta})} \\leq \\sqrt{\\mathbb{V}(c^T\\tilde{\\beta})},\\]\nand therefore \\(\\mathbb{V}(c^T\\hat{\\beta}) \\leq \\mathbb{V}(c^T\\tilde{\\beta})\\). Since this inequality is valid for any vector \\(c\\), this means that \\(\\mathbb{V}(\\tilde{\\beta}) - \\mathbb{V}(\\hat{\\beta})\\) is positive semi-definite, which is what we wanted to show."
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html#formula",
    "href": "teaching/linear_model/lectures/estimation.html#formula",
    "title": "Linear Regression Model",
    "section": "",
    "text": "By definition of the orthogonal projection, the vector \\(Y - X\\hat{\\beta}\\) is orthogonal to any vector in the space \\([X]\\), in particular to each of the vectors \\(X^{(j)}\\). Thus, for \\(j = 1, \\ldots, p\\), the scalar product between \\(Y - X\\hat{\\beta}\\) and \\(X^{(j)}\\) is zero, i.e.\n\\[\\left(X^{(j)}\\right)^T(Y - X\\hat{\\beta}) = 0.\\]\nSince this relation is true for all \\(j = 1, \\ldots, p\\), we have\n\\[X^T(Y - X\\hat{\\beta}) = 0,\\]\n\n\n\n\n\n\nOther possible idea\n\n\n\nWe get the same equation if we instead use that the gradient of \\(\\|Y-X\\beta\\|_2^2\\) must be \\(0\\) at \\(\\beta=\\hat \\beta\\).\n\n\nwhere this time \\(0\\) denotes the zero vector of size \\(p\\). This implies \\(X^T X\\hat{\\beta} = X^T Y\\). Finally, the hypothesis \\(\\text{rg}(X) = p\\) guarantees that the matrix \\(X^T X\\) is invertible, and we obtain the result\n\n\\[\\hat{\\beta} = (X^T X)^{-1}X^T Y \\enspace .\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html#expectation",
    "href": "teaching/linear_model/lectures/estimation.html#expectation",
    "title": "Linear Regression Model",
    "section": "",
    "text": "We compute:\n\\[\\mathbb{E}(\\hat{\\beta}) = \\mathbb{E}((X^T X)^{-1}X^T Y) = (X^T X)^{-1}X^T \\mathbb{E}(Y)\\]\nsince \\((X^T X)^{-1}X^T\\) is a deterministic matrix and expectation is linear. Furthermore\n\\[\\mathbb{E}(Y) = \\mathbb{E}(X\\beta + \\varepsilon) = X\\beta + \\mathbb{E}(\\varepsilon) = X\\beta\\]\nsince \\(\\mathbb{E}(\\varepsilon) = 0\\). We indeed obtain\n\n\\[\\mathbb{E}(\\hat{\\beta}) = (X^T X)^{-1}X^T X\\beta = \\beta \\enspace .\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html#variance",
    "href": "teaching/linear_model/lectures/estimation.html#variance",
    "title": "Linear Regression Model",
    "section": "",
    "text": "For the variance, let us recall that for any deterministic matrix \\(A\\) and for any random vector \\(Z\\), \\(\\mathbb{V}(AZ) = A\\mathbb{V}(Z)A^T\\). We thus have\n\\[\\mathbb{V}(\\hat{\\beta}) = \\mathbb{V}((X^T X)^{-1}X^T Y) = (X^T X)^{-1}X^T \\mathbb{V}(Y)((X^T X)^{-1}X^T)^T.\\]\nNow \\(\\mathbb{V}(Y) = \\mathbb{V}(X\\beta + \\varepsilon) = \\mathbb{V}(\\varepsilon) = \\sigma^2 I_n\\) and \\(((X^T X)^{-1}X^T)^T = (X^T)^T ((X^T X)^{-1})^T = X(X^T X)^{-1}\\), since the matrix \\(X^T X\\) (and therefore also \\((X^T X)^{-1}\\)) is symmetric.\nWe therefore arrive at\n\\[\\mathbb{V}(\\hat{\\beta}) = (X^T X)^{-1}X^T \\sigma^2 I_n X(X^T X)^{-1} = \\sigma^2(X^T X)^{-1}X^T X(X^T X)^{-1} = \\sigma^2(X^T X)^{-1}.\\]\nHence the result:\n\n\\[\\mathbb{V}(\\hat{\\beta}) = \\sigma^2(X^T X)^{-1}.\\]\n\n\n\nFor the vector estimator \\(\\hat{\\beta}\\), the mean squared error is a scalar defined by: \\[\\text{MSE}(\\hat{\\beta}) = \\mathbb{E}[\\|\\hat{\\beta} - \\beta\\|^2]\\]\nThe MSE decomposes into variance plus squared bias. For a vector estimator: \\[\\text{MSE}(\\hat{\\beta}) = \\mathbb{E}[\\|\\hat{\\beta} - \\beta\\|^2] = \\mathbb{E}[\\text{Tr}((\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T)]= \\text{Tr}(\\mathbb{E}[(\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T])=\\sigma^2 \\text{Tr}((X^TX)^{-1})\\]\nHence, we have convergence as soon as \\(\\text{Tr}((X^TX)^{-1})\\) goes to \\(0\\) as \\(n \\to \\infty\\)."
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html#gauss-markov",
    "href": "teaching/linear_model/lectures/estimation.html#gauss-markov",
    "title": "Linear Regression Model",
    "section": "",
    "text": "\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\) \\(\\newcommand{\\E}{\\mathbb E}\\) \\(\\newcommand{\\P}{\\mathbb P}\\) \\(\\newcommand{\\Var}{\\mathbb V}\\)\n\n\n\n\n\n\nNote\n\n\n\nUnder the same assumptions, if \\(\\tilde \\beta\\) is another linear and unbiased estimator then \\[\\mathbb V(\\hat \\beta) \\preceq \\mathbb V(\\tilde \\beta),\\]\nwhere \\(A\\preceq B\\) means that \\(B-A\\) is a symmetric positive semidefinite matrix\n\n\nSince \\(\\tilde \\beta\\) is a linear estimator, then there exists a known matrix \\(M\\) such that \\(\\tilde \\beta = MY\\). Since it is also assumed to be unbiased, we have \\(\\mathbb{E}(\\tilde{\\beta}) = \\beta\\) for all \\(\\beta\\), that is \\(M\\mathbb{E}(Y) = \\beta\\), in other words \\(MX\\beta = \\beta\\) for all \\(\\beta\\). This last relation means that \\(MX = I_p\\), which is the only matrix satisfying this equality for all \\(\\beta\\).\nThere are several ways to prove the theorem. One of them consists in starting from the calculation of the covariance matrix between \\(\\tilde{\\beta}\\) and \\(\\hat{\\beta}\\):\n\\[\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta}) = \\mathbb{E}((\\tilde{\\beta} - \\mathbb{E}(\\tilde{\\beta}))(\\hat{\\beta} - \\mathbb{E}(\\hat{\\beta}))^T).\\]\nNow \\(\\tilde{\\beta} - \\mathbb{E}(\\tilde{\\beta}) = MY - \\beta = M(X\\beta + \\varepsilon) - \\beta = M\\varepsilon\\) since \\(MX\\beta = \\beta\\). Similarly \\(\\hat{\\beta} - \\mathbb{E}(\\hat{\\beta}) = (X^TX)^{-1}X^T\\varepsilon\\) (this is the case \\(M = (X^TX)^{-1}X^T\\)). We therefore obtain\n\\[\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta}) = \\mathbb{E}(M\\varepsilon\\varepsilon^T((X^TX)^{-1}X^T)^T) = M\\mathbb{E}(\\varepsilon\\varepsilon^T)X(X^TX)^{-1}.\\]\nNow since \\(\\varepsilon\\) is centered, \\(\\mathbb{E}(\\varepsilon\\varepsilon^T) = \\mathbb{V}(\\varepsilon) = \\sigma^2I_n\\), and since \\(MX = I_p\\) we conclude\n\\[\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta}) = \\sigma^2(X^TX)^{-1}.\\]\nThis means that \\(\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta}) = \\mathbb{V}(\\hat{\\beta})\\). We wish at this stage to use the Cauchy-Schwarz inequality, but since this last equality concerns matrices, we must first reduce to scalars. Let \\(c\\) be any vector of dimension \\(p\\). From the previous equality we deduce \\(c^T\\text{Cov}(\\tilde{\\beta}, \\hat{\\beta})c = c^T\\mathbb{V}(\\hat{\\beta})c\\), that is \\(\\text{Cov}(c^T\\tilde{\\beta}, c^T\\hat{\\beta}) = \\mathbb{V}(c^T\\hat{\\beta})\\). This equality concerns scalars. We now apply the Cauchy-Schwarz inequality:\n\\[\\mathbb{V}(c^T\\hat{\\beta}) = \\text{Cov}(c^T\\tilde{\\beta}, c^T\\hat{\\beta}) \\leq \\sqrt{\\mathbb{V}(c^T\\tilde{\\beta})\\mathbb{V}(c^T\\hat{\\beta})}.\\]\nBy simplifying both sides by \\(\\sqrt{\\mathbb{V}(c^T\\hat{\\beta})}\\), we arrive at\n\\[\\sqrt{\\mathbb{V}(c^T\\hat{\\beta})} \\leq \\sqrt{\\mathbb{V}(c^T\\tilde{\\beta})},\\]\nand therefore \\(\\mathbb{V}(c^T\\hat{\\beta}) \\leq \\mathbb{V}(c^T\\tilde{\\beta})\\). Since this inequality is valid for any vector \\(c\\), this means that \\(\\mathbb{V}(\\tilde{\\beta}) - \\mathbb{V}(\\hat{\\beta})\\) is positive semi-definite, which is what we wanted to show."
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html#expectation-unbiasedness",
    "href": "teaching/linear_model/lectures/estimation.html#expectation-unbiasedness",
    "title": "Linear Regression Model",
    "section": "Expectation (unbiasedness)",
    "text": "Expectation (unbiasedness)\nFirst, let’s verify that \\(\\hat{\\sigma}^2\\) is unbiased:\n\\[\\hat{\\varepsilon} = P_{[X]^\\perp}Y = P_{[X]^\\perp}(X\\beta + \\varepsilon) = P_{[X]^\\perp}\\varepsilon\\]\nsince \\(P_{[X]^\\perp}X = 0\\).\nTherefore: \\[\\hat \\sigma^2=\\|\\hat{\\varepsilon}\\|^2 = \\varepsilon^T P_{[X]^\\perp}\\varepsilon\\]\nSince \\(P_{[X]^\\perp}\\) is idempotent and symmetric with \\(\\text{tr}(P_{[X]^\\perp}) = n - p\\):\n\\[\\mathbb{E}[\\|\\hat{\\varepsilon}\\|^2] = \\mathbb{E}[\\varepsilon^T P_{[X]^\\perp}\\varepsilon] = \\sigma^2 \\text{tr}(P_{[X]^\\perp}) = \\sigma^2(n-p)\\]\nThus:\n\n\\[\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{1}{n-p}\\mathbb{E}[\\|\\hat{\\varepsilon}\\|^2] = \\sigma^2\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html#var_sigma",
    "href": "teaching/linear_model/lectures/estimation.html#var_sigma",
    "title": "Linear Regression Model",
    "section": "Variance",
    "text": "Variance\nSetup: Consider the linear regression model \\(Y = X\\beta + \\varepsilon\\) where:\n\n\\(\\hat{\\varepsilon} = P_{[X]^\\perp}\\varepsilon\\) with \\(P_{[X]^\\perp} = I - X(X^TX)^{-1}X^T\\)\n\\(P_{[X]^\\perp}\\) is symmetric and idempotent with \\(\\text{tr}(P_{[X]^\\perp}) = n - p\\)\n\nThe estimator is: \\(\\hat{\\sigma}^2 = \\frac{1}{n-p}\\|\\hat{\\varepsilon}\\|^2 = \\frac{1}{n-p}\\varepsilon^T P_{[X]^\\perp}\\varepsilon\\)\n\nVariance in the Gaussian case (simpler proof)\nLet us first assume that \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\)\nKey Observation: Since \\(P_{[X]^\\perp}\\) is idempotent with rank \\(n-p\\), its eigenvalues are \\((n-p)\\) ones and \\(p\\) zeros. By spectral decomposition: \\[P_{[X]^\\perp} = Q\\Lambda Q^T\\] where \\(Q\\) is orthogonal and \\(\\Lambda = \\text{diag}(1, \\ldots, 1, 0, \\ldots, 0)\\).\nTransform to Standard Form: Let \\(Z = Q^T\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\). Then: \\[\\varepsilon^T P_{[X]^\\perp}\\varepsilon = Z^T\\Lambda Z = \\sum_{i=1}^{n-p} Z_i^2\\]\nChi-Square Distribution: Since \\(Z_i \\stackrel{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)\\): \\[\\frac{(n-p)\\hat{\\sigma}^2}{\\sigma^2} = \\sum_{i=1}^{n-p} \\left(\\frac{Z_i}{\\sigma}\\right)^2 \\sim \\chi^2_{n-p}\\]\nMoments from Chi-Square: Using properties of \\(\\chi^2_{n-p}\\): - \\(\\mathbb{E}[\\hat{\\sigma}^2] = \\sigma^2\\) (unbiased) - \\(\\mathbb{V}(\\hat{\\sigma}^2) = \\frac{2\\sigma^4}{n-p}\\)\nL² Consistency: Since \\(\\hat{\\sigma}^2\\) is unbiased:\n\n\\[\\mathbb{E}[(\\hat{\\sigma}^2 - \\sigma^2)^2] = \\mathbb{V}(\\hat{\\sigma}^2) = \\frac{2\\sigma^4}{n-p} \\to 0 \\text{ as } n \\to \\infty\\]\n\nTherefore, \\(\\hat{\\sigma}^2 \\xrightarrow{L^2} \\sigma^2\\) as \\(n \\to \\infty\\).\nBonus: We also get the exact distribution: \\(\\frac{(n-p)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{n-p}\\) for all \\(n\\).\n\n\nBounded fourth moment case (more technical)\nNow we do not assume that \\(\\epsilon\\) is a gaussian vector. We assume the \\(\\varepsilon_i\\) are iid, such that\n\n\\(\\mathbb E[\\varepsilon_i]=0\\)\n\\(\\mathbb V[\\varepsilon_i] = \\sigma^2\\)\n\\(\\mathbb E[\\varepsilon_i^4]= \\mu_4 &lt; \\infty\\) \\[\\mathbb{V}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^4] - (\\mathbb{E}[\\hat{\\sigma}^2])^2 = \\mathbb{E}[\\hat{\\sigma}^4] - \\sigma^4\\]\n\nWe have \\[\\mathbb{E}[\\hat \\sigma^4] = \\mathbb{E}[(\\varepsilon^  T P_{[X]^\\perp}\\varepsilon)^2]\\]\nHence, the computation of the variance of a quadratic form gives\n\\[\\mathbb V(\\hat \\sigma^2) = \\frac{1}{(n-p)^2}\\left(2\\sigma^4\\text{tr}(P_{[X]^\\perp}^2) + (\\mu_4 - 3\\sigma^4)\\sum_{i}(P_{[X]^\\perp})_{ii}^2 \\right) \\enspace .\\]\nThe fact that \\(P_{[X]^\\perp}^2=P_{[X]^\\perp}\\) and \\(\\text{tr}(P_{[X]^\\perp}) = n-p\\) gives We have\n\n\\[\\mathbb V(\\hat \\sigma^2) = \\frac{1}{(n-p)^2}\\left(2\\sigma^4(n-p) + (\\mu_4 - 3\\sigma^4)\\sum_{i}(P_{[X]^\\perp})_{ii}^2\\right) \\enspace .\\]\n\nSince \\((P_{[X]^\\perp})_{ii} = e_i^TP_{[X]^\\perp}e_i \\in [0,1]\\) (the eigen values of an orthogonal projector are in \\(\\{0,1\\}\\)), we have \\((P_{[X]^\\perp})_{ii}^2 \\leq (P_{[X]^\\perp})_{ii}\\).\nTherefore:\n\n\\[\\mathbb V(\\hat \\sigma^2) \\leq \\frac{1}{(n-p)}\\left(2\\sigma^4 + (\\mu_4 - 3\\sigma^4)\\right) \\to 0\\]\n\nwhen \\(n \\to \\infty\\)."
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html#proof-mle",
    "href": "teaching/linear_model/lectures/estimation.html#proof-mle",
    "title": "Linear Regression Model",
    "section": "Maximum Likelihood Estimator",
    "text": "Maximum Likelihood Estimator\n\n\n\n\n\n\nMLE\n\n\n\nLet \\(\\hat \\beta_{MLE}\\) and \\(\\hat \\sigma_{MLE}^2\\) be the MLE of \\(\\beta\\) and \\(\\sigma^2\\), respectively.\n\n\\(\\hat{\\beta}_{MLE} = \\hat{\\beta}\\) et \\(\\hat{\\sigma}^2_{MLE} = \\frac{SCR}{n} = \\frac{n-p}{n} \\hat{\\sigma}^2\\).\n\\(\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\\).\n\\(\\frac{n-p}{\\sigma^2} \\hat{\\sigma}^2 = \\frac{n}{\\sigma^2} \\hat{\\sigma}^2_{MLE} \\sim \\chi^2(n - p)\\).\n\\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent\n\n\n\n\nSetup\nModel: \\(Y = X\\beta + \\varepsilon\\) where \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\)\nThis means: \\(Y \\sim N(X\\beta, \\sigma^2 I_n)\\)\nFor \\(n\\) observations, the likelihood function is:\n\\[L(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|Y - X\\beta\\|^2\\right)\\]\n\n\\[\\ell(\\beta, \\sigma^2) = \\log L(\\beta, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|Y - X\\beta\\|^2\\]\n\n\n\nExistence and Uniqueness of Global Maximum\nLet \\(\\beta^*\\) be the minimizer of \\(\\|Y-X\\tilde \\beta\\|\\) over all \\(\\tilde \\beta \\in \\mathbb R^p\\). \\(\\beta^*\\) the OLS estimator by definition, and it satisfies \\(\\ell(\\beta, \\sigma^2) \\leq -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|Y - X\\beta^*\\|^2 = \\ell(\\beta^*, \\sigma^2)\\). Since \\(p &lt; n\\), we almost surely have that \\(Y \\not \\in [X]\\), so that \\(\\|Y - X\\beta^*\\|^2 &gt; 0\\).\nNow that \\(\\beta^*\\) is fixed, the function \\(\\sigma^2 \\to -\\frac{n}{2} \\log(2\\pi\\sigma^2  ) - \\frac{1}{2\\sigma^2} \\|Y - X\\beta^*\\|^2\\) is concave on \\((0, +\\infty)\\), and cancelling the gradient gives that it achieves its maximum at \\((\\sigma^*)^2 = \\frac{1}{n}\\|Y - X\\beta^*\\|^2\\).\nHence, we obtain that for any \\(\\beta\\) and \\(\\sigma\\), \\(\\ell(\\beta, \\sigma^2) \\leq \\ell(\\beta^*, \\sigma^2) \\leq \\ell(\\beta^*, (\\sigma^*)^2)\\). We just proved that \\((\\beta^*, (\\sigma^*))^2\\) is the unique global maximum, which implies that\n\n\\[\n\\begin{aligned}\n\\hat \\beta_{MLE}&= \\beta^* = \\hat \\beta \\\\\n\\hat \\sigma_{MLE}&= \\sigma^* = \\frac{1}{n}\\|Y-X\\hat \\beta\\|^2\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/estimation.html#efficient-beta",
    "href": "teaching/linear_model/lectures/estimation.html#efficient-beta",
    "title": "Linear Regression Model",
    "section": "\\(\\hat \\beta\\) is an Efficient Estimator in the Gaussian Model",
    "text": "\\(\\hat \\beta\\) is an Efficient Estimator in the Gaussian Model\n\n\n\n\n\n\nTheorem\n\n\n\nIn the Gaussian Model, \\(\\hat \\beta\\) is an efficient estimator of \\(\\hat \\beta\\). This means that \\[\n\\Var(\\hat \\beta) \\preceq \\Var(\\tilde \\beta)\\; ,\n\\] for any estimator \\(\\tilde \\beta\\)\n\n\n\nCramer-Rao Lower Bound\nIt holds that for any estimator \\(\\tilde \\beta\\), \\[\\mathbb V(\\tilde \\beta) \\succeq [I(\\beta)]^{-1}\\] where \\(I(\\beta)\\) is the Fisher Information Matrix. This comes from the Cramér-Rao lower bound. Hence, it is enough to prove that \\(\\mathbb V(\\tilde \\beta) = [I(\\beta)]^{-1}\\).\n\n\nFisher Information Matrix\nThe log-likelihood function is: \\[\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}(Y - X\\beta)^T(Y - X\\beta)\\]\nFirst derivative with respect to \\(\\beta\\): \\[\\frac{\\partial \\ell}{\\partial \\beta} = \\frac{1}{\\sigma^2}X^T(Y - X\\beta)\\]\nSecond derivative: \\[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^T} = -\\frac{1}{\\sigma^2}X^TX\\]\nHence, we obtain that \\[[I(\\beta)]^{-1} = -\\left(\\mathbb{E}\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^T}\\right]\\right)^{-1} = \\sigma^2(X^TX)^{-1} = \\mathbb V(\\hat \\beta) \\enspace ,\\]\nwhich proves that \\(\\hat \\beta\\) is an efficient estimator of \\(\\beta\\).\n\n\nAdditional Notes\n\nThis efficiency holds specifically under the normality assumption\n\\(\\hat \\beta\\) is also the Best Linear Unbiased Estimator (BLUE) by the Gauss-Markov theorem\nUnder normality, \\(\\hat \\beta\\) is the Best Unbiased Estimator (BUE) among all estimators, not just linear ones"
  },
  {
    "objectID": "teaching/linear_model/notes/variance_quadratic_form.html",
    "href": "teaching/linear_model/notes/variance_quadratic_form.html",
    "title": "Variance of a Quadratic Form",
    "section": "",
    "text": "Setup: Consider the quadratic form \\(Q = \\varepsilon^T A \\varepsilon\\) where:\n\n\\(\\varepsilon_i\\) are i.i.d. with \\(\\mathbb{E}[\\varepsilon_i] = 0\\), \\(\\mathbb{E}[\\varepsilon_i^2] = \\sigma^2\\), \\(\\mathbb{E}[\\varepsilon_i^4] = \\mu_4\\)\n\\(A\\) is a symmetric \\(n \\times n\\) matrix (\\(A^T = A\\))\n\nComputing the Expectation: \\[\\mathbb{E}[Q] = \\mathbb{E}\\left[\\sum_{i,j} A_{ij}\\varepsilon_i\\varepsilon_j\\right] = \\sum_{i,j} A_{ij}\\mathbb{E}[\\varepsilon_i\\varepsilon_j] = \\sigma^2 \\sum_i A_{ii} = \\sigma^2 \\text{tr}(A)\\]\nComputing the Second Moment: \\[Q^2 = \\left(\\sum_{i,j} A_{ij}\\varepsilon_i\\varepsilon_j\\right)^2 = \\sum_{i,j,k,\\ell} A_{ij}A_{k\\ell}\\varepsilon_i\\varepsilon_j\\varepsilon_k\\varepsilon_\\ell\\]\nTaking expectation: \\[\\mathbb{E}[Q^2] = \\sum_{i,j,k,\\ell} A_{ij}A_{k\\ell}\\mathbb{E}[\\varepsilon_i\\varepsilon_j\\varepsilon_k\\varepsilon_\\ell]\\]\nFourth Moment Structure: For i.i.d. \\(\\varepsilon_i\\) with mean zero: \\[\\mathbb{E}[\\varepsilon_i\\varepsilon_j\\varepsilon_k\\varepsilon_\\ell] = \\begin{cases}\n\\mu_4 & \\text{if } i = j = k = \\ell \\\\\n\\sigma^4 & \\text{if two pairs of equal indices} \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\nComputing Each Contribution:\nAll indices equal \\((i = j = k = \\ell)\\): \\[\\mu_4 \\sum_i A_{ii}^2\\]\nPattern \\((i = j \\neq k = \\ell)\\): \\[\\sigma^4 \\sum_{i \\neq k} A_{ii}A_{kk} = \\sigma^4[\\text{tr}(A)^2 - \\sum_i A_{ii}^2]\\]\nPattern \\((i = k \\neq j = \\ell)\\): \\[\\sigma^4 \\sum_{i,j} A_{ij}^2 = \\sigma^4 \\text{tr}(A^2)\\]\nPattern \\((i = \\ell \\neq j = k)\\): Since \\(A\\) is symmetric: \\(A_{ij} = A_{ji}\\) \\[\\sigma^4 \\sum_{i,j} A_{ij}A_{ji} = \\sigma^4 \\sum_{i,j} A_{ij}^2 = \\sigma^4 \\text{tr}(A^2)\\]\nFinal Formula for Second Moment: \\[\\mathbb{E}[Q^2] = \\mu_4 \\sum_i A_{ii}^2 + \\sigma^4[\\text{tr}(A)^2 - \\sum_i A_{ii}^2] + 2\\sigma^4 \\text{tr}(A^2)\\]\nRearranging: \\[\\mathbb{E}[Q^2] = \\sigma^4[\\text{tr}(A)^2 + 2\\text{tr}(A^2)] + (\\mu_4 - 3\\sigma^4)\\sum_i A_{ii}^2\\]\nVariance of Q: \\[\\mathbb{V}(Q) = \\mathbb{E}[Q^2] - [\\mathbb{E}[Q]]^2 = \\mathbb{E}[Q^2] - \\sigma^4[\\text{tr}(A)]^2\\]\n\n\\[\\mathbb{V}(Q) = 2\\sigma^4 \\text{tr}(A^2) + (\\mu_4 - 3\\sigma^4)\\sum_i A_{ii}^2\\]\n\nSpecial Cases:\nGaussian errors (\\(\\mu_4 = 3\\sigma^4\\)): \\[\\mathbb{V}(Q) = 2\\sigma^4 \\text{tr}(A^2)\\]\nIdempotent matrix (\\(A^2 = A\\)): \\[\\mathbb{V}(Q) = 2\\sigma^4 \\text{tr}(A) + (\\mu_4 - 3\\sigma^4)\\sum_i A_{ii}^2\\]\nProjection matrix (idempotent with \\(A_{ii} \\in [0,1]\\)): \\[\\sum_i A_{ii}^2 \\leq \\sum_i A_{ii} = \\text{tr}(A)\\] Therefore: \\[\\mathbb{V}(Q) \\leq 2\\sigma^4 \\text{tr}(A) + |\\mu_4 - 3\\sigma^4|\\text{tr}(A)\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#teachers",
    "href": "teaching/linear_model/slides/introduction.html#teachers",
    "title": "Introduction",
    "section": "Teachers",
    "text": "Teachers\n\n\ncours\n\nEmmanuel Pilliat (Me, in english)\nFrédéric Lavancier (in French)\n\nTD\n\n\nThéo Paquier (In English)\nJulien Jamme\nThéo Leroy\nDenis Mottin\nMarie Christiane Wambo / Koffi Amezouwui"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#about-me",
    "href": "teaching/linear_model/slides/introduction.html#about-me",
    "title": "Introduction",
    "section": "About Me",
    "text": "About Me\n\nI Arrived at ENSAI in sep. 2024\nI give also lectures on hypothesis testing\nBefore: PhD in Montpellier\nResearch fields: crowdsourcing, active learning and parallel computing"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#course-structure",
    "href": "teaching/linear_model/slides/introduction.html#course-structure",
    "title": "Introduction",
    "section": "Course Structure",
    "text": "Course Structure\n\n8 lecture sessions\nCourse materials and slides (both evolving) available on Moodle (French)\nAnd https://epilliat.github.io/\n8 TD/TP sessions (tutorial/practical work)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#evaluation",
    "href": "teaching/linear_model/slides/introduction.html#evaluation",
    "title": "Introduction",
    "section": "Evaluation",
    "text": "Evaluation\n\nA short quiz at the beginning of each tutorial/practical session\nA one-hour midterm exam (1h) on November 4th (date to be confirmed)\nAn exam (3h) on December 19th (date to be confirmed)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#objectives-of-a-regression-model",
    "href": "teaching/linear_model/slides/introduction.html#objectives-of-a-regression-model",
    "title": "Introduction",
    "section": "Objectives of a Regression Model",
    "text": "Objectives of a Regression Model\n\nExplain a quantity \\(Y\\)\n\n\nbased on \\(p\\) quantities \\(X^{(1)}, ..., X^{(p)}\\) (explanatory variables, or regressors).\n\n\nFor this purpose, we have \\(n\\) observations of each quantity from \\(n\\) individuals."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#examples",
    "href": "teaching/linear_model/slides/introduction.html#examples",
    "title": "Introduction",
    "section": "Examples",
    "text": "Examples\n\n\\(Y\\): daily electricity consumption in France\n\n\n\\(X= X^{(1)}\\): average daily temperature\n\n\nThe data consists of a history of \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\) over \\(n\\) days\n\n\nQuestion: Do we have \\(Y \\approx f(X)\\) for a certain function f?\nSimplifying: Do we have \\(Y ≈ aX + b\\) for certain values \\(a\\) and \\(b\\)?\nIf yes, what is \\(a\\)? What is \\(b\\)? Is the relationship “reliable”?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#examples-1",
    "href": "teaching/linear_model/slides/introduction.html#examples-1",
    "title": "Introduction",
    "section": "Examples",
    "text": "Examples\n\n\\(Y \\in \\{0,1\\}\\): customer quality (\\(1\\): good; \\(0\\): not good)\n\n\\(X^{(1)}\\): customer income\n\n\\(X^{(2)}\\): socio-professional category (6-7 possibilities)\n\n\\(X^{(3)}\\): age\n\n\n\nData: n customers.\nIn this case, we model \\(p = P(Y = 1)\\).\nDo we have \\(p \\approx f(X^{(1)}, X^{(2)}, X^{(3)})\\) for a function f with values in \\([0, 1]\\)?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#predictivedescriptive-model",
    "href": "teaching/linear_model/slides/introduction.html#predictivedescriptive-model",
    "title": "Introduction",
    "section": "Predictive/Descriptive Model",
    "text": "Predictive/Descriptive Model\n\nApproximate relationship between \\(Y\\) and \\(X^{(1)}\\), …, \\(X^{(p)}\\) is a model.\n\n\nWhy seek to establish such a model? Two main reasons:\n\n\nDescriptive objective: quantify the marginal effect of each variable.\n\n\nFor example, if \\(X^{(1)}\\) increases by 10%, how does \\(Y\\) change?\n\n\nPredictive objective: given new values for \\(X^{(1)}\\), …, \\(X^{(p)}\\), we can estimate an approximation of corresponding \\(Y\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#course-outline",
    "href": "teaching/linear_model/slides/introduction.html#course-outline",
    "title": "Introduction",
    "section": "Course Outline",
    "text": "Course Outline\n\nIntroduction: Bivariate analysis, and general aspects of modelling\nLinear Regression: Quantitative \\(Y\\) as a function of quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)\nAnalysis of Variance and Covariance: Quantitative \\(Y\\) as a function of qualitative and/or quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)\nGeneralized Linear Regression: Qualitative or quantitative \\(Y\\) as a function of qualitative and/or quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#two-types-of-variables",
    "href": "teaching/linear_model/slides/introduction.html#two-types-of-variables",
    "title": "Introduction",
    "section": "Two Types of Variables",
    "text": "Two Types of Variables\n\n\\(X\\) and \\(Y\\) can be\n\nQuantitative (they represent numbers)\nor Qualitative (modalities, e.g. colors)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#quantitative-variables",
    "href": "teaching/linear_model/slides/introduction.html#quantitative-variables",
    "title": "Introduction",
    "section": "Quantitative Variables",
    "text": "Quantitative Variables\n\nA variable whose observation is a measured quantity.\n\nDiscrete quantitative variables whose possible values are finite or countable (Examples: number of children, number of infractions, etc.)\nContinuous quantitative variables which can take any value within an interval (Examples: height, salary, etc.)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#qualitative-variables-or-factors",
    "href": "teaching/linear_model/slides/introduction.html#qualitative-variables-or-factors",
    "title": "Introduction",
    "section": "Qualitative Variables (or Factors)",
    "text": "Qualitative Variables (or Factors)\n\nA variable whose observation results in a category or code. Possible observations are called modalities\n\n\n\nordinal qualitative variable: a natural order appears in the modalities (Examples: high school honors, etc.).\nnominal qualitative variable otherwise (Examples: gender, socio-professional category, etc.)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-of-the-pottery-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-of-the-pottery-dataset",
    "title": "Introduction",
    "section": "Example of the “Pottery” Dataset",
    "text": "Example of the “Pottery” Dataset\n\nData: chemical composition of pottery found at different archaeological sites in the United Kingdom\n\n\n\n\n\nSite\nAl\nFe\nMg\nCa\nNa\n\n\n\n\n1\nLlanedyrn\n14.4\n7.00\n4.30\n0.15\n0.51\n\n\n2\nLlanedyrn\n13.8\n7.08\n3.43\n0.12\n0.17\n\n\n3\nLlanedyrn\n14.6\n7.09\n3.88\n0.13\n0.20\n\n\n4\nLlanedyrn\n10.9\n6.26\n3.47\n0.17\n0.22\n\n\n5\nCaldicot\n11.8\n5.44\n3.94\n0.30\n0.04\n\n\n6\nCaldicot\n11.6\n5.39\n3.77\n0.29\n0.06\n\n\n7\nIsleThorns\n18.3\n1.28\n0.67\n0.03\n0.03\n\n\n8\nIsleThorns\n15.8\n2.39\n0.63\n0.01\n0.04\n\n\n9\nIsleThorns\n18.0\n1.88\n0.68\n0.01\n0.04\n\n\n10\nIsleThorns\n20.8\n1.51\n0.72\n0.07\n0.10\n\n\n11\nAshleyRails\n17.7\n1.12\n0.56\n0.06\n0.06\n\n\n12\nAshleyRails\n18.3\n1.14\n0.67\n0.06\n0.05\n\n\n13\nAshleyRails\n16.7\n0.92\n0.53\n0.01\n0.05\n\n\n\n\n\nIndividuals: pottery numbered from 1 to 13\n\nVariables: the archaeological site (factor with 4 modalities) and different chemical compounds (quantitative)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-of-the-no2traffic",
    "href": "teaching/linear_model/slides/introduction.html#example-of-the-no2traffic",
    "title": "Introduction",
    "section": "Example of the “NO2traffic”",
    "text": "Example of the “NO2traffic”\n\nData: NO2 concentration inside cars in Paris, type of road, (P, T, A, V or U) and traffic fluidity (A to D).\n\n\n\n\n\nNO2\nType\nFluidity\n\n\n\n\n1\n378.94\nP\nA\n\n\n2\n806.67\nT\nD\n\n\n3\n634.58\nA\nD\n\n\n4\n673.35\nT\nC\n\n\n5\n589.75\nP\nA\n\n\n…\n…\n…\n…\n\n\n283\n184.16\nP\nB\n\n\n284\n121.88\nV\nD\n\n\n285\n152.39\nU\nA\n\n\n286\n129.12\nU\nC\n\n\n\n\n\nIndividuals: vehicles numbered from 1 to 286\n\nVariables: NO2 (quantitative), type (factor with 5 modalities) and fluidity (ordinal factor with 4 modalities)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#pairwise-scatter-plots",
    "href": "teaching/linear_model/slides/introduction.html#pairwise-scatter-plots",
    "title": "Introduction",
    "section": "Pairwise Scatter Plots",
    "text": "Pairwise Scatter Plots\n\nWe observe\n\\(X=(X_1, \\ldots, X_n) \\in \\mathbb R^n\\) and \\(Y=(Y_1, \\ldots, Y_n) \\in \\mathbb R^n\\), (quantitative variables)\n\n\nRelationship between \\(X\\) and \\(Y\\): scatter plot of points \\((X_i, Y_i)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-pottery-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-pottery-dataset",
    "title": "Introduction",
    "section": "Example: Pottery Dataset",
    "text": "Example: Pottery Dataset"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-plot",
    "href": "teaching/linear_model/slides/introduction.html#correlation-plot",
    "title": "Introduction",
    "section": "Correlation Plot",
    "text": "Correlation Plot\n\nplot_cor=@df pottery_num corrplot(cols(1:4),grid=false, compact=true) #Julia"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-empirical-covariance-and-variance",
    "href": "teaching/linear_model/slides/introduction.html#linear-empirical-covariance-and-variance",
    "title": "Introduction",
    "section": "Linear Empirical Covariance and Variance",
    "text": "Linear Empirical Covariance and Variance\n\\(\\DeclareMathOperator{\\cov}{cov}\\) \\(\\DeclareMathOperator{\\var}{var}\\)\n\nLet \\(\\hat \\var\\) and \\(\\hat \\cov\\) denote the empirical variance and covariance:\n\n\n\n\\(\\hat \\cov(X,Y)= \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\overline X)(Y_i - \\overline Y)\\)\n\n\n\n\n\\(\\hat \\var(X) = \\hat \\cov(X,X)\\), \\(\\hat \\var(Y)=\\hat\\cov(Y,Y)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-empirical-correlation",
    "href": "teaching/linear_model/slides/introduction.html#linear-empirical-correlation",
    "title": "Introduction",
    "section": "Linear Empirical Correlation",
    "text": "Linear Empirical Correlation\n\nThe linear relationship is quantified by Pearson’s linear correlation:\n\n\\[\\hat \\rho = \\frac{\\hat\\cov(X,Y)}{\\sqrt{\\hat \\var(X)\\hat \\var(Y)}}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#properties-of-empirical-correlation",
    "href": "teaching/linear_model/slides/introduction.html#properties-of-empirical-correlation",
    "title": "Introduction",
    "section": "Properties of Empirical Correlation",
    "text": "Properties of Empirical Correlation\n\nFrom the Cauchy-Schwarz inequality, we deduce that:\n\n\nThe correlation \\(\\hat \\rho\\) is always between \\(-1\\) and \\(1\\):\n\nIf \\(\\hat \\rho = 1\\): for all \\(i\\), \\(Y_i = aX_i + b\\) for some \\(a &gt; 0\\)\nIf \\(\\hat \\rho = -1\\): for all \\(i\\), \\(Y_i = aX_i + b\\) for some \\(a &lt; 0\\)\nIf \\(\\hat \\rho = 0\\): no linear relationship. notebook"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-test",
    "href": "teaching/linear_model/slides/introduction.html#correlation-test",
    "title": "Introduction",
    "section": "Correlation test",
    "text": "Correlation test\n\n\\(\\hat \\rho(X, Y)\\) is an estimator of the unknown theoretical correlation \\(\\rho\\) between \\(X\\) and \\(Y\\) defined by \\[\\rho = \\frac{\\mathbb E[(X - \\mathbb E(X))(Y - \\mathbb E(Y))]}{\\sqrt{\\mathbb V(X)\\mathbb V(Y)}}\\]\n\n\nCorrelation test problem:\n\\[H_0: \\rho = 0 \\quad \\text{VS}\\quad  H_1: \\rho \\neq 0\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-test-1",
    "href": "teaching/linear_model/slides/introduction.html#correlation-test-1",
    "title": "Introduction",
    "section": "Correlation test",
    "text": "Correlation test\n\nTest statistic (here we use \\(\\psi\\) for test statistics and \\(T\\) for tests) \\[\\psi(X,Y) = \\frac{\\hat \\rho\\sqrt{n-2}}{\\sqrt{1-\\hat \\rho^2}}\\]\n\n\nTest\nUnder \\(H_0\\), if \\((X,Y)\\) is Gaussian, \\(\\psi(X,Y) \\sim \\mathcal T(n-2)\\) (Student distribution of degree of freedom \\(n-2\\))\n\n\n\\[T(X,Y) = \\mathbf{1}\\{|\\psi(X,Y)| &gt; t_{1-\\alpha/2}\\}\\]\nIn R: cor.test"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#least-square-p1",
    "href": "teaching/linear_model/slides/introduction.html#least-square-p1",
    "title": "Introduction",
    "section": "Least Square \\((p=1)\\)",
    "text": "Least Square \\((p=1)\\)\n\nGiven observations \\((X_i, Y_i)\\), we consider \\(\\hat \\alpha\\), \\(\\hat \\mu\\) that minimize, over all \\((\\alpha, \\mu) \\in \\mathbb R^2\\):\n\n\\[\nL(\\alpha, \\mu) = \\sum_{i=1}^n (Y_i - \\alpha X_i - \\mu)^2\n\\]\n\n\n\nSolution: (check homogeneity!)\n\n\n\\[\\hat \\alpha = \\frac{\\hat \\cov(X,Y)}{\\hat \\var(X)}  \\quad \\text{and} \\quad \\hat \\mu = \\overline Y - \\hat \\alpha \\overline X\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#contingency-table-and-notation",
    "href": "teaching/linear_model/slides/introduction.html#contingency-table-and-notation",
    "title": "Introduction",
    "section": "Contingency Table and Notation",
    "text": "Contingency Table and Notation\n\nWe observe \\(X=(X_1, \\dots, X_n)\\) and \\(Y=(Y_1, \\dots, Y_n)\\), where\n\n\\(X_k \\in \\{1, \\dots, I\\}\\) (factor with \\(I\\) categories, “colors”)\n\\(Y_k \\in \\{1, \\dots, J\\}\\) (factor with \\(J\\) categories, “bags”)\n\n\n\n\n\n\nCategory X/Y\nBag 1\nBag 2\nBag 3\nTotals\n\n\n\n\nCol 1\n\\(n_{11}\\)\n\\(n_{12}\\)\n\\(n_{13}\\)\n\\(R_1\\)\n\n\nCol 2\n\\(n_{21}\\)\n\\(n_{22}\\)\n\\(n_{23}\\)\n\\(R_2\\)\n\n\nTotals\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\\(n_{ij}\\): number of individuals having category \\(i\\) for \\(X\\) and \\(j\\) for \\(Y\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-no2trafic-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-no2trafic-dataset",
    "title": "Introduction",
    "section": "Example: NO2trafic dataset",
    "text": "Example: NO2trafic dataset\ncontingency table of variable “Type” and “Fluidity”\n\n\n\nFluidity/Type\nP\nU\nA\nT\nV\n\n\n\n\nA\n21\n21\n19\n9\n9\n\n\nB\n20\n17\n16\n8\n7\n\n\nC\n17\n17\n16\n8\n7\n\n\nD\n20\n20\n18\n8\n8\n\n\n\nIn R: table(X,Y)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#in-julia",
    "href": "teaching/linear_model/slides/introduction.html#in-julia",
    "title": "Introduction",
    "section": "In Julia",
    "text": "In Julia\n\nfluidity_types = [\"A\", \"B\", \"C\", \"D\"]\ntype_p = [21, 20, 17, 20]\ntype_u = [21, 17, 17, 20]\ntype_a = [19, 16, 16, 18]\ntype_t = [9, 8, 8, 8]\ntype_v = [9, 7, 7, 8]\n\n# Create a matrix for the grouped bar plot\n# Each row represents a fluidity type, each column represents a measurement type\ndata_matrix = hcat(type_p, type_u, type_a, type_t, type_v)\n\n# Create a grouped bar plot\np1 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Dodge (Beside)\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:dodge,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\np2 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Stack\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:stack,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\nplot(p1,p2)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-problem",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-problem",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test Problem",
    "text": "\\(\\chi^2\\) Dependency Test Problem\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)\n\nWe observe\n\\(X=(X_1, \\dots, X_n) \\in \\{1, \\dots, I\\}^n\\) and \\(Y=(Y_1, \\dots, Y_n) \\in \\{1, \\dots, J\\}^n\\)\n\n\nAssumptions: \\((X_k,Y_k)\\) are independent, each pair has unknown distribution \\(P_{XY}\\)\n\n\ndependency test problem:\n\n\\[H_0: P_{XY}=P_{X}P_Y \\VS H_1: P_{XY} \\neq P_{X}P_{Y}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#definitions",
    "href": "teaching/linear_model/slides/introduction.html#definitions",
    "title": "Introduction",
    "section": "Definitions",
    "text": "Definitions\n\nEntries of the table:\n\n\\[n_{ij} = \\sum_{k=1}^n \\mathbf 1\\{X_{k} = i\\}\\mathbf 1\\{Y_k=j\\}\\]\n\n\n\nTotal proportion of individuals \\(k\\) of color \\(X_k =i\\):\n\n\n\\(\\hat p_{i}=\\frac{R_i}{N}\\) \\(= \\tfrac{1}{N}\\sum_{j=1}^{J}n_{ij}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test",
    "text": "\\(\\chi^2\\) Dependency Test\n\nChi-squared statistic, or chi-squared distance:\n\n\\[\\psi(X,Y) = \\sum_{i=1}^I\\sum_{j=1}^J \\frac{(n_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\]\n\n\nApproximation: \\(\\psi(X,Y) \\sim \\chi^2((I-1)(J-1))\\) when \\(n \\to \\infty\\)\nTest: \\(T=\\mathbf 1\\{\\psi(X,Y) \\geq t_{1-\\alpha/2}\\}\\), where\n\\(t_{0.975}\\) = quantile(Chisq(I-1,J-1), 0.975)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#setting",
    "href": "teaching/linear_model/slides/introduction.html#setting",
    "title": "Introduction",
    "section": "Setting",
    "text": "Setting\n\nWe observe \\(X=(X_1, \\dots, X_n)\\) and \\(Y=(Y_1, \\dots, Y_n)\\), where\n\n\\(X_k \\in \\{1, \\dots, I\\}\\) (Quali)\n\\(Y_k \\in \\mathbb R\\) (Quanti)\n\n\n\nBoxplot: represents \\(0, 25, 75\\) and \\(100\\) percentiles."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#singer-dataset-julia-statsplots",
    "href": "teaching/linear_model/slides/introduction.html#singer-dataset-julia-statsplots",
    "title": "Introduction",
    "section": "Singer Dataset (Julia StatsPlots)",
    "text": "Singer Dataset (Julia StatsPlots)\n\n\\(X\\): Height (in inches), \\(Y\\): Type of singer\n\n\nBoxplot: (min, \\(q_{1/4}\\), \\(q_{3/5}\\), max) for each modality"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#definitions-1",
    "href": "teaching/linear_model/slides/introduction.html#definitions-1",
    "title": "Introduction",
    "section": "Definitions",
    "text": "Definitions\n\n\\(X=(X_1, \\dots, X_n) \\in \\{1, \\dots, I\\}^n\\), \\(Y=(Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\nif \\(i \\in \\{1, \\dots, I\\}\\), we define partial means as\n\n\n\\[N_i = \\sum_{k=1}^n \\mathbf 1\\{X_k=i\\} \\and \\overline Y_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\mathbf 1\\{X_k=i\\}\\]\n\n\n\n\nTotal mean:\n\\(\\overline Y = \\frac{1}{N}\\sum_{k=1}^n Y_k = \\frac{1}{N}\\sum_{i=1}^I\\sum N_i \\overline Y_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#variance-decomposition",
    "href": "teaching/linear_model/slides/introduction.html#variance-decomposition",
    "title": "Introduction",
    "section": "Variance Decomposition",
    "text": "Variance Decomposition\n\n\n\n\\[\\frac{1}{n}\\underbrace{\\sum_{k=1}^n(Y_k - \\overline Y)^2}_{SST} =\n\\frac{1}{n}\\underbrace{\\sum_{i=1}^IN_i(\\overline Y_i - \\overline Y)^2}_{SSB}\n+ \\frac{1}{n}\\underbrace{\\sum_{k=1}^n\\mathbf 1\\{X_k=i\\}(Y_k - \\overline Y_i)^2}_{SSW}\\]\n\n\n\n\ncorrelation ratio:\n\n\\[ \\hat \\eta^2 = \\frac{SSB}{SST}  \\in [0,1]\\]\n\n\n\nThis is an estimator of unknown \\(\\eta = \\frac{\\mathbb V(\\mathbb E[Y|X])}{\\mathbb V(Y)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#anova-test",
    "href": "teaching/linear_model/slides/introduction.html#anova-test",
    "title": "Introduction",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\\((X_1, \\dots, X_n) \\in \\{1, \\dots I\\}^n\\)\n\\((Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\nAssumption: \\(Y_k\\) are independent, Gaussian of same variance. \\(\\mu_i = \\mathbb E[Y|X=i]=\\frac{\\mathbb E[Y\\mathbf 1\\{X=i\\}]}{\\mathbb P(X=i)}\\) (unknown)\n\n\nProblem:\n\n\\[H_0: \\mu_1=\\dots \\mu_I \\VS H_1: \\mu_i \\neq \\mu_j \\text{ for some $i,j$}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#anova-test-1",
    "href": "teaching/linear_model/slides/introduction.html#anova-test-1",
    "title": "Introduction",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\nTest Statistic\n\n\\[\\psi(X,Y) = \\frac{SSB/(I-1)}{SSW/(N-I)}\\]\n\n\n\n\\(\\psi(X,Y) \\sim \\mathcal F(I-1, N-I)\\) under \\(H_0\\)\n\n\npvalue:\n1-cdf(FDist(I-1, N-1), psiobs)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#context",
    "href": "teaching/linear_model/slides/introduction.html#context",
    "title": "Introduction",
    "section": "Context",
    "text": "Context\n\n\\(n\\) individuals, \\(p\\) explanatory variables \\(X=(X^{(1)}, \\dots, X^{(p)})\\).\nGoal: Explain/Predict \\(Y\\) in function of \\(X\\)\n\n\nWe observe\n\\[Y=\\begin{pmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X=\\begin{pmatrix}\nX^{(1)}_1 & \\cdots & X^{(p)}_1 \\\\\nX^{(1)}_2 & \\cdots & X^{(p)}_1 \\\\\n\\vdots & & \\vdots \\\\\nX^{(1)}_n & \\cdots & X^{(p)}_1\n\\end{pmatrix}\\]\nEach individual \\(k\\) correspond to \\(Y_k\\) a row \\((X^{(1)}_k, \\dots, X^{(p)}_k)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#where-is-randomness",
    "href": "teaching/linear_model/slides/introduction.html#where-is-randomness",
    "title": "Introduction",
    "section": "Where is Randomness?",
    "text": "Where is Randomness?\nGenerally, we don’t know any values a priori. Example:\n\nindividual characteristics of a customer\n\n\n\\(Y\\) and \\(X^{(1)}, \\ldots, X^{(p)}\\) are random variables.\n\n\nWe observe realizations the \\(Y\\)’s and \\(X\\)’s\n\n\nSometimes \\(X = (X^{(1)}, \\ldots, X^{(p)})\\) is chosen a priori. Example:\n\n\\(X\\): medication dosages (and \\(Y\\): a physiological response)\n\n\n\nIn this context, \\(Y\\) is random, but \\(X^{(1)}, \\ldots, X^{(p)}\\) are not."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#summary",
    "href": "teaching/linear_model/slides/introduction.html#summary",
    "title": "Introduction",
    "section": "Summary:",
    "text": "Summary:\n\n\\(Y\\) is always viewed as a random variable\n\n\n\\(X^{(1)}, \\ldots, X^{(p)}\\) are viewed as random variables or deterministic variables, depending on the context"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#general-model",
    "href": "teaching/linear_model/slides/introduction.html#general-model",
    "title": "Introduction",
    "section": "General Model",
    "text": "General Model\n\n\\(Y=(Y_1, \\dots, Y_n)\\)\n\\(X^{(k)} = (X^{(k)}_1, \\dots, X^{(k)}_n)\\), \\(k= 1, \\dots, p\\) (row notation)\n\n\nGeneral model:\n\n\\[Y_i = F(X_i^{(1)}, \\dots, X_i^{(p)}, \\varepsilon_i)\\]\n\n\n\\(F\\) is an unknown and deterministic function of \\(p\\) variables.\n\\(\\varepsilon_i\\) are iid random representing external independent noise\n\n\n\n\nNonparametric problem: space of all \\(F\\) is of infinite dimension!"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-model",
    "href": "teaching/linear_model/slides/introduction.html#linear-model",
    "title": "Introduction",
    "section": "Linear Model",
    "text": "Linear Model\n\nIdea: reduce to a smaller class of function \\(F \\in \\mathcal F\\).\n\n\nLinear Model:\n\n\\[\nY_i = \\mu + \\beta_1 X^{(1)}_i + \\beta_2 X^{(2)}_i + \\dots + \\beta_p X^{(p)}_i + \\sigma \\varepsilon_i\n\\]\n\n\n\nSpace of affine function:\n\\[\n\\mathcal F = \\{F:~ F(x, \\varepsilon) = \\mu + \\beta^T x + \\sigma \\varepsilon, (\\mu, \\beta, \\sigma) \\in \\mathbb R^{p+2}\\}\n\\]\n\n\n\\(\\dim(\\mathcal F) = p+2\\) (number of unknown parameters)\n\n\nmuch easier to estimate \\(f\\) (and perhaps less overfitting)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#wait-what-is",
    "href": "teaching/linear_model/slides/introduction.html#wait-what-is",
    "title": "Introduction",
    "section": "Wait, what is + ?",
    "text": "Wait, what is + ?\n\nIf the \\(X^{(k)}\\) are qualitative factors,\n\n\nWhat is the meaning of\n\\[\nY_i = \\mu + \\beta_1 X^{(1)}_i + \\beta_2 X^{(2)}_i + \\dots + \\beta_p X^{(p)}_i + \\sigma \\varepsilon_i\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#case-of-categorical-variables",
    "href": "teaching/linear_model/slides/introduction.html#case-of-categorical-variables",
    "title": "Introduction",
    "section": "Case of categorical variables",
    "text": "Case of categorical variables\n\nEncode each category\n\n\nIf \\(Y \\in \\{A, B\\}\\) has \\(2\\) categories, we encode \\[\\widetilde Y = \\mathbf 1\\{Y = A\\}\\]\n\n\nIf \\(Y\\in \\{A_1, \\dots, A_K\\}\\), we use one hot encoding:\n\\[\\widetilde{Y}_k = \\mathbf 1\\{Y=A_k\\}\\]\n\nAlso encode \\(X^{(1)}, \\ldots, X^{(p)}\\) if needed.\nsee also the chapter on ANOVA and ANCOVA."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x",
    "href": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x",
    "title": "Introduction",
    "section": "From random to deterministic \\(X\\)",
    "text": "From random to deterministic \\(X\\)\n\nIf \\(X^{(1)}, \\ldots, X^{(p)}\\) are random,\n\n\nThen for all deterministic \\(x^{(1)}, \\dots, x^{(p)}\\)\n\n\nConditionnally to \\((X^{(1)}=x^{(1)}, \\dots, X^{(p)} =x^{(p)})\\), we have the general model\n\\[Y = F(x^{(1)},\\dots, x^{(p)}, \\varepsilon)\\]\n\nBecause \\(\\varepsilon\\) is independent of \\(X\\)\nReplace \\(X^{(k)}\\) by their observations \\(x^{(k)}\\).\nThe only randomness is now in \\(\\varepsilon\\)!"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#next-class",
    "href": "teaching/linear_model/slides/introduction.html#next-class",
    "title": "Introduction",
    "section": "Next Class",
    "text": "Next Class\nDefinition of the Linear Model"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#outline",
    "href": "teaching/linear_model/slides/linear_model.html#outline",
    "title": "Definition of the Linear Model",
    "section": "Outline",
    "text": "Outline\n\nIntroduction of the model\nModel in the matrix form\nRemarks and identifiability condition"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#general-setting",
    "href": "teaching/linear_model/slides/linear_model.html#general-setting",
    "title": "Definition of the Linear Model",
    "section": "General Setting",
    "text": "General Setting\n\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\n\nIn other words, we observe\n\\(Y= (Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\n\n\\(X^{(1)} = (X^{(1)}_1, \\dots, X^{(1)}_n) \\in \\mathbb R^n\\)\n\\(X^{(2)} = (X^{(2)}_1, \\dots, X^{(2)}_n) \\in \\mathbb R^n\\)\n…\n\\(X^{(p)} = (X^{(p)}_1, \\dots, X^{(p)}_n)\\in \\mathbb R^n\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#non-parametric-model",
    "href": "teaching/linear_model/slides/linear_model.html#non-parametric-model",
    "title": "Definition of the Linear Model",
    "section": "Non Parametric Model",
    "text": "Non Parametric Model\n\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\n\nWe assume that\n\n\\[Y_i = F(X^{(1)}_i, X^{(2)}_i, \\dots, X^{(p)}_i, \\varepsilon_i)\\]\n\n\n\nwhere \\(\\varepsilon = (\\varepsilon_1, \\dots, \\varepsilon_n) \\in \\mathbb R^n\\) are iid random noise\n\n\n\n\\(\\varepsilon\\) is not observed\n\n\n\\(F\\) is unknown\n–&gt; Too ambitious, risk of overfitting"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model",
    "title": "Definition of the Linear Model",
    "section": "Linear Model",
    "text": "Linear Model\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\nWe assume that\n\n\\[Y = \\beta_1 X^{(1)}+ \\beta_2 X^{(2)}+ \\dots+ \\beta_p X^{(p)}+ \\varepsilon\\]\n\n\n\nThat is, we know that \\(F\\) is of the form \\(F(x_1, \\dots, x_p, \\varepsilon) = \\beta_1 x_1+ \\beta_2 x_2+ \\dots+ \\beta_p x_p+ \\varepsilon\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-1",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-1",
    "title": "Definition of the Linear Model",
    "section": "Linear Model",
    "text": "Linear Model\n\\(Y\\) and the \\(X^{(k)}\\)’s are vectors in \\(\\mathbb R^n\\).\n\nFor all \\(i\\),\n\n\\[Y_i = \\beta_1 X^{(1)}_i+ \\beta_2 X^{(2)}_i+ \\dots+ \\beta_p X^{(p)}_i+ \\varepsilon_i\\]\n\nWe assume that\n\n\\(X^{(k)}\\) are known and deterministic (otherwise we condition on \\(X^{(k)}\\)’s)\n\\(\\mathbb E[\\varepsilon_i] = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-with-intercept",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-with-intercept",
    "title": "Definition of the Linear Model",
    "section": "Linear Model with Intercept",
    "text": "Linear Model with Intercept\n\\(Y\\) and the \\(X^{(k)}\\)’s are vectors in \\(\\mathbb R^n\\).\n\nIf we set \\(X^{(1)}= (1, \\dots, 1)\\), then the model rewrites\n\n\\[Y_i = \\beta_1+ \\beta_2 X^{(2)}_i+ \\dots+ \\beta_p X^{(p)}_i+ \\varepsilon_i\\]\n\n\n\n\nThe model is linear in \\(X^{(2)}, \\dots, X^{(p)}\\)\n\\(\\beta_1\\) is then called the intercept"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation",
    "href": "teaching/linear_model/slides/linear_model.html#notation",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\nWe write \\(Y = (Y_1, \\dots, Y_n)\\) and \\(X^{(k)} = (X^{(k)}_1, \\dots, X^{(k)}_n)\\) as columns:\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\) \\(\\newcommand{\\E}{\\mathbb E}\\) \\(\\newcommand{\\P}{\\mathbb P}\\) \\(\\newcommand{\\Var}{\\mathbb V}\\)\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X^{(k)}=\\begin{pmatrix}\nX^{(k)}_1 \\\\\n\\vdots \\\\\nX^{(k)}_n\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation-1",
    "href": "teaching/linear_model/slides/linear_model.html#notation-1",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\n\nTo get a matrix form, we write \\(X_{ik} = X^{(k)}_i\\). Then:\n\n\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X^{(k)}=\\begin{pmatrix}\nX_{1k} \\\\\n\\vdots \\\\\nX_{n,k}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation-2",
    "href": "teaching/linear_model/slides/linear_model.html#notation-2",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\n\nTo get a matrix form, we write \\(X\\) for the matrix \\((X_{ik}) \\in \\mathbb R^{n \\times p}\\)\n\n\nThat is, \\(X = (X^{(1)}, \\dots, X^{(p)})\\)\n\n\nAnd:\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X=\\begin{pmatrix}\n&X_{1,1} &\\dots &X_{1,p} \\\\\n&\\vdots &~ &\\vdots \\\\\n&X_{n,1} &\\dots &X_{n,p}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#model-matrix-form",
    "href": "teaching/linear_model/slides/linear_model.html#model-matrix-form",
    "title": "Definition of the Linear Model",
    "section": "Model, Matrix Form",
    "text": "Model, Matrix Form\n\nLet \\(\\beta = (\\beta_1, \\dots, \\beta_p) \\in \\mathbb R^p\\) be unknown parameters, and \\(\\varepsilon = (\\varepsilon_1, \\dots, \\varepsilon_n)\\) be iid noise.\n\n\nIn column notation:\n\n\\[\\beta = \\begin{pmatrix}\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{pmatrix} \\and \\varepsilon=\\begin{pmatrix}\n\\varepsilon_{1} \\\\\n\\vdots \\\\\n\\varepsilon_{n}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-matrix-form",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-matrix-form",
    "title": "Definition of the Linear Model",
    "section": "Linear Model, Matrix Form",
    "text": "Linear Model, Matrix Form\n\nWe observe \\(Y = (Y_1, \\dots, Y_n) \\in \\mathbb R^n\\) and \\(X \\in \\mathbb R^{n \\times p}\\)\n\n\nWe assume that\n\n\\[Y = X \\beta + \\varepsilon\\]\n\nwhere\n\n\\(X\\) is known,\n\\(\\beta \\in \\mathbb R^p\\) is unknown\n\\(\\varepsilon \\in \\mathbb R^n\\) is a vector of iid random noise with \\(\\mathbb E[\\varepsilon_i] = 0\\) and \\(\\mathbb V(\\varepsilon_i) = \\sigma^2\\)(unknown)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#remarks",
    "href": "teaching/linear_model/slides/linear_model.html#remarks",
    "title": "Definition of the Linear Model",
    "section": "Remarks",
    "text": "Remarks\n\n\\((\\varepsilon_1, \\dots, \\varepsilon_n)\\) independent implies no correlation between individuals\n\\(\\mathbb V(\\varepsilon_i)= \\sigma^2\\) does not depend on \\(i\\): this is called homoscedasticity assumption\nWe can write \\(\\mathbb V(\\varepsilon) = \\sigma^2 I_n\\) (covariance matrix of \\(\\varepsilon\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#identifiability-condition",
    "href": "teaching/linear_model/slides/linear_model.html#identifiability-condition",
    "title": "Definition of the Linear Model",
    "section": "Identifiability Condition",
    "text": "Identifiability Condition\n\nRecall that \\(X \\in \\mathbb R^{n \\times p}\\)\n\n\nWe assume that \\(rk(X)=p\\).\n\n\nThis implies \\(p \\leq n\\)\n\n\nIf this condition is not satisfied:\n\n\nIt means that there is a linear relation between the \\(X^{(k)}\\)!\n\n\nIt means that \\(X\\alpha=\\alpha_1X^{(1)} + \\dots + \\alpha_p X^{(p)}=0\\) for some \\(\\alpha \\in \\mathbb R^p\\setminus \\{0\\}\\)\n\n\nWe can take infinitely many possible \\(\\beta\\), since for \\(t \\in \\mathbb R\\),\n\\[\nX(\\beta + t\\alpha) = X\\beta\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#next-class",
    "href": "teaching/linear_model/slides/linear_model.html#next-class",
    "title": "Definition of the Linear Model",
    "section": "Next Class",
    "text": "Next Class\nInference in the Linear Model"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#practical-question",
    "href": "teaching/linear_model/slides/selection.html#practical-question",
    "title": "Selection",
    "section": "Practical Question",
    "text": "Practical Question\n\nIn practice, we often hesitate between several models:\n\nWhich variables to include in the model?\nHow to choose between one model and another?\nIdeally: How to select the “best” model among all possible sub-models of a large linear regression model?"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#selection-criteria",
    "href": "teaching/linear_model/slides/selection.html#selection-criteria",
    "title": "Selection",
    "section": "Selection Criteria",
    "text": "Selection Criteria\n\nSeveral criteria exist. The main ones:\n\n\\(R_a^2\\): Adjusted \\(R^2\\) (already seen)\nFisher test for nested models (already seen)\n\nMallows’ \\(C_p\\)\nAIC criterion\nBIC criterion"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#setup",
    "href": "teaching/linear_model/slides/selection.html#setup",
    "title": "Selection",
    "section": "Setup",
    "text": "Setup\n\nSuppose we have \\(p_{\\max}\\) explanatory variables, forming the “maximal” design matrix \\(X_{\\max}\\).\n\n\nTrue model (unknown):\n\n\\[Y = X^*\\beta^* + \\varepsilon\\]\n\nwhere \\(X^*\\) is a sub-matrix of \\(X_{\\max}\\) formed by \\(p^* \\leq p_{\\max}\\) columns.\n\n\nWe don’t know \\(p^*\\) nor which variables are involved.\nGoal: Select the correct matrix \\(X^*\\) and estimate \\(\\beta^*\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#practice",
    "href": "teaching/linear_model/slides/selection.html#practice",
    "title": "Selection",
    "section": "Practice",
    "text": "Practice\n\nWe regress \\(Y\\) on \\(p \\leq p_{\\max}\\) variables, assuming: \\[Y = X\\beta + \\varepsilon\\] where \\(X\\): sub-matrix of \\(X_{\\max}\\) containing the \\(p\\) chosen columns (yielding \\(\\hat{\\beta}\\)).\n\n\nThis model is potentially wrong (bad choice of variables).\n\n\nObjective: Calculate a quality score for this submodel."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#adjusted-r2---reminder",
    "href": "teaching/linear_model/slides/selection.html#adjusted-r2---reminder",
    "title": "Selection",
    "section": "Adjusted \\(R^2\\) - Reminder",
    "text": "Adjusted \\(R^2\\) - Reminder\n\nFor a model with constant:\n\n\\[R_a^2 = 1 - \\frac{n-1}{n-p} \\cdot \\frac{SSR}{SST}\\]\n\n\n\\(SST = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\) (independent of chosen model)\n\\(SSR = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) (specific to considered model)\n\n\n\nSelection rule: Between two models, prefer highest \\(R_a^2\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#fisher-test-for-nested-models",
    "href": "teaching/linear_model/slides/selection.html#fisher-test-for-nested-models",
    "title": "Selection",
    "section": "Fisher Test for Nested Models",
    "text": "Fisher Test for Nested Models\n\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR}\\]\n\n\n\\(SSR\\): residual sum of squares of the larger model\n\\(SSR_c\\): SSR of the sub-model (fewer variables)\n\\(p\\): number of variables in the larger model\n\\(q\\): number of constraints (\\(p-q\\) variables in sub-model)\n\n\nIf \\(F &lt; f_{q,n-p}(1-\\alpha)\\): prefer sub-model (\\(H_0\\) at level \\(\\alpha\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#mallows-c_p",
    "href": "teaching/linear_model/slides/selection.html#mallows-c_p",
    "title": "Selection",
    "section": "Mallows’ \\(C_p\\)",
    "text": "Mallows’ \\(C_p\\)\n\nTrue model (unknown): \\(Y = X^*\\beta^* + \\varepsilon\\)\n\nTested model (possibly wrong): \\(Y = X\\beta + \\varepsilon\\) with OLS estimate \\(\\hat{\\beta}\\)\n\n\nMallows’ \\(C_p\\) aims to estimate the prediction risk: \\[\\E(\\|\\tilde{Y} - X\\hat{\\beta}\\|^2)\\]\nwhere \\(\\tilde{Y}\\) follows the same distribution as \\(Y\\) but is independent."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#formula-for-mallows-c_p",
    "href": "teaching/linear_model/slides/selection.html#formula-for-mallows-c_p",
    "title": "Selection",
    "section": "Formula for Mallows’ \\(C_p\\)",
    "text": "Formula for Mallows’ \\(C_p\\)\n\n\n\\(C_p = \\frac{SSR}{\\hat{\\sigma}^2} - n + 2p\\)\n\n\n\\(p\\): number of variables in the considered model\n\\(SSR\\): residual sum of squares of the considered model\n\n\\(\\hat{\\sigma}^2\\): estimation of \\(\\sigma^2\\) in the largest model\nSame for all tested models\n\n\n\nSelection rule: Among all tested models, choose the one with lowest \\(C_p\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#aic-criterion",
    "href": "teaching/linear_model/slides/selection.html#aic-criterion",
    "title": "Selection",
    "section": "AIC Criterion",
    "text": "AIC Criterion\n\nAIC (Akaike Information Criterion) is motivated like \\(C_p\\).\nIt also focuses on prediction error \\(\\tilde{Y} - X\\hat{\\beta}\\), but Kullback distance instead of Quadratic distance.\n\n\n\n\\[AIC = n \\ln\\left(\\frac{SSR}{n}\\right) + 2(p+1)\\]\n\n\n\nSelection rule: choose model with lowest AIC.\nIn practice, AIC and \\(C_p\\) are very close (choose same model)"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#bic-criterion",
    "href": "teaching/linear_model/slides/selection.html#bic-criterion",
    "title": "Selection",
    "section": "BIC Criterion",
    "text": "BIC Criterion\n\nBIC (Bayesian Information Criterion) seeks the “most probable” model in a Bayesian formalism.\n\n\\[BIC = n \\ln\\left(\\frac{SSR}{n}\\right) + (p+1) \\ln n\\]\n\n\n\nSelection rule: choose the one with lowest BIC.\n\nKey difference: The “2” in front of \\((p+1)\\) is replaced by \\(\\ln n\\)\nThis difference frequently leads to a different model choice between AIC and BIC"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#relationship-between-criteria",
    "href": "teaching/linear_model/slides/selection.html#relationship-between-criteria",
    "title": "Selection",
    "section": "Relationship Between Criteria",
    "text": "Relationship Between Criteria\n\n“Large” model: low \\(SSR\\), but high number of variables \\(p\\)\n(if too large: overfitting)\n“Small” model: high \\(SSR\\), but low number of variables \\(p\\)   (if two small: underfitting)\n\n\nAll previous criteria try to find a compromise between:\n\nGood fit to data (low \\(SSR\\))\nSmall model size (low \\(p\\))\n\n\n\nThis is a permanent trade-off in statistics (not just in regression)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#general-form",
    "href": "teaching/linear_model/slides/selection.html#general-form",
    "title": "Selection",
    "section": "General Form",
    "text": "General Form\n\n\\(C_p\\), AIC, and BIC consist of minimizing an expression of the form:\n\n\\[f(SSR) + c(n) \\cdot p\\]\n\n\n\nBIC: \\(c(n) = \\ln n\\) \\(\\quad\\quad\\) AIC: \\(c(n) = 2\\)\n\n\\(f\\) is an increasing function of \\(SSR\\)\n\\(c(n) \\cdot p\\) is a term penalizing models with many variables\n\n\n\n(Other criteria exist built on the same principle.)"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#relationship-between-criteria-1",
    "href": "teaching/linear_model/slides/selection.html#relationship-between-criteria-1",
    "title": "Selection",
    "section": "Relationship Between Criteria",
    "text": "Relationship Between Criteria\n\n\\[f(SSR) + c(n) \\cdot p\\]\n\n\nWhen \\(\\ln n &gt; 2\\), BIC penalizes large models more than AIC.\n\n\nOrdering criteria by their propensity to select the most sparse model:\n\\[BIC \\leq F\\text{ test} \\leq C_p \\approx AIC \\leq R_a^2\\]\n\nBIC will favor a smaller model than \\(C_p\\) or AIC\n\\(R_a^2\\) will tend to favor an even larger model"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#theoretical-aspects",
    "href": "teaching/linear_model/slides/selection.html#theoretical-aspects",
    "title": "Selection",
    "section": "Theoretical Aspects",
    "text": "Theoretical Aspects\n\n\n\n\n\n\n\n\nProbability as \\(n \\to \\infty\\)                                                             \nBIC\n\\(C_p\\), AIC, \\(R_a^2\\)\n\n\n\n\n\\(\\mathbb{P}\\)(selects model smaller than true)\n\\(\\to 0\\)\n\\(\\to 0\\)\n\n\n\\(\\mathbb{P}\\)(selects model larger than true)\n\\(\\to 0\\)\n\\(\\not\\to 0\\)\n\n\n\\(\\mathbb{P}\\)(selects correct model)\n\\(\\to 1\\)\n\\(\\not\\to 1\\)\n\n\n\n\nBIC is asymptotically consistent, while other criteria tend to overfit."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#exhaustive-search",
    "href": "teaching/linear_model/slides/selection.html#exhaustive-search",
    "title": "Selection",
    "section": "Exhaustive Search",
    "text": "Exhaustive Search\n\nGiven \\(p_{\\max}\\) available explanatory variables:\n\nTempting approach: Test all possible sub-models\nSelection: Keep the one with lowest BIC (or other criterion)\nComputational cost: \\(2^{p_{\\max}}\\) models to test (that’s a lot!)\n\n\n\nIf \\(p_{\\max}\\) is not too large, this remains feasible.\nR function: regsubsets from leaps library"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#exhaustive-search-1",
    "href": "teaching/linear_model/slides/selection.html#exhaustive-search-1",
    "title": "Selection",
    "section": "Exhaustive Search",
    "text": "Exhaustive Search\n\n\n\n\nImportant Warning\n\n\nAutomatic selection does not guarantee that the selected model is good.\nIt’s simply the best model according to the chosen criterion.\nThe selected model may be bad in terms of:\n\nExplanatory power\nMulticollinearity problems\n\nHeteroscedasticity issues\nAuto-correlation problems"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#stepwise-procedures",
    "href": "teaching/linear_model/slides/selection.html#stepwise-procedures",
    "title": "Selection",
    "section": "Stepwise Procedures",
    "text": "Stepwise Procedures\n\nIf \\(p_{\\max}\\) is too large for exhaustive search:\n\n\nStepwise Backward (according to chosen criterion, e.g., BIC):\n\nStart with largest model (\\(p_{\\max}\\) variables)\nRemove least significant variable\nRepeat: remove remaining least significant variable\nStop when no removal improves the model"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#stepwise-procedures-1",
    "href": "teaching/linear_model/slides/selection.html#stepwise-procedures-1",
    "title": "Selection",
    "section": "Stepwise Procedures",
    "text": "Stepwise Procedures\nIf \\(p_{\\max}\\) is too large for exhaustive search:\n\nStepwise Forward:\n\nStart with smallest model (constant only)\nAdd most significant variable at each step\n\n\n\nStepwise Backward (or Forward) Hybrid:\n\nLike backward (or forward), but also try adding (or removing) a variable at each step"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#limitations-and-characteristics",
    "href": "teaching/linear_model/slides/selection.html#limitations-and-characteristics",
    "title": "Selection",
    "section": "Limitations and Characteristics",
    "text": "Limitations and Characteristics\n\nStepwise procedures do not explore all possible sub-models:\n\nMay miss the best model\n\n\n\nSpeed comparison:\n\nForward: fastest (small models are quicker to estimate)\nHybrid procedures: slower, but explore more possible models"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#r-implementation",
    "href": "teaching/linear_model/slides/selection.html#r-implementation",
    "title": "Selection",
    "section": "R Implementation",
    "text": "R Implementation\n\nFunction: step with option direction:\n\n\"backward\" or \"forward\" or \"both\"\nDefault criterion: AIC (k = 2)\nFor BIC: use k = ln(n)\n\n\n\nThe option k corresponds to the penalty \\(c(n)\\) introduced earlier."
  },
  {
    "objectID": "teaching/linear_model/TD/TD2-5.html",
    "href": "teaching/linear_model/TD/TD2-5.html",
    "title": "TD2-5",
    "section": "",
    "text": "Exercise 1. R Lab on Eucalyptus Data\nWe want to explain the height of eucalyptus trees based on their circumference using simple linear regression. We have measurements of heights (ht) and circumferences (circ) of 1429 eucalyptus trees, which are found in the file “eucalyptus.txt”.\n\nExtract and plot the data in a scatter plot.\nPerform the regression \\(y = \\beta_1 + \\beta_2 x + \\varepsilon\\) where \\(y\\) represents the height and \\(x\\) the circumference. Comment on the results.\nWhat is the theoretical formula for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)? Retrieve the estimates provided by R using it.\nCalculate a 95% confidence interval for \\(\\beta_1\\) and \\(\\beta_2\\), assuming the normality of the data.\nIf the noise \\(\\varepsilon\\) does not follow a normal distribution, do the previous confidence intervals remain valid?\nPlot the estimator of the regression line and a 95% confidence interval for it. What do you conclude about the quality of the estimation?\nWe now want to predict the height of a new series of eucalyptus trees with circumferences 50, 100, 150, and 200. Give the estimators of the height of each of them and the associated 95% prediction intervals, assuming the normality of the data.\nAdd to the graphical representation from question 6 the prediction intervals (associated with the same circumference values).\nIf the noise \\(\\varepsilon\\) does not follow a normal distribution, do the previous prediction intervals remain valid?\n\n\n\nExercise 2. The Fisher Test and \\(R^2\\)\nConsider a multiple linear regression model \\(y = X\\beta + \\varepsilon\\) where \\(\\beta \\in \\mathbb{R}^p\\), \\(X\\) is a matrix of size \\((n, p)\\) and \\(\\varepsilon\\) is a random vector of size \\(n\\), centered with covariance matrix \\(\\sigma^2 I_n\\) (\\(I_n\\) is the identity matrix).\nWe want to test \\(q\\) linear constraints on the parameter \\(\\beta\\), that is to test \\(H_0: R\\beta = 0\\) against \\(H_1: R\\beta \\neq 0\\), where \\(R\\) is a matrix of size \\((q, p)\\).\nWe denote SSR the sum of squared residuals of the initial model, and \\(\\text{SSR}_c\\) the sum of squared residuals of the constrained model (i.e., for which hypothesis \\(H_0\\) is verified).\n\nRecall the statistic used to perform this test. Denote it \\(F\\) and give its expression as a function of SSR and \\(\\text{SSR}_c\\).\nWhat distribution does this statistic follow under \\(H_0\\) when \\(\\varepsilon\\) follows a normal distribution? What can be said about its distribution if no normality assumption is made on \\(\\varepsilon\\)?\nShow that if a constant is present in the constrained model, \\[F = \\frac{R^2 - R^2_c}{1 - R^2} \\cdot \\frac{n - p}{q}\\] where \\(R^2\\) (respectively \\(R^2_c\\)) denotes the coefficient of determination of the initial model (respectively of the constrained model).\n\n\n\nExercise 3. R Lab on Ice Cream Consumption\nWe study ice cream consumption in the United States over a period of 30 weeks from March 18, 1950 to July 11, 1953. The variables are the period (from week 1 to week 30), and on average for each period: ice cream consumption per person (“Consumption”, in 1/2 liter), price of ice cream (“Price”, in dollars), average weekly salary per household (“Income”, in dollars), and temperature (“Temp”, in degrees Fahrenheit). The data are available in the file “icecream-R.dat”.\n\nExtract the data and plot consumption as a function of the different variables.\nWe propose to linearly regress consumption on the three variables “Price”, “Income” and “Temp”, also assuming that a constant is present in the model. Denote the constant \\(\\beta_1\\) and the three coefficients associated with the previous variables \\(\\beta_2\\), \\(\\beta_3\\) and \\(\\beta_4\\) respectively. Perform the estimation phase of this regression and comment on the sign of the estimated coefficients.\nTest the global significance of the proposed model, i.e. \\(H_0: \\beta_2 = \\beta_3 = \\beta_4 = 0\\), using the global Fisher test.\nTest the significance of the variable “Price” in this model at the 5% level. Similarly test the significance of “Income”, then of “Temp”.\nCompare the previous complete model and the model without the “Price” variable using a Fisher test:\n\nBasing the calculation on the sum of squared residuals of each model;\nBasing the calculation on the coefficient of determination of each model;\nUsing the linearHypothesis function from the car library.\nUsing the anova function.\n\nWhat is the difference between this test and the Student’s t-test of significance of the “Price” variable?\nCompare the complete model and the model without the “Price” variable and without the constant using a Fisher test. Proceed according to the 4 methods described above. Comment.\nWe now want to predict ice cream consumption for the following data: \\(\\text{Price} = 0.3\\), \\(\\text{Income} = 85\\) and \\(\\text{Temp} = 65\\). Propose the prediction that seems best to you given the models studied previously. Give a 95% prediction interval around this prediction.\nUnder what hypothesis is the previous prediction interval valid? Verify it by observing the QQ-plot of the regression residuals and by performing a statistical test.\nVerify the other hypotheses by recalling the definition and calculating the VIF (Variance Inflation Factor) of each explanatory variable and by performing a graphical analysis of the residuals.\nObserve the 3D scatter plot of the variables “Consumption”, “Income” and “Temp”, and the fit by the linear model, using scatter3d from the car library.\n\n\n\nExercise 4. The Multiple Correlation Coefficient with or without Constant\nConsider the regression model \\[y_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)} + \\varepsilon_i, \\quad i = 1, \\ldots, n, \\quad (*)\\]\nwhere the variables \\(\\varepsilon_i\\) are centered, with variance \\(\\sigma^2\\) and uncorrelated. Let \\(Y = (y_1, \\ldots, y_n)^T\\), \\(X^{(k)} = (x_1^{(k)}, \\ldots, x_n^{(k)})^T\\) and \\(\\mathbf{1} = (1, \\ldots, 1)^T\\). We assume that the variables \\(X^{(k)}\\) are not linearly related to \\(\\mathbf{1}\\). We denote \\(\\bar{y}\\) the empirical mean of \\(Y\\) and \\(\\hat{Y} = \\hat{\\beta}_0 \\mathbf{1} + \\hat{\\beta}_1 X^{(1)} + \\cdots + \\hat{\\beta}_p X^{(p)}\\) where the estimators are those obtained by ordinary least squares.\n\nWhat does \\(\\hat{Y}\\) represent geometrically? Draw a diagram showing the vectors \\(Y\\), \\(\\hat{Y}\\), \\(\\bar{y}\\mathbf{1}\\), \\(Y - \\bar{y}\\mathbf{1}\\), \\(\\hat{Y} - \\bar{y}\\mathbf{1}\\) and \\(\\hat{\\varepsilon}\\).\nDeduce the following equalities:\n\n\\(\\sum_{i=1}^n y_i^2 = \\sum_{i=1}^n \\hat{\\varepsilon}_i^2 + \\sum_{i=1}^n \\hat{y}_i^2\\)\n\\(\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n \\hat{\\varepsilon}_i^2 + \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\)\n\nConsider the ratios: \\[R_1^2 = \\frac{\\sum_{i=1}^n \\hat{y}_i^2}{\\sum_{i=1}^n y_i^2} \\quad \\quad R_2^2 = \\frac{\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\nJustify that \\(R_1^2 \\geq R_2^2\\). In which case do we have equality?\nWhat is the definition of the multiple correlation coefficient for model \\((*)\\)?\nWe now consider a regression model without constant, i.e., we set \\(\\beta_0 = 0\\) in \\((*)\\). Do the equalities shown in 2) remain valid? What is in this case the definition of the multiple correlation coefficient?\nAfter estimating model \\((*)\\) with or without the constant, we obtain \\(R^2 = 0.72\\) with the constant and \\(R^2 = 0.96\\) without the constant. Is the introduction of the constant relevant?\n\n\n\nExercise 5. The Interpretation of \\(R^2\\) as Multiple Correlation Coefficient\nWe place ourselves in a regression model containing a constant. We define \\[\\rho(Y, X) = \\sup_{\\beta} \\text{cor}(Y, X\\beta)\\] where cor denotes the empirical correlation. This quantity is therefore the maximum correlation that can be obtained between \\(Y\\) and a linear combination of the explanatory variables.\n\nShow that \\[\\text{cor}(Y, X\\beta) = \\frac{(X\\hat{\\beta} - \\bar{Y}\\mathbf{1})^T(X\\beta - \\bar{X\\beta}\\mathbf{1})}{\\|Y - \\bar{Y}\\mathbf{1}\\| \\|X\\beta - \\bar{X\\beta}\\mathbf{1}\\|}\\] where \\(\\hat{\\beta}\\) is the OLS estimator of the regression of \\(Y\\) on \\(X\\), and \\(\\bar{X}\\) denotes the vector of the \\(p\\) empirical means of each explanatory variable.\nDeduce that for all \\(\\beta\\), \\(\\text{cor}(Y, X\\beta)^2 \\leq R^2\\), where \\(R^2\\) is the multiple correlation coefficient of the regression of \\(Y\\) on \\(X\\).\nShow that the previous bound is attained when \\(\\beta = \\hat{\\beta}\\).\nConclude that \\(\\rho(Y, X)^2 = R^2\\) justifying the terminology “multiple correlation coefficient”.\n\n\n\nExercise 6. Effect of Multicollinearity\nConsider a model with two explanatory variables, assumed to be centered. From the estimation on \\(n\\) individuals, we obtained the following matrices \\(X^T X\\) and \\(X^T Y\\): \\[X^T X = \\begin{pmatrix} 200 & 150 \\\\ 150 & 113 \\end{pmatrix} \\quad X^T Y = \\begin{pmatrix} 350 \\\\ 263 \\end{pmatrix}\\]\nThe removal of one observation has modified these matrices as follows: \\[X^T X = \\begin{pmatrix} 199 & 149 \\\\ 149 & 112 \\end{pmatrix} \\quad X^T Y = \\begin{pmatrix} 347.5 \\\\ 261.5 \\end{pmatrix}\\]\n\nCalculate the estimated coefficients of the regression in both cases.\nCalculate the linear correlation coefficient between the two explanatory variables.\nComment.\n\n\n\nExercise 7. Modeling the Daily Maximum Ozone Concentration\nThe data set “ozone.txt” contains the maximum ozone concentration (maxO3) measured each day of summer 2001 in Rennes. It also contains temperatures, cloudiness and wind speed measured at 9am, 12pm and 3pm (respectively T9, T12, T15, Ne9, Ne12, Ne15 and Vx9, Vx12, Vx15), as well as the main wind direction and the presence or absence of rain. We want to best explain the ozone concentration using the variables available in the dataset.\n\nAnalyze the scatter plot and the linear correlation between maxO3 and each of the available quantitative variables (i.e., T9, T12, T15, Ne9, Ne12, Ne15, Vx9, Vx12 and Vx15). Is it reasonable to assume that there is a linear relationship between maxO3 and these variables?\nFit the linear regression model explaining maxO3 as a function of all the previous quantitative variables. Test the significance of each of the explanatory variables in this model. Is the result in agreement with the observations from the previous question? Also test the global significance of the model. Comment.\nCalculate the VIF (Variance Inflation Factor) for each of the explanatory variables of the previous model. How do these values explain the results of the Student’s t-tests performed above?\nWe decide to remove variables from the previous model. Which variables does it seem natural to remove in view of the previous question? Fit the new proposed model and repeat the analyses performed in the two previous questions.\nImplement an automatic selection of the best possible sub-model of the “large” model fitted in question 2, according to the BIC criterion. You can use the regsubsets function from the leaps library (then plot.regsubsets) or step. Compare the retained model with the model chosen in the previous question.\nApply the previous automatic selection based on criteria other than BIC. Are the retained models the same? If not, which seems preferable?\nAnalyze the residuals of the model selected in the previous question through graphical representations and by performing tests of homoscedasticity and non-correlation of residuals. Do all the assumptions of a linear model seem to be verified?\nIn order to solve the problem of auto-correlation of residuals, we propose to add the previous day’s ozone maximum to the model. Create this variable, which we will call maxO3v and add it to the dataset. Do we observe a linear relationship between maxO3 and maxO3v?\nFit the regression model containing maxO3v as an additional explanatory variable. Analyze the fitting results: are the assumptions of a linear model verified?\nCompare this last model to the model without maxO3v using a Fisher test and by comparing the different selection criteria (AIC, BIC, Mallows’ Cp, adjusted \\(R^2\\)).\n\n\n\nExercise 8. Comparison of Model Selection Criteria\nConsider a linear regression model aimed at explaining \\(Y\\) as a function of variables \\(X^{(1)}, \\ldots, X^{(p)}\\). We want to choose between the model with \\(X^{(p)}\\) and the model without \\(X^{(p)}\\) (the other variables being included in both cases), based on a sample of \\(n\\) individuals.\nWe denote \\(F\\) the statistic: \\[F = (n - p) \\frac{\\text{SSR}_c - \\text{SSR}}{\\text{SSR}}\\] where SSR denotes the sum of squared residuals in the model with \\(X^{(p)}\\), and \\(\\text{SSR}_c\\) denotes the sum of squared residuals in the model without \\(X^{(p)}\\).\n\nBy applying a Fisher test of nested models, according to which decision rule, based on \\(F\\), will we choose to include the variable \\(X^{(p)}\\) in the model?\nWe recall that the adjusted \\(R^2\\) in a model with \\(k\\) variables and \\(n\\) individuals is defined by \\[R^2_a = 1 - \\frac{n - 1}{n - k} \\frac{\\text{SSR}_k}{\\text{SST}}\\] where \\(\\text{SSR}_k\\) denotes the sum of squared residuals in the model, and SST the total sum of squares.\nShow that we will decide to include \\(X^{(p)}\\) according to the adjusted \\(R^2\\) criterion if \\(F &gt; 1\\).\nWe recall that the Mallows’ Cp in a model with \\(k\\) variables and \\(n\\) individuals is defined by \\[C_p = \\frac{\\text{SSR}_k}{\\hat{\\sigma}^2} - n + 2k\\] where \\(\\text{SSR}_k\\) denotes the sum of squared residuals in the model, and \\(\\hat{\\sigma}^2\\) is an estimator of \\(\\sigma^2\\) based on the largest possible model. We will take here \\(\\hat{\\sigma}^2 = \\text{SSR}/(n - p)\\), where SSR denotes the sum of squared residuals in the model with \\(X^{(p)}\\).\nShow that we will decide to include \\(X^{(p)}\\) according to the Mallows’ Cp criterion if \\(F &gt; 2\\).\nWe recall that the AIC criterion in a model with \\(k\\) variables, with \\(n\\) individuals, with Gaussian residuals, is defined by \\[\\text{AIC} = n(1 + \\log(2\\pi)) + n \\log \\frac{\\text{SSR}_k}{n} + 2(k + 1)\\] where \\(\\text{SSR}_k\\) denotes the sum of squared residuals in the model.\nShow that we will decide to include \\(X^{(p)}\\) according to the AIC criterion if \\(F &gt; (n - p)(e^{2/n} - 1)\\).\nWe recall that the BIC criterion (also sometimes called SBC) in a model with \\(k\\) variables, with \\(n\\) individuals, with Gaussian residuals, is defined by \\[\\text{BIC} = n(1 + \\log(2\\pi)) + n \\log \\frac{\\text{SSR}_k}{n} + \\log(n)(k + 1)\\] where \\(\\text{SSR}_k\\) denotes the sum of squared residuals in the model.\nShow that we will decide to include \\(X^{(p)}\\) according to the BIC criterion if \\(F &gt; (n - p)(e^{\\log(n)/n} - 1)\\).\nBy admitting that the 95% quantiles of a Fisher distribution with degrees of freedom \\((1, \\nu)\\) take their values in the interval \\([3.8, 5]\\) as soon as \\(\\nu &gt; 10\\), rank the previous criteria from the most conservative (i.e., tending to more easily reject the introduction of \\(X^{(p)}\\)) to the least conservative (i.e., tending to more easily accept the introduction of \\(X^{(p)}\\)). You can use a Taylor expansion for the study of the AIC and BIC criteria, assuming that \\(n\\) is sufficiently large.\n\n\n\nExercise 9. Over-fitting Probability of Selection Criteria\nWe place ourselves in the framework of the previous exercise, but we additionally assume that the variable \\(X^{(p)}\\) is not significant in the model (i.e., its coefficient is zero in the regression) and that the residuals are i.i.d. Gaussian. We admit the results stated in the questions of the previous exercise.\n\nWhat distribution does the statistic \\(F\\) follow? Show that when \\(n \\to \\infty\\), this distribution is equivalent to a \\(\\chi^2(1)\\) distribution.\nWhen implementing the Fisher test of nested models at level \\(\\alpha \\in [0, 1]\\), what is the probability of (wrongly) deciding to include the variable \\(X^{(p)}\\) in the model?\nWhat does the previous probability tend to if we base the decision on the adjusted \\(R^2\\)?\nSame question if the decision is based on Mallows’ Cp.\nSame question if the decision is based on the AIC criterion.\nSame question if the decision is based on the BIC criterion.\nWhich criterion is preferable to choose if we want to minimize the risk of including an extra variable in the model?\n\nSupplement: In the opposite situation where \\(X^{(p)}\\) is significant in the model and it is therefore preferable to include it, we can show (but it is more difficult) that by relying on any of the above criteria, the probability of (wrongly) deciding not to include \\(X^{(p)}\\) tends to 0 when \\(n \\to \\infty\\).\n\n\nExercise 10. ANCOVA: Return to Ozone Modeling\nWe again consider the “ozone.txt” data studied in Exercise 7. We want to take advantage of the qualitative variables present in the dataset (i.e., the presence or absence of rain, and the main wind direction) to possibly improve the model constructed in Exercise 7.\n\nWe take the model selected in Exercise 7, namely “maxO3” as a function of “T12”, “Ne9”, “Vx9” and “maxO3v” where “maxO3v” represents the maximum ozone concentration from the previous day (create this variable if needed). Fit this model on the data.\nGraphically represent “maxO3” as a function of the presence of rain. Does a relationship seem present? Confirm it by a statistical test.\nAdd to the model from the first question the variable “rain” in the most general way possible (i.e., including an interaction with each variable in addition to an effect on the constant). Test the significance of these additions by performing a Fisher test of nested models between this model and the initial model.\nSimilarly test the simpler model in which only an additive effect of the “rain” variable is integrated, and not its interactions with the other variables. Is the result in disagreement with the graphical analysis from question 2? How to explain the result?\nSimilarly: graphically represent “maxO3” as a function of wind direction and study the relevance of including a wind effect in the initial model.\n\n\n\nExercise 11. Generalized Least Squares\nLet a multiple linear regression model \\[Y = X\\beta + \\varepsilon\\] where \\(\\beta \\in \\mathbb{R}^p\\), \\(X\\) is a matrix of size \\(n \\times p\\) and \\(\\varepsilon\\) is a random vector of size \\(n\\), centered. We consider here the situation where the variables \\(\\varepsilon_i\\) are no longer homoscedastic and uncorrelated, but generally \\(\\mathbb{V}(\\varepsilon) = \\Sigma\\) where \\(\\Sigma\\) is an invertible matrix. We assume in this exercise that \\(\\Sigma\\) is known (it should be estimated in practice).\n\nSpecify the matrix \\(\\Sigma\\) when the variables \\(\\varepsilon_i\\) are uncorrelated but heteroscedastic with variance \\(\\sigma_i^2\\) (\\(i = 1, \\ldots, n\\)).\nDetermine the expectation and the variance of the ordinary least squares estimator \\(\\hat{\\beta}\\) (in the general case of any matrix \\(\\Sigma\\)).\nFor \\(S \\in \\mathbb{R}^n\\) and \\(T \\in \\mathbb{R}^n\\), we define the scalar product between \\(S\\) and \\(T\\) associated with the matrix \\(\\Sigma^{-1}\\) by \\(S^T \\Sigma^{-1} T\\), and thus the norm of \\(T\\) associated with \\(\\Sigma^{-1}\\) is \\(\\|T\\|_\\Sigma^2 = T^T \\Sigma^{-1} T\\). Show that the explicit form of the generalized least squares estimator \\(\\hat{\\beta}_G\\) defined as the minimizer of \\(\\|Y - X\\beta\\|_\\Sigma\\) is \\[\\hat{\\beta}_G = (X^T \\Sigma^{-1} X)^{-1} X^T \\Sigma^{-1} Y\\]\nDeduce its expectation and its variance.\nShow that the covariance matrix between \\(\\hat{\\beta}\\) and \\(\\hat{\\beta}_G\\) is equal to the variance-covariance matrix of \\(\\hat{\\beta}_G\\). Deduce that \\(\\hat{\\beta}_G\\) is better than \\(\\hat{\\beta}\\) in the sense of quadratic cost."
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Tests provide the theoretical basis for decision-making based on data. For example, doctors diagnose diseases using specific biological markers, industrial quality engineers evaluate the quality of a production batch, and climate scientists determine whether there are significant changes in measurements compared to the pre-industrial era.\n\n\nHypothesis testing is a key statistical method that enables professionals to make informed decisions. The process involves setting up two competing hypotheses:\n\nthe null hypothesis \\(H_0\\), which assumes no effect or no difference. This is usually an a priori on the data.\nthe alternative hypothesis \\(H_1\\), which suggests that there is a significant effect or difference.\n\nLet’s illustrate this with the example of a doctor diagnosing a disease, such as diabetes, using blood glucose levels:\n\nThe doctor begins by assuming the null hypothesis \\(H_0\\): the patient’s blood glucose levels are normal, indicating no diabetes.\nThe alternative hypothesis (\\(H_1\\)) would suggest that the patient’s glucose levels are abnormally high, indicating diabetes.\n\nAfter collecting the patient’s blood glucose data, the doctor compares it to a standard threshold for diagnosing diabetes. A hypothesis test is then performed to evaluate whether the observed glucose level is significantly higher than the threshold, or whether the difference could simply be due to random variation.\nThe testing process can be summarized as follows:\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests?\n\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)\n\n\n\n\nIn general, observing \\(T=1\\) is just one piece of evidence supporting \\(H_1\\). It’s possible that we were simply unlucky and the data led the test to reject \\(H_0\\). Additionally, the sample used might not be representative, or the elevated glucose level could be caused by another condition, not diabetes. Therefore, we cannot blindly accept \\(H_1\\) based on this result alone. Further investigation is essential when \\(T=1\\) to minimize the risk of making a Type I error.\nThe same applies when \\(T=0\\). A result of \\(T=0\\) simply indicates that the test provides no evidence in favor of \\(H_1\\), but it doesn’t mean that \\(H_1\\) is false. It is possible that the patient exhibits other symptoms that could still suggest a diagnosis of diabetes, despite the lack of evidence from the test.\n\n\n\n\n\n\nObjective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?\n\nIn the above example, we formally observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\) Imagine that instead of ten \\(6\\), we observed \\(600\\) tosses leading to the following counts:\n\nThe number of \\(6\\) is really close to \\(100 = 600/6\\), which implies that we would not reject \\(H_0\\) in the previous example. We would reject however if we wanted to test whether the dice is fair, since it is highly biased toward \\(2\\).\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_1\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_0\\) is “unlikely”)\n\n\n\n\n\n\n\n\nSee also the (Pluto notebook: illustration of pvalue)\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\) (“\\(95^{th}\\) percentile”)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Hypothesis testing is a key statistical method that enables professionals to make informed decisions. The process involves setting up two competing hypotheses:\n\nthe null hypothesis \\(H_0\\), which assumes no effect or no difference. This is usually an a priori on the data.\nthe alternative hypothesis \\(H_1\\), which suggests that there is a significant effect or difference.\n\nLet’s illustrate this with the example of a doctor diagnosing a disease, such as diabetes, using blood glucose levels:\n\nThe doctor begins by assuming the null hypothesis \\(H_0\\): the patient’s blood glucose levels are normal, indicating no diabetes.\nThe alternative hypothesis (\\(H_1\\)) would suggest that the patient’s glucose levels are abnormally high, indicating diabetes.\n\nAfter collecting the patient’s blood glucose data, the doctor compares it to a standard threshold for diagnosing diabetes. A hypothesis test is then performed to evaluate whether the observed glucose level is significantly higher than the threshold, or whether the difference could simply be due to random variation.\nThe testing process can be summarized as follows:\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Decision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)\n\n\n\n\nIn general, observing \\(T=1\\) is just one piece of evidence supporting \\(H_1\\). It’s possible that we were simply unlucky and the data led the test to reject \\(H_0\\). Additionally, the sample used might not be representative, or the elevated glucose level could be caused by another condition, not diabetes. Therefore, we cannot blindly accept \\(H_1\\) based on this result alone. Further investigation is essential when \\(T=1\\) to minimize the risk of making a Type I error.\nThe same applies when \\(T=0\\). A result of \\(T=0\\) simply indicates that the test provides no evidence in favor of \\(H_1\\), but it doesn’t mean that \\(H_1\\) is false. It is possible that the patient exhibits other symptoms that could still suggest a diagnosis of diabetes, despite the lack of evidence from the test.",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Objective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?\n\nIn the above example, we formally observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\) Imagine that instead of ten \\(6\\), we observed \\(600\\) tosses leading to the following counts:\n\nThe number of \\(6\\) is really close to \\(100 = 600/6\\), which implies that we would not reject \\(H_0\\) in the previous example. We would reject however if we wanted to test whether the dice is fair, since it is highly biased toward \\(2\\).\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_1\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_0\\) is “unlikely”)\n\n\n\n\n\n\n\n\nSee also the (Pluto notebook: illustration of pvalue)\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\) (“\\(95^{th}\\) percentile”)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#section",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\n\n\nTest",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#type-of-problems",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#type-of-problems",
    "title": "Hypothesis Testing",
    "section": "Type of Problems",
    "text": "Type of Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\n\n\nMultiple VS Multiple",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nParametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nNon-parametric: otherwise\n\n\nExample of multiple VS multiple parametric problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\nDecision rule\n\n\n\n\nA decision rule or test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\n\n\n\n\n\n\n\n\nTest statistic\n\n\n\n\na test statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nExample of \\(\\mathcal R\\), for a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) \\not \\in [t_1, t_2]\\}:~~~~~~\\mathcal R = (-\\infty,t_1)\\cup (t_2, +\\infty)\\;\n\\end{aligned}\n\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#continous-measures",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#continous-measures",
    "title": "Hypothesis Testing",
    "section": "Continous Measures",
    "text": "Continous Measures\n\ndensity wrp to Lebesgue: \\(dP(x) = p(x)dx\\)\nPDF (proba density function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha} = \\alpha)\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#discrete-measures",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#discrete-measures",
    "title": "Hypothesis Testing",
    "section": "Discrete Measures",
    "text": "Discrete Measures\n\ndensity wrp to counting measure: \\(P(X=x) = p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\nGaussian/Bernoulli\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)\n\n\n\n\n\nExponential/Geometric\n\n\n\nExponential \\(\\mathcal E(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)\n\n\n\n\n\nGamma/Poisson\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\]\n\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\lambda\\)\n\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\n\nNumber of tics before time \\(1\\) of an atomic clock of rate \\(\\lambda\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\)?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nEquivalently, we can consider the log-likelihood ratio test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#neyman-pearson",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#neyman-pearson",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson",
    "text": "Neyman Pearson\n\n\n\n\n\n\nNeyman Pearson’s theorem\n\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\nProof of Neyman Pearson’s theorem\nWe prove the theorem in the case where \\(P\\) and \\(Q\\) each have a density \\(p\\) and \\(q\\) on \\(\\mathbb R^n\\). For any \\(t &gt; 0\\), define \\[I_t(P, Q) = \\int_{x \\in \\mathbb R^n} |q(x) - tp(x)|dx \\; .\\]\nWhen \\(t=1\\), this quantity is equal to the total-variation distance between \\(P\\) and \\(Q\\). For any event A in \\(\\mathbb R^n\\) , it holds that\n\\[\n\\begin{split}\n  I_t(P, Q) &= \\int_{x \\in \\mathbb R^n} |q(x) - tp(x)|dx \\\\\n  &= \\int_{q(x)&gt; tp(x)} q(x) - tp(x)dx + \\int_{q(x)&lt; tp(x)} tp(x) - q(x)dx\\\\\n  &=  2\\int_{x \\in \\mathbb R^n} \\mathbf1_{q(x)&gt; tp(x)}(q(x) - tp(x))dx + t-1 \\\\\n  &\\geq 2\\int_{x \\in \\mathbb R^n} \\mathbf 1_A \\mathbf1_{q(x)&gt; tp(x)}(q(x) - tp(x))dx + t-1\\\\\n  &\\geq t-1 + 2(Q(A) - tP(A)) \\; .\n\\end{split}\n\\]\nIf \\(A  = \\{x \\in \\mathbb R^n : q(x) &gt; tp(x)\\}\\), then the two last inequalities are equalities. In particular, \\[I_t(P, Q) = t-1 + 2\\sup_{A \\subset \\mathbb R^n}(Q(A) - tP(A))) \\; .\\]\nAssume that the type-1 error of \\(T\\) is smaller than \\(\\alpha\\): \\(P(T=1) \\leq \\alpha\\).\nThe power of \\(T\\), \\(Q(T = 1)\\), is upper-bounded as follows: \\[\n\\begin{split}\n  Q(T=1) &\\leq Q(T=1) + t(\\alpha - P(T=1))\\\\\n  &= \\alpha t + Q(T=1) - tP(T=1) \\\\\n  &\\leq \\alpha t + \\frac{t-1}{2} + \\frac{1}{2}I_t \\; .\n\\end{split}\n\\]\n\nThere is equality in the second inequality if \\(T=1\\) is the event \\(\\{ \\frac{q(X)}{p(X)} &gt; t \\}\\).\nThere is equality in the first inequality if \\(P(T=1) = \\alpha\\).\n\nLet \\(t_{\\alpha}\\) be such that \\(P(\\frac{q(X)}{p(X)}&gt; t_{\\alpha}) = \\alpha\\). The test \\(T^*(X) = \\mathbf1 \\{\\frac{q(X)}{p(X)}&gt; t_{\\alpha}\\}\\) satisfies the two above points, since its rejection event is exactly \\(\\{T^*(X) = 1\\} = \\{\\frac{q(X)}{p(X)}&gt; t_{\\alpha}\\}\\) and since \\(P(T^* = 1) = \\alpha.\\) Hence, for any test \\(T\\) of type 1 error smaller than \\(\\alpha\\), it holds that \\(Q(T = 1) \\leq Q(T^* = 1)\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples-2",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples-2",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\nExample (Gaussians)\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\nLog-likelihood ratio test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\n\n\n\nExample: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\n\n\\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]\n\n\nExample: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): reject if \\(N \\geq 19\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#generalities",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#generalities",
    "title": "Hypothesis Testing",
    "section": "Generalities",
    "text": "Generalities\n\n\\(H_0 = \\{ \\mathcal P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(right-tailed: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(left-tailed: unilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(simple/multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( multiple/multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-tailed test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (right-tailed test).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#pivotal-test-statistic",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#pivotal-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic",
    "text": "Pivotal Test Statistic\n\nConsider \\(\\Theta_0\\) not singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\nPivotal test statistic\n\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#p-value",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#p-value",
    "title": "Hypothesis Testing",
    "section": "P-value",
    "text": "P-value\nSee the (Pluto notebook: Illustration of pvalue)\n\n\n\n\n\n\nP-value: definition\n\n\n\nWe define \\(p_{value}(x_{\\mathrm{obs}}) =\\mathbb P(\\psi(X) &gt; x_{\\mathrm{obs}})\\) for a right-tailed test.\nFor a two-tailed test, \\(p_{value}(x_{\\mathrm{obs}}) =2\\min(\\mathbb P(\\psi(X) &gt; x_{\\mathrm{obs}}),\\mathbb P(\\psi(X) &lt; x_{\\mathrm{obs}}))\\)\n\n\n\n\n\n\n\n\nP-value under \\(H_0\\)\n\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(p_{value}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\nProof.\nThe p-value is a probability, so it belongs to \\([0,1]\\). Let \\(F_{\\psi}\\) be the cumulative distribution function of a random variable \\(\\psi(X)\\) when \\(X\\) follows distribution \\(P\\), that is \\(G_{\\psi}(t) = \\mathbb P(\\psi(X) &gt; t)\\). It holds that \\[\n\\mathbb P(\\psi(X') &gt; \\psi(X) ~|~ \\psi(X)) = F_{\\psi}(\\psi(X))\n\\] If \\(u \\in [0,1]\\), then \\[\n\\begin{aligned}\n\\mathbb P(\\mathrm{pvalue}(X) &gt; u) &= \\mathbb P(F_{\\psi}(\\psi(X)) &gt; u) \\\\\n&= \\mathbb P(\\psi(X) &gt; F_{\\psi}^{-1}(u)) \\\\\n&= 1- F_{\\psi}(F_{\\psi}^{-1}(u)) = 1-u\n\\end{aligned}\n\\] Hence, \\(\\mathrm{pvalue}(X)\\) is uniform when \\(X\\) follows distribution \\(\\mathbb P\\). \\[\\tag*{$\\blacksquare$}\\]\n\n\n\nIn practice: reject if \\(p_{value}(X) &lt; \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\) under \\(H_0\\):",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html",
    "href": "teaching/hypothesis_testing/lectures/dependency.html",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "We observe iid paired data \\((X_1, Y_1), \\dots, (X_n,Y_n)\\) of unknown mean \\(\\mu_X, \\mu_Y\\) and cov matrix \\(\\Sigma\\).\nCov matrix: \\(\\Sigma =\n\\left(\\begin{matrix}\n\\sigma_X^2 & \\mathrm{Cov(X,Y)} \\\\\n\\mathrm{Cov(X,Y)} & \\sigma_Y^2 \\\\\n\\end{matrix}\\right)\\)\n\\(H_0: \\mathrm{Cov}(X,Y)=0\\) or \\(H_1: \\mathrm{Cov}(X,Y)\\neq 0\\)\n\\(\\mathrm{Cov(X,Y)} = \\mathbb E[(X- \\mathbb E[X])(Y- \\mathbb E[Y])]\\)\n\\(\\sigma_X^2 = \\mathrm{Cov(X,X)}\\)\n\\(\\sigma_Y^2 = \\mathrm{Cov(Y,Y)}\\)\n\\(\\mathrm{Cor(X,Y)} = \\frac{\\mathrm{Cov(X,Y)}}{\\sigma_X \\sigma_Y}\\)\n\n\n\n\n\n\n\n\n\n\nPearson’s Correlation Test\n\n\n\n\n\\(r =  \\frac{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline X)^2\\sum_{i=1}^n (Y_i - \\overline Y)^2}}\\)\n\\(\\psi(X,Y) = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\)\nUnder \\(H_0\\), \\(\\psi(X,Y) \\approx \\mathcal T(n-2)\\)\n\n\n\n\n\n\nMonte Carlo Simulation with \\(n=4\\):\n\n\n\n\n\n\n\n\n\n\n\n\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nGroup \\(k\\): \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\). Ex: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\)\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2 \\sim \\sigma^2\\chi^2(N_k-1)\\) (under \\(H_0\\))\nVar of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\sim\\sigma^2\\chi^2(d-1)\\) (under \\(H_0\\))\n\n\n\n\n\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}{\\tfrac{1}{d-1}SSB}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\).\n\n\n\n\n\n\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#correlation-test-1",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#correlation-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "Monte Carlo Simulation with \\(n=4\\):",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#anova-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#anova-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nGroup \\(k\\): \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\). Ex: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\)\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2 \\sim \\sigma^2\\chi^2(N_k-1)\\) (under \\(H_0\\))\nVar of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\sim\\sigma^2\\chi^2(d-1)\\) (under \\(H_0\\))\n\n\n\n\n\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}{\\tfrac{1}{d-1}SSB}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#interpretation-of-variances-in-anova",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#interpretation-of-variances-in-anova",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#chi2-homogeneity-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#chi2-homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Homogeneity Test",
    "text": "\\(\\chi^2\\) Homogeneity Test\n\n\\(d\\) different bags (or groups), each containing balls of \\(m\\) potential colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix of counts:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\n\n\n\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all color \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#example-soft-drink-preferences",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\)\n\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#chi2-independence-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#chi2-independence-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Independence Test",
    "text": "\\(\\chi^2\\) Independence Test\n\n\n\n\n\n\n\nObservations: Paired Categorical Variables \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\ne.g. \\(X_i \\in \\{\\mathrm{male}, \\mathrm{female}\\}\\), \\(Y_i \\in \\{\\mathrm{coffee}, \\mathrm{tea}\\}\\)\nIdea: regroup the data into bags, e.g. \\(\\{\\mathrm{male}, \\mathrm{female}\\}\\)\nBuild the contingency table\nPerform a chi-square homogeneity test\n\n\n\n\nExample of contingency table:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n30\n20\n50\n\n\nTea\n28\n22\n50\n\n\nTotal\n58\n42\n100\n\n\n\nExpected counts:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n29\n21\n50\n\n\nTea\n29\n21\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\nDegree of freedom: \\((2-1)(2-1) = 1\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#symetric-random-variable",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)\n\n\n\n\n\n\n\n\n\n\n\nSymetrization\n\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t)\n&=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy \\\\\n&= \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]\n\\(X\\) indep of \\(Y\\) \\(\\implies\\) \\(X-Y\\) symmetric \\(\\implies\\) \\(\\mathrm{median}(X-Y) = 0\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#dependency-problem-for-paired-data",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The median of \\((X_i - Y_i)\\) is \\(0\\) for all \\(i\\)\n\\(H_1:\\) The median of \\((X_i - Y_i)\\) is not \\(0\\) for some \\(i\\)\n\n\n\n\n\n\n\n\n\n\nGenerality of \\(H_0\\)\n\n\n\n\nIf \\(X_i-Y_i\\) is symmetric \\(i\\), then we are under \\(H_0\\).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\).\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\)).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\n\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]\n\n\n\n\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\n\nUnder \\(H_0\\),\n\nSigns of the \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &gt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\nif \\(H_1\\): \\(\\mathrm{median}(D_i) &gt; 0\\): left-one sided test on \\(W_-\\).\n\n\n\nThis approximation fits well the exact distribution. Monte-Carlo simulation:\n\nTo generate a \\(W_-\\) under \\(H_0\\) in Julia:\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#effect-of-drug-on-blood-pressure",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#effect-of-drug-on-blood-pressure",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Effect of Drug on Blood Pressure",
    "text": "Effect of Drug on Blood Pressure\n\n\\(H_0\\): the drug has no effect. \\(H_1\\): it lowers the blood pressure\n\n\n\n\nPatient\n\\(X_i\\) (Before)\n\\(Y_i\\)​ (After)\n\\(D_i = X_i-Y_i\\)​\n\\(R_i\\)\n\n\n\n\n1\n150\n140\n10\n6 (+)\n\n\n2\n135\n130\n5\n5 (+)\n\n\n3\n160\n162\n-2\n2 (-)\n\n\n4\n145\n146\n-1\n1 (-)\n\n\n5\n154\n150\n4\n4 (+)\n\n\n6\n171\n160\n11\n7 (+)\n\n\n7\n141\n138\n3\n3 (+)\n\n\n\n\n\\(W_- = 1+2 = 3\\).\nFrom a simulation, we approx \\(\\mathbb P(W_-=i)\\), for \\(i \\in \\{0, 1, 2, 3, 4, 5,6\\}\\) under \\(H_0\\) by\n\n[0.00784066, 0.00781442, 0.00781534, 0.01563892, 0.01562184, 0.02343478]\n\nFrom a simulation, \\(p_{value}=\\mathbb P(W_- \\leq 3) \\approx 0.039 &lt; 0.05\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "",
    "text": "Note sur la correction\n\n\n\nComme répété à plusieurs reprises en cours, la qualité de la rédaction ainsi que les détails d’explications sont grandement pris en compte dans l’évaluation, surtout pour les introductions des modèles (premières questions des exercices)."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables",
    "text": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables\nNous cherchons à déterminer si les citoyens d’une région ont une quelconque préférence pour les sources d’énergie renouvelables (par exemple, solaire, éolienne) ou les sources d’énergie non renouvelables (par exemple, charbon, gaz naturel). Nous supposons que, a priori, il n’y a aucune préférence en moyenne. Nous interrogeons \\(n\\) individus, et nous notons \\(X\\) le nombre de répondants qui préfèrent les énergies renouvelables."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#questions",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#questions",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions :",
    "text": "Questions :\n\nFormalisez le problème de test d’hypothèse, et définissez \\(H_0\\) et \\(H_1\\)​. Indiquez si ce test est unilatéral ou bilatéral.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\(X\\) le nombre d’individus qui préfèrent les ENR.\nOn suppose que \\(X\\) suit une loi \\(Bin(n,p)\\), où \\(p\\) est un paramètre inconnu.\nOn considère le problème de test: \\(H_0: p=1/2\\) VS \\(H_1: p\\neq 1/2\\)\n\nC’est un problème de test bilatéral\n\n\n\nNous interrogeons \\(n=100\\) individus, et \\(X=58\\) préfèrent les sources d’énergie renouvelables. Écrivez la p-valeur en fonction de \\(F\\), la fonction de répartition d’une distribution Binomiale Bin\\((100,0.5)\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\nSous \\(H_0\\), \\(X\\) suit une loi \\(Bin(100, 0.5)\\) de cdf \\(F\\). Ainsi, \\(p_{valeur} = 2\\min(\\mathbb P(X \\leq 58), \\mathbb P(X \\geq 58)) =  2\\mathbb P(X \\geq 58) = 2(1- F(57))\\).\nd’où \\(p_{valeur} = 2(1- F(57))\\).\n(58 accepté aussi car \\(\\mathbb P(X=58)\\) est petit)\n\n\n\nÉcrivez une ligne de code qui calculerait la p-valeur exacte en Julia, Python, ou R.\n\n2*(1-cdf(Binomial(100, 0.5), 57)) # Julia\n2*(1-pbinom(57,100,0.5)) # R\n# Résultat: 0.133\n\nDonnez une approximation de la p-valeur en utilisant une approximation gaussienne et le graphique de la fonction de répartition de \\(\\mathcal N(0,1)\\) fourni dans le sujet. Quelle est votre conclusion ?\n\n\n\n\n\n\n\nNote\n\n\n\nSous \\(H_0\\), \\(\\mathbb E_0[X]=n/2\\) et \\(\\mathbb V_0(X)=n\\frac{1}{2}(1-\\frac{1}{2})=n/4\\)\n\nOn renormalise \\(X\\) pour obtenir la statistique de test suivante: \\[\\psi(X) = \\frac{X-\\mathbb E_0[X]}{\\sqrt{\\mathbb V_0(X)}}=\\frac{X-n/2}{\\sqrt{n/4}}\\]\nLorsque \\(n \\to \\infty\\), \\(\\psi(X)\\) converge en loi vers \\(\\mathcal N(0,1)\\). Ainsi, on obtient suite à cette approximation Gaussienne (\\(n/2\\) est assez grand):\n\\[\np_{valeur} = 2\\mathbb P(\\psi(X) \\geq \\psi(X_{obs})) \\asymp 2\\mathbb P(Z \\geq \\psi(58)),\n\\] où \\(Z\\) est une VA qui suit une loi \\(\\mathcal N(0,1)\\) sous \\(\\mathbb P\\). On calcule \\(\\psi(58) \\asymp 1.6\\), et par lecture graphique, \\(2*P(Z \\geq \\psi(58)) \\asymp 0.11\\)\nLa \\(p_{valeur}\\) est grande, on ne donc rejette pas à un niveau de \\(5\\%\\).\n\n\n\nRedéfinissez l’hypothèse alternative \\(H_1\\)​ et calculez une p-valeur approximative si nous cherchons à déterminer si les citoyens ont une préférence pour :\n\nles sources d’énergie renouvelables.\nles sources d’énergie non renouvelables. Quelles sont vos conclusions pour ces deux autres problèmes ?\n\n\n\n\n\n\n\n\nRéponse\n\n\n\nLes problèmes de test deviennent unilatéraux.\n\n\\(H_1: p &gt; 0.5\\), \\(p_{valeur} \\asymp 0.11/2 \\asymp 0.055\\) on pourrait rejeter à un niveau \\(10\\%\\)\n\\(H_1: p &lt; 0.5\\) , \\(p_{valeur} \\asymp (1-0.11)/2 \\asymp 0.445\\), on ne rejette pas."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 2 : Surveillance environnementale de la pollution fluviale",
    "text": "Exercice 2 : Surveillance environnementale de la pollution fluviale\nUne agence environnementale surveille les niveaux de pollution d’une rivière pour déterminer si une usine à proximité provoque une augmentation de la concentration de produits chimiques nocifs. La concentration cible pour un produit chimique spécifique est de \\(15 \\, \\text{ppm}\\) (parties par million), ce qui est considéré comme sûr pour la vie aquatique. Pour un échantillon de \\(n = 20\\) prélèvements d’eau effectués en aval de l’usine, la concentration moyenne empirique est \\(\\bar{X}_n = 16,3 \\, \\text{ppm}\\), et la variance empirique est \\(S^2_n = 2,4 \\, \\text{ppm}^2\\).\nA priori, on suppose que la rivière respecte le seuil de pollution sans danger de \\(15 \\, \\text{ppm}\\).\nNous visons à tester, avec un niveau de signification \\(\\alpha = 0,05\\), si la concentration chimique en aval dépasse le seuil de sécurité, indiquant une pollution provenant de l’usine.\n\nQuestions :\n\nEn utilisant une hypothèse Gaussienne, formalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\). S’agit-il d’un test unilatéral ou bilatéral ?\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\((X_1, \\dots, X_n)\\) les concentrations des prélèvement en ppm (toujours bien de mettre les unités)\nOn suppose que les \\(X_i\\) sont iid de loi \\(\\mathcal N(\\mu, \\sigma^2)\\) où \\(\\mu\\) et \\(\\sigma^2\\) sont des paramètres inconnus\nOn souhaite tester s’il y a un problème de pollution, cad\n\\(H_0: \\mu = 15\\) ppm VS \\(H_1: \\mu &gt; 15\\) ppm\n\nC’est un problème de test unilatéral droit (\\(\\mu \\leq 15\\) ppm aussi accepté pour \\(H_0\\))\n\n\n\nDéfinissez la statistique de test. Quelle est sa distribution sous \\(H_0\\) ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn utilise la statistique de test de Student \\[\\psi(X) = \\sqrt{n}\\frac{\\overline X - 15}{\\hat \\sigma},\\]\noù \\(\\overline X=\\frac{1}{n}\\sum_{i=1}^n X_i\\) et \\(\\hat \\sigma^2 = \\tfrac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline X)^2\\) est un estimateur non biaisé de \\(\\sigma^2\\) sous \\(H_0\\). Comme les \\(X_i\\) sont iid de loi normale, \\(\\psi(X)\\) suit une loi de student \\(\\mathcal T(n-1)\\) sous \\(H_0\\). Ce n’est pas une approximation ici, on peut même dire que \\(\\psi\\) est une statistique de test pivot\n\n\n\nDéterminez la zone de rejet. Vous pouvez utiliser une approximation Gaussienne et le graphe de l’exercice 1.\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn souhaite tester (à droite) au niveau \\(0.05\\). Ainsi, on rejette si \\(\\psi(X) \\geq t_{0.95}\\), où \\(t_{0.95}\\) est le quantile \\(0.95\\) de \\(\\mathcal T(n-1)\\)\nLorsque \\(n\\) tend vers \\(+\\infty\\), \\(\\mathcal T(n-1)\\) converge en loi vers \\(\\mathcal N(0,1)\\). On approxime donc \\(t_{0.95}\\) par le quantile de la loi Gaussienne (inverse de la cdf sur le graphe), d’où \\(t_{0.95} \\asymp 1.6\\)\nOn rejette si \\(\\psi(X) \\geq 1.6\\)\n\n\n\nEcrivez une ligne de code permettant de calculer le seuil de rejet exact.\n\nquantile(TDist(19), 0.95) # julia\nqt(0.95,n) # R, (1.73 légèrement plus grand que 1.6)\n\nLa rivière présente-t-elle une concentration chimique accrue qui pourrait indiquer une pollution provenant de l’usine ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn calcule \\(\\psi(X_{\\mathrm{obs}}) = 3.75\\), on rejette donc \\(H_0\\), la rivière est peut être polluée"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux",
    "text": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux\nUn chercheur en faune sauvage étudie le comportement d’une certaine espèce d’oiseaux qui migrent vers une réserve naturelle. Le chercheur a une hypothèse sur la façon dont les oiseaux se répartissent entre différents types d’habitats dans la réserve. La distribution attendue, basée sur des données historiques, est la suivante :\n\nPrairie : 40%\nZones humides : 30%\nForêts : 20%\nZones rocheuses : 10%\n\nPour tester cette hypothèse, le chercheur observe 200 oiseaux et enregistre leurs préférences d’habitat. Les comptages observés sont les suivants :\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé\n90\n60\n30\n20\n\n\n\n\nQuestions :\n\nFormalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\((X_1, X_2,X_3,X_4)\\) les effectifs d’oiseaux présent respectivement dans les Prairie,…, zones rocheuses.\nOn suppose que le vecteur \\((X_1, X_2,X_3,X_4)\\) suit une loi multinomiale de paramètre \\(n=200\\) et \\(q=(q_1,q_2,q_3,q_4)\\) inconnu\nOn veut tester si les observations correspondent à la distribution attendue, c’est à dire \\(H_0: q=(0.4,0.3,0.2,0.1)\\) VS \\(H_1: q\\neq (0.4,0.3,0.2,0.1)\\) Ce problème correspond au test d’adéquation du Chi2 (Goodness of fit).\n\n\n\n\nCalculez les effectifs attendus.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé (\\(X_i\\))\n90\n60\n30\n20\n\n\nAttendus (\\(E_i\\))\n80\n60\n40\n20\n\n\n\n\n\n\nCalculez la statistique du chi-deux.\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\psi(X) = \\sum_{i=1}^4 \\frac{(X_i - E_i)^2}{E_i} = 100/80 + 100/40 = 3.75\\)\n\n\n\nDéterminez le degré de liberté \\(df\\) de la statistique du chi-deux, et lisez la p-value sur le graphique suivant de la fonction de répartition.\n\n\n\n\n\n\n\nNote\n\n\n\nLorsque \\(n \\to +\\infty\\), \\(\\psi(X)\\) converge en loi vers une loi de \\(\\chi^2\\) de degré \\(df=3\\). On lit pvaleur = 1-cdf(Chisq(df), 3.75)=0.3\n\n\n\nQuelle est votre conclusion ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nLa \\(p_{valeur}\\) est assez grande, la distribution colle avec celle attendue et on ne rejette donc pas \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-4",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 4",
    "text": "Exercice 4\n\nProductivité des employés entre départements\nUne entreprise souhaite évaluer si un nouveau style de management a eu un effet uniforme sur la productivité des employés dans cinq départements. Chaque département a adopté une variation spécifique du style de management pendant trois mois, et l’entreprise a enregistré le nombre moyen de tâches accomplies par employé durant cette période.\nDonnées :\n\n\n\nDépartement\n1\n2\n3\n4\n5\n\n\n\n\nNombre d’employés\n12\n10\n8\n9\n11\n\n\nMoyenne des tâches accomplies\n72,4\n68,9\n75,6\n74,3\n69,7\n\n\nVariance des tâches\n8,5\n9,2\n10,1\n7,8\n9,6\n\n\n\nL’entreprise cherche à comprendre si les niveaux de productivité varient significativement entre les départements, indiquant que les styles de management pourraient avoir des impacts différents.\nSoit \\(d=5\\) le nombre de départements et \\(N_{\\text{tot}} = 50\\) le nombre total d’employés. Pour tout département \\(j\\), nous notons \\(N_k\\) le nombre d’employés dans le département \\(k\\), et \\(P_{ik}\\) le nombre de tâches accomplies par l’employé \\(i\\) dans le département \\(k\\). Nous supposons que les \\(P_{ik}\\) sont indépendants et suivent une distribution normale de moyenne \\(\\mu_k\\) et de variance \\(\\sigma^2\\).\nNous écrivons \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDéfinissez les hypothèses du problème pour tester si les styles de management ont eu un impact uniforme sur la productivité.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nDans le département \\(k\\), on observe \\(P_{ik}\\) le nombre de tâche accompli par l’employé \\(i\\)\nOn suppose que les \\(P_{ik}\\) sont indépendants, et que \\(P_{ik}\\) suit une \\(\\mathcal N(\\mu_k, \\sigma^2)\\) où \\((\\mu_1, \\dots, \\mu_5)\\) et \\(\\sigma^2\\) sont des paramètres inconnus.\nProblème de test: \\(H_0\\): \\(\\mu_1 = \\dots = \\mu_5\\) VS \\(H_1\\): \\(\\exists k,l\\) tels que \\(\\mu_k \\neq \\mu_l\\)\n\n\n\n\nDonnez une brève interprétation de chacune des quantités \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\n\n\n\n\n\n\n\nNote\n\n\n\nCf cours\n\n\n\nDémontrez la formule d’analyse de la variance : \\(V_T = V_W + V_B\\)\n\n\n\n\n\n\n\nNote\n\n\n\nExercice à refaire\n\n\n\nCalculez \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), et \\(V_T\\).\n\n\n\n\n\n\n\nNote\n\n\n\nOn applique les formules…\n\\(\\overline P=71.96\\)\n\\(V_W = 9.01\\)\n\\(V_B=6.15\\)\n\\(V_T=15.16\\)\n\n\n\n\nExprimez la statistique de test ANOVA en termes de \\(V_W\\) et \\(V_B\\).\nQuelles sont les distributions de \\(N_k V_k\\) et de \\(N_{\\mathrm{tot}}V_W\\) sous \\(H_0\\) ? Changent-elles sous \\(H_1\\) ?\n\n\n\n\n\n\n\nNote\n\n\n\n\\(N_kV_k\\) suit une loi \\(\\sigma^2\\chi^2(N_k-1)\\).\nAinsi, \\(N_{\\mathrm{tot}}V_W/\\sigma^2\\) suit une loi \\(\\chi^2\\) de degré \\(\\sum_{k=1}^5(N_k-1) = N_{tot}-5\\) Ces lois ne changes pas sous \\(H_1\\)! C’est en fait la loi de la variance interclasses \\(V_B\\) qui va changer sous \\(H_1\\).\n\n\n\nRappelez la définition de la statistique de test ANOVA, donnez sa distribution \\(\\mathcal D\\) sous \\(H_0\\) et effectuez le test ANOVA au niveau \\(\\alpha =0,05\\). On donne les quantiles \\(0.05\\) et \\(0.95\\) de \\(\\mathcal D\\): \\(0.18\\) and \\(2.58\\). Concluez si la productivité diffère significativement entre les départements.\n\n\n\n\n\n\n\nRéponse\n\n\n\nLa statistique ANOVA se définit comme \\(\\psi((P_{ik})) = \\frac{N_k-1}{d-1}\\frac{V_B}{V_W}\\). Ici, on calcule \\(\\psi((P_{ik}))=7.7\\). Comme \\(7.7 &gt; 2.5\\) (On regarde le quantile à droite), on rejette \\(H_0\\) au niveau \\(0.05\\). La productivité n’est pas homogène entre les départements."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#questions-de-cours",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#questions-de-cours",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions de cours",
    "text": "Questions de cours\n\nRappelez la définition d’une statistique de test \\(\\psi\\) et d’un test (ou règle de décision) \\(T\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nUne statistique de test \\(\\psi\\) est une fonction mesurable des donnée à valeurs réelles et qui ne dépend pas des paramètres inconnus du modèle.\nUne règle de décision \\(T\\) a la même définition, sauf qu’elle prend ses valeurs dans \\(\\{0,1\\}\\) (\\(0\\) correspond à conserver \\(H_0\\) et \\(1\\) correspond au rejet de \\(H_0\\))\n\n\n\n\nQuels sont les deux types d’erreur que nous pouvons commettre ?\n\n\n\n\n\n\n\nNote\n\n\n\n\nL’erreur de type 1: on rejette \\(H_0\\) alors qu’elle est vraie\nL’erreur de type 2: on “accepte” \\(H_0\\) alors qu’elle est fausse Ne pas écrire de probabilité, à moins de préciser le cadre! (exemple: \\(H_0\\) est simple)\n\n\\(\\mathbb P_{H_0}(X \\in A)\\) n’a aucun sens si \\(H_0\\) est multiple. C’est quoi \\(\\mathbb P_{H_0}\\)??\n\n\n\nPour une statistique de test \\(\\psi\\) donnée et un problème de test bilatéral, rappelez la définition de la p-valeur.\n\n\n\n\n\n\n\nRéponse\n\n\n\nSoit un problème de test où \\(H_0\\) est simple. Soit \\(X\\) la variable aléatoire, et \\(X_{obs}\\) une observation (c’est formellement une réalisation de \\(X\\))\nPour un problème de test bilatéral et une statistique de test \\(\\psi\\) donnée, la pvaleur s’écrit \\(2*\\min(\\mathbb P(\\psi(X) \\geq \\psi(X_{\\mathrm{obs}})), \\mathbb P(\\psi(X) \\leq \\psi(X_{\\mathrm{obs}})))\\)\n\n\n\nÉnoncez le théorème de Neyman-Pearson.\n\n\n\n\n\n\n\nRéponse\n\n\n\nDans le cas d’un problème simple VS simple, le rapport de vraissemblance est optimal pour un niveau de test \\(\\alpha\\) fixé, au sens où aucun test de niveau \\(\\alpha\\) ne peut avoir une plus grande puissance. Cf cours"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#definition-and-link-with-clt",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#definition-and-link-with-clt",
    "title": "Gaussian Populations",
    "section": "Definition and link with CLT",
    "text": "Definition and link with CLT\nWe observe \\((X_1, \\dots, X_n)\\) iid real valued random variables.\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n X_i\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when } n \\to \\infty \\]\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\nEquality when \\(X_i\\)’s are Gaussian \\(\\mathcal N(\\mu, \\sigma^2)\\), that is \\[\n\\mathbb P(X_1 \\in [x,x+dx]) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Known Variance",
    "text": "Testing Mean with Known Variance\n\n\\(X = (X_1, \\dots, X_n)\\), iid with distribution \\(\\mathcal N(\\mu, \\sigma^2)\\).\n\n\n\n\n\n\n\nTest problems\n\n\n\\[\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTest statistic: \\[ \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\\]\n\\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nQ1, Q2\n\n\n\n\n\n\nTests\n\n\n\\[\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{(left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "Why 0.05 and 1.96 ?",
    "text": "Why 0.05 and 1.96 ?\n\n\n\n\n\nFisher’s Quote\n\n\nThe value for which \\(p=0.05\\), or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Unknown Variance",
    "text": "Testing Mean with Unknown Variance\n\nMultiple VS multiple test problem: \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n\\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\) no longer test statistic.\nIdea: replace \\(\\sigma\\) by its estimator \\[ \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\mu_0)^2} \\; .\\]\nThis gives \\[\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\\]\nIs \\(\\psi(X)\\) pivotal under \\(H_0\\) ? What is its distribution ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "Chi-Square and Student Distributions",
    "text": "Chi-Square and Student Distributions\n\n\n\n\n\n\nChi-squared distribution \\(\\chi^2(k)\\)\n\n\n\nDistrib of \\(\\sum_{i=1}^k Z_i^2\\) where the \\(Z_i\\)’s are iid \\(\\mathcal N(0,1)\\).\n\\(\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\\)\n\\(k\\): degree of freedom\n\\(\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\n\nStudent distribution \\(\\mathcal T(k)\\)\n\n\n\nDistrib of \\(\\tfrac{Z}{\\sqrt{U/k}}\\) where \\(Z\\), \\(U\\) are independent and follow resp. \\(\\mathcal N(0,1)\\) and a \\(\\chi^2(k)\\)\n\\(k\\): degree of freedom\n\\(\\mathcal T(k) \\sim \\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\nAssume \\(X_i\\) are iid \\(\\mathcal N(\\mu_0, \\sigma^2)\\).\n\nThe test statistic \\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma}\\) pivotal (indep. of \\(\\sigma\\)).\nIt follows a Student Distribution \\(\\mathcal T(n-1)\\).\n\n\n\n\n\nProof idea: \\(\\overline X \\cdot (1, \\dots, 1)\\) and \\((X_1 - \\overline X, \\dots, X_n - \\overline X)\\) are orthogonal in \\(\\mathbb R^n\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "Student T-Test",
    "text": "Student T-Test\n\nMultiple VS multiple test problem \\(X=(X_1, \\dots, X_n)\\): \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n(Student) T-test statistic: \\[\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variance-unknown-mean",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variance-unknown-mean",
    "title": "Gaussian Populations",
    "section": "Testing Variance, Unknown Mean",
    "text": "Testing Variance, Unknown Mean\n\n\n\n\nWe observe \\(X=(X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu, \\sigma^2)\\). \\(\\mu\\), \\(\\sigma\\) are unknown. \\(\\sigma_0\\) is fixed and known.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(H_0\\): \\(\\sigma \\leq \\sigma_0\\), \\(H_1\\): \\(\\sigma &gt; \\sigma_0\\)\n\\(\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum_{i=1}^n (X_i - \\overline X)^2\\) Wooclap\n\\(T(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\\)\n\\(q_{1-\\alpha}\\): quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\\(H_0\\): \\(\\sigma \\geq \\sigma_0\\), \\(H_1\\): \\(\\sigma &lt; \\sigma_0\\)\n\\(\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum_{i=1}^n (X_i - \\overline X)^2\\)\n\\(T(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\\)\n\\(q_{\\alpha}\\): quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\) are known, \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nTest problem: \\(H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\\)\nIdea: normalize \\(\\overline X - \\overline Y\\): \\[\n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\\]\nTwo-tailed test for testing means: \\[\nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\\]\n\\(t_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of a Gaussian distribution"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example",
    "title": "Gaussian Populations",
    "section": "Example",
    "text": "Example\n\nObjective. Test if a new medication is efficient to lower cholesterol level\nExperiment.\n\nGroup A: \\(n_A = 45\\) patients receiving the new medication\nGroup B: \\(n_B = 50\\) patients receiving a placebo\n\nTest Problem.\n\nWe observe \\((X_1, \\dots, X_{n_A})\\) iid \\(\\mathcal N(\\mu_A,\\sigma^2)\\) and \\((Y_1, \\dots, Y_{n_B})\\) iid \\(N(\\mu_B,\\sigma^2)\\) the chol levels. \\(\\sigma = 8\\) mg/dL is known from calibration.\n\\(H_0: \\mu_A = \\mu_B\\) VS \\(H_1: \\mu_A &lt; \\mu_B\\)\n\nTest Statistic. \\(\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2}}}\\)\nData. \\(\\overline X = 24.5\\) mg/dL and \\(\\overline Y = 21.3\\) mg/dL. Hence \\(\\psi(X,Y)= 5.5\\).\nConclusion. Do not reject, and do not use this medication!"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe \\((X_1, \\dots, X_{n})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nVariance Testing Problem: \\[\nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\\]\nF-Test Statistic of the Variances (ANOVA) \\[\n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher distribution \\(\\mathcal F(k_1,k_2)\\)\n\n\n\nDistribution of \\(\\frac{U_1/k_1}{U_2/k_2}\\), where \\(U_1\\), \\(U_2\\) are indep. and follow \\(\\chi^2(k_1)\\), \\(\\chi^2(k_2)\\). wiki Wooclap\n\\(\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right)\\) when \\(k_1,k_2 \\to +\\infty\\)\n\\((k_1, k_2)\\): degrees of freedom\nExample: \\(\\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1)\\) if \\(Z_i \\sim \\mathcal N(0,1)\\)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2}\\) is independent of \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1\\), \\(\\sigma_2\\). It is pivotal\nIt follow distribution \\(\\mathcal F(n_1-1, n_2-1)\\)\n\n\n\n\n\n\n\n\nTwo-tailed test: \\[ \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown, but we know that \\(\\sigma_1=\\sigma_2\\)\nEquality of mean testing problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}\\).\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\\(\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\\)\nNormalize \\(\\overline X - \\overline Y\\): \\[\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\\]\n\\(\\psi(X,Y)\\) is pivotal because \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nEquality of Mean Testing Problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}\\).\n\n\n\n\n\nStudent Welch test statistic\n\n\n\\[\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\\]\n\nWooclap \\(\\psi(X,Y)\\) is not pivotal\nGaussian approximation: \\(\\psi(X,Y) \\approx \\mathcal N(0,1)\\) when \\(n_1, n_2 \\to \\infty\\)\nBetter approximation: Student Welch"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \\]\nEquality when \\(X_i\\)’s are \\(\\mathcal N(\\mu, \\sigma^2)\\)\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\n\n\n\nExample: binomials\n\n\n\nIf \\(p \\in (0,1)\\)\n\\(\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1)\\) when \\(n \\to \\infty\\)\n\\(n\\) should be \\(\\gg \\frac{1}{p}\\)\n\n\n\n\n\n\n\n\n\nGood Approx for (\\(n=100\\), \\(p=0.2\\))\n\n\n\n\nBad Approx for (\\(n=100\\), \\(p=0.01\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe \\(X \\sim Bin(n_1, p_1)\\) and \\(Y \\sim Bin(n_2, p_2)\\).\n\\(n_1\\), \\(n_2\\) are known but \\(p_1\\), \\(p_2\\) are unknown in \\((0,1)\\)\n\\(H_0\\): \\(p_1 = p_2\\) or \\(H_1\\): \\(p_1 \\neq p_2\\)\n\n\n\n\n\nTest Statistic\n\n\n\\[ \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\\]\n\n\\(\\hat p_1 = X/n_1\\), \\(\\hat p_2 = Y/n_2\\)\n\\(\\hat p = \\frac{X+Y}{n_1+n_2}\\) [Wooclap]\nIf \\(np_1, np_2 \\gg 1\\): \\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nWe reject if \\(|\\psi(X,Y)| \\geq t_{1-\\alpha/2}\\) (gaussian quantile)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion: “should we raise taxes on cigarettes to pay for a healthcare reform ?”\n\\(p_1\\), \\(p_2\\): proportion of non-smokers or smokers willing to raise taxes\n\\(H_0\\): \\(p_1=p_2\\) or \\(H_1\\): \\(p_1 &gt; p_2\\)\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\(\\hat p_1 = \\overline X= \\approx 0.58\\), \\(\\hat p_2=\\overline Y= \\approx 0.21\\).\n\\(\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\\)\n\\(\\mathbb P(\\psi(X,Y) &gt; 8.99)\\) = 1-cdf(Normal(0,1), 8.99)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#multinomials",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#multinomials",
    "title": "Goodness of Fit Tests",
    "section": "Multinomials",
    "text": "Multinomials\n\n\n\n\n\n\nBinomial distribution\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red [Wooclap]\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1), Y=n-X \\sim \\mathrm{Bin}(n,1-p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color [Wooclap]\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m)) [Wooclap]\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit Test",
    "text": "\\chi^2 Goodness of Fit Test\n\n\n\n\nWe observe (X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q). This corresponds to n counts: X_1 + \\dots + X_m = n\nq = (q_1, \\dots, q_m) corresponds to probabilities of getting color 1, \\dots, m\nLet p = (p_1, \\dots, p_m) be a known vector s.t. p_1 + \\dots + p_m = 1.\nH_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\n\n\n\n\n\n\n\n\n\n\n\\chi^2 Goodness of Fit Test (Adéquation)\n\n\n\nChi-squared test statistic:: \\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; , where n_i = np_i = \\mathbb E[X_i] is the expected number of counts for color i.\nWhen np_i = n_i are large, under H_0, we approximate the distribution of \\psi(X) as \\psi(X) \\sim \\chi^2(m-1)\nReject if \\psi(X) &gt; t_{1- \\alpha} (right-tail of \\chi^2(m-1))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "title": "Goodness of Fit Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\n\n\n\nWe observe a bag of sweets containing n=100 sweets of m=3 different colors: red, green, and yellow.\nManufacturer: p_1= 40\\% red, p_2=35\\% green, and p_3=25\\% yellow.\nH_0: q=p (manufacturer’s claim is correct)\nH_1: q\\neq p (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\n\n\n\nColor\nObserved Counts\n\n\n\n\nRed\nX_1=50\n\n\nGreen\nX_2=30\n\n\nYellow\nX_3=20\n\n\n\n\n\n\n\n\n\nExpected Counts\n\n\n\n\nn_1=40\n\n\nn_2=35\n\n\nn_3=25\n\n\n\n\n\n\n\n\n\n\n\n\n\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\n\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878 (p_{value} \\approx 0.222)\nConclusion: do not reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#histograms",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#histograms",
    "title": "Goodness of Fit Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nhistogram\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n\n\n\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\n\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\n\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\nwhere I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\n\n\n\n\n\n\n\n\n\n\n\n\nNormalization\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of large numbers, Monte Carlo (informal)\n\n\n\nAssume that (X_1, \\dots, X_n) are iid of distrib P, and that a,b, k are fixed\nThe histogram \\mathrm{hist}(a,b,k) converges to the histogram of the density P"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit to a given distribution",
    "text": "\\chi^2 Goodness of Fit to a given distribution\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n, iid with unknown distrib P\nH_0: P = P_0, where P_0 is known\nH_1: P \\neq P_0 [Wooclap]\nIdea: under H_0, the counts in disjoint intervals I_1, I_2, \\dots, I_k follow a multinomial distribution \\mathrm{Mult}(n, (p_1, \\dots, p_{k})) where p_1 = P_0(I_1), \\dots, p_{k} = P_0(I_k)\n\n\n\n\n\n\n\n\n\n\nReduction to chi-squared test statistic\n\n\n\nCount the number of data (c_1, \\dots, c_k) in I_1, \\dots, I_k\nTheoretical counts nP_0(I_1) \\dots, nP_0(I_k)\nChi-squared statistic: \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_j)}\nDecide using an 1-\\alpha-quantile of a \\chi^2(k-1) distribution\n\n\n\n\n\n\n\n\n\n\nCorrected \\chi^2 Test\n\n\n\nIf P_0 is unknown\nIn a class \\mathcal P parameterized by \\ell parameters\nThen estimate the \\ell parameters\nCompute theoretical counts with \\hat P_{\\hat \\theta}\nBut decide with \\chi^2(k - 1 - \\ell)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Goodness of Fit Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\nH_0: X_i iid \\mathcal P(2)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n0\n1\n2 \n\\geq 3\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\n\n\n\n\n\n\nTo get 9.7, we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\gtrsim \\frac{(16-4)^2}{4} = 36\n(1-cdf(Chisq(3),36)) is very small: Reject\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf H_0 is X_i iid \\mathcal P(\\lambda) with unknown \\lambda\n\\hat \\lambda = 0.8, \\sum_{i=1}^4 \\frac{(X_i - n_i)^2}{n_i} \\approx 9.4\nNot \\chi^2(3) but \\chi^2(2)\n(1-cdf(Chisq(2),9.4)) \\approx 0.009. Reject at level 1%\n\n\n\n\n\n\n\n[Wooclap]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "title": "Goodness of Fit Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)} [Wooclap]\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\n\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with y=x"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "title": "Goodness of Fit Tests",
    "section": "Kolmogorov-Smirnov Test",
    "text": "Kolmogorov-Smirnov Test\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile:\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n} [Wooclap]\nEmpirical CDF: \\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf 1\\{X_i \\leq x\\}\nIdea: Max distance between empirical and true CDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\\psi(X) = \\sup_{x}|\\hat F(x) - F_0(x)|\nApprox: \\mathbb P_0(\\psi(X) &gt;c/\\sqrt{n}) \\to 2\\sum_{r=1}^{+\\infty}(-1)^{r-1}\\exp(-2c^2r^2) when n \\to +\\infty\nIn practice, use Julia, Python or R for KS Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html",
    "href": "teaching/hypothesis_testing/TDs/TP.html",
    "title": "TP: Hypothesis Testing",
    "section": "",
    "text": "We observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\). We assume that the vectors \\(X\\) and \\(Y\\) are independent. We want to test \\(H_0\\): \\(\\mu_1 = \\mu_2\\) VS \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\). We observe the data\nX=[-0.2657064426519085, -0.27538323622274347, 0.11419811877193782, 0.1158736466676504, 1.7071154417851981, 0.9306910454777643, 0.5834941669559498, -1.536447927372139, -1.4158768806157345, 1.0532694288697444, 1.2955133629200777, -0.4195557179577367]\nY=[-0.6452416530469819, 0.3662048411679129, -0.09943069837472361, 0.8738423322164134, 0.7163913715056272, -0.32450102319617485, 0.9159821874321818, -2.3583609849887224]\n\nCompute the student-Welch test statistic \\(\\tfrac{\\overline X - \\overline Y}{\\sqrt{\\hat\\sigma_1/n_1 + \\hat\\sigma_2/n_2}}\\)\nConclude using a Gaussian approximation (use the cdf of a N(0,1))\nConclude using an UnequalVarianceTTest (julia) or test.welch (R) or scipy.stats.ttest_ind(a, b, equal_var=False) (python)\nBonus. Conclude using a better chi-squared approximation. Compare these result to 3."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#student-test",
    "href": "teaching/hypothesis_testing/TDs/TP.html#student-test",
    "title": "TP: Hypothesis Testing",
    "section": "",
    "text": "We observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\). We assume that the vectors \\(X\\) and \\(Y\\) are independent. We want to test \\(H_0\\): \\(\\mu_1 = \\mu_2\\) VS \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\). We observe the data\nX=[-0.2657064426519085, -0.27538323622274347, 0.11419811877193782, 0.1158736466676504, 1.7071154417851981, 0.9306910454777643, 0.5834941669559498, -1.536447927372139, -1.4158768806157345, 1.0532694288697444, 1.2955133629200777, -0.4195557179577367]\nY=[-0.6452416530469819, 0.3662048411679129, -0.09943069837472361, 0.8738423322164134, 0.7163913715056272, -0.32450102319617485, 0.9159821874321818, -2.3583609849887224]\n\nCompute the student-Welch test statistic \\(\\tfrac{\\overline X - \\overline Y}{\\sqrt{\\hat\\sigma_1/n_1 + \\hat\\sigma_2/n_2}}\\)\nConclude using a Gaussian approximation (use the cdf of a N(0,1))\nConclude using an UnequalVarianceTTest (julia) or test.welch (R) or scipy.stats.ttest_ind(a, b, equal_var=False) (python)\nBonus. Conclude using a better chi-squared approximation. Compare these result to 3."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#monte-carlo-and-chi-squared-tests",
    "href": "teaching/hypothesis_testing/TDs/TP.html#monte-carlo-and-chi-squared-tests",
    "title": "TP: Hypothesis Testing",
    "section": "1. Monte Carlo and Chi-squared Tests",
    "text": "1. Monte Carlo and Chi-squared Tests\nA statistician observes \\(X = (X_1, \\dots, X_n)\\) where the \\(X_i's\\) are iid of distribution \\(P\\). If the problem is to test whether \\(P\\) is Gaussian with known \\(\\mu\\) and \\(\\sigma\\), the problem is:\n\\[H_0: P=\\mathcal N(\\mu, \\sigma) \\quad \\text{VS} \\quad H_1: P\\neq \\mathcal N(\\mu, \\sigma)\\]\nIf \\(\\mu\\) and \\(\\sigma\\) are unknown, the problem is \\[H_0: P\\in \\{\\mathcal N(\\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt;0\\}\\quad \\text{VS} \\quad H_1: P\\not \\in \\{\\mathcal N(\\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt;0\\}\\]\nWe first assume that \\(\\mu\\) and \\(\\sigma\\) are known, and that:\nmu = 0\nsigma = 1\nn = 100\nm = 5\nThis practical exercise aims to empirically demonstrate how a chi-squared test statistic converges to a chi-squared distribution in both known and unknown parameter scenarios. We will:\n\nDivide the observation space into 5 disjoint intervals\nCount how many observations fall into each interval for randomly generated data\nCalculate the chi-squared test statistic for randomly generated data\nRepeat this process 1,000 times to build an empirical distribution (an histogram)\nThe resulting empirical histogram should approach a theoretical chi-squared distribution as both the sample size \\(n\\) and the number of repetitions \\(N\\) approach infinity.\n\n\nQuestions\n\nGenerate a vector \\(X\\) made of \\(n\\) iid \\(\\mathcal N(\\mu, \\sigma)\\)\nCompute the vector \\(Y = \\frac{X-\\mu}{\\sigma}\\)\nCompute the list of counts \\(C\\) of \\(Y\\) in \\((-\\infty, -3)\\), \\([\\tfrac{3i}{m}, \\frac{3(i+1)}{m})\\) for \\(i\\) in \\(\\{-m, \\dots, m-1\\}\\) and \\([3,+\\infty)\\).\n\nHow many intervals do we have here?\nWhat is the expected number of entries of \\(Y\\) falling in \\([3, +\\infty)\\)? (compute this using the cdf function). Change the value of \\(n\\) so that we have at least \\(5\\) expected counts in \\([3, +\\infty)\\).\n\n\n\n\n#Julia: use the broadcasting .&lt;\nsum(x .&lt;= Y .&lt; y) # counts in [x, y)\n\n#R: use bitwise operator &\nsum(Y &gt;= x & Y &lt; y) # counts in [x, y)\n\n\n\nUsing the cdf of \\(\\mathcal N(0,1)\\), compute the list of expected counts in the same intervals\nCompute the Chi-squared test statistic using the two preceeding questions. We recall that \\(\\psi(Y) = \\sum_{i=1}^n \\tfrac{(c_i - e_i)^2}{e_i}\\) where \\(c_i\\) and \\(e_i\\) are the counts and expected counts.\nSummarize the preceeding questions into a function trial_chisq(X, mu, sigma, m) that normalizes \\(X\\), computes counts, expected counts and the chisq test statistic:\n\n# function trial_chisq(X, mu, sigma, m)\n# n = length(X)\n# Y = (X-mu)/sigma\n# Compute counts \n# Compute expcounts\n# Compute and Return chisq\n\nUsing the previous question, write a function monte_carlo_known that computes \\(N\\) chi-squared test statistics on iid random samples \\(X\\sim \\mathcal N(\\mu, \\sigma)^{\\otimes n}\\). It returns a list trials of length \\(N\\).\n\nN = 1000\n# function monte_carlo_known(N, mu, sigma, n, m)\n# empty list trials\n\n# for i = 1 ... N\n\n# Generate X made of n iid gaussian (mu, sigma)\n# append trial_chisq(X, mu, sigma, m) to trials\n\n# endfor\n# return trials\n\nPlot a histogram of a list of trials using a builtin function. Normalize it in density (area=1), and precise the bins (0:0.5:30).\nWhat is a good distribution to approximate the histogram? Plot the distribution’s density and check that it fits the histogram. Vary the parameters \\(m\\), \\(n\\), and \\(N\\).\n\nNow, we assume that \\(\\mu\\) and \\(\\sigma\\) are unknown.\n\nGiven \\(X\\), compute to estimators hatmu and hatsigma of mu and sigma\nSimilarly to Q.7, write a function monte_carlo_unknown(N, n, m) that computes a Monte-Carlo simulation. \\(\\hat \\mu\\) and \\(\\hat \\sigma\\) must be computed for all trial \\(i=1,\\dots,N\\).\nRevisit questions 8 and 9, considering the case where \\(\\mu\\) and \\(\\sigma\\) are unknown. How does this affect the distribution of the histogram?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#application-with-bitcoin",
    "href": "teaching/hypothesis_testing/TDs/TP.html#application-with-bitcoin",
    "title": "TP: Hypothesis Testing",
    "section": "2. Application with Bitcoin",
    "text": "2. Application with Bitcoin\n\nUse your favorite AI to write the code to import the last \\(500\\) hourly close prices of bitcoin in usdt from binance. Plot the prices and compute the returns defined as \\(R_t = \\tfrac{P_t}{P_{t-1}}-1\\), where \\(P_t\\) is the price at time \\(t\\) (in hours).\n\n\n\n\n\n\n\nR (Click to See a Solution)\n\n\n\n\n\nlibrary(httr)\nlibrary(jsonlite)\n# Define the API endpoint and parameters\napi_url &lt;- \"https://api.binance.com/api/v3/klines\"\nsymbol &lt;- \"BTCUSDT\" # Bitcoin to USDT trading pair\ninterval &lt;- \"1h\" # 1-hour interval\nlimit &lt;- 500 # Limit to 500 data points\n\n# Create the query URL with parameters\nquery_params &lt;- list(\n    symbol = symbol,\n    interval = interval,\n    limit = limit\n)\n\n# Fetch the data from Binance API\nresponse &lt;- GET(api_url, query = query_params)\nresponse.body\ndata &lt;- content(response, as = \"text\", encoding = \"UTF-8\")\ndata &lt;- fromJSON(data)\ndata &lt;- data.frame(data)[2]\ndata &lt;- data.frame(lapply(data, as.numeric))\nn &lt;- length(data$X2)\nR &lt;- (data[2:n,1] / data[1:(n - 1),1]) - 1\n\n\n\n\n\n\n\n\n\nJulia (Click to See a Solution)\n\n\n\n\n\nusing HTTP\nusing JSON\nusing DataFrames\n\nfunction BTC_returns()\n    # Define the API endpoint and parameters\n    api_url = \"https://api.binance.com/api/v3/klines\"\n    symbol = \"BTCUSDT\"  # Bitcoin to USDT trading pair\n    interval = \"1h\"     # 1-hour interval\n    limit = 1000         # Limit to 500 data points\n    \n    # Construct the full query URL\n    query_url = \"$api_url?symbol=$symbol&interval=$interval&limit=$limit\"\n    \n    # Fetch the data from Binance API\n    response = HTTP.get(query_url)\n    data = JSON.parse(String(response.body))\n    P = [parse(Float64, data[i][2]) for i in 1:length(data)]\n    R = [P[t] / P[t-1] - 1 for t in 2:length(P)]\n    return R\nend\n    R=BTC_returns()\n\n\n\n\nWe first test\n\\(H_0\\): the mean of the returns is zero VS \\(H_1\\): it is nonzero.\nCompute \\(\\hat \\sigma\\) as std(R) and the Student statistic \\(\\psi(R) = \\sqrt{n}\\tfrac{\\overline R}{\\hat \\sigma}\\). Compute the p-value using the cdf function of a Student(499) (or Gaussian). Obtain the same result with a library function like OneSampleTTest in Julia, t.test in R or ttest_1samp in Python\nPlot a histogram of the returns, normalized in density. Plot on the same graph the density of a Gaussian of mean mean(R) and of std std(R).\nUsing the previous exercise with \\(m=5\\), compute a chi-squared statistic and an approximated p-value.\nDo a scatter plot of \\((R_{t-1}, R_t)\\). Do you see any correlation between \\(R_{t-1}\\) and \\(R_t\\)?\nCompute the correlation \\(r\\) between \\((R_t)\\) and \\((R_{t-1})\\).\nCompute the p-value of a two-sided Pearson’s correlation test, using the test statistic \\(\\tfrac{r}{\\sqrt{1-r^2}} \\sqrt{n-2}\\) and the cdf of a Student distribution. Compare with the function CorrelationTest in Julia or cor.test in R or pearsonr in Python."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/MoreExercises.html",
    "href": "teaching/hypothesis_testing/TDs/MoreExercises.html",
    "title": "More Exercises",
    "section": "",
    "text": "A quality control analyst at a semiconductor manufacturing plant is tracking chip failures during production. For each production batch, the analyst records the number of chips that pass inspection before the batch is terminated.\nDataset: Number of working chips produced in \\(150\\) different production batches.\n\n\n\nWorking Chips\nNumber of Batches\nPercentage\n\n\n\n\n0\n8\n5.3%\n\n\n1\n10\n6.7%\n\n\n2\n16\n10.7%\n\n\n3\n17\n11.3%\n\n\n4\n21\n14.0%\n\n\n5\n20\n13.3%\n\n\n6\n17\n11.3%\n\n\n7\n14\n9.3%\n\n\n8\n10\n6.7%\n\n\n9\n7\n4.7%\n\n\n10\n3\n2.0%\n\n\n11\n5\n3.3%\n\n\n12\n2\n1.3%\n\n\nTotal\n150\n100%\n\n\n\nWe assume that each batch terminates after \\(r= 5\\) failures.\n\nWhat distribution the number of working chips in a batch should follow?\nTest the goodness of fit to this distribution.\nWhat does it change if the number of failures after which each batch ends is unknown?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/MoreExercises.html#exercise",
    "href": "teaching/hypothesis_testing/TDs/MoreExercises.html#exercise",
    "title": "More Exercises",
    "section": "",
    "text": "A quality control analyst at a semiconductor manufacturing plant is tracking chip failures during production. For each production batch, the analyst records the number of chips that pass inspection before the batch is terminated.\nDataset: Number of working chips produced in \\(150\\) different production batches.\n\n\n\nWorking Chips\nNumber of Batches\nPercentage\n\n\n\n\n0\n8\n5.3%\n\n\n1\n10\n6.7%\n\n\n2\n16\n10.7%\n\n\n3\n17\n11.3%\n\n\n4\n21\n14.0%\n\n\n5\n20\n13.3%\n\n\n6\n17\n11.3%\n\n\n7\n14\n9.3%\n\n\n8\n10\n6.7%\n\n\n9\n7\n4.7%\n\n\n10\n3\n2.0%\n\n\n11\n5\n3.3%\n\n\n12\n2\n1.3%\n\n\nTotal\n150\n100%\n\n\n\nWe assume that each batch terminates after \\(r= 5\\) failures.\n\nWhat distribution the number of working chips in a batch should follow?\nTest the goodness of fit to this distribution.\nWhat does it change if the number of failures after which each batch ends is unknown?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html",
    "href": "teaching/hypothesis_testing/TDs/TD4.html",
    "title": "TD4: Homogeneity/Dependency",
    "section": "",
    "text": "A sociologist wants to investigate whether the choice of transportation mode (Car, Bicycle, or Public Transit) varies among residents of three different cities: City A, City B, and City C.\nThe sociologist conducted a survey, and the responses are summarized in the contingency table below:\n\n\n\nTransportation Mode\nCity A\nCity B\nCity C\nTotal\n\n\n\n\nCar\n120\n150\n100\n370\n\n\nBicycle\n80\n60\n90\n230\n\n\nPublic Transit\n100\n90\n110\n300\n\n\nTotal\n300\n300\n300\n900\n\n\n\n\nFormulate the Hypothesis Testing Problem corresponding to the initial objective of the sociologist. Introduce the notation\nAnswer to the initial question using a chi-squared test at a \\(0.05\\) significance level"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-1",
    "title": "TD4: Homogeneity/Dependency",
    "section": "",
    "text": "A sociologist wants to investigate whether the choice of transportation mode (Car, Bicycle, or Public Transit) varies among residents of three different cities: City A, City B, and City C.\nThe sociologist conducted a survey, and the responses are summarized in the contingency table below:\n\n\n\nTransportation Mode\nCity A\nCity B\nCity C\nTotal\n\n\n\n\nCar\n120\n150\n100\n370\n\n\nBicycle\n80\n60\n90\n230\n\n\nPublic Transit\n100\n90\n110\n300\n\n\nTotal\n300\n300\n300\n900\n\n\n\n\nFormulate the Hypothesis Testing Problem corresponding to the initial objective of the sociologist. Introduce the notation\nAnswer to the initial question using a chi-squared test at a \\(0.05\\) significance level"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-2",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe statistician of an insurance company is tasked with studying the impact of an advertising campaign conducted in 7 regions where the company operates. To do this, he has extracted from the database the number of new clients acquired by a certain number of agents in each region.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nNumber of agents\n9\n7\n7\n6\n7\n6\n6\n\n\nAverage number of new clients\n26.88\n22.34\n19.54\n18.95\n27.17\n25.87\n25.72\n\n\nVariance of new clients\n13.54\n12.59\n12.87\n13.42\n13.17\n12.56\n12.64\n\n\n\nThe statistician decides to perform an analysis of variance to test whether the regional factor influences the number of new clients. Let \\(X_{ik}\\) denote the number of new clients of agent \\(i\\) in region \\(k\\), \\(N_k\\) the number of agents in region \\(k\\), \\(d = 7\\) the number of regions and \\(N_{\\mathrm{tot}} = 48\\) the total number of agents. Assume that the random variables \\(X_{ik}\\) are normal with mean \\(\\mu_k\\) and variance \\(\\sigma^2\\). Define:\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\nFormulate the hypothesis testing problem to test whether the number of new clients is homogeneous accross the regions.\nWhat do \\(\\overline X_k\\), \\(\\overline X\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\) represent?\nProve the analysis of variance formula: \\[V_T = V_W + V_B \\; .\\] substract and add \\(\\overline X_k\\) in the definition of \\(V_T\\)\nCompute \\(\\overline X\\), \\(V_W\\), \\(V_B\\), and \\(V_T\\).\nWrite the definition of the ANOVA test statistic in terms of \\(V_W\\) and \\(V_B\\).\nDid the advertising campaign have the same impact in all regions?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-3",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 3",
    "text": "Exercise 3\nSome data are collected from 7 students and we want to analyze the correlation between the number of hours students spend studying before an exam and their test scores.\n\n\n\nStudent\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nStudy Hours\n2.5\n3.0\n1.5\n4.0\n3.5\n5.0\n3.0\n\n\nTest Score\n56\n64\n45\n72\n68\n80\n59\n\n\n\n\nFormulate the hypothesis testing problem for a linear correlation test\nPerform the linear correlation test at level \\(0.05\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-4",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 4",
    "text": "Exercise 4\nBelow are stress scores for \\(10\\) patients before and after a sport session:\n\n\n\n\n\n\n\n\n\n\nParticipant\nStress Score (Before)\nStress Score (After)\nDifference\nRank/Sign\n\n\n\n\n1\n40\n32\n\n\n\n\n2\n38\n35\n\n\n\n\n3\n45\n40\n\n\n\n\n4\n50\n42.5\n\n\n\n\n5\n44\n41.5\n\n\n\n\n6\n48\n48\n\n\n\n\n7\n39\n30\n\n\n\n\n8\n42\n38\n\n\n\n\n9\n47\n46\n\n\n\n\n10\n46.5\n40\n\n\n\n\n\nWe want to test if sport has an effect on the stress of the patients\n\nFormulate the hypothesis testing problem\nComplete the above table\nPerform a Wilcoxon signed rank test"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-5",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-5",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 5",
    "text": "Exercise 5\nLet \\(X=(X_1, \\dots, X_N)\\) be a Gaussian vectors \\(\\mathcal N(0, I_N)\\) in \\(\\mathbb R^N\\) (i.e. \\(X_i\\) are iid \\(\\mathcal N(0,1)\\)).\n\n\nWhat is the distribution of \\(QX\\), if \\(Q\\) is an orthogonal matrix ? (\\(QQ^T = I_n\\))\nWhat is the distribution of \\(\\|PX\\|^2\\) if \\(P\\) is an orthogonal projector ?\nUse the rank of \\(P\\) defined as \\(rk(P) = dim(Im(P))\\)\nDefinition of orthogonal projector: (\\(P^2=P\\) and \\(P = P^T\\))\nShow that if \\(P\\) is an orthogonal projector, then \\(PX\\) is independent of \\((I-P)X\\).\nUse the fact that two centered gaussian vectors \\(X\\),\\(Y\\) are independent iif \\(\\mathbb E[X_iY_j] = 0\\) for all \\(i,j\\). Translate this fact in a matrix form.\nWhat is the distribution of \\(\\frac{n-rk(P)}{rk(P)}\\frac{\\|PX\\|^2}{\\|(I-P)X\\|^2}\\) ?\nShow that if \\(P\\), \\(P_0\\) are two orthogonal projectors such that \\(Im(P_0) \\subset Im(P)\\), then \\(P(I-P_0)X\\) is independent of \\((I-P)(I-P_0)X\\). What is the distribution of \\(\\|P(I-P_0)X\\|^2\\) ?\nShow first that \\(PP_0=P_0P= P_0\\), and that \\(P-P_0\\) is an orthogonal projector\nWhat is the orthogonal projector \\(P_0\\) on \\(\\mathrm{Span}(1, \\dots, 1)\\) ? Deduce that \\((X_i - \\overline X)\\) is independent of \\(\\overline X\\) for all \\(i\\).\n\nWe divide \\(N\\) into \\(d\\) blocks: \\(N = N_1 + \\dots + N_d\\). We write \\((X_1, \\dots, X_n) = ((X_{11}, \\dots, X_{N_11}), (X_{12}, \\dots X_{N_2 2}), \\dots, (X_{N_d 1}, \\dots, X_{N_d d}))\\).\n\nWhat is the orthogonal projection on \\(E_k =((0, \\dots, 0), \\dots, (0, \\dots, 0),(1, \\dots, 1), (0,\\dots, 0) ,\\dots, (0, \\dots, 0))\\) ? (\\(0\\) everywhere except on block \\(k\\) where we put ones everywhere)\nGive the orthogonal projection \\(P\\) on \\(\\mathrm{Span}(E_1, \\dots, E_d)\\). Explicit \\((I-P)(I-P_0)X\\) and \\(P(I-P_0)X\\).\nDeduce the distribution of \\(\\frac{d-1}{n-d}\\frac{\\|(I-P)(I-P_0)X\\|^2}{\\|P(I-P_0)X\\|^2}\\) and of the ANOVA Test Statistic under \\(H_0\\)."
  },
  {
    "objectID": "teaching/glossary.html",
    "href": "teaching/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "General\n\n\n\nEnglish\n\nincreasing function\nnondecreasing function\ninterval \\([a,b)\\)\n\n\n\n\nFrench\n\nfonction strictement croissante\nfonction croissante\nintervalle \\([a,b[\\)\n\n\n\n\n\n\nProbability/Statistics\n\n\n\nEnglish\n\nprobability distribution\nexpectation\nlikelihood\ndistribution tail\nCDF (Cumul. Distrib. Func.)\nPDF (Proba. Density Func.)\nCLT (Central Limit Theorem)\nLLN (Law of Large Numbers)\n\n\n\n\nFrench\n\nloi de probabilité\nespérance\nvraissemblance\nqueue de distribution\nFonction de Répartition\nDensité d’une distribution\nTLC (Th. de la limite centrale)\nLoi des grands nombres\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nEnglish\n\nA sample\none-sided test\ntwo-sided test\nchi-squared goodness of fit test\nchi-squared homogeneity test\n\n\n\n\nFrench\n\nUn échantillon\ntest unilatéral\ntest bilatéral\ntest d’adéquation du Khi-deux\ntest d’homogénéité du Khi-deux\n\n\n\n\n\n\nProgramming Languages (JupytR)\n\n\n\n\nJulia\nusing Distributions\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\ncdf(Binomial(n, p), x)\nquantile(Binomial(n,p), q)\ncdf(Normal(0,1), x)\nquantile(Normal(0,1), q)\n\ncdf(Chisq(n), x) # Chi-squared\ncdf(TDist(n), x) # Student\n\nn1,n2 = 5,10\ncdf(FDist(n1, n2), x) # Fisher\n\nlmbda = 2\ncdf(Gamma(n, lmbda), x) # Gamma\ncdf(Poisson(lmbda), x) # Poisson\n\n\n\nPython\nfrom scipy.stats import *\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\nbinom.cdf(x, n, p)\nbinom.ppf(q, n, p) # Quantile\nnorm.cdf(x)\nnorm.ppf(q)\n\nchi2.cdf(x, n)\nt.cdf(x, n) # Student\n\nn1,n2 = 5,10\nf.cdf(x, n1, n2) # Fisher\n\nlmbda = 2\ngamma.cdf(x,n,lmbda) # Gamma\npoisson.cdf(x, lmbda) # Poisson\n\n\n\nR\n\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\npbinom(x, n, p)\nqbinom(q, n, p)\npnorm(x)\nqnorm(q)\n\npchisq(x, n)\npt(x, n)\n\nn1,n2 = 5,10\npf(x, n1,n2)\n\nlmbda = 2\npgamma(x,n,lmbda) \nppois(x, lmbda)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#this-class",
    "href": "teaching/glm/slides/counting_models.html#this-class",
    "title": "Counting Models",
    "section": "This Class",
    "text": "This Class"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#poisson-regression-model",
    "href": "teaching/glm/slides/counting_models.html#poisson-regression-model",
    "title": "Counting Models",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#model-assumption",
    "href": "teaching/glm/slides/counting_models.html#model-assumption",
    "title": "Counting Models",
    "section": "Model Assumption",
    "text": "Model Assumption\n\nWe assume that\n\n\\[Y_i|X_i=x_i \\sim P(\\lambda_\\beta(x_i))\\quad \\text{with} \\quad \\lambda_\\beta(x) = \\exp(x^T \\beta)\\]\n\nwith \\(\\lambda_\\beta(x) = \\exp(x^T \\beta)\\).\n\n\nIn particular, \\(\\mathbb E[Y_i|X_i=x_i] = \\lambda_\\beta(x)\\)\n\n\n\n\n\nWarning\n\n\n\\(Y|X\\) follows a Poisson distribution but not \\(Y\\)!\n(We can only say that it is a mixture of Poisson)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#illustration",
    "href": "teaching/glm/slides/counting_models.html#illustration",
    "title": "Counting Models",
    "section": "Illustration",
    "text": "Illustration\n\nExample: binary regressor \\(X\\) (with as many \\(X = 0\\) as \\(X = 1\\))\n\n\n\n\\(Y|(X = 0) \\sim P(2) \\and Y|(X = 1) \\sim P(10)\\)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#rate-ratio",
    "href": "teaching/glm/slides/counting_models.html#rate-ratio",
    "title": "Counting Models",
    "section": "Rate Ratio",
    "text": "Rate Ratio\n\nThere is no notion of OR since we are not estimating a probability (or a probability ratio) but an expectation.\n\n\nThe equivalent notion here is the rate ratio (RR), \\(\\lambda(x)\\) being seen as an average rate of occurrence of \\(Y\\).\n\n\nFor two characteristics \\(x_1\\) and \\(x_2\\), the rate ratio is simply:\n\n\\[RR(x_1, x_2) = \\frac{\\lambda_\\beta(x_1)}{\\lambda_\\beta(x_2)} = \\exp((x_1 - x_2)^T \\beta)\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#particular-cases-for-rate-ratio",
    "href": "teaching/glm/slides/counting_models.html#particular-cases-for-rate-ratio",
    "title": "Counting Models",
    "section": "Particular Cases for Rate Ratio",
    "text": "Particular Cases for Rate Ratio\n\nSingle Regressor Difference: If \\(x_1\\) and \\(x_2\\) differ only by regressor \\(j\\):\n\\(RR(x_1, x_2) = \\exp((x_{1j} - x_{2j})\\beta_j)\\)\n\n\nBinary regressor: (\\(x_{1j} = 1\\) and \\(x_{2j} = 0\\)): \\(RR_j = e^{\\beta_j}\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#log-likelihood",
    "href": "teaching/glm/slides/counting_models.html#log-likelihood",
    "title": "Counting Models",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n\n\n\\[P(Y = k|X = x) = e^{-\\lambda_\\beta(x)} \\frac{\\lambda_\\beta(x)^k}{k!}\\]\n\n\n\nThus the sample likelihood equals \\(\\prod_{i=1}^n e^{-\\lambda_\\beta(x_i)} \\frac{\\lambda_\\beta(x_i)^{y_i}}{y_i!}\\)\n\n\nSince \\(\\lambda_\\beta(x) = \\exp(x^T \\beta)\\), the log-likelihood therefore equals\n\n\\[L = \\sum_{i=1}^n \\left[y_i x_i^T \\beta - e^{x_i^T \\beta} - \\ln(y_i!)\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#estimation-of-beta-by-mle",
    "href": "teaching/glm/slides/counting_models.html#estimation-of-beta-by-mle",
    "title": "Counting Models",
    "section": "Estimation of \\(\\beta\\) by MLE",
    "text": "Estimation of \\(\\beta\\) by MLE\n\nBy setting the gradient with respect to \\(\\beta\\) to zero, we find that the MLE \\(\\hat{\\beta}\\) must verify:\n\n\\[\\sum_{i=1}^n y_i x_i = \\sum_{i=1}^n \\lambda_{\\hat{\\beta}}(x_i) x_i\\]\n\n\n\nThis is a system with \\(p\\) unknowns (recall that \\(x_i \\in \\mathbb R^p\\)) that we solve numerically."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#properties-of-hat-beta",
    "href": "teaching/glm/slides/counting_models.html#properties-of-hat-beta",
    "title": "Counting Models",
    "section": "Properties of \\(\\hat \\beta\\)",
    "text": "Properties of \\(\\hat \\beta\\)\n\nUnder regularity conditions, when \\(n \\to \\infty\\):\n\n\\[\\hat{\\beta} \\sim N(\\beta, (X^T W_{\\hat{\\beta}} X)^{-1})\\]\n\nwhere \\(W_{\\hat{\\beta}} = \\text{diag}(\\lambda_{\\hat{\\beta}}(x_1), \\ldots, \\lambda_{\\hat{\\beta}}(x_n))\\).\n\n\nWe can therefore perform Wald significance tests."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#validation-deviance",
    "href": "teaching/glm/slides/counting_models.html#validation-deviance",
    "title": "Counting Models",
    "section": "Validation: Deviance",
    "text": "Validation: Deviance\n\nThe saturated model (one parameter per different observation) leads to\n\n\\[\\hat{\\lambda}(x) = \\frac{y_x}{n_x}\\]\n\n\n\\(y_x = \\sum_{i:x_i=x} y_i\\) is the total number of \\(Y\\) observed for characteristic \\(x\\) on the sample\n\\(n_x = \\sum_{i:x_i=x} 1\\) is the number of times \\(x\\) was observed."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#saturated-log-likelihood",
    "href": "teaching/glm/slides/counting_models.html#saturated-log-likelihood",
    "title": "Counting Models",
    "section": "Saturated Log-Likelihood",
    "text": "Saturated Log-Likelihood\n\nThe log-likelihood of the saturated model therefore equals:\n\n\\[L_{\\text{sat}} = \\sum_x \\left[y_x \\ln\\left(\\frac{y_x}{n_x}\\right) - y_x\\right] - \\text{cste}\\]\n\nwhere \\(\\text{cste} = \\sum_{i=1}^n \\ln(y_i!)\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#deviance-formula",
    "href": "teaching/glm/slides/counting_models.html#deviance-formula",
    "title": "Counting Models",
    "section": "Deviance Formula",
    "text": "Deviance Formula\n\nThus the deviance equals\n\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}}) = 2\\sum_x y_x \\ln\\left(\\frac{y_x}{n_x \\lambda_{\\hat{\\beta}}(x)}\\right) - (y_x - n_x \\lambda_{\\hat{\\beta}}(x))\\]\n\n\n\n\nIf a constant is in the model (one coordinate of \\(x\\) equals \\(1\\)), we have from the likelihood equations \\(\\sum_x y_x = \\sum_x n_x \\lambda_{\\hat{\\beta}}(x)\\) and then\n\n\n\\[D = 2\\sum_x y_x \\ln\\left(\\frac{y_x}{\\hat{y}_x}\\right)\\]\n\n\nwhere \\(\\hat{y}_x = n_x \\lambda_{\\hat{\\beta}}(x)\\) are the expected theoretical counts."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#validation-deviance-test",
    "href": "teaching/glm/slides/counting_models.html#validation-deviance-test",
    "title": "Counting Models",
    "section": "Validation: Deviance Test",
    "text": "Validation: Deviance Test\n\nAs in logistic regression, we can compare two nested models by a deviance test (or likelihood ratio test)\n\n\nIf model 2 has \\(q\\) fewer parameters compared to model 1, we have under \\(H_0\\): “the \\(q\\) coefficients in question are zero”:\n\n\\[D_2 - D_1 = 2(L_1 - L_2) \\xrightarrow{L} \\chi^2_q\\]\n\nRejection region at level \\(\\alpha\\):\n\n\\[\\text{CR}_\\alpha = \\{D_2 - D_1 &gt; \\chi^2_q(1-\\alpha)\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#global-significance-test",
    "href": "teaching/glm/slides/counting_models.html#global-significance-test",
    "title": "Counting Models",
    "section": "Global Significance Test",
    "text": "Global Significance Test\n\nThe global significance test corresponds to the case where model 2 contains only the constant.\n\n\nIn this case \\(D_2 = D_0\\) and \\(q = p - 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#validation-graphical-inspection",
    "href": "teaching/glm/slides/counting_models.html#validation-graphical-inspection",
    "title": "Counting Models",
    "section": "Validation: Graphical Inspection",
    "text": "Validation: Graphical Inspection\n\nPlot the predicted counts \\(\\hat{y}_x = n_x \\lambda_{\\hat{\\beta}}(x)\\) against the observed counts \\(y_x\\).\n\n\n\n\n\nWarning\n\n\nthe predicted counts \\(\\hat{y}_x\\) represent the expectation of the expected counts given \\(x\\).\nIt is therefore normal that the observed counts \\(y_x\\) are dispersed around the \\(\\hat{y}_x\\)\n\n\n\n\n\nIt is appropriate to have sufficiently large “classes” \\(x\\) (\\(n_x &gt; 5\\)) for the graph to be relevant."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#alternative-graphical-validation",
    "href": "teaching/glm/slides/counting_models.html#alternative-graphical-validation",
    "title": "Counting Models",
    "section": "Alternative Graphical Validation",
    "text": "Alternative Graphical Validation\n\nIdea: empirical distribution of \\(Y\\) VS its predicted distribution.\n\n\nThe empirical distribution of \\(Y\\) is simply given by\n\n\\(p_k = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}_{y_i = k}, \\quad k \\in \\mathbb{N}\\)\n\n\n\nWhile its predicted distribution is given by\n\n\\(\\hat{p}_k = \\frac{1}{n}\\sum_{i=1}^n \\hat{P}(Y = k|X = x_i), \\quad k \\in \\mathbb{N}\\)\n\n\n\nwhere \\(\\hat{P}(Y = k|X = x_i)\\) is the poisson distribution \\(\\mathcal P(\\lambda_{\\hat{\\beta}}(x_i))\\), i.e., \\(\\hat{P}(Y = k|X = x_i) = \\frac{\\lambda_{\\hat{\\beta}}(x_i)^k e^{-\\lambda_{\\hat{\\beta}}(x_i)}}{k!}\\)\nwith \\(\\lambda_{\\hat{\\beta}}(x_i) = \\exp(x_i^T \\hat{\\beta})\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-number-of-plant-species",
    "href": "teaching/glm/slides/counting_models.html#example-number-of-plant-species",
    "title": "Counting Models",
    "section": "Example: Number of Plant Species",
    "text": "Example: Number of Plant Species\n\nNumber of plant species recorded on a plot according to soil pH (Neutral, Acidic or Basic) and biomass collected.\n\n\n\nSpecies\npH\nBiomass\n\n\n\n\n14\nlow\n3.538\n\n\n31\nmid\n0.740\n\n\n36\nhigh\n7.242\n\n\n20\nmid\n3.216\n\n\n…\n…\n…\n\n\n\n\n\nWe want to model \\(Y =\\) “Species” as a function of pH and Biomass."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-r-output",
    "href": "teaching/glm/slides/counting_models.html#example-r-output",
    "title": "Counting Models",
    "section": "Example: R output",
    "text": "Example: R output\n\nglm(Species ∼ pH + Biomass, family=poisson)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n3.84894\n0.05281\n72.885\n&lt; 2e-16\n***\n\n\npHlow\n-1.13639\n0.06720\n-16.910\n&lt; 2e-16\n***\n\n\npHmid\n-0.44516\n0.05486\n-8.114\n4.88e-16\n***\n\n\nBiomass\n-0.12756\n0.01014\n-12.579\n&lt; 2e-16\n***\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n(Dispersion parameter for poisson family taken to be 1)\nNull deviance: 452.346\nResidual deviance: 99.242\nAIC: 526.43"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-number-of-plant-species-1",
    "href": "teaching/glm/slides/counting_models.html#example-number-of-plant-species-1",
    "title": "Counting Models",
    "section": "Example: Number of Plant Species",
    "text": "Example: Number of Plant Species\n\nThus, the average number of species, given pH and Biomass, is estimated as\n\n\\(\\lambda_{\\hat{\\beta}}(\\text{pH}, \\text{Biomass}) = \\exp(3.85 - 1.14 \\mathbf{1}_{\\text{pH=low}} - 0.46 \\mathbf{1}_{\\text{pH=mid}} - 0.13 \\text{Biomass})\\)\n\n\n\nRate Ratio for low pH (acidic) compared to high pH (basic):\n\n\\(RR(\\text{acidic}, \\text{basic}) = \\exp(-1.14) = 0.32\\)\n\n\n\nOn average, there are therefore about 3 times fewer species in acidic soil than in basic soil."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-interraction-ph-biomass",
    "href": "teaching/glm/slides/counting_models.html#example-interraction-ph-biomass",
    "title": "Counting Models",
    "section": "Example: Interraction Ph-Biomass",
    "text": "Example: Interraction Ph-Biomass\n\nWe can try to introduce an interaction between pH and Biomass\nglm(Species ~ pH + Biomass + pH:Biomass, family=poisson)\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n3.76812\n0.06153\n61.240\n&lt; 2e-16\n***\n\n\npHlow\n-0.81557\n0.10284\n-7.931\n2.18e-15\n***\n\n\npHmid\n-0.33146\n0.09217\n-3.596\n0.000323\n***\n\n\nBiomass\n-0.10713\n0.01249\n-8.577\n&lt; 2e-16\n***\n\n\npHlow:Biomass\n-0.15503\n0.04003\n-3.873\n0.000108\n***\n\n\npHmid:Biomass\n-0.03189\n0.02308\n-1.382\n0.166954\n\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n(Dispersion parameter for poisson family taken to be 1)\nNull deviance: 452.346\nResidual deviance: 83.201\nAIC: 514.39"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-model-comparison",
    "href": "teaching/glm/slides/counting_models.html#example-model-comparison",
    "title": "Counting Models",
    "section": "Example: Model Comparison",
    "text": "Example: Model Comparison\n\nThe model with interaction seems preferable (via AIC and deviance test).\n\n\nThe average number of species, given pH and Biomass, is this time:\n\n\n\\[\\begin{aligned}\n\\lambda_{\\hat{\\beta}}(\\text{pH}, \\text{Bm}) &= \\exp(3.77 - 0.82 \\mathbf{1}_{\\text{pH=low}} - 0.33 \\mathbf{1}_{\\text{pH=mid}}) \\\\\n&- 0.11\\text{Bm} - 0.16\\text{Bm}\\mathbf{1}_{\\text{pH=low}} - 0.032\\text{Bm}\\mathbf{1}_{\\text{pH=mid}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-bm-dependent-rr",
    "href": "teaching/glm/slides/counting_models.html#example-bm-dependent-rr",
    "title": "Counting Models",
    "section": "Example: Bm-dependent RR",
    "text": "Example: Bm-dependent RR\n\nThe Rate Ratio for low pH (acidic) compared to high pH (basic) depends on Biomass and equals:\n\n\n\\(RR(\\text{acidic}, \\text{basic}) = \\exp(-0.82 - 0.16\\text{Bm})\\)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-predicted-vs-observed-counts",
    "href": "teaching/glm/slides/counting_models.html#example-predicted-vs-observed-counts",
    "title": "Counting Models",
    "section": "Example: Predicted vs Observed Counts",
    "text": "Example: Predicted vs Observed Counts\n\nPredicted mean counts \\(\\hat{y}_i = \\lambda_{\\hat{\\beta}}(\\text{pH}_i, \\text{Biomass}_i)\\) as a function of observed counts \\(y_i\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#model-predictions-by-ph",
    "href": "teaching/glm/slides/counting_models.html#model-predictions-by-ph",
    "title": "Counting Models",
    "section": "Model Predictions by pH",
    "text": "Model Predictions by pH\n\nLines: predicted mean counts by pH as a function of biomass\nPoints: observed counts by pH as a function of biomass\n\n\n\nBlack: pH=basic; Green: pH=neutral; Red: pH=acidic"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#predicted-distribution",
    "href": "teaching/glm/slides/counting_models.html#predicted-distribution",
    "title": "Counting Models",
    "section": "Predicted Distribution",
    "text": "Predicted Distribution\n\nHistogram: empirical distribution\nPoints: predicted distribution"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#model-limitations",
    "href": "teaching/glm/slides/counting_models.html#model-limitations",
    "title": "Counting Models",
    "section": "Model Limitations",
    "text": "Model Limitations\n\nWhen we model \\(Y|(X = x) \\sim \\mathcal P(\\lambda(x))\\), we have\n\n\\[\\E(Y|X = x) = \\lambda(x)\\]\n\n\n\nbut also\n\n\\[\\Var(Y|X = x) = \\lambda(x)\\]\n\n\n\nThis constraint is a limitation of the Poisson model."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#overdispersion-1",
    "href": "teaching/glm/slides/counting_models.html#overdispersion-1",
    "title": "Counting Models",
    "section": "Overdispersion",
    "text": "Overdispersion\n\nSome data are overdispersed, in the sense that\n\n\\[\\Var(Y|X = x) &gt; \\E(Y|X = x)\\]\n\n\n\nMore rarely, we can find underdispersed data.\n\n\nIn case of overdispersion, the estimated variance of estimators is underestimated."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#how-to-detect-overdispersion",
    "href": "teaching/glm/slides/counting_models.html#how-to-detect-overdispersion",
    "title": "Counting Models",
    "section": "How to Detect Overdispersion?",
    "text": "How to Detect Overdispersion?\n\nBy assuming that \\(\\Var(Y|X = x) = \\phi \\E(Y|X = x)\\) where \\(\\phi &gt; 0\\), we can estimate \\(\\phi\\) by\n\n\\[\\hat{\\phi} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\hat{y}_i)^2}{\\hat{y}_i}\\]\n\nand test if \\(\\phi = 1\\) or not (if \\(\\phi = 1\\), \\(\\hat{\\phi} \\sim N(1, 1/n)\\) when \\(n \\to \\infty\\)).\n\n\nWe can fit a negative binomial model (cf the following), and test if it is better than the Poisson model."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#the-quasi-poisson-model",
    "href": "teaching/glm/slides/counting_models.html#the-quasi-poisson-model",
    "title": "Counting Models",
    "section": "The Quasi-Poisson Model",
    "text": "The Quasi-Poisson Model\n\nEstimate \\(E(Y|X = x) = \\lambda_\\beta(x)\\) with \\(\\lambda_\\beta(x) = \\exp(x^T \\beta)\\) in the same way as with a Poisson model (same likelihood).\nEstimate \\(\\phi\\) as in the previous slide.\nEstimate the variance of \\(\\hat{\\beta}\\) taking into account \\(\\hat{\\phi}\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#quasi-poisson-vs-poisson-model",
    "href": "teaching/glm/slides/counting_models.html#quasi-poisson-vs-poisson-model",
    "title": "Counting Models",
    "section": "Quasi-Poisson VS Poisson Model",
    "text": "Quasi-Poisson VS Poisson Model\n\nThe estimated coefficients \\(\\hat{\\beta}\\) are identical.\nThe predictions \\(\\hat{y}_i = \\exp(\\lambda_{\\hat{\\beta}}(x_i))\\) are identical.\nOnly the estimation of standard errors differ, and therefore possibly the significance of coefficients.\n\n\nThe estimation procedure does not rely on the “true” likelihood (because of \\(\\phi\\)): we therefore do not have access to \\(L_{\\text{mod}}\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-plant-species-with-quasi-poisson",
    "href": "teaching/glm/slides/counting_models.html#example-plant-species-with-quasi-poisson",
    "title": "Counting Models",
    "section": "Example: Plant Species with Quasi Poisson",
    "text": "Example: Plant Species with Quasi Poisson\n\nglm(Species∼ pH+ Biomass+ pH:Biomass, family=quasipoisson)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;\nt\n\n\n\n\n(Intercept)\n3.76812\n0.06144\n61.332\n&lt; 2e-16\n***\n\n\npHlow\n-0.81557\n0.10268\n-7.943\n7.90e-12\n***\n\n\npHmid\n-0.33146\n0.09203\n-3.602\n0.000534\n***\n\n\nBiomass\n-0.10713\n0.01247\n-8.590\n3.97e-13\n***\n\n\npHlow:Biomass\n-0.15503\n0.03997\n-3.878\n0.000208\n***\n\n\npHmid:Biomass\n-0.03189\n0.02304\n-1.384\n0.169985\n\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n(Dispersion parameter for quasipoisson family taken to be 0.9970074)\nNull deviance: 452.346 on 89 degrees of freedom\nResidual deviance: 83.201 on 84 degrees of freedom\nAIC: NA"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#interpretation",
    "href": "teaching/glm/slides/counting_models.html#interpretation",
    "title": "Counting Models",
    "section": "Interpretation",
    "text": "Interpretation\n\n\\(\\hat{\\phi} = 0.997 \\approx 1\\) therefore there was no overdispersion issue\n\n\nThe Residual deviance is incorrect: it’s that of the Poisson model."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#negative-binomial-distribution",
    "href": "teaching/glm/slides/counting_models.html#negative-binomial-distribution",
    "title": "Counting Models",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\n\nAlternative to Poisson: Negative Binomial (NB), which depends on 2 parameters:\n\nits expectation \\(\\lambda &gt; 0\\)\nthe “number of successes” (or size) \\(\\theta &gt; 0\\)\n\n\n\nIf \\(Y \\sim NB(\\lambda, \\theta)\\), then for all \\(k \\in \\mathbb{N}\\),\n\n\\[P(Y = k) = \\frac{\\Gamma(k + \\theta)}{\\Gamma(k + 1)\\Gamma(\\theta)} \\left(\\frac{\\lambda}{\\lambda + \\theta}\\right)^k \\left(\\frac{\\theta}{\\lambda + \\theta}\\right)^\\theta\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#properties-of-negative-binomial",
    "href": "teaching/glm/slides/counting_models.html#properties-of-negative-binomial",
    "title": "Counting Models",
    "section": "Properties of Negative Binomial",
    "text": "Properties of Negative Binomial\n\nThe expectation of \\(NB(\\lambda, \\theta)\\) equals \\(\\lambda\\).\nThe variance of \\(NB(\\lambda, \\theta)\\) equals \\(\\lambda + \\lambda^2/\\theta\\).\n\n\nThis distribution can therefore model overdispersion (but not underdispersion).\n\n\nIf \\(\\theta \\to +\\infty\\), \\(NB(\\lambda, \\theta) \\approx P(\\lambda)\\).\n\n\nThe Poisson distribution is therefore a special case of the NB distribution."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#illustration-of-nb",
    "href": "teaching/glm/slides/counting_models.html#illustration-of-nb",
    "title": "Counting Models",
    "section": "Illustration of NB",
    "text": "Illustration of NB"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#negative-binomial-glm-model",
    "href": "teaching/glm/slides/counting_models.html#negative-binomial-glm-model",
    "title": "Counting Models",
    "section": "Negative Binomial GLM Model",
    "text": "Negative Binomial GLM Model\n\nThe negative binomial GLM model assumes that\n\n\\[Y|(X = x) \\sim NB(\\lambda(x), \\theta)\\]\n\nwhere as usual \\(\\lambda(x) = \\exp(x^T \\beta)\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#inference-an-other-tools-in-nb-glm-model",
    "href": "teaching/glm/slides/counting_models.html#inference-an-other-tools-in-nb-glm-model",
    "title": "Counting Models",
    "section": "Inference an other Tools in NB GLM model",
    "text": "Inference an other Tools in NB GLM model\n\nThe estimation of \\(\\beta\\) and \\(\\theta\\) is done by maximum likelihood\n\n\nAll usual inference tools are available:\n\nWald tests; Deviance; AIC, BIC.\n\n\n\nIf \\(\\hat{\\theta}\\) is large, this is equivalent to the Poisson model.\n\n\nWe can test the interest of the NB model compared to the Poisson model by inspecting \\(\\hat{\\theta}\\), or by comparing their AIC and BIC criteria."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-plant-species-and-nb-model",
    "href": "teaching/glm/slides/counting_models.html#example-plant-species-and-nb-model",
    "title": "Counting Models",
    "section": "Example: Plant Species and NB Model",
    "text": "Example: Plant Species and NB Model\n\nglm.nb(Species∼ pH+ Biomass+ pH:Biomass)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n3.76813\n0.06154\n61.235\n&lt; 2e-16\n***\n\n\npHlow\n-0.81557\n0.10284\n-7.930\n2.19e-15\n***\n\n\npHmid\n-0.33146\n0.09217\n-3.596\n0.000323\n***\n\n\nBiomass\n-0.10713\n0.01249\n-8.577\n&lt; 2e-16\n***\n\n\npHlow:Biomass\n-0.15503\n0.04003\n-3.873\n0.000108\n***\n\n\npHmid:Biomass\n-0.03189\n0.02308\n-1.382\n0.166978\n\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n(Dispersion parameter for Negative Binomial(212058.3) family taken to be 1)\nNull deviance: 452.307 on 89 degrees of freedom\nResidual deviance: 83.194 on 84 degrees of freedom\nAIC: 516.39\n\n\n\n\\(\\hat{\\theta} = 212058.3\\) therefore the model is equivalent to the Poisson model.\n\n\nThis is confirmed via the AIC."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#the-problem",
    "href": "teaching/glm/slides/counting_models.html#the-problem",
    "title": "Counting Models",
    "section": "The Problem",
    "text": "The Problem\n\nWhen \\(Y\\) is a count variable, it is not uncommon for \\(Y = 0\\) to appear very often in the sample.\n\n\nExample 1: \\(Y\\): amount of rain (in mm) that fell each day.\n\n\nExample 2: \\(Y\\): amount of alcohol (in glasses) consumed each week."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#model-inadequacy",
    "href": "teaching/glm/slides/counting_models.html#model-inadequacy",
    "title": "Counting Models",
    "section": "Model Inadequacy",
    "text": "Model Inadequacy\n\nA Poisson or Negative Binomial model is not adapted.\n\n\nGenerally two “populations” that explain this phenomenon:\n\none for which \\(Y = 0\\) systematically\none for which \\(Y \\geq 0\\) (or \\(Y &gt; 0\\))\n\n\n\nWe can consider two modelings in this spirit:\n\nThe zero-inflated model (\\(Y = 0\\) versus \\(Y \\geq 0\\))\nThe Hurdle model (\\(Y = 0\\) versus \\(Y &gt; 0\\))\n\n\n\nWe present below the zero-inflated model."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#zip-model-definition",
    "href": "teaching/glm/slides/counting_models.html#zip-model-definition",
    "title": "Counting Models",
    "section": "ZIP Model Definition",
    "text": "ZIP Model Definition\n\nThe ZIP (Zero-Inflated Poisson) model assumes that\n\n\\[Y|(X = x) = \\begin{cases}\n0 & \\text{with probability } q(x) \\\\\nP(\\lambda(x)) & \\text{with probability } 1 - q(x)\n\\end{cases}\\]\n\nwhere\n\n\\[\\lambda(x) = \\exp(x^T \\beta) \\quad \\text{and} \\quad q(x) = \\text{logit}^{-1}(x^T \\gamma)\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#two-mixed-populations",
    "href": "teaching/glm/slides/counting_models.html#two-mixed-populations",
    "title": "Counting Models",
    "section": "Two Mixed Populations",
    "text": "Two Mixed Populations\n\nThus two populations mix:\n\nFor one, \\(Y\\) is always \\(0\\). For the other, \\(Y \\in \\mathbb{N}\\).\nA logistic model explains membership to one or the other population.\nA Poisson log-linear model is used for the second."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#probability-distribution",
    "href": "teaching/glm/slides/counting_models.html#probability-distribution",
    "title": "Counting Models",
    "section": "Probability Distribution",
    "text": "Probability Distribution\n\nWith this model, we therefore have\n\n\\(P(Y = 0|X = x) = q_\\gamma(x) + (1 - q_\\gamma(x))e^{-\\lambda_\\beta(x)}\\)\n\n\n\n\n\\(P(Y = k|X = x) = (1 - q_\\gamma(x))e^{-\\lambda_\\beta(x)} \\frac{\\lambda_\\beta(x)^k}{k!}\\)\n\nwhere \\(\\lambda_\\beta(x) = \\exp(x^T \\beta)\\) and \\(q_\\gamma(x) = \\text{logit}^{-1}(x^T \\gamma)\\).\n\n\nWe deduce\n\n\\(E(Y|X = x) = (1 - q_\\gamma(x))\\lambda_\\beta(x)\\)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#inference",
    "href": "teaching/glm/slides/counting_models.html#inference",
    "title": "Counting Models",
    "section": "Inference",
    "text": "Inference\n\nFor inference:\n\nWe can write the log-likelihood as a function of \\(\\beta\\) and \\(\\gamma\\)\nWe obtain estimators by maximum likelihood\nAnd we have access to usual inference tools\n\n\n\nUnder R: zeroinfl function from the pscl package."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#zinb-model-definition",
    "href": "teaching/glm/slides/counting_models.html#zinb-model-definition",
    "title": "Counting Models",
    "section": "ZINB Model Definition",
    "text": "ZINB Model Definition\n\nSimilarly, the ZINB (Zero-Inflated Negative Binomial) model is\n\n\\[Y|(X = x) = \\begin{cases}\n0 & \\text{with probability } q(x) \\\\\nNB(\\lambda(x), \\theta) & \\text{with probability } 1 - q(x)\n\\end{cases}\\]\n\nwhere \\(\\theta &gt; 0\\) and\n\\(\\lambda(x) = \\exp(x^T \\beta) \\quad \\text{and} \\quad q(x) = \\text{logit}^{-1}(x^T \\gamma)\\)\n\n\nUnder R: zeroinfl function with the option dist=\"negbin\"."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-horseshoe-crabs",
    "href": "teaching/glm/slides/counting_models.html#example-horseshoe-crabs",
    "title": "Counting Models",
    "section": "Example: Horseshoe Crabs",
    "text": "Example: Horseshoe Crabs\n\nNumber of male satellites on female horseshoe crabs.\n\n\n\n\n\n\n\n\nWe want to model the number of satellites (satell) as a function of the weight of the horseshoe crab (weight) and its color (color, from 1 to 4, ≈ age)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-poisson-log-linear-model",
    "href": "teaching/glm/slides/counting_models.html#example-poisson-log-linear-model",
    "title": "Counting Models",
    "section": "Example: Poisson log-linear Model :",
    "text": "Example: Poisson log-linear Model :\n\nglm(satell∼weight+color,family=’poisson’,data=crabs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n0.08855\n0.25443\n0.348\n0.72783\n\n\n\nweight\n0.54588\n0.06749\n8.088\n6.05e-16\n***\n\n\ncolor\n-0.17282\n0.06155\n-2.808\n0.00499\n**\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n(Dispersion parameter for poisson family taken to be 1)\nNull deviance: 632.79 on 172 degrees of freedom\nResidual deviance: 552.79 on 170 degrees of freedom\nAIC: 914.09\n\nThe model is significant but the fit is poor (the deviance is very high)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-log-linear-poisson",
    "href": "teaching/glm/slides/counting_models.html#example-log-linear-poisson",
    "title": "Counting Models",
    "section": "Example: Log-Linear Poisson",
    "text": "Example: Log-Linear Poisson\n\n\n\n\nIn red: empirical distribution of satell on the sample\nPoints: predicted distribution"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-nb-model",
    "href": "teaching/glm/slides/counting_models.html#example-nb-model",
    "title": "Counting Models",
    "section": "Example: NB Model",
    "text": "Example: NB Model\n\nglm.nb(satell∼weight+color,data=crabs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-0.3220\n0.5540\n-0.581\n0.561\n\n\n\nweight\n0.7072\n0.1612\n4.387\n1.15e-05\n***\n\n\ncolor\n-0.1734\n0.1199\n-1.445\n0.148\n\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n(Dispersion parameter for Negative Binomial(0.9555) family taken to be 1)\nNull deviance: 219.50 on 172 degrees of freedom\nResidual deviance: 196.64 on 170 degrees of freedom\nAIC: 754.45\n\n\n\nIt’s better… (by the way: color doesn’t seem significant)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-nb-model-1",
    "href": "teaching/glm/slides/counting_models.html#example-nb-model-1",
    "title": "Counting Models",
    "section": "Example: NB Model",
    "text": "Example: NB Model\n\n\n\n\nStill not very satisfactory."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-zip-model",
    "href": "teaching/glm/slides/counting_models.html#example-zip-model",
    "title": "Counting Models",
    "section": "Example: ZIP Model",
    "text": "Example: ZIP Model\n\nzeroinfl(satell∼weight | weight+color,dist=\"poisson\",data=crabs)\n\n\n\nCount model coefficients (poisson with log link):\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n1.00152\n0.20793\n4.817\n1.46e-06\n***\n\n\nweight\n0.19020\n0.07572\n2.512\n0.012\n*\n\n\n\nZero-inflation model coefficients (binomial with logit link):\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n1.9621\n1.1448\n1.714\n0.0866\n.\n\n\nweight\n-1.6630\n0.3943\n-4.218\n2.47e-05\n***\n\n\ncolor\n0.5329\n0.2305\n2.312\n0.0208\n*\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nNumber of iterations in BFGS optimization: 9\nLog-likelihood: -360.8 on 5 Df"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-zip-model-1",
    "href": "teaching/glm/slides/counting_models.html#example-zip-model-1",
    "title": "Counting Models",
    "section": "Example: ZIP Model",
    "text": "Example: ZIP Model\n\nWe see the estimation result explaining the mixture of the two “populations” forming the model.\n\n\nFirst population: Poisson log-linear model with expectation \\(\\lambda\\) where \\(\\lambda(\\text{weight}) = \\exp(1 + 0.19 \\times \\text{weight})\\)\n\n\nSecond population is \\(0\\). The probability \\(q\\) of belonging to the second one is\n\\(q(\\text{wgt}, \\text{col}) = \\text{logit}^{-1}(1.96 - 1.66 \\times \\text{wgt} + 0.53 \\times \\text{col})\\)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-zip-model-2",
    "href": "teaching/glm/slides/counting_models.html#example-zip-model-2",
    "title": "Counting Models",
    "section": "Example: ZIP Model",
    "text": "Example: ZIP Model\n\n\n\n\nMuch more convincing."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-zinb-model",
    "href": "teaching/glm/slides/counting_models.html#example-zinb-model",
    "title": "Counting Models",
    "section": "Example: ZINB Model",
    "text": "Example: ZINB Model\n\nzeroinfl(satell∼weight | weight+color,dist=\"negbin\",data=crabs)\n\n\n\nCount model coefficients (negbin with log link):\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n0.8961\n0.3070\n2.919\n0.00351\n**\n\n\nweight\n0.2169\n0.1125\n1.928\n0.05383\n.\n\n\nLog(theta)\n1.5802\n0.3574\n4.422\n9.79e-06\n***\n\n\n\nZero-inflation model coefficients (binomial with logit link):\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n1.8663\n1.2415\n1.503\n0.133\n\n\n\nweight\n-1.7531\n0.4429\n-3.958\n7.55e-05\n***\n\n\ncolor\n0.5985\n0.2572\n2.326\n0.020\n*\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nTheta = 4.8558\nNumber of iterations in BFGS optimization: 11\nLog-likelihood: -349.9 on 6 Df"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-zinb-model-1",
    "href": "teaching/glm/slides/counting_models.html#example-zinb-model-1",
    "title": "Counting Models",
    "section": "Example: ZINB Model",
    "text": "Example: ZINB Model"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#zinb-model-interpretation",
    "href": "teaching/glm/slides/counting_models.html#zinb-model-interpretation",
    "title": "Counting Models",
    "section": "ZINB Model Interpretation",
    "text": "ZINB Model Interpretation\n\nThe output is read in the same way as for the ZIP model\n\n\nThere is the parameter \\(\\theta\\) in addition, from the NB distribution\n\n\n\\(\\theta\\) is not “infinite”: the model therefore does not reduce to the ZIP model.\n\nA deviance test shows that ZINB is preferable to ZIP\nThis is also confirmed by the AIC and BIC criteria."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-conclusion",
    "href": "teaching/glm/slides/counting_models.html#example-conclusion",
    "title": "Counting Models",
    "section": "Example: Conclusion",
    "text": "Example: Conclusion\n\nA portion of horseshoe crabs (the smallest and oldest) have no male satellites.\n\n\nThe probability of belonging to this population is estimated at\n\\(q(\\text{wht}, \\text{col}) = \\text{logit}^{-1}(1.87 - 1.75 \\times \\text{wht} + 0.60 \\times \\text{col})\\)\n\n\nFor the other portion of horseshoe crabs, they have on average more satellites the larger they are. This average is estimated at\n\\(\\lambda(\\text{wht}) = \\exp(0.90 + 0.22 \\times \\text{wht})\\)\n\n\nDistribution cna be modeled as \\(NB(\\lambda(\\text{wht}), \\theta)\\) where \\(\\theta = 4.86\\)."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#recalling-the-linear-model",
    "href": "teaching/glm/slides/Introduction_glm.html#recalling-the-linear-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Recalling the Linear Model",
    "text": "Recalling the Linear Model\n. . .\nWe observe \\(Y = (Y_1, \\dots, Y_n)\\) and \\(X = (X^{(1)} , . . . , X^{(p)}) \\in \\mathbb R^{n \\times p}\\),\n. . .\nIn the Linear Model, We assume that for some unknown \\(\\beta\\), \\(\\sigma^2\\) where \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2 I_n)\\),\n\n\\[Y = X\\beta + \\varepsilon\\]\n\n. . .\nThe hypothesis can be written in the form \\(\\mathbb E[Y|X] = X\\beta\\)\n. . .\nThe OLS estimator of \\(\\beta\\) is \\(\\hat \\beta= (X^TX)^{-1}X^TY\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#when-this-linearity-is-reasonable",
    "href": "teaching/glm/slides/Introduction_glm.html#when-this-linearity-is-reasonable",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When this Linearity is Reasonable",
    "text": "When this Linearity is Reasonable\n. . .\nThe hypothesis \\(\\E(Y|X) = X^T\\beta\\) in linear regression models implies that \\(\\E(Y|X)\\) can take any real value.\n. . .\nThis is not a restriction when:\n\n\\(Y|X\\) follows a Gaussian distribution\n\\(Y|X\\) follows any other “nice” continuous distribution on \\(\\mathbb{R}\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#when-it-is-not",
    "href": "teaching/glm/slides/Introduction_glm.html#when-it-is-not",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When it is not",
    "text": "When it is not\n. . .\nThe linear assumption is inappropriate for certain variables \\(Y\\), particularly when \\(Y\\) is qualitative or discrete."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#when-it-is-not-1",
    "href": "teaching/glm/slides/Introduction_glm.html#when-it-is-not-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When it is not",
    "text": "When it is not\n. . .\nBinary outcomes (\\(Y = 0\\) or \\(1\\)):\n\ndisease presence\n\n. . .\nCategorical outcomes (\\(Y \\in \\{A_1, \\ldots, A_k\\}\\)):\n\nTransportation choice: \\(\\{\\) Car, Bus, Bike \\(\\}\\)\n\n. . .\nCount data (\\(Y \\in \\mathbb{N}\\)):\n\nNumber of traffic accident per month"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#key-differences-by-response-type",
    "href": "teaching/glm/slides/Introduction_glm.html#key-differences-by-response-type",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Key Differences by Response Type",
    "text": "Key Differences by Response Type\n. . .\nIn all examples, the objective remains to link \\(Y\\) to \\(X = (X^{(1)}, \\ldots, X^{(p)})\\) through modeling \\(\\E(Y|X)\\).\n. . .\nHowever, \\(\\E(Y|X)\\) has different interpretations depending on the situation:\n\nBinary \\(Y\\): \\(Y = 0\\) or \\(1\\)\nCategorical \\(Y\\): \\(Y \\in \\{A_1, \\ldots, A_k\\}\\)\n\nCount data: \\(Y \\in \\mathbb{N}\\)\n\n. . .\nIn all these cases, the linear model \\(\\E(Y|X) = X^T\\beta\\) is inappropriate."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#objectives-of-the-glm-1",
    "href": "teaching/glm/slides/Introduction_glm.html#objectives-of-the-glm-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Objectives of the GLM",
    "text": "Objectives of the GLM\n. . .\nWe model \\(\\E(Y|X)\\) differently using generalized linear models.\nAs in linear regression, we focus on:\n\nSpecific effects: The individual effect of a given regressor, (all other things being equal)\nExplanation: Understanding relationships\nPrediction: Forecasting outcomes"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#three-fundamental-cases",
    "href": "teaching/glm/slides/Introduction_glm.html#three-fundamental-cases",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Three Fundamental Cases",
    "text": "Three Fundamental Cases\n. . .\nWe detail the modeling challenges for \\(\\E(Y|X)\\) in three fundamental cases:\n\nCase 1: Binary: \\(Y\\) is binary (takes values 0 or 1)\nCase 2: Categorical: \\(Y \\in \\{A_1, \\ldots, A_k\\}\\) (general qualitative variable)\nCase 3: Count: \\(Y \\in \\mathbb{N}\\) (count variable)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-1-binary-case",
    "href": "teaching/glm/slides/Introduction_glm.html#case-1-binary-case",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 1: Binary Case",
    "text": "Case 1: Binary Case\n. . .\nWithout loss of generality, \\(Y \\in \\{0, 1\\}\\)\n. . .\nIf \\(Y\\) models membership in a category \\(A\\), this is equivalent to studying the variable \\(Y = \\mathbf{1}_A\\)\n. . .\nThe distribution of \\(Y\\) given \\(X = x\\) is entirely determined by \\(p(x) = P(Y = 1|X = x)\\)\n. . .\nWe deduce \\(P(Y = 0|X = x) = 1 - p(x)\\)\n. . .\n\\(Y|X = x\\) follows a Bernoulli distribution with parameter \\(p(x)\\)\n. . .\n\\(\\E(Y|X = x) = p(x)\\)\n. . .\nkey constraint: \\(p(x) \\in [0, 1]\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#modelling-px",
    "href": "teaching/glm/slides/Introduction_glm.html#modelling-px",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Modelling \\(p(x)\\)",
    "text": "Modelling \\(p(x)\\)\n. . .\n\n\\[E(Y|X = x) = P(Y = 1|X = x) = p(x) \\in [0, 1]\\]\n\n. . .\nWhat NOT to do: \\(p(x) = x^T\\beta\\) (for some \\(\\beta \\in \\mathbb{R}^p\\) to be estimated)\n. . .\nProposed approach: We can propose a model of the type:\n\n\\[p(x) = f(x^T\\beta)\\]\n\nwhere \\(f\\) is a function from \\(\\mathbb{R}\\) to \\([0, 1]\\)\n. . .\nBenefits: Coherent model that depends only on \\(\\beta\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-2-categorical-y",
    "href": "teaching/glm/slides/Introduction_glm.html#case-2-categorical-y",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Categorical \\(Y\\)",
    "text": "Case 2: Categorical \\(Y\\)\n. . .\nIf \\(Y\\) represents membership in \\(k\\) different classes \\(A_1, \\ldots, A_k\\), its distribution is determined by the probabilities:\n\n\\[p_j(x) = P(Y \\in A_j|X = x), \\quad \\text{for } j = 1, \\ldots, k\\]\n\n. . .\nConstraint: \\(\\sum_{j=1}^{k} p_j(x) = 1\\) (If \\(k = 2\\), this reduces to the previous case)\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-2-categorical-y-1",
    "href": "teaching/glm/slides/Introduction_glm.html#case-2-categorical-y-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Categorical \\(Y\\)",
    "text": "Case 2: Categorical \\(Y\\)\n. . .\n\\(Y = (\\mathbf{1}_{A_1}, \\ldots, \\mathbf{1}_{A_k})\\) follows a multinomial distribution and:\n\n\\[\\E(Y|X = x) = \\begin{pmatrix} p_1(x) \\\\ \\vdots \\\\ p_k(x) \\end{pmatrix}\\]\n\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-2-model-for-categorical-y",
    "href": "teaching/glm/slides/Introduction_glm.html#case-2-model-for-categorical-y",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Model for Categorical \\(Y\\)",
    "text": "Case 2: Model for Categorical \\(Y\\)\n. . .\nTo model \\(\\E(Y|X = x)\\), it suffices to model \\(p_1(x), \\ldots, p_{k-1}(x)\\) since \\(p_k(x) = 1 - \\sum_{j=1}^{k-1} p_j(x)\\)\n. . .\nProposed model: As in the binary case, we can propose:\n\n\\[p_j(x) = f(x^T\\beta_j), \\quad j = 1, \\ldots, k-1\\]\n\nwhere \\(f: \\mathbb{R} \\to [0,1]\\)\n. . .\nParameters: There will be \\(k-1\\) unknown parameters to estimate, each in \\(\\mathbb{R}^p\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-3-count-y---non-negative-integer-values",
    "href": "teaching/glm/slides/Introduction_glm.html#case-3-count-y---non-negative-integer-values",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 3: Count Y - Non-negative Integer Values",
    "text": "Case 3: Count Y - Non-negative Integer Values\n. . .\nIf \\(Y\\) takes integer values, we have for all \\(x\\), \\(E(Y|X = x) \\geq 0\\)\n. . .\nCoherent choice: A coherent approach is:\n\n\\[\\E(Y|X = x) = f(x^T\\beta)\\]\n\nwhere \\(f\\) is a function from \\(\\mathbb{R}\\) to \\([0, +\\infty)\\)\n. . .\nExample of possible choice for f: The exponential function"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#model-formulation",
    "href": "teaching/glm/slides/Introduction_glm.html#model-formulation",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Model Formulation",
    "text": "Model Formulation\n. . .\nLet \\(g\\) be a strictly monotonic function, called the link function\n. . .\nA generalized linear model (GLM) establishes a relationship of the type:\n\n\\[g(\\E(Y|X = x)) = x^T\\beta\\]\n\nEquivalently,\n\n\\[\\E(Y|X = x) = g^{-1}(x^T\\beta)\\]\n\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#remarks-on-model",
    "href": "teaching/glm/slides/Introduction_glm.html#remarks-on-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Remarks on Model",
    "text": "Remarks on Model\n\nIn the previous examples, \\(g^{-1}\\) was denoted \\(f\\)\nWe generally assume that the distribution \\(Y|X\\) belongs to the exponential family, with unknown parameter \\(\\beta\\)\nThis allows us to compute the likelihood and facilitates inference"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#remark-on-the-intercept",
    "href": "teaching/glm/slides/Introduction_glm.html#remark-on-the-intercept",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Remark on the Intercept",
    "text": "Remark on the Intercept\n. . .\nAmong the explanatory variables \\(X^{(1)}, \\ldots, X^{(p)}\\), we often assume that \\(X^{(1)}=\\1\\) to account for the presence of a constant. Thus: \\[X\\beta = \\beta_1 X^{(1)} + \\cdots + \\beta_p X^{(p)}\\]\n. . .\nAlternative notation: Sometimes indexed differently to write \\(\\beta_0 + \\beta_1 X^{(1)} + \\cdots + \\beta_p X^{(p)}\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#example-1-linear-regression-model",
    "href": "teaching/glm/slides/Introduction_glm.html#example-1-linear-regression-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 1: Linear Regression Model",
    "text": "Example 1: Linear Regression Model\n. . .\nLink function: We recover linear regression by taking the identity link function \\(g(t) = t\\)\n. . .\nExpected value: Then:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) = x^T\\beta\\]\n\n. . .\nIn the Gaussian linear model:\n\n\\[Y|X \\sim \\mathcal{N}(X\\beta, \\sigma^2)\\]\n\n. . .\nLinear regression is therefore a special case of GLM models!"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#example-2-binary-case-y-in-0-1",
    "href": "teaching/glm/slides/Introduction_glm.html#example-2-binary-case-y-in-0-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)",
    "text": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)\n. . .\nLink function requirement: The link function \\(g\\) must satisfy:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) \\in [0, 1]\\]\n\n. . .\nSince \\(Y\\in \\{0,1\\}\\), \\(Y|X\\) follows a Bernoulli distribution\n\n\\[Y|X \\sim \\mathcal{B}(g^{-1}(X^T\\beta))\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#example-2-binary-case-y-in-0-1-1",
    "href": "teaching/glm/slides/Introduction_glm.html#example-2-binary-case-y-in-0-1-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)",
    "text": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)\n. . .\n\n\\[Y|X \\sim \\mathcal{B}(g^{-1}(X^T\\beta))\\]\n\nPossible choices for \\(g^{-1}\\): A CDF of a continuous distrib. on \\(\\mathbb{R}\\)\n. . .\nStandard choice for \\(g^{-1}\\): The CDF of a logistic distribution:\n\n\\[g^{-1}(t) = \\frac{e^t}{1 + e^t} \\quad \\text{i.e.} \\quad g(t) = \\ln\\left(\\frac{t}{1-t}\\right) = \\text{logit}(t)\\]\n\n. . .\nThis leads to the logistic model, the most important model in this chapter"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#example-3-count-data-yin-mathbb-n",
    "href": "teaching/glm/slides/Introduction_glm.html#example-3-count-data-yin-mathbb-n",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 3: Count Data \\(Y\\in \\mathbb N\\)",
    "text": "Example 3: Count Data \\(Y\\in \\mathbb N\\)\n. . .\nLink function: \\(g(t) = \\ln(t)\\), \\(g^{-1}(t) = e^t\\) gives:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) = e^{x^T\\beta}\\]\n\n. . .\nFor the distribution of \\(Y|X\\), defined on \\(\\mathbb{N}\\), we often assume it follows a Poisson distribution (exp. familily)\n. . .\nIn this context:\n\n\\[Y|X \\sim \\mathcal{P}(e^{X^T\\beta})\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#summary",
    "href": "teaching/glm/slides/Introduction_glm.html#summary",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Summary",
    "text": "Summary\nThere are 2 choices to make when setting up a GLM model:\n\nThe distribution of \\(Y|X\\)\nThe link function \\(g\\) defining \\(E(Y|X) = g^{-1}(X^T\\beta)\\)\n\n. . .\nKey insight: The second choice is linked to the first"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#the-3-common-cases",
    "href": "teaching/glm/slides/Introduction_glm.html#the-3-common-cases",
    "title": "Introduction to the Generalized Linear Model",
    "section": "The 3 Common Cases",
    "text": "The 3 Common Cases\n. . .\nBinary (\\(Y \\in \\{0, 1\\}\\)):\n→ \\(Y|X\\): it’s a Bernoulli distribution\n→ By default \\(g = \\text{logit}\\) (see later)\n. . .\nMulti-category (\\(Y \\in \\{A_1, \\ldots, A_k\\}\\)):\n→ \\(Y|X\\): it’s a multinomial distribution\n→ By default \\(g = \\text{logit}\\)\n. . .\nCount (\\(Y \\in \\mathbb{N}\\)):\n→ \\(Y|X\\): Poisson (often) or negative binomial\n→ Choice of \\(g\\): by default \\(g = \\ln\\)"
  },
  {
    "objectID": "teaching/probability/density_likelihood.html",
    "href": "teaching/probability/density_likelihood.html",
    "title": "Density, Likelihood and Radon Nikodym",
    "section": "",
    "text": "Notation\nIn this note, \\((\\Omega,\\mathbb P)\\) denote a common probability measured space for all the random variables introduced in this note. We also write \\(\\mathbb E\\) for the expectation. Let \\(X\\) be a random variable with values in a measurable space \\((\\mathcal X, \\mathcal A)\\). Let say for simplicity that \\(\\mathcal X = \\mathbb R^n\\).\nWe say that \\(X\\) has distribution \\(P\\) if \\(\\mathbb P(X \\in A)=P(A)\\) for any measurable set \\(A\\). For clarity, we sometimes write \\(\\mathbb P_{X \\sim P}(X \\in A)\\), which means “the probability of \\(X\\) being in \\(A\\) if \\(X\\) follows distribution \\(P\\)”. Sometimes, we do the slight abuse of notation by writing that \\(P(A) = P(X \\in A)\\).\n\\(P\\) can be seen as the “pushforward” measure of the common probability measure \\(\\mathbb P\\) by random variable \\(X\\), since by definition, \\(\\mathbb P(X \\in A) = \\mathbb P(\\{\\omega \\in \\Omega, X(\\omega) \\in A\\})= \\mathbb P(X^{-1}(A))\\).\n\n\nContinuous Densities\nA measure \\(P\\) has density \\(p\\) with respect to the Lebesgue measure if for any event \\(A\\) (which is simply a measurable set of \\(\\mathbb R^n\\)), \\[P(A)= \\int_{x \\in \\mathbb R^n}\\mathbf 1\\{x \\in A\\}p(x)dx \\; .\\] \\(p(x)\\) is sometimes called the likelihood of a random variables that has distribution \\(P\\) at point \\(x\\). An equivalent condition is that for any “kind” real valued function \\(f\\) (e.g. continuous with bounded support), \\[\\mathbb E_{X \\sim P}[f(X)] \\stackrel{\\mathrm{def}}{=} \\int_{x \\in \\mathbb R^n}f(x)dP =\\int_{x \\in \\mathbb R^n}f(x)p(x)dx \\; .\\] We write that \\[dP(x) = p(x)dx ~~~ \\text{or}~~~~ \\tfrac{dP}{dx}(x) = p(x) \\; ,\\] and \\(\\tfrac{dP}{dx}\\) is called the Radon-Nikodym derivative of \\(P\\) with respect to the Lebesgue measure. The intuition of this abstract notation is the following. If \\(x \\in \\mathbb R^n\\) and \\(h\\) is a small quantity that goes to \\(0\\), \\(dP\\) represents the measure of the interval \\([x, x+h]\\), with respect to the measure \\(P\\). Then, \\(d P(x) = P([x, x+h]) = \\int_x^{x+h}p(x)dx \\sim p(x)h\\).\n\n\nThe Counting Measure for Discrete Random Variables\nRandom variables in \\(\\mathbb{R}\\) that take on a finite number of values are referred to as discrete random variables, and they do not have a density with respect to the Lebesgue measure. However, this case is much simpler and is handled within measure theory using the counting measure. As its name indicates, the counting measure \\(\\mu\\) on \\(\\mathcal X=\\mathbb R^n\\) counts the elements of a given set \\(A\\): \\[ \\mu(A) = |A| \\enspace .\\] In particular, \\(\\mu(A)\\) is infinite if \\(A\\) is an infinite set.\nLet \\(X\\) be a discrete random variable that takes values in \\(\\{x_1, \\dots, x_N\\}\\), e.g. a Bernoulli, Binomial or Poisson random variable, and let \\(P\\) be its probability distribution. Let \\(p(x_i)\\) be the probability that \\(X = x_i\\), that is \\(p(x_i) = P(\\{x_i\\}) = P(X = x_i) \\in [0,1]\\). In this discrete case, the probability \\(p(x_i)\\) represents the likelihood of the value \\(x_i\\) for the random variable \\(X\\). While \\(X\\) has not a density with respect to the Lebesgue measure, it has density \\(p\\) with respect to the counting measure \\(\\mu\\), that is \\[\\mathbb E_{X \\sim P}[f(X)] =\\int_{x \\in \\mathbb R^n}f(x)p(x)d\\mu, ~~~~~~~ \\frac{dP}{d\\mu}(x) = p(x) \\; .\\] \\[ \\] ### The General Radon Nikodym Theorem\nThe Radon Nikodym Theorem tells us that any probability \\(P\\) admits a density with respect to a given measure \\(\\nu\\) if it is absolutely continuous with respect to \\(\\nu\\), that is \\[ \\nu(A) = 0 \\implies P(A) = 0 \\; .\\] In this case, the density is the Radon Nikodym derivative \\(\\frac{dP}{d\\nu}\\) of \\(P\\) with respect to \\(\\nu\\) and satisfies\n\\[ \\mathbb E_{X \\sim P}[f(X)] =\\int_{x \\in \\mathbb R^n}f(x)\\frac{dP}{d\\nu}(x)d\\nu \\; .\\] Informally, the \\(d\\nu\\) simplify so that \\(\\frac{dP}{d\\nu}d\\nu\\) = \\(dP\\).\n\n\nGeneralized Likelihood Ratio\nIf \\(P\\) and \\(Q\\) are two probability measures such that \\(P\\) is absolutely continuous with respect to \\(Q\\), then the Radon Nikodym derivative \\(\\frac{dP}{d\\nu}\\) of \\(P\\) with respect to \\(Q\\) is a generalized likelihood ratio.\nIf \\(P\\) and \\(Q\\) are both absolutely continuous with respect to another measure \\(\\nu\\) (for example the Lebesgue measure), then the generalized likelihood ratio can be written \\[\n\\frac{dP}{dQ} = \\frac{\\frac{dP}{d\\nu}}{\\frac{dQ}{d\\nu}} \\; .\n\\]\nIn particular, if \\(P\\) and \\(Q\\) have positive densities \\(p\\) and \\(q\\) with respect to the Lebesgue measure, that is \\(\\frac{dP}{dx} = p(x)\\) and \\(\\frac{dQ}{dx} = q(x)\\), then\n\\[\n\\frac{dP}{dQ}(x) = \\frac{p(x)}{q(x)} \\; .\n\\]\nIn particular, the likelihood ratio does not depend on the reference measure (here Lebesgue).\n\n\nChange of Measure\nIf \\(\\mathbb E_{P}\\) (resp. \\(\\mathbb E_{Q}\\)) denotes the expectation when the random variable \\(X\\) follows distribution \\(P\\) (resp. \\(Q\\)), then for any measurable and bounded function \\(f\\),\n\\[\\mathbb E_{P}[f(X)] = \\mathbb E_{Q}\\left[f(X) \\frac{dP}{dQ}(X)\\right] \\; .\\] In other words, we simply replace the real random variable \\(f(X)\\) by the random variable \\(f(X) \\frac{dP}{dQ}(X)\\) when we observe \\(X\\) under \\(Q\\) instead of \\(P\\). This results directly follows from Radon-Nikodym: \\[\n\\int_{x \\in R^n} f(x) dP(x) = \\int_{x \\in R^n} f(x) \\frac{dP}{dQ}(x) dQ(x) \\; .\n\\]",
    "crumbs": [
      "Probability",
      "Density and Likelihood"
    ]
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#context",
    "href": "teaching/glm/slides/logistic_model.html#context",
    "title": "The Logistic Model",
    "section": "Context",
    "text": "Context\n\\(Y\\) is a binary variable \\(Y_k \\in \\{0,1\\}\\)\n. . .\n\\(X = (X^{(1)}, \\ldots, X^{(p)})\\) are \\(p\\) regressors\n. . .\n\\(Y|X = x\\) follows a Bernoulli distribution with parameter \\(p(x) = P(Y = 1|X = x)\\). Model:\n. . .\n\n\\[p(x) = g^{-1}(x^T\\beta)\\]\n\nwhere \\(g^{-1}\\) is a strictly increasing function with values in \\([0, 1]\\)\n. . .\nApproach: We begin by discussing the choice of \\(g^{-1}\\) (or \\(g\\))"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-coronary-heart-disease",
    "href": "teaching/glm/slides/logistic_model.html#example-coronary-heart-disease",
    "title": "The Logistic Model",
    "section": "Example: Coronary Heart Disease",
    "text": "Example: Coronary Heart Disease\n. . .\nData Description: Presence of chd as a function of age \\(Y = \\text{chd} \\in \\{0,1\\}\\), \\(X = \\text{age}\\)\n. . .\nWe want to estimate \\(p(x) = \\E(Y|X = x) = \\P(\\text{chd} = 1|X = x)\\) for all \\(x\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-simple-idea",
    "href": "teaching/glm/slides/logistic_model.html#example-simple-idea",
    "title": "The Logistic Model",
    "section": "Example: Simple Idea",
    "text": "Example: Simple Idea\n\nGroup the \\(x\\) values by age class\nCalculate the proportion of \\(chd = 1\\) in the class containing \\(x\\)\n\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-glm-approach",
    "href": "teaching/glm/slides/logistic_model.html#example-glm-approach",
    "title": "The Logistic Model",
    "section": "Example: GLM Approach",
    "text": "Example: GLM Approach\n. . .\nObjective: We want to model \\(p(x) = P(\\text{chd} = 1|X = x)\\) by: \\[p(x) = g^{-1}(\\beta_0 + \\beta_1 x)\\]\n. . .\nConstraint: We need \\(g^{-1}\\) to have values in \\([0, 1]\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.html#example-the-logit-model",
    "title": "The Logistic Model",
    "section": "Example: The Logit Model",
    "text": "Example: The Logit Model\n. . .\n\n\n\\[g^{-1}(t) = \\frac{e^t}{1 + e^t} \\quad \\text{i.e.} \\quad g(t) = \\ln\\left(\\frac{t}{1-t}\\right) = \\text{logit}(t)\\]\n\n\n. . .\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-the-probit-model",
    "href": "teaching/glm/slides/logistic_model.html#example-the-probit-model",
    "title": "The Logistic Model",
    "section": "Example: The Probit Model",
    "text": "Example: The Probit Model\n. . .\nIf \\(\\Phi\\) is the CDF of a \\(\\mathcal{N}(0, 1)\\) distribution, we take \\(g^{-1}(t) = \\Phi(t)\\)\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-model-cloglog",
    "href": "teaching/glm/slides/logistic_model.html#example-model-cloglog",
    "title": "The Logistic Model",
    "section": "Example: Model cloglog",
    "text": "Example: Model cloglog\n. . .\n\n\\[g^{-1}(t) = 1 - e^{-e^t}\\]\n\n. . .\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-comparison-of-the-three-models",
    "href": "teaching/glm/slides/logistic_model.html#example-comparison-of-the-three-models",
    "title": "The Logistic Model",
    "section": "Example: Comparison of the Three Models",
    "text": "Example: Comparison of the Three Models\n. . .\nlogit and probit give approximately the same result cloglog differs slightly and is not “symmetric”"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#which-link-function-to-choose-for-binary-case",
    "href": "teaching/glm/slides/logistic_model.html#which-link-function-to-choose-for-binary-case",
    "title": "The Logistic Model",
    "section": "Which Link Function to Choose for Binary Case?",
    "text": "Which Link Function to Choose for Binary Case?\n. . .\nQuestion: Which link function to choose in practice when \\(Y\\) is binary?\n. . .\nDefault choice: By default, we favor the logit model\n. . .\nExceptions: Unless there is a good reason to choose something else (probit model, complementary log-log, or log-log)\n. . .\nNext steps: We return to the 3 usual choices to justify this preference"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#details-on-the-probit-model",
    "href": "teaching/glm/slides/logistic_model.html#details-on-the-probit-model",
    "title": "The Logistic Model",
    "section": "Details on the Probit Model",
    "text": "Details on the Probit Model\n. . .\nThe probit model is justified when the binary variable \\(Y|X = x\\) comes from thresholding a Gaussian latent variable \\(Z(x)\\):\n\n\\[(Y|X = x) = \\mathbf{1}_{Z(x) \\geq \\tau}\\]\n\nwhere \\(Z(x) \\sim \\mathcal{N}(x^T\\beta, \\sigma^2)\\)\n. . .\nIf \\(\\Phi\\) is the CDF of a \\(\\mathcal N(0,1)\\) \\[P(Y = 1|X = x) = P(Z(x) \\geq \\tau) = \\Phi\\left(\\frac{x^T\\beta - \\tau}{\\sigma}\\right)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#examples-in-probit-model",
    "href": "teaching/glm/slides/logistic_model.html#examples-in-probit-model",
    "title": "The Logistic Model",
    "section": "Examples in Probit Model",
    "text": "Examples in Probit Model\n\n\\(Y\\) represents a purchase decision, and \\(Z(x)\\) quantifies the utility of the good\n\\(Y\\) is a declared psychological state (happiness, depression) and \\(Z(x)\\) is a latent, unobserved measure of personal satisfaction"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#probit-vs-logit-current-trends",
    "href": "teaching/glm/slides/logistic_model.html#probit-vs-logit-current-trends",
    "title": "The Logistic Model",
    "section": "Probit vs Logit: Current Trends",
    "text": "Probit vs Logit: Current Trends\n. . .\nEconometricians: The probit model remains relatively popular among econometricians…\n. . .\nGeneral trend: but it tends to be replaced by the logistic model\n. . .\nAdvantages of logit: The logit model has many advantages that probit does not have:\n\nInterpretation of results\nExplicit formulas\n\n. . .\nTheoretical justification: CDF of probit close to CDF of logit"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#remarks-on-cloglog-model",
    "href": "teaching/glm/slides/logistic_model.html#remarks-on-cloglog-model",
    "title": "The Logistic Model",
    "section": "Remarks on cloglog Model",
    "text": "Remarks on cloglog Model\n. . .\nThe modeling approach is \\(p(x) = g^{-1}(x^T\\beta)\\) with\n\n\\[g(t) = \\ln(-\\ln(1-t)) \\quad \\text{i.e.} \\quad g^{-1}(t) = 1 - e^{-e^t}\\]\n\n. . .\nNot symmetric in the sense that \\(g(t) \\neq -g(1-t)\\).\n\\(p(x)\\) approaches \\(0\\) slowly but \\(1\\) very rapidly\n. . .\nIf the opposite is true: take \\(g(t) = -\\ln(-\\ln(t))\\) (loglog model)\n. . .\nUseful in survival models (e.g. Cox)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#details-on-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.html#details-on-the-logit-model",
    "title": "The Logistic Model",
    "section": "Details on the Logit Model",
    "text": "Details on the Logit Model\n\nHighly valued interpretation tool: odds-ratios.\nMore “practical” from a theoretical point of view.\nNatural model in many situations."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#theoretical-motivation-of-logit",
    "href": "teaching/glm/slides/logistic_model.html#theoretical-motivation-of-logit",
    "title": "The Logistic Model",
    "section": "Theoretical Motivation of Logit",
    "text": "Theoretical Motivation of Logit\n. . .\nWe will show in exercises that:\nIf the two groups of individuals associated with \\(Y = 0\\) and \\(Y = 1\\) have a Gaussian distribution of \\(X\\) with different means, i.e. for \\(m_0 \\neq m_1\\),\n\n\\(X|(Y = 0) \\sim \\mathcal N(m_0, \\Sigma) \\and X|(Y = 1) \\sim \\mathcal N(m_1, \\Sigma)\\)\n\nthen \\(\\mathbb P(Y = 1|X = x)\\) follows a logistic model.\n. . .\nThe previous result remains true for any distribution from the exponential family instead of \\(\\mathcal N\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#summary-for-binary-variables",
    "href": "teaching/glm/slides/logistic_model.html#summary-for-binary-variables",
    "title": "The Logistic Model",
    "section": "Summary for Binary Variables:",
    "text": "Summary for Binary Variables:\n. . .\nIf \\(Y\\) is a binary variable, \\((Y|X = x) \\sim \\mathcal B(p(x))\\).\nIn a GLM model for \\(Y\\), we set \\(p(x) = g^{-1}(x^T\\beta)\\) where \\(g\\) is:\n\nby default the logit function, which is the most natural;\npossibly probit if we have good reasons to justify it (but the results will be similar to logit);\ncloglog (or loglog) if we have good reasons to justify it (strong asymmetry of \\(p(x)\\), connection with a Cox model).\n\n. . .\nIn the following, we will focus on the logit model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#outline",
    "href": "teaching/glm/slides/logistic_model.html#outline",
    "title": "The Logistic Model",
    "section": "Outline",
    "text": "Outline\n\ninterpret the model,\nestimate \\(\\beta\\) from a dataset,\nevaluate the quality of estimation,\nexploit it to make predictions/classification."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#interpretation-of-the-logistic-model",
    "href": "teaching/glm/slides/logistic_model.html#interpretation-of-the-logistic-model",
    "title": "The Logistic Model",
    "section": "Interpretation of the Logistic Model",
    "text": "Interpretation of the Logistic Model\nIf \\(x=(x^{(1)}, \\dots, x^{(p)}) \\in \\mathbb R^{p \\times 1}\\)\n\n\\[p(x) = \\text{logit}^{-1}(x^T\\beta) = \\frac{e^{x^T\\beta}}{1 + e^{x^T\\beta}}\\]\n\n\n\\(x^{(j)} \\to p(x)\\) is increasing if \\(\\beta_j &gt; 0\\), decreasing otherwise.\nThe larger \\(|\\beta_j|\\) is, the stronger the discriminatory power of regressor \\(X^{(j)}\\) (a small variation in \\(x^{(j)}\\) can cause a large variation in \\(p(x)\\))."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#shape-of-logit-function",
    "href": "teaching/glm/slides/logistic_model.html#shape-of-logit-function",
    "title": "The Logistic Model",
    "section": "Shape of logit function",
    "text": "Shape of logit function\n. . .\n\n\n\nshape of \\(x^{(j)} \\to p(x)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-bmi-study-french-imc",
    "href": "teaching/glm/slides/logistic_model.html#example-bmi-study-french-imc",
    "title": "The Logistic Model",
    "section": "Example: BMI Study (French: IMC)",
    "text": "Example: BMI Study (French: IMC)\nFor each of the 5300 patients, we observe:\n\n\\(Y\\): 1 if BMI &gt; 35, 0 otherwise\nAGE\nDBP: low pressure (diastolic)\nSEXE: male or female\nACTIV: 1 if intense sports activity, 0 otherwise\nWALK: 1 if walking or cycling to work, 0 otherwise\nMARITAL: marital status (6 categories: married, widowed, divorced, separated, single or cohabiting)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#model-definition",
    "href": "teaching/glm/slides/logistic_model.html#model-definition",
    "title": "The Logistic Model",
    "section": "Model Definition",
    "text": "Model Definition\n. . .\nWe seek to model \\(P(Y = 1|X)\\) where \\(X\\) groups the previous variables (excluding \\(Y\\)).\n. . .\nIn R, we use the glm function with the family=binomial option.\nglm(Y ~ AGE + DBP + SEXE + ACTIV + WALK + MARITAL, family=binomial)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#model-results",
    "href": "teaching/glm/slides/logistic_model.html#model-results",
    "title": "The Logistic Model",
    "section": "Model Results",
    "text": "Model Results\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-2.810240\n0.294316\n-9.548\n&lt; 2e-16\n***\n\n\nAGE\n-0.004407\n0.002717\n-1.622\n0.105\n\n\n\nDBP\n0.017581\n0.003283\n5.356\n8.53e-08\n***\n\n\nSEXEFEMME\n0.544916\n0.081261\n6.706\n2.00e-11\n***\n\n\nWALK1\n-0.409344\n0.095972\n-4.265\n2.00e-05\n***\n\n\nACTIV1\n-0.789734\n0.126653\n-6.235\n4.51e-10\n***\n\n\nMARITAL2\n0.070132\n0.149638\n0.469\n0.639\n\n\n\nMARITAL3\n-0.071318\n0.127510\n-0.559\n0.576\n\n\n\nMARITAL4\n0.188228\n0.206598\n0.911\n0.362\n\n\n\nMARITAL5\n0.070613\n0.115928\n0.609\n0.542\n\n\n\nMARITAL6\n-0.150165\n0.157687\n-0.952\n0.341\n\n\n\n\n\n. . .\nThe interpretation is similar to that of a linear regression model.\n. . .\nWe want to remove the MARITAL variable from the model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#model-results-with-age2",
    "href": "teaching/glm/slides/logistic_model.html#model-results-with-age2",
    "title": "The Logistic Model",
    "section": "Model Results with \\(AGE^2\\)",
    "text": "Model Results with \\(AGE^2\\)\n. . .\nglm(Y ~ AGE + I(AGE^2) + DBP + SEXE + WALK + ACTIV, family=binomial)\nGives\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\nFor someone for which WALK1=0 and ACTIV1=0:\n\n\\(P(Y = 1|\\text{AGE}, \\text{DBP}) = \\text{logit}^{-1}(-3.95 + 0.064 \\times \\text{AGE} - 0.00068 \\times \\text{AGE}^2 + 0.0122 \\times \\text{DBP})\\)\n\n. . .\nFor someone for which WALK1=0 and ACTIV1=1:\n\n\\(P(Y = 1|\\text{AGE}, \\text{DBP}) = \\text{logit}^{-1}(-3.95 + 0.064 \\times \\text{AGE} - 0.00068 \\times \\text{AGE}^2 + 0.0122 \\times \\text{DBP} \\color{red}{ - 0.657})\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#odds",
    "href": "teaching/glm/slides/logistic_model.html#odds",
    "title": "The Logistic Model",
    "section": "Odds",
    "text": "Odds\n\n\\[\\text{odds} = \\frac{p}{1-p}\\]\n\n. . .\nBetting interpretation for example, \\(3\\) to \\(1\\) means that for \\(3\\) people betting on \\(A\\), \\(1\\) person bets on \\(B\\).\n. . .\nSo a randomly chosen bettor has a probability of \\(p=3/4\\) of betting on \\(A\\) and \\(1-p=1/4\\) on betting on \\(B\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#odds-given-xx",
    "href": "teaching/glm/slides/logistic_model.html#odds-given-xx",
    "title": "The Logistic Model",
    "section": "Odds given \\(X=x\\)",
    "text": "Odds given \\(X=x\\)\n. . .\nSimilarly, the odds of obtaining \\(Y = 1\\) given \\(X = x\\) is:\n\n\\[\\text{odds}(x) = \\frac{p(x)}{1 - p(x)}\\]\n\n. . .\nwhere \\(p(x) = P(Y = 1|X = x)\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#odds-ratio",
    "title": "The Logistic Model",
    "section": "Odds Ratio",
    "text": "Odds Ratio\n. . .\nIf two individuals have characteristics \\(x_1\\) and \\(x_2\\) respectively, we call the odds ratio between \\(x_1\\) and \\(x_2\\):\n\\[OR(x_1, x_2) = \\frac{\\text{odds}(x_1)}{\\text{odds}(x_2)} = \\frac{\\frac{p(x_1)}{1-p(x_1)}}{\\frac{p(x_2)}{1-p(x_2)}}\\]\n. . .\n\n\n\n\n\n\nWarning\n\n\n\nDO NOT CONFUSE ODDS RATIO WITH PROBABILITY RATIO\nOnly possible exception: if \\(p(x_1)\\) and \\(p(x_2)\\) are very small because then \\(1 - p(x_1) \\approx 1\\) and \\(1 - p(x_2) \\approx 1\\), so that \\(OR(x_1, x_2) \\approx p(x_1)/p(x_2)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#link-with-proba.-ratio",
    "href": "teaching/glm/slides/logistic_model.html#link-with-proba.-ratio",
    "title": "The Logistic Model",
    "section": "Link with Proba. Ratio",
    "text": "Link with Proba. Ratio\n. . .\nHowever, it remains that:\n\\[\\begin{aligned}\nOR(x_1, x_2) &gt; 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} &gt; 1\\\\\nOR(x_1, x_2) &lt; 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} &lt; 1\\\\\nOR(x_1, x_2) = 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} = 1\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#other-property-of-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#other-property-of-odds-ratio",
    "title": "The Logistic Model",
    "section": "Other Property of Odds Ratio",
    "text": "Other Property of Odds Ratio\n. . .\n\\(OR(x_1, x_2)\\) accentuates the differences compared to \\(p(x_1)/p(x_2)\\):\n\\[\\begin{aligned}\nOR(x_1, x_2) &gt; \\frac{p(x_1)}{p(x_2)} &\\text{ when } OR(x_1, x_2) &gt; 1\\\\\nOR(x_1, x_2) &lt; \\frac{p(x_1)}{p(x_2)} &\\text{ when } OR(x_1, x_2) &lt; 1\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#examples-using-odds-ratios",
    "href": "teaching/glm/slides/logistic_model.html#examples-using-odds-ratios",
    "title": "The Logistic Model",
    "section": "Examples Using Odds Ratios",
    "text": "Examples Using Odds Ratios\n. . .\nA logistic regression is most often used to compare the behavior of two individuals with respect to the variable of interest.\n. . .\nExamples:\n\nprobability of purchase depending on whether or not one has been the subject of a personalized promotion;\nfor a given vehicle, probability of experiencing a breakdown according to age;\nprobability of recovery according to the treatment used;"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#odds-ratio-in-logistic-regression",
    "href": "teaching/glm/slides/logistic_model.html#odds-ratio-in-logistic-regression",
    "title": "The Logistic Model",
    "section": "Odds Ratio in Logistic Regression",
    "text": "Odds Ratio in Logistic Regression\n. . .\nIt holds that \\(\\text{odds}(x) = \\frac{p(x)}{1 - p(x)} = \\exp(x^T \\beta)\\)\nHence,\n\n\\(OR(x_1, x_2) = \\frac{\\text{odds}(x_1)}{\\text{odds}(x_2)} = \\exp((x_1 - x_2)^T \\beta)\\)\n\n. . .\nIf the two individuals differ only by regressor \\(j\\), then\n\\(OR(x_1, x_2) = \\exp(\\beta_j (x_1^{(j)} - x_2^{(j)}))\\)\n. . .\nIf regressor \\(j\\) is binary (\\(x_1^{(j)} = 1\\) while \\(x_2^{(j)} = 0\\)):\n\\(OR(x_1, x_2) = \\exp(\\beta_j)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#key-summary-statement",
    "href": "teaching/glm/slides/logistic_model.html#key-summary-statement",
    "title": "The Logistic Model",
    "section": "Key Summary Statement",
    "text": "Key Summary Statement\n. . .\nIn a logistic regression model, \\(\\beta_j\\) is interpreted as the logarithm of the odds-ratio between two individuals differing by a quantity of \\(1\\) on regressor \\(j\\), all else being equal.\n. . .\nIn brief: \\(\\exp(\\beta_j) = OR(x^{(j)} + 1, x^{(j)})\\)\nIf regressor \\(j\\) is binary (absence or presence of a certain characteristic):\n. . .\n\\(\\exp(\\beta_j)\\) is simply the OR between the presence or absence of this characteristic, all else being equal."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-1-intense-sports-activity",
    "href": "teaching/glm/slides/logistic_model.html#example-1-intense-sports-activity",
    "title": "The Logistic Model",
    "section": "Example 1: Intense Sports Activity",
    "text": "Example 1: Intense Sports Activity\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\n. . .\nThe Odds Ratio corresponding to practicing or not practicing intense sports activity is, all else being equal:\n. . .\n\\(\\exp(-0.657) \\approx 0.52\\)\n. . .\nThe odds of obesity occurrence therefore decrease by half for individuals practicing intense sports activity.\n(The odds, not the probability!)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-2-diastolic-pressure",
    "href": "teaching/glm/slides/logistic_model.html#example-2-diastolic-pressure",
    "title": "The Logistic Model",
    "section": "Example 2: Diastolic Pressure",
    "text": "Example 2: Diastolic Pressure\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\nThe OR for a diastolic pressure difference of \\(+20\\) is:\n. . .\n\\(\\exp(0.0121546 \\times 20) \\approx 1.28\\)\n. . .\nThe odds of obesity occurrence therefore increase by \\(28\\%\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#the-framework",
    "href": "teaching/glm/slides/logistic_model.html#the-framework",
    "title": "The Logistic Model",
    "section": "The Framework",
    "text": "The Framework\n. . .\nWe observe \\(n\\) i.i.d. realizations \\((Y_i, X_i)\\) where \\(Y_i \\in \\{0, 1\\}\\) and \\(X_i \\in \\mathbb{R}^p\\).\n. . .\nWe denote \\(p(x_i) = P(Y_i = 1|X_i = x_i)\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#the-logistic-model",
    "href": "teaching/glm/slides/logistic_model.html#the-logistic-model",
    "title": "The Logistic Model",
    "section": "The Logistic Model",
    "text": "The Logistic Model\n. . .\nWe assume the logistic model: for all \\(i\\),\n\n\\[p(x_i) = \\text{logit}^{-1}(x_i^T \\beta) = \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\]\n\n. . .\nwhere \\(\\beta = (\\beta_1, \\ldots, \\beta_p)^T\\) and \\(x_i^T \\beta = \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#parameter-estimation",
    "href": "teaching/glm/slides/logistic_model.html#parameter-estimation",
    "title": "The Logistic Model",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n. . .\n\n\\[p(x_i) = \\text{logit}^{-1}(x_i^T \\beta) = \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\]\n\nWe will estimate \\(\\beta\\) by maximizing the likelihood.\n. . .\nWe will denote \\(p_\\beta(x_i)\\) to emphasize the dependence of \\(p(x_i)\\) on \\(\\beta\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#likelihood-calculation",
    "href": "teaching/glm/slides/logistic_model.html#likelihood-calculation",
    "title": "The Logistic Model",
    "section": "Likelihood Calculation",
    "text": "Likelihood Calculation\n. . .\nFor all \\(i\\), \\(Y_i|(X_i = x_i)\\) follows the distribution \\(B(p_\\beta(x_i))\\). Therefore\n\n\n\\[P(Y_i = y_i|X_i = x_i) = p_\\beta(x_i)^{y_i}(1 - p_\\beta(x_i))^{1-y_i}\\]\n\n\n. . .\nfor all \\(y_i \\in \\{0, 1\\}\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#likelihood",
    "href": "teaching/glm/slides/logistic_model.html#likelihood",
    "title": "The Logistic Model",
    "section": "Likelihood",
    "text": "Likelihood\n. . .\nBy independence, we obtain the likelihood\n\n\\[\\ell(\\beta, y_1, \\ldots, y_n, x_1, \\ldots, x_n) = \\prod_{i=1}^n p_\\beta(x_i)^{y_i}(1 - p_\\beta(x_i))^{1-y_i}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#log-likelihood",
    "href": "teaching/glm/slides/logistic_model.html#log-likelihood",
    "title": "The Logistic Model",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n. . .\nTaking the log and replacing \\(p(x_i)\\) by its expression, we obtain the log-likelihood:\n\n\n\\[\\begin{aligned}\nL(\\beta, y_1, \\ldots, y_n, x_1, \\ldots, x_n) &= \\ln(\\ell) \\\\\n&=\\sum_{i=1}^n \\left[y_i x_i^T \\beta - \\ln(1 + e^{x_i^T \\beta})\\right]\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#mle-calculation",
    "href": "teaching/glm/slides/logistic_model.html#mle-calculation",
    "title": "The Logistic Model",
    "section": "MLE Calculation",
    "text": "MLE Calculation\n. . .\nThe MLE \\(\\hat{\\beta}\\), if it exists, cancels the gradient of \\(L\\) with respect to \\(\\beta\\). This gradient equals\n\n\\[\\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^n x_i \\left(y_i - \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\right)\\]\n\nWe therefore need to solve a system of \\(p\\) equations with \\(p\\) unknowns.\n. . .\nBut the solution is not explicit: we resort to numerical methods (Newto-Raphso algo)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#remarks",
    "href": "teaching/glm/slides/logistic_model.html#remarks",
    "title": "The Logistic Model",
    "section": "Remarks",
    "text": "Remarks\n. . .\nThis is a classic situation when using advanced statistical models: we often resort to optimization algorithms.\n. . .\nDoes the solution exist? Is it unique?"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#mle-uniqueness",
    "href": "teaching/glm/slides/logistic_model.html#mle-uniqueness",
    "title": "The Logistic Model",
    "section": "MLE Uniqueness",
    "text": "MLE Uniqueness\n. . .\nLet \\(X\\) be the design matrix \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\)\n. . .\n\n\n\n\n\n\nProposition\n\n\n\nIf \\(\\text{rank}(X) = p\\), then the MLE, if it exists, is unique."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#proof-of-mle-uniqueness",
    "href": "teaching/glm/slides/logistic_model.html#proof-of-mle-uniqueness",
    "title": "The Logistic Model",
    "section": "Proof of MLE Uniqueness",
    "text": "Proof of MLE Uniqueness\n. . .\nIt suffices to show that \\(L\\) is strictly concave in \\(\\beta\\).\n. . .\nHessian Matrix of \\(L\\):\n\\[\\frac{\\partial^2 L}{\\partial \\beta^2} = -\\sum_{i=1}^n p_\\beta(x_i)(1 - p_\\beta(x_i)) x_i x_i^T\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#hessian-properties",
    "href": "teaching/glm/slides/logistic_model.html#hessian-properties",
    "title": "The Logistic Model",
    "section": "Hessian Properties",
    "text": "Hessian Properties\n. . .\nIt is negative semi-definite. Moreover, for all \\(u \\in \\mathbb{R}^p\\),\n\\[\\begin{aligned}\nu^T \\frac{\\partial^2 L}{\\partial \\beta^2} u = 0 &\\Leftrightarrow u^T x_i x_i^T u = 0 \\text{ for all } i\\\\\n&\\Leftrightarrow u^T x_i = 0 \\text{ for all } i\\\\\n&\\Leftrightarrow Xu = 0\\\\\n&\\Leftrightarrow u = 0\n\\end{aligned}\\]\nsince \\(\\text{rank}(X) = p\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#conclusion",
    "href": "teaching/glm/slides/logistic_model.html#conclusion",
    "title": "The Logistic Model",
    "section": "Conclusion",
    "text": "Conclusion\n. . .\nThus, for all \\(u \\neq 0\\),\n\n\\[u^T \\frac{\\partial^2 L}{\\partial \\beta^2} u &lt; 0\\]\n\nThe Hessian matrix is negative definite and therefore \\(L\\) is strictly concave,\nThe MLE is unique"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#about-mle-existence",
    "href": "teaching/glm/slides/logistic_model.html#about-mle-existence",
    "title": "The Logistic Model",
    "section": "About MLE Existence",
    "text": "About MLE Existence\n. . .\nAlthough \\(L\\) is strictly concave, its maximum can occur at infinity (think of the \\(\\ln\\) function), in which case \\(\\hat{\\beta}\\) does not exist.\n. . .\nThis occurs if there is non-overlap, i.e., separation by a hyperplane of the \\(x_i\\) for which \\(y_i = 0\\) and those for which \\(y_i = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#non-overlap-situation",
    "href": "teaching/glm/slides/logistic_model.html#non-overlap-situation",
    "title": "The Logistic Model",
    "section": "Non-Overlap Situation",
    "text": "Non-Overlap Situation\n. . .\nMathematically, there is non-overlap if there exists \\(\\alpha \\in \\mathbb{R}^p\\) such that\n\\[\\begin{cases}\n\\text{for all } i \\text{ such that } y_i = 0, & \\alpha^T x_i \\geq 0 \\\\\n\\text{for all } i \\text{ such that } y_i = 1, & \\alpha^T x_i \\leq 0\n\\end{cases}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#illustration-of-non-overlap-situation",
    "href": "teaching/glm/slides/logistic_model.html#illustration-of-non-overlap-situation",
    "title": "The Logistic Model",
    "section": "Illustration of Non-Overlap Situation",
    "text": "Illustration of Non-Overlap Situation"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#non-overlap-and-existence",
    "href": "teaching/glm/slides/logistic_model.html#non-overlap-and-existence",
    "title": "The Logistic Model",
    "section": "Non-Overlap and Existence",
    "text": "Non-Overlap and Existence\n. . .\n\n\n\n\n\n\nProposition (admitted)\n\n\n\nIn case of non-overlap, the estimator \\(\\hat{\\beta}\\) does not exist, in the sense that \\(L(\\beta)\\) is maximal when \\(\\|\\beta\\| \\to \\infty\\) (in one or several directions).\n\n\n. . .\nFor all \\(x\\), \\(\\hat{p}(x) = \\in \\{0,1\\}\\), depending on the position of \\(x\\) relative to the separating hyperplane.\n. . .\nNevertheless, there is a “dead zone” in the middle of the \\(2\\) point clouds, because the separating hyperplane is not necessarily unique."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#beyond-the-dead-zone",
    "href": "teaching/glm/slides/logistic_model.html#beyond-the-dead-zone",
    "title": "The Logistic Model",
    "section": "Beyond the Dead Zone",
    "text": "Beyond the Dead Zone\n. . .\nBeyond this dead zone, classification is very simple (\\(0\\) or \\(1\\)).\n. . .\nBut no interpretation of the model is possible (the OR are worth \\(0\\) or \\(+\\infty\\))."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#existence-and-uniqueness-of-mle",
    "href": "teaching/glm/slides/logistic_model.html#existence-and-uniqueness-of-mle",
    "title": "The Logistic Model",
    "section": "Existence and Uniqueness of MLE",
    "text": "Existence and Uniqueness of MLE\n. . .\nWe say there is overlap when no hyperplane can separate the red points from the blue points.\n. . .\n\n\n\n\n\n\nProposition (admitted)\n\n\n\nIf \\(\\text{rank}(X) = p\\) and there is overlap, then the MLE exists and is unique.\n\n\nUnder these conditions, we can therefore search for the MLE using the Newton-Raphson algorithm.\n\nthe maximum exists,\nthe function to optimize is strictly concave and there is therefore no local maximum, only a global maximum."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#fisher-information-recall",
    "href": "teaching/glm/slides/logistic_model.html#fisher-information-recall",
    "title": "The Logistic Model",
    "section": "Fisher Information (Recall)",
    "text": "Fisher Information (Recall)\n. . .\nLet \\(X\\) be the design matrix (whose rows are the vectors \\(x_i\\)).\n. . .\nLet \\(J_n(\\beta)\\) be the Fisher information matrix:\n\n\\[J_n(\\beta) = -E\\left[\\frac{\\partial^2 L}{\\partial \\beta^2}(\\beta) \\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#asymptotic-efficiency",
    "href": "teaching/glm/slides/logistic_model.html#asymptotic-efficiency",
    "title": "The Logistic Model",
    "section": "Asymptotic Efficiency",
    "text": "Asymptotic Efficiency\n\n\n\n\n\n\nProposition (admitted)\n\n\n\nIn the logistic regression model, if\n\nthe distribution of the regressors \\((X_1, \\ldots, X_p)\\) has compact support,\n\\(\\text{rank}(X) = p\\),\nthe smallest eigenvalue of \\(X^T X\\) tends to infinity with \\(n\\),\n\nthen\n\nthe maximum likelihood estimator \\(\\hat{\\beta}\\) is consistent;\n\\(J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\)\n\nwhere \\(I_p\\) is the identity matrix of size \\(p\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#comments",
    "href": "teaching/glm/slides/logistic_model.html#comments",
    "title": "The Logistic Model",
    "section": "Comments",
    "text": "Comments\n. . .\nUnder these conditions, the MLE therefore exists for sufficiently large \\(n\\). In fact, there is necessarily overlap when \\(n\\) is large.\n. . .\nIt is asymptotically efficient (= minimal asymptotic variance)\n. . .\nThe Fisher information matrix \\(J_n(\\beta)\\) can be calculated\n. . .\nWe will be able to rely on asymptotic normality to perform tests and construct confidence intervals"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#comparison-with-linear-regression",
    "href": "teaching/glm/slides/logistic_model.html#comparison-with-linear-regression",
    "title": "The Logistic Model",
    "section": "Comparison with Linear Regression",
    "text": "Comparison with Linear Regression\n. . .\nThe formula for \\(\\hat{\\beta}\\) is explicit: \\(\\hat{\\beta} = (X^T X)^{-1} X^T Y\\);\n. . .\nIts expectation and variance are explicit;\n. . .\nIn the Gaussian model (\\(Y|X\\) Gaussian), the distribution of \\(\\hat{\\beta}\\) is explicit, which allows constructing exact tests (Student, Fisher).\n. . .\nIf the model is not Gaussian, these tests are valid asymptotically."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#comparison-with-linear-regression-1",
    "href": "teaching/glm/slides/logistic_model.html#comparison-with-linear-regression-1",
    "title": "The Logistic Model",
    "section": "Comparison with Linear Regression",
    "text": "Comparison with Linear Regression\n. . .\nNo explicit formula for \\(\\hat{\\beta}\\), the solution is obtained numerically;\n. . .\nWe know neither the bias nor the variance of \\(\\hat{\\beta}\\);\n. . .\nThe distribution of \\(Y|X\\) is simple (a Bernoulli), but we don’t know the distribution of \\(\\hat{\\beta}\\).\n. . .\nWe only know its asymptotic distribution.\n. . .\nWe’ll do asymptotic tests!"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#asymptotic-framework",
    "href": "teaching/glm/slides/logistic_model.html#asymptotic-framework",
    "title": "The Logistic Model",
    "section": "Asymptotic Framework",
    "text": "Asymptotic Framework\n. . .\nUnder “good conditions”,\n\n\\[J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]\n\nwhere \\(J_n(\\beta)\\) is the Fisher information matrix.\n. . .\nTo build asymptotic tests, we need to understand \\(J_n(\\beta)\\) and be able to estimate it."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#computation-of-j_nbeta",
    "href": "teaching/glm/slides/logistic_model.html#computation-of-j_nbeta",
    "title": "The Logistic Model",
    "section": "Computation of \\(J_n(\\beta)\\)",
    "text": "Computation of \\(J_n(\\beta)\\)\n. . .\n\n\\[J_n(\\beta) = -E\\left[\\frac{\\partial^2 L}{\\partial \\beta^2}(\\beta) \\bigg| X\\right]\\]\n\nwhere \\(L\\) is the log-likelihood of the model.\n. . .\nFrom the proof of existence of MLE,\n\n\\[J_n(\\beta) = \\sum_{i=1}^n p_\\beta(x_i)(1 - p_\\beta(x_i)) x_i x_i^T\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#equivalent-form",
    "href": "teaching/glm/slides/logistic_model.html#equivalent-form",
    "title": "The Logistic Model",
    "section": "Equivalent Form",
    "text": "Equivalent Form\n. . .\nWe can write equivalently\n\n\\[J_n(\\beta) = X^T W_\\beta X\\]\n\nwhere \\(X\\) is the design matrix and \\(W_\\beta\\) is the diagonal matrix\n. . .\n\n\n\\[W_\\beta = \\begin{pmatrix}\np_\\beta(x_1)(1 - p_\\beta(x_1)) & 0 & \\cdots & 0 \\\\\n0 & p_\\beta(x_2)(1 - p_\\beta(x_2)) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & p_\\beta(x_n)(1 - p_\\beta(x_n))\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#estimation",
    "href": "teaching/glm/slides/logistic_model.html#estimation",
    "title": "The Logistic Model",
    "section": "Estimation",
    "text": "Estimation\n. . .\nTo estimate \\(J_n(\\beta)\\), we simply replace \\(\\beta\\) by the MLE \\(\\hat{\\beta}\\)\n. . .\nUnder the same regularity assumptions, we can show that\n\n\\[J_n(\\hat{\\beta})^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#estimated-variance-of-hat-beta_j",
    "href": "teaching/glm/slides/logistic_model.html#estimated-variance-of-hat-beta_j",
    "title": "The Logistic Model",
    "section": "Estimated Variance of \\(\\hat \\beta_j\\)",
    "text": "Estimated Variance of \\(\\hat \\beta_j\\)\n. . .\n\n\\[J_n(\\hat{\\beta})^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]\n\n. . .\nDenoting \\(\\hat{\\sigma}_j^2\\) the \\(j\\)-th diagonal element of \\(J_n(\\hat{\\beta})^{-1}\\), we obtain (admitted):\n\n\\[\\frac{\\hat{\\beta}_j - \\beta_j}{\\hat{\\sigma}_j} \\xrightarrow{L} N(0, 1)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#confidence-interval",
    "href": "teaching/glm/slides/logistic_model.html#confidence-interval",
    "title": "The Logistic Model",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n. . .\nWe deduce a confidence interval for \\(\\beta_j\\), at asymptotic confidence level \\(1 - \\alpha\\):\n\n\\[\\text{CI}_{1-\\alpha}(\\beta_j) = \\left[\\hat{\\beta}_j - q_{1-\\alpha/2}\\hat{\\sigma}_j \\,;\\, \\hat{\\beta}_j + q_{1-\\alpha/2}\\hat{\\sigma}_j\\right]\\]\n\nwhere \\(q(1 - \\alpha/2)\\) is the quantile of order \\(1 - \\alpha/2\\) of the \\(N(0, 1)\\) distribution.\n. . .\nWe verify that we have \\(\\P(\\beta_j \\in \\text{CI}_{1-\\alpha}(\\beta_j)) \\to 1 - \\alpha\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#significance-test-for-one-coefficient",
    "href": "teaching/glm/slides/logistic_model.html#significance-test-for-one-coefficient",
    "title": "The Logistic Model",
    "section": "Significance Test for One Coefficient",
    "text": "Significance Test for One Coefficient\n. . .\nWe want to test \\(H_0: \\beta_j = 0\\) against \\(H_1: \\beta_j \\neq 0\\).\nUnder \\(H_0\\) we know that \\(\\frac{\\hat{\\beta}_j}{\\hat{\\sigma}_j} \\xrightarrow{L} N(0, 1)\\)\nWe deduce a critical region at asymptotic level \\(\\alpha\\):\n\n\\[\\mathcal R_\\alpha = \\left\\{\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha/2}\\right\\}\\]\n\n. . .\nIndeed \\(P_{H_0}(\\mathcal R_\\alpha) \\to \\alpha\\).\nThis test is called the Wald test. (As any other test that relies on asymptotic normality)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#p-value",
    "href": "teaching/glm/slides/logistic_model.html#p-value",
    "title": "The Logistic Model",
    "section": "P-value",
    "text": "P-value\n. . .\nDenoting \\(\\Phi\\) the cdf of the \\(\\mathcal N(0, 1)\\) distribution, the p-value of the test equals\n\n\\[p\\text{-value} = 2\\left(1 - \\Phi\\left(\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j}\\right)\\right)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-in-r",
    "href": "teaching/glm/slides/logistic_model.html#example-in-r",
    "title": "The Logistic Model",
    "section": "Example in R",
    "text": "Example in R\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\n. . .\nEach columns corresponds resp. to\n\n\n\\(\\hat \\beta_j\\)\n\\(\\hat \\sigma_j\\)\n\\(\\hat \\beta_j/\\hat \\sigma_j\\) (z-score)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#estimation-of-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#estimation-of-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Estimation of an Odds-Ratio",
    "text": "Estimation of an Odds-Ratio\n. . .\nWe consider two individuals \\(1\\) and \\(2\\) who differ only by their regressor \\(j\\). Then,\n\n\\[OR(x_1^{(j)}, x_2^{(j)}) = e^{\\beta_j(x_1^{(j)} - x_2^{(j)})}\\]\n\n. . .\nDo we have \\(OR(x_1^{(j)}, x_2^{(j)})\\approx 1\\)?\n. . .\nThe estimation of \\(OR(x_1^{(j)}, x_2^{(j)})\\) is simply\n\n\\[\\widehat{OR}(x_1^{(j)}, x_2^{(j)}) = e^{\\hat{\\beta}_j(x_1^{(j)} - x_2^{(j)})}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#important-example",
    "href": "teaching/glm/slides/logistic_model.html#important-example",
    "title": "The Logistic Model",
    "section": "Important Example",
    "text": "Important Example\n. . .\nIf regressor \\(j\\) is binary with \\(x_1^{(j)} = 1\\) and \\(x_2^{(j)} = 0\\), then\n\n\\[\\widehat{OR}(x_1^{(j)}, x_2^{(j)}) = e^{\\hat{\\beta}_j}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#asymptotic-ci-for-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#asymptotic-ci-for-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Asymptotic CI for an Odds-Ratio",
    "text": "Asymptotic CI for an Odds-Ratio\n. . .\nWe have seen that an asymptotic CI at confidence level \\(1 - \\alpha\\) for \\({\\beta}_j\\) is\n\n\\[\\text{CI}_{1-\\alpha}(\\beta_j) = \\left[\\hat{\\beta}_j - q_{1-\\alpha/2}\\hat{\\sigma}_j \\,;\\, \\hat{\\beta}_j + q_{1-\\alpha/2}\\hat{\\sigma}_j\\right]= [l,r]\\]\n\n. . .\nIf \\(x_1^{(j)} &gt; x_2^{(j)}\\), an asymptotic CI for \\(OR(x_1^{(j)}, x_2^{(j)})= e^{\\beta_j(x^{(j)}_1 - x^{(j)}_2)}\\) at level \\(1 - \\alpha\\) is\n\n\\[\\text{CI}_{1-\\alpha}(OR(x_1^{(j)}, x_2^{(j)})) = \\left[e^{l(x_1^{(j)} - x_2^{(j)})}, e^{r(x_1^{(j)} - x_2^{(j)})}\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#significance-test-for-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#significance-test-for-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Significance Test for an Odds-Ratio",
    "text": "Significance Test for an Odds-Ratio\n. . .\nWe generally want to compare \\(OR(x_1^{(j)}, x_2^{(j)})\\) to \\(1\\).\n\n\\[OR(x_1^{(j)}, x_2^{(j)}) = 1 \\Leftrightarrow e^{\\beta_j(x_1^{(j)} - x_2^{(j)})} = 1 \\Leftrightarrow \\beta_j = 0\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#two-sided-test",
    "href": "teaching/glm/slides/logistic_model.html#two-sided-test",
    "title": "The Logistic Model",
    "section": "Two-Sided Test",
    "text": "Two-Sided Test\n. . .\n\n\\(H_0: OR(x_1^{(j)}, x_2^{(j)}) = 1\\) VS \\(H_1: OR(x_1^{(j)}, x_2^{(j)}) \\neq 1\\)\n\n. . .\namounts to testing \\(H_0: \\beta_j = 0\\) against \\(H_1: \\beta_j \\neq 0\\). The Rejection region at level \\(\\alpha\\) is\n. . .\n\n\\[\\mathcal R_\\alpha = \\left\\{\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha/2}\\right\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#one-sided-tests",
    "href": "teaching/glm/slides/logistic_model.html#one-sided-tests",
    "title": "The Logistic Model",
    "section": "One-Sided Tests",
    "text": "One-Sided Tests\n. . .\nNevertheless, for ORs, we often prefer one-sided tests.\n. . .\n\n\\(H_0: OR(x_1^{(j)}, x_2^{(j)}) = 1\\) VS \\(H_1: OR(x_1^{(j)}, x_2^{(j)}) &gt; 1\\)\n\n. . .\nIf \\(x_1^{(j)} &gt; x_2^{(j)}\\). Since \\(OR(x_1^{(j)}, x_2^{(j)}) \\geq 1 \\Leftrightarrow \\beta_j \\geq 0\\) rejection region at level \\(\\alpha\\) is\n\n\\[\\mathcal R_{\\alpha}=\\left\\{\\frac{\\hat{\\beta}_j}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha}\\right\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#saturated-model",
    "href": "teaching/glm/slides/logistic_model.html#saturated-model",
    "title": "The Logistic Model",
    "section": "Saturated Model",
    "text": "Saturated Model\n. . .\nSuppose we have \\(n\\) observations \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\) (categorical)\n. . .\nHere \\(X_k\\) can represent the vector \\(X_k = (X^{(1)}_k, \\dots, X^{(p)}_k)^T\\).\n. . .\nAssume that indivudal are iid, with \\(\\mathbb P(Y=1|X_k=x) = p(x)\\).\n. . .\nHow do we estimate \\(p(x)\\)?"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#saturated-estimator",
    "href": "teaching/glm/slides/logistic_model.html#saturated-estimator",
    "title": "The Logistic Model",
    "section": "Saturated Estimator",
    "text": "Saturated Estimator\n. . .\nSuppose we have \\(n\\) observations \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\)\n. . .\n\\(n(x) = |\\{k:~ X_k = x\\}|\\) (number of indiv. \\(k\\) s.t. \\(X_k=x\\))\n. . .\n\\(n_1(x) = |\\{k:~ X_k = x ~~\\text{and}~~Y_k=1\\}|\\)\n. . .\nThe saturated model is one that estimates \\(p(x)\\), for an observed \\(x\\), by\n\n\\[\\hat{p}_{\\text{sat}}(x) = \\frac{n_1(x)}{n(x)}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#remark",
    "href": "teaching/glm/slides/logistic_model.html#remark",
    "title": "The Logistic Model",
    "section": "Remark",
    "text": "Remark\n. . .\n\n\\[\\hat{p}_{\\text{sat}}(x) = \\frac{n_1(x)}{n(x)}\\]\n\nIf all observations are distinct, i.e., each observed \\(x\\) is only for a single individual, then for an observed \\(x\\):\n. . .\n\\(n(x) = 1\\), \\(n_1(x) \\in \\{0,1\\}\\), and \\(\\hat{p}_{\\text{sat}}(x) = 0\\) or \\(1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#remarks-1",
    "href": "teaching/glm/slides/logistic_model.html#remarks-1",
    "title": "The Logistic Model",
    "section": "Remarks",
    "text": "Remarks\n. . .\nThe saturated model is the simplest model to imagine.\n. . .\nIt fits the data perfectly.\n. . .\nHowever, it has no explanatory power (effect of regressors on \\(Y\\)?).\n. . .\nAnd it says nothing about \\(p(x)\\) if \\(x\\) is not observed.\n. . .\nIt will serves as a reference for fit"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#likelihood-of-saturated-estimator",
    "href": "teaching/glm/slides/logistic_model.html#likelihood-of-saturated-estimator",
    "title": "The Logistic Model",
    "section": "Likelihood of Saturated Estimator",
    "text": "Likelihood of Saturated Estimator\n. . .\nFor the saturated model with probabilities \\(p(x)\\), the Log-likelihood is:\n\n\n\\[L(y_1, \\ldots, y_n|x_1, \\ldots, x_n) = \\sum_{i=1}^n y_i \\ln(p(x_i)) + (1 - y_i) \\ln(1 - p(x_i))\\]\n\n\n. . .\nThe saturated model minimizes this likelihood, and we denote\n\n\n\\[L_{\\text{sat}} = \\sum_{i=1}^n y_i \\ln(\\hat p_{\\text{sat}}(x_i)) + (1 - y_i) \\ln(1 - \\hat p_{\\text{sat}}(x_i))\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#case-1-distinct-observations",
    "href": "teaching/glm/slides/logistic_model.html#case-1-distinct-observations",
    "title": "The Logistic Model",
    "section": "Case 1: Distinct Observations",
    "text": "Case 1: Distinct Observations\n. . .\nIf all observations \\(x_i\\) are distinct, we have \\(\\hat{p}_{\\text{sat}}(x_i) = y_i\\) with \\(y_i \\in \\{0, 1\\}\\). We thus have\n\n\\[L_{\\text{sat}} = 0\\]\n\n. . .\nThe saturated estimator has highest possible log-likelihood: it fits the data perfectly (too well)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#case-2-non-distinct-observations",
    "href": "teaching/glm/slides/logistic_model.html#case-2-non-distinct-observations",
    "title": "The Logistic Model",
    "section": "Case 2: Non-Distinct Observations",
    "text": "Case 2: Non-Distinct Observations\n. . .\nIf the observations \\(x_i\\) are not distinct, we obtain\n\n\n\\[L_{\\text{sat}} = \\sum_x \\left[n_1(x) \\ln\\left(\\frac{n_1(x)}{n(x)}\\right) + (n(x) - n_1(x)) \\ln\\left(1 - \\frac{n_1(x)}{n(x)}\\right)\\right]\\]\n\n\nwhere the sum runs over the set of values \\(x\\) taken by the \\(x_i\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#deviance-as-a-substitute-for-ssr",
    "href": "teaching/glm/slides/logistic_model.html#deviance-as-a-substitute-for-ssr",
    "title": "The Logistic Model",
    "section": "Deviance as a Substitute for SSR",
    "text": "Deviance as a Substitute for SSR"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#definition",
    "href": "teaching/glm/slides/logistic_model.html#definition",
    "title": "The Logistic Model",
    "section": "Definition",
    "text": "Definition\n\nThe deviance of a model measures how much this model deviates from the saturated model (the ideal model in terms of likelihood).\n\n\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\]\n\nwhere \\(L_{\\text{mod}}\\) denotes the log-likelihood for the model parameters.\n\n\nWe always have \\(D \\geq 0\\).\n\n\nIf all observations are distinct, \\(L_{\\text{sat}} = 0\\) therefore \\(D = -2L_{\\text{mod}}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#role-of-deviance-and-computation-in-r",
    "href": "teaching/glm/slides/logistic_model.html#role-of-deviance-and-computation-in-r",
    "title": "The Logistic Model",
    "section": "Role of Deviance and Computation in R",
    "text": "Role of Deviance and Computation in R\n. . .\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\]\n\n. . .\nDeviance plays the role of the SSR of a linear model: the higher the deviance, the less well the model is fitted to the data.\n. . .\nIn R, The returned deviance is \\(-2L_{\\text{mod}}\\): the term \\(L_{\\text{sat}}\\) is therefore omitted."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#linear-constraint-test",
    "href": "teaching/glm/slides/logistic_model.html#linear-constraint-test",
    "title": "The Logistic Model",
    "section": "Linear Constraint Test",
    "text": "Linear Constraint Test\n. . .\nAs in linear regression, we would like to test \\(H_0: R\\beta = 0\\) VS \\(H_1: R\\beta \\neq 0\\)\nwhere \\(R\\) is a constraint matrix of size \\((q, p)\\) of full rank.\n. . .\nFor recall, depending on the choice of \\(R\\) this allows:\n\ntesting the minimum: is there at least one relevant regressor?\ncomparing nested models\nexamining the collective significance of a family of regressors"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#linear-constraint-testing-test-procedures",
    "href": "teaching/glm/slides/logistic_model.html#linear-constraint-testing-test-procedures",
    "title": "The Logistic Model",
    "section": "Linear Constraint Testing: Test Procedures",
    "text": "Linear Constraint Testing: Test Procedures"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#available-procedures",
    "href": "teaching/glm/slides/logistic_model.html#available-procedures",
    "title": "The Logistic Model",
    "section": "Available Procedures",
    "text": "Available Procedures\n. . .\nIn GLM, several test procedures address the problem.\nThe Wald test, based on the asymptotic normality of \\(\\hat{\\beta}\\), which generalizes the one seen for testing \\(\\beta_j = 0\\) against \\(\\beta_j \\neq 0\\).\n. . .\nThe likelihood ratio test, called in this context the deviance test.\n. . .\nThe score test, based on the behavior of the gradient of the log-likelihood at the critical point.\n. . .\nThe most used is the deviance test."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#the-deviance-test-or-likelihood-ratio-test",
    "href": "teaching/glm/slides/logistic_model.html#the-deviance-test-or-likelihood-ratio-test",
    "title": "The Logistic Model",
    "section": "The Deviance Test (or Likelihood Ratio Test)",
    "text": "The Deviance Test (or Likelihood Ratio Test)\n. . .\nTo test \\(H_0: R\\beta = 0\\) against \\(H_1: R\\beta \\neq 0\\), the principle of the test is as follows:\n. . .\nWe calculate the MLE in each model to obtain \\(\\hat{\\beta}\\) in the complete model and \\(\\hat{\\beta}_{H_0}\\) in the constrained model.\n. . .\nLogic: If \\(H_0\\) is true, the constrained model should be as “likely” as the complete model, so \\(L(\\hat{\\beta})\\) and \\(L(\\hat{\\beta}_{H_0})\\) should be similar."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#deviance-test-statistic",
    "href": "teaching/glm/slides/logistic_model.html#deviance-test-statistic",
    "title": "The Logistic Model",
    "section": "Deviance Test Statistic",
    "text": "Deviance Test Statistic\n. . .\nThe test statistic is the difference of deviances:\n\n\\[D_{H_0} - D_{H_1} = 2\\left(L(\\hat{\\beta}) - L(\\hat{\\beta}_{H_0})\\right)\\]\n\n. . .\nUnder \\(H_0\\), denoting \\(q\\) the number of constraints, we have the convergence (admitted):\n\n\\[D_{H_0} - D_{H_1} = 2\\left(L(\\hat{\\beta}) - L(\\hat{\\beta}_{H_0})\\right) \\xrightarrow{L} \\chi^2_q\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#rejection-region-and-p-value",
    "href": "teaching/glm/slides/logistic_model.html#rejection-region-and-p-value",
    "title": "The Logistic Model",
    "section": "Rejection Region and P-value",
    "text": "Rejection Region and P-value\n. . .\nThe asymp. rejection region at asymptotic level \\(\\alpha\\) is therefore\n\n\\[\\mathcal R_\\alpha = \\{D_{H_0} - D_{H_1} &gt; \\chi^2_{q,1-\\alpha}\\}\\]\n\nwhere \\(\\chi^2_{q,1-\\alpha}\\): quantile \\(1 - \\alpha\\) of a \\(\\chi^2_q\\) distribution.\n. . .\nThe p-value equals\n\n\\[p\\text{-value} = 1 - F(D_{H_0} - D_{H_1})\\]\n\nwhere \\(F\\) is the cdf of a \\(\\chi^2_q\\) distribution."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#special-case-1-significance-test",
    "href": "teaching/glm/slides/logistic_model.html#special-case-1-significance-test",
    "title": "The Logistic Model",
    "section": "Special Case 1: Significance Test",
    "text": "Special Case 1: Significance Test\n. . .\nWe want to test if a model (having a constant) is significant\n. . .\nWe therefore test \\(H_0\\): all its coefficients are zero except the constant. This corresponds to the special case\n\n\\(R = [0 | I_{p-1}]\\).\n\n. . .\nWe compare the deviance of the model to the null deviance \\(D_0\\), corresponding to a model that contains only the constant."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#test-statistic",
    "href": "teaching/glm/slides/logistic_model.html#test-statistic",
    "title": "The Logistic Model",
    "section": "Test Statistic",
    "text": "Test Statistic\n. . .\nThe test statistic is \\(D_0 - D\\). Under \\(H_0\\), when \\(n \\to \\infty\\):\n\n\\[D_0 - D \\sim \\chi^2_{p-1}\\]\n\n. . .\nThe model is therefore significant (relative to the null model) if the sample is in the critical region of asymptotic level \\(\\alpha\\):\n\n\\[\\mathcal R_\\alpha = \\{D_0 - D &gt; \\chi^2_{p-1,1-\\alpha/2}\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#special-case-2-nested-models",
    "href": "teaching/glm/slides/logistic_model.html#special-case-2-nested-models",
    "title": "The Logistic Model",
    "section": "Special Case 2: Nested Models",
    "text": "Special Case 2: Nested Models\n. . .\nSuppose that model \\(1\\) (with deviance \\(D_1\\)) is a sub-model of model \\(2\\) (with deviance \\(D_2\\))\n. . .\nModel \\(1\\) is therefore obtained from model \\(2\\), with parameter \\(\\beta\\), via a constraint of the type \\(R\\beta = 0\\) where \\(R\\) is a \\((q, p)\\) matrix.\n. . .\nUnder \\(H_0: R\\beta = 0\\), we have asymptotically \\(D_1 - D_2 \\sim \\chi^2_q\\)\n. . .\nHence the asymptotic test: .\n\n\\(\\mathcal R_\\alpha = \\{D_1 - D_2 &gt; \\chi^2_{q,1 - \\alpha}\\}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#aic-and-bic-criteria",
    "href": "teaching/glm/slides/logistic_model.html#aic-and-bic-criteria",
    "title": "The Logistic Model",
    "section": "AIC and BIC Criteria",
    "text": "AIC and BIC Criteria\n. . .\nThe AIC and BIC criteria are defined similarly to linear regression, i.e.\n\n\\(\\text{AIC} = -2L_{\\text{mod}} + 2p\\)\n\n\n\\(\\text{BIC} = -2L_{\\text{mod}} + \\ln(n)p\\)\n\nwhere \\(L_{\\text{mod}}\\) is the log-likelihood of the estimated model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#aic-and-bic-criteria-1",
    "href": "teaching/glm/slides/logistic_model.html#aic-and-bic-criteria-1",
    "title": "The Logistic Model",
    "section": "AIC and BIC Criteria",
    "text": "AIC and BIC Criteria\n. . .\nIf we ignore saturated likelihood and set \\(L_{\\text{sat}}=0\\),\n\n\\(\\text{AIC} = D + 2p\\)\n\n\n\\(\\text{BIC} = D + \\ln(n)p\\)\n\nIn practice, we choose the model having the minimal AIC or BIC\n. . .\nAs in linear regression, we can use automatic selection procedures (backward, forward, etc)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-obesity-study-in-the-us",
    "href": "teaching/glm/slides/logistic_model.html#example-obesity-study-in-the-us",
    "title": "The Logistic Model",
    "section": "Example: Obesity Study in the US",
    "text": "Example: Obesity Study in the US\n\nmodel=glm(Y~AGE+DBP+SEXE+ACTIV+WALK+MARITAL, family=binomial)\nsummary(model)\n\n\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4459.5\\)\non \\(5290\\)\n\n\nAIC\n\\(4481.5\\)\n\n\n\n\n\n\n\nThe model deviance is therefore \\(D = 4459.5\\).\nSignificance Test: We compare \\(D\\) to the null deviance \\(D_0 = 4610.8\\): \\(D_0 - D = 151.3\\). The p-value of the test equals \\(1 - F_{10}(151.3) \\approx 0\\) where \\(F_{10}\\) is the cdf of a \\(\\chi^2_{10}\\).\n\n\nThe model is significant."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-significance-test-of-marital",
    "href": "teaching/glm/slides/logistic_model.html#example-significance-test-of-marital",
    "title": "The Logistic Model",
    "section": "Example: Significance Test of MARITAL",
    "text": "Example: Significance Test of MARITAL\n. . .\nmodel=glm(Y~AGE+DBP+SEXE+ACTIV+WALK, family=binomial) # We want to test if `MARITAL` is significant\nsummary(model)\n. . .\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4462.7\\)\non \\(5295\\)\n\n\nAIC\n\\(4474.7\\)\n\n\n\n\n\n. . .\nThe deviance is now \\(D_2 = 4462.7\\). To compare with the previous model, we calculate: \\(D_2 - D = 3.2\\).\n. . .\nThe p-value of the test equals \\(1 - F_5(3.2) \\approx 0.67\\), where \\(F_5\\): cdf of a \\(\\chi^2_5\\).\nWe therefore accept \\(H_0\\): the coefficients related to MARITAL are zero. (Also confirmed with AIC)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-significance-test-of-age2",
    "href": "teaching/glm/slides/logistic_model.html#example-significance-test-of-age2",
    "title": "The Logistic Model",
    "section": "Example: Significance Test of AGE\\(~^2\\)",
    "text": "Example: Significance Test of AGE\\(~^2\\)\n. . .\nmodel=glm(Y∼AGE+I(AGE2 )+DBP+SEXE+WALK+ACTIV, family=binomial) # We add AGE^2\nsummary(model)\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4439.5\\)\non \\(5294\\)\n\n\nAIC\n\\(4453.5\\)\n\n\n\n\n\n. . .\nThe deviance test with the previous model has p-value \\(1 - F_1(4462.7 - 4439.5) = 1 - F_1(23.2) \\approx 10^{-6}\\)\n. . .\nThis model is therefore preferable, (confirmed with AIC).\n. . .\nHowever, we cannot compare this model with the first one by deviance test because they are not nested.\n. . .\nWe can however compare their AIC: this model is preferable."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#setup-for-prevision",
    "href": "teaching/glm/slides/logistic_model.html#setup-for-prevision",
    "title": "The Logistic Model",
    "section": "Setup for Prevision",
    "text": "Setup for Prevision\n\nSuppose we are interested in a new individual for whom\n\nwe know their characteristics \\(x \\in \\mathbb{R}^p\\),\nwe do not know their \\(Y\\).\n\n\n\nWe want to predict \\(Y\\) for this new individual."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#probability-estimation",
    "href": "teaching/glm/slides/logistic_model.html#probability-estimation",
    "title": "The Logistic Model",
    "section": "Probability Estimation",
    "text": "Probability Estimation\n\nIf we have fitted a logistic regression model, we can estimate\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nby\n\n\\[p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta}) = \\frac{e^{x^T \\hat{\\beta}}}{1 + e^{x^T \\hat{\\beta}}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#what-we-will-see",
    "href": "teaching/glm/slides/logistic_model.html#what-we-will-see",
    "title": "The Logistic Model",
    "section": "What We Will See",
    "text": "What We Will See\n\nWe will see:\n\nhow to construct a confidence interval around this estimation;\nhow to exploit this estimation to classify the individual in category \\(Y = 0\\) or \\(Y = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#asymptotic-distribution",
    "href": "teaching/glm/slides/logistic_model.html#asymptotic-distribution",
    "title": "The Logistic Model",
    "section": "Asymptotic Distribution",
    "text": "Asymptotic Distribution\n\nWe know that when \\(n \\to \\infty\\):\n\n\\[\\hat{\\beta} \\sim N(\\beta, (X^T W_{\\hat{\\beta}} X)^{-1})\\]\n\nHence, when \\(n \\to \\infty\\), for any \\(x \\in \\mathbb R^p\\),\n\n\\[x^T \\hat{\\beta} \\sim N(x^T \\beta, x^T (X^T W_{\\hat{\\beta}} X)^{-1} x)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#ci-for-linear-predictor",
    "href": "teaching/glm/slides/logistic_model.html#ci-for-linear-predictor",
    "title": "The Logistic Model",
    "section": "CI for Linear Predictor",
    "text": "CI for Linear Predictor\n. . .\nWe deduce that when \\(n \\to +\\infty\\), \\(x^T \\hat{\\beta} \\sim N(x^T \\beta, x^T (X^T W_{\\hat{\\beta}} X)^{-1} x)\\), and the asymptotic CI of \\(x^T\\beta\\):\n. . .\n\n\\[\\text{CI}_{1-\\alpha}(x^T \\beta) = \\left[x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#setting-and-objective",
    "href": "teaching/glm/slides/logistic_model.html#setting-and-objective",
    "title": "The Logistic Model",
    "section": "Setting and Objective",
    "text": "Setting and Objective\n. . .\nSuppose we are interested in a new individual for whom\n\nwe know their characteristics \\(x \\in \\mathbb{R}^p\\),\nwe do not know their \\(Y\\).\n\n. . .\nWe want to predict \\(Y\\) for this new individual."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#recall-in-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.html#recall-in-the-logit-model",
    "title": "The Logistic Model",
    "section": "Recall in the Logit Model",
    "text": "Recall in the Logit Model\n. . .\nIf we have fitted a logistic regression model, we can estimate\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nby\n. . .\n\n\\[p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta}) = \\frac{e^{x^T \\hat{\\beta}}}{1 + e^{x^T \\hat{\\beta}}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#outline-for-prediction",
    "href": "teaching/glm/slides/logistic_model.html#outline-for-prediction",
    "title": "The Logistic Model",
    "section": "Outline for Prediction",
    "text": "Outline for Prediction\n. . .\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nWe will see:\n\nhow to construct a confidence interval around the estimation \\(p_{\\hat{\\beta}}(x)\\);\nhow to exploit this estimation to classify the new individual in category \\(Y = 0\\) or \\(Y = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#ci-asymptotic-distribution-of-p_betax",
    "href": "teaching/glm/slides/logistic_model.html#ci-asymptotic-distribution-of-p_betax",
    "title": "The Logistic Model",
    "section": "CI: Asymptotic Distribution of \\(p_\\beta(x)\\)",
    "text": "CI: Asymptotic Distribution of \\(p_\\beta(x)\\)\n. . .\nWe know that when \\(n \\to \\infty\\):\n\n\\[\\hat{\\beta} \\sim N(\\beta, (X^T W_{\\hat{\\beta}} X)^{-1})\\]\n\n\n\\(X=(X^{(1)}, \\dots, X^{(p)})\\) is the \\(n \\times p\\) design matrix\n\\(W_{\\hat{\\beta}}\\) is the \\(n \\times n\\) diagonal matrix with coefs \\(p_{\\hat \\beta}(x_i)(1-p_{\\hat \\beta}(x_i))\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#ci-for-linear-predictor-1",
    "href": "teaching/glm/slides/logistic_model.html#ci-for-linear-predictor-1",
    "title": "The Logistic Model",
    "section": "CI for Linear Predictor",
    "text": "CI for Linear Predictor\n\nWe deduce that when \\(n \\to +\\infty\\), \\(x^T \\hat{\\beta} \\sim N(x^T \\beta, x^T (X^T W_{\\hat{\\beta}} X)^{-1} x)\\), and the asymptotic CI of \\(x^T\\beta\\):\n\n\n\n\\[\\text{CI}_{1-\\alpha}(x^T \\beta) = \\left[x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#ci-for-probability-p_betax",
    "href": "teaching/glm/slides/logistic_model.html#ci-for-probability-p_betax",
    "title": "The Logistic Model",
    "section": "CI for Probability \\(p_{\\beta}(x)\\)",
    "text": "CI for Probability \\(p_{\\beta}(x)\\)\n. . .\nSince \\(p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta})\\), we have therefore by application of the increasing function \\(\\text{logit}^{-1}\\), the CI at asymptotic level \\(1 - \\alpha\\):\n. . .\n\n\n\\[\\text{CI}_{1-\\alpha}(p_\\beta(x)) = \\left[\\text{logit}^{-1}\\left(x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right)\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#classification-1",
    "href": "teaching/glm/slides/logistic_model.html#classification-1",
    "title": "The Logistic Model",
    "section": "Classification",
    "text": "Classification\n\nWe have estimated \\(p_\\beta(x) = P(Y = 1|X = x)\\) by \\(p_{\\hat{\\beta}}(x)\\).\n\n\nFor a threshold to choose \\(s \\in [0, 1]\\), we use the rule:\n\n\\[\\begin{cases}\n\\text{if } p_{\\hat{\\beta}}(x) &gt; s, & \\hat{Y} = 1 \\\\\n\\text{if } p_{\\hat{\\beta}}(x) &lt; s, & \\hat{Y} = 0\n\\end{cases}\\]\n\n\n\nThe “natural” choice of threshold is \\(s = 0.5\\) but this choice can be optimized."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#evaluation-of-classification-quality",
    "href": "teaching/glm/slides/logistic_model.html#evaluation-of-classification-quality",
    "title": "The Logistic Model",
    "section": "Evaluation of Classification Quality",
    "text": "Evaluation of Classification Quality\n. . .\nWe proceed by cross-validation:\n. . .\nUsing a train sample, predict \\(Y\\) on a test sample and form the confusion matrix.\n. . .\n\n\n\n\n\\(Y = 0\\)\n\\(Y = 1\\)\n\n\n\n\n\\(\\hat{Y} = 0\\)\nTN\nFN\n\n\n\\(\\hat{Y} = 1\\)\nFP\nTP\n\n\n\nReading: T: true, F:False, N: Negative, P: Positive.\n. . .\nFP: false positives: number of individuals who were classified positive who were actually negative"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#evaluation-of-classification-quality-1",
    "href": "teaching/glm/slides/logistic_model.html#evaluation-of-classification-quality-1",
    "title": "The Logistic Model",
    "section": "Evaluation of Classification Quality",
    "text": "Evaluation of Classification Quality\n. . .\nThe ideal is to have a confusion matrix that is as diagonal as possible.\n. . .\nWe generally seek to maximize the following indicators:\n. . .\n\n\nThe sensitivity (or recall, or true positive rate) estimates \\(\\P(\\hat{Y} = 1|Y = 1)\\) by\n\n\\[\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\]\n\n\nThe specificity (or selectivity, or true negative rate) estimates \\(\\P(\\hat{Y} = 0|Y = 0)\\)\n\n\\[\\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\\]\n\n\n\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#other-indicators",
    "href": "teaching/glm/slides/logistic_model.html#other-indicators",
    "title": "The Logistic Model",
    "section": "Other Indicators",
    "text": "Other Indicators\n. . .\nThe precision (or positive predictive value) estimates \\(\\P(Y = 1|\\hat{Y} = 1)\\) by\n\n\\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\)\n\n. . .\nThe \\(F\\)-score is the harmonic mean between sensitivity and precision:\n\n\\[F_1 = 2 \\frac{\\text{precision} \\times \\text{sensitivity}}{\\text{precision} + \\text{sensitivity}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#choice-of-threshold-s",
    "href": "teaching/glm/slides/logistic_model.html#choice-of-threshold-s",
    "title": "The Logistic Model",
    "section": "Choice of Threshold \\(s\\)",
    "text": "Choice of Threshold \\(s\\)\n. . .\nFor each threshold \\(s\\), from a test sample:\n\nwe can form the confusion matrix\ncalculate scores (sensitivity, \\(F\\)-score, etc.)\n\n. . .\nWe finally choose the optimal threshold \\(s\\), according to the chosen score."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#choosing-the-score",
    "href": "teaching/glm/slides/logistic_model.html#choosing-the-score",
    "title": "The Logistic Model",
    "section": "Choosing the score",
    "text": "Choosing the score\n\nIt depends on the context of the study\nIt can be much more serious to wrongly predict \\(\\hat{Y} = 0\\) than \\(\\hat{Y} = 1\\)\n\n. . .\n\\(\\hat Y=1\\) (treatment) while the patient is not ill (\\(Y = 0\\))\n. . .\n\\(\\hat Y=0\\) (no treatment) while the patient has a serious illness (\\(Y=1\\))"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#roc-curve",
    "href": "teaching/glm/slides/logistic_model.html#roc-curve",
    "title": "The Logistic Model",
    "section": "ROC Curve",
    "text": "ROC Curve\n. . .\nWe can also plot the ROC curve (TP rate as a function of FP rate for \\(s \\in [0, 1]\\)):\n\n\n\\(ROC:~~\\mathrm{sensitiv.} = \\frac{TP}{TP+FN} = F\\left(\\frac{FP}{FP+TN}\\right) = F(1-\\mathrm{specific.})\\)\n\n\n. . .\nThe AUC (area under the curve) is a quality indicator of the model (\\(0 \\leq \\text{AUC} \\leq 1\\)).\n. . .\nOr equivalently, the Gini index: \\(2 \\times \\text{AUC} - 1\\).\n. . .\nUse: compare \\(2\\) models by plotting the 2 ROC curves."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#roc-curve-illustration",
    "href": "teaching/glm/slides/logistic_model.html#roc-curve-illustration",
    "title": "The Logistic Model",
    "section": "ROC Curve Illustration",
    "text": "ROC Curve Illustration\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#this-class",
    "href": "teaching/glm/slides/categorical_data.html#this-class",
    "title": "Models for Categorical Data",
    "section": "This Class",
    "text": "This Class\n\nFor each individual \\(i\\), \\(Y_i\\) takes now \\(K\\) values\n\n\nTwo class of models:\n\nNominal: no relation a priori between the \\(K\\) values\nOrdinal: a natural relation exists between these values"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#binary-case",
    "href": "teaching/glm/slides/categorical_data.html#binary-case",
    "title": "Models for Categorical Data",
    "section": "Binary Case",
    "text": "Binary Case\n\nIn the binary case (\\(K = 2\\)), the logistic model assumes that there exists \\(\\beta \\in \\mathbb{R}^p\\) such that:\n\n\n\n\\[\\frac{p^{(1)}(x)}{p^{(0)}(x)} = e^{x^T \\beta}\\]\n\n(because \\(\\frac{p^{(1)}(x)}{p^{(0)}(x)} = \\frac{p^{(1)}(x)}{1-p^{(1)}(x)}\\))\n\n\nCategory “\\(0\\)” can be seen as a reference category."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#general-case",
    "href": "teaching/glm/slides/categorical_data.html#general-case",
    "title": "Models for Categorical Data",
    "section": "General Case",
    "text": "General Case\n\nIn the general case (\\(K\\) arbitrary), the nominal logistic model (or multinomial, or reference category model) similarly assumes for \\(k \\in \\{1, \\ldots, K-1\\}\\):\n\n\n\\(\\frac{p^{(k)}(x)}{p^{(0)}(x)} = e^{x^T \\beta^{(k)}}\\)\n\n\n\n\nwhere \\(\\beta^{(k)} \\in \\mathbb{R}^p\\) is the parameter associated with category \\(k\\).\nCategory “\\(0\\)” is the reference category, whose probability is deduced from the others.\n\n\nThere are in total \\((K-1) \\times p\\) unknown parameters."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#probability-formulas",
    "href": "teaching/glm/slides/categorical_data.html#probability-formulas",
    "title": "Models for Categorical Data",
    "section": "Probability Formulas",
    "text": "Probability Formulas\n\nWe deduce that in this model, for all \\(k \\in \\{1, \\ldots, K-1\\}\\),\n\n\\(p^{(k)}(x) = p_\\beta^{(k)}(x) = \\frac{e^{x^T \\beta^{(k)}}}{1 + \\sum_{r=1}^{K-1} e^{x^T \\beta^{(r)}}}\\)\n\n\n\n\n\\(p^{(0)}(x) = p_\\beta^{(0)}(x) = \\frac{1}{1 + \\sum_{r=1}^{K-1} e^{x^T \\beta^{(r)}}}\\)\n\n\n\n(which is consistent with the previous formula by taking \\(\\beta^{(0)} = 0\\))"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#remarks",
    "href": "teaching/glm/slides/categorical_data.html#remarks",
    "title": "Models for Categorical Data",
    "section": "Remarks",
    "text": "Remarks\n\nWe note that each \\(p_\\beta^{(k)}(x)\\) depends on all parameters \\(\\beta = (\\beta^{(1)}, \\ldots, \\beta^{(K-1)})\\) and not only on \\(\\beta^{(k)}\\)\n\n\nHence the notation \\(p_\\beta^{(k)}(x)\\) with the index \\(\\beta\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#odd-ratio-of-yk",
    "href": "teaching/glm/slides/categorical_data.html#odd-ratio-of-yk",
    "title": "Models for Categorical Data",
    "section": "Odd Ratio of \\(Y=k\\)",
    "text": "Odd Ratio of \\(Y=k\\)\n\n\\(\\beta^{(k)} \\in \\mathbb R^p\\) depends on the reference category.\n\n\nWe call “odds” of event \\(Y = k\\), the ratio \\(p_\\beta^{(k)}(x)/p_\\beta^{(0)}(x)\\).\nThe OR of \\(Y = k\\) for two characteristics \\(x_1\\) and \\(x_2\\) is therefore\n\n\\[OR^{(k)}(x_1, x_2) = \\frac{p_\\beta^{(k)}(x_1)/p_\\beta^{(0)}(x_1)}{p_\\beta^{(k)}(x_2)/p_\\beta^{(0)}(x_2)} = e^{(x_1 - x_2)^T \\beta^{(k)}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#interpretation-of-odds-ratio",
    "href": "teaching/glm/slides/categorical_data.html#interpretation-of-odds-ratio",
    "title": "Models for Categorical Data",
    "section": "Interpretation of Odds Ratio",
    "text": "Interpretation of Odds Ratio\n\n\\(OR^{(k)}(x_1, x_2)\\) depends only on \\(\\beta^{(k)}\\), and even, only on \\(\\beta_j^{(k)}\\) if \\(x_1\\) and \\(x_2\\) differ only by regressor \\(X^{(j)}\\).\n\n\nWhile probabilities \\(p_\\beta^{(k)}(x)\\) depend on other \\(\\beta^{(k')}\\), \\(k'\\neq k\\)!!\n\n\nWe find the same interpretation of OR as in logistic regression, except that here the odds is relative to the reference category.\n\n\nIt is therefore important to judiciously choose the reference category for interpretations."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#change-of-reference",
    "href": "teaching/glm/slides/categorical_data.html#change-of-reference",
    "title": "Models for Categorical Data",
    "section": "Change of Reference",
    "text": "Change of Reference\n\nThat said, for two categories \\(k \\neq l\\), the probability ratio\n\n\\[\\frac{p_\\beta^{(k)}(x)}{p_\\beta^{(l)}(x)} = e^{x^T(\\beta^{(k)} - \\beta^{(l)})}\\]\n\ndoes not depend on the chosen reference category.\n\n\nSimilarly, the value of probabilities \\(p_\\beta^{(k)}(x)\\) and their estimation do not depend on the chosen reference category either."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#change-of-reference-justification",
    "href": "teaching/glm/slides/categorical_data.html#change-of-reference-justification",
    "title": "Models for Categorical Data",
    "section": "Change of Reference, Justification",
    "text": "Change of Reference, Justification\n\nIf the reference category is \\(Y = j\\), denoting the associated parameters \\(\\gamma^{(k)}\\), \\(k \\neq j\\), and \\(\\gamma^{(j)} = 0\\),\n\n\nwe have the relation \\(\\gamma^{(k)} = \\beta^{(k)} - \\beta^{(j)}\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#maximum-likelihood-estimation",
    "href": "teaching/glm/slides/categorical_data.html#maximum-likelihood-estimation",
    "title": "Models for Categorical Data",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nFor each individual \\(i\\), \\(Y_i|X_i = x\\) follows a multinomial distribution \\(\\mathrm{Mult}(1,(p_\\beta^{(0)}(x), \\ldots, p_\\beta^{(K-1)}(x)))\\).\n\n\nThe likelihood of a sample \\((Y_1|X_1 = x_1), \\ldots, (Y_n|X_n = x_n)\\) is written:\n\n\\[\\prod_{i=1}^n \\prod_{k=0}^{K-1} \\left(p_\\beta^{(k)}(x_i)\\right)^{\\mathbf{1}\\{Y_i = k\\}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#log-likelihood",
    "href": "teaching/glm/slides/categorical_data.html#log-likelihood",
    "title": "Models for Categorical Data",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n\nTherefore the log-likelihood\n\n\\[L = \\sum_{i=1}^n \\sum_{k=0}^{K-1} \\mathbf{1}_{y_i = k} \\ln\\left(p_\\beta^{(k)}(x_i)\\right)\\]\n\n\n\nIn the case of the nominal logistic model, we deduce\n\n\n\\[L = \\sum_{i=1}^n \\left[\\sum_{k=1}^{K-1} x_i^T \\beta^{(k)} \\mathbf{1}_{y_i = k} - \\ln\\left(1 + \\sum_{k=1}^{K-1} e^{x_i^T \\beta^{(k)}}\\right)\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#maximum-likelihood-estimator-of-beta",
    "href": "teaching/glm/slides/categorical_data.html#maximum-likelihood-estimator-of-beta",
    "title": "Models for Categorical Data",
    "section": "Maximum Likelihood Estimator of \\(\\beta\\)",
    "text": "Maximum Likelihood Estimator of \\(\\beta\\)\n\nBy setting the gradient of \\(L\\) to zero, we obtain that \\(\\hat{\\beta} = (\\hat{\\beta}^{(1)}, \\ldots, \\hat{\\beta}^{(K-1)})\\) must verify, for all \\(k \\in \\{1, \\ldots, K-1\\}\\):\n\n\\[\\sum_{i=1}^n x_i \\mathbf{1}_{y_i = k} = \\sum_{i=1}^n x_i p_{\\hat{\\beta}}^{(k)}(x_i)\\]\n\n\n\\(K-1\\) equations, each with \\(p\\) parameters, i.e. \\((K-1) \\times p\\) equations.\nWe solve it numerically to find \\(\\hat{\\beta}\\) of size \\((K-1) \\times p\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#uniqueness-and-existence",
    "href": "teaching/glm/slides/categorical_data.html#uniqueness-and-existence",
    "title": "Models for Categorical Data",
    "section": "Uniqueness and Existence",
    "text": "Uniqueness and Existence\n\nAs in logistic regression:\nWe can show that \\(L\\) is strictly concave as soon as \\(\\text{rank}(X) = p\\).\n\n\nThis ensures the uniqueness of the MLE (if it exists).\n\n\nExistence is ensured if no category is separated from the others by a hyperplane."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#asymptotic-normality",
    "href": "teaching/glm/slides/categorical_data.html#asymptotic-normality",
    "title": "Models for Categorical Data",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\n\nUnder regularity assumptions similar to the case of logistic regression, we have\n\n\n\n\\[J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_{(K-1) \\times p})\\]\n\nwhere \\(J_n(\\beta)\\) is the Fisher information matrix and \\(I_{(K-1) \\times p}\\) is the identity matrix of size \\((K-1) \\times p\\).\n\n\n\\(J_n(\\beta)\\), not detailed here, is a matrix of \\((K-1) \\times (K-1)\\) blocks, each having a similar form to the Fisher information matrix of logistic regression."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#inference-tools",
    "href": "teaching/glm/slides/categorical_data.html#inference-tools",
    "title": "Models for Categorical Data",
    "section": "Inference Tools",
    "text": "Inference Tools\n\nThe inference tools are based on the asymptotic distribution of \\(\\hat{\\beta}\\) and are similar to those of logistic regression:\n\n\nThe significance of each coefficient can be tested by an (asymptotic) Wald test.\n\n\nConfidence intervals, for coefficients and OR, are deduced analogously."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#model-selection-criteria",
    "href": "teaching/glm/slides/categorical_data.html#model-selection-criteria",
    "title": "Models for Categorical Data",
    "section": "Model Selection Criteria",
    "text": "Model Selection Criteria\n\nDeviance is defined similarly: \\(D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\).\n\n\nAs we do for logistic regression:\n\nWe can test the significance of the model,\nor compare two nested models.\nUse the AIC, BIC criteria. Since we have \\(p(K-1)\\) parameters:\n\n\n\n\n\n\\(\\text{AIC} = D + 2p(K-1) \\and \\text{BIC} = D + \\ln(n)p(K-1)\\)"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-car-equipment-preference",
    "href": "teaching/glm/slides/categorical_data.html#example-car-equipment-preference",
    "title": "Models for Categorical Data",
    "section": "Example: Car Equipment Preference",
    "text": "Example: Car Equipment Preference\n\nPreference for an equipped car (with air conditioning and power steering), according to age group and gender.\n\n\n\n\n\n\nGender\nAge Category\nNot Important\nImportant\nVery Important\n\n\n\n\nFemale\n18-23\n26\n12\n7\n\n\n\n24-40\n9\n21\n15\n\n\n\n&gt;40\n5\n14\n41\n\n\nMale\n18-23\n40\n17\n8\n\n\n\n24-40\n17\n15\n12\n\n\n\n&gt;40\n8\n15\n18"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#model-setup",
    "href": "teaching/glm/slides/categorical_data.html#model-setup",
    "title": "Models for Categorical Data",
    "section": "Model Setup",
    "text": "Model Setup\n\nWe want to model the variable \\(Y =\\) “importance” (3 categories)\n\n\nThe regressors are gender (2 classes) and age (3 classes).\n\n\nThese are grouped observations: each category gender/age is observed for several individuals"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#r-implementation",
    "href": "teaching/glm/slides/categorical_data.html#r-implementation",
    "title": "Models for Categorical Data",
    "section": "R Implementation",
    "text": "R Implementation\n\nUnder R, we can use the vglm function from the VGAM library:\nvglm(Y ~ age + sexe, family=multinomial) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept):1\n-0.5908\n0.2840\n-2.080\n0.037484\n*\n\n\n(Intercept):2\n-1.0391\n0.3305\n-3.144\n0.001667\n**\n\n\nage2:1\n1.1283\n0.3416\n3.302\n0.000958\n***\n\n\nage2:2\n1.4781\n0.4009\n3.687\n0.000227\n***\n\n\nage3:1\n1.5877\n0.4029\n3.941\n8.12e-05\n***\n\n\nage3:2\n2.9168\n0.4229\n6.897\n5.32e-12\n***\n\n\nsexeM:1\n-0.3881\n0.3005\n-1.292\n0.196510\n\n\n\nsexeM:2\n-0.8130\n0.3210\n-2.532\n0.011326\n*\n\n\n\n\nNames of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#r-output-remarks",
    "href": "teaching/glm/slides/categorical_data.html#r-output-remarks",
    "title": "Models for Categorical Data",
    "section": "R output, remarks",
    "text": "R output, remarks\n\nIn the output \\(Y\\) is encoded 1, 2 or 3 (from “Not important” to “Very important”)\nWe estimate \\(p^{(2)}(x)/p^{(1)}(x)\\) and \\(p^{(3)}(x)/p^{(1)}(x)\\), the first category being the reference.\nThe reference category for \\(Y\\) is \\(Y = 1\\) (“Not important”)\nAge is encoded 1, 2 or 3 (increasing)\n\\((3-1) + (2-1) + 1 = 4\\) parameters for each \\(p^{(2)}(x)/p^{(1)}(x)\\) and \\(p^{(3)}(x)/p^{(1)}(x)\\)\n\\(2\\times 4=8\\) parameters to estimate!"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#output",
    "href": "teaching/glm/slides/categorical_data.html#output",
    "title": "Models for Categorical Data",
    "section": "Output",
    "text": "Output\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept):1\n-0.5908\n0.2840\n-2.080\n0.037484\n*\n\n\n(Intercept):2\n-1.0391\n0.3305\n-3.144\n0.001667\n**\n\n\nage2:1\n1.1283\n0.3416\n3.302\n0.000958\n***\n\n\nage2:2\n1.4781\n0.4009\n3.687\n0.000227\n***\n\n\nage3:1\n1.5877\n0.4029\n3.941\n8.12e-05\n***\n\n\nage3:2\n2.9168\n0.4229\n6.897\n5.32e-12\n***\n\n\nsexeM:1\n-0.3881\n0.3005\n-1.292\n0.196510\n\n\n\nsexeM:2\n-0.8130\n0.3210\n-2.532\n0.011326\n*"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-calculation",
    "href": "teaching/glm/slides/categorical_data.html#example-calculation",
    "title": "Models for Categorical Data",
    "section": "Example Calculation",
    "text": "Example Calculation\n\nFor example, for a woman aged 18 to 23 years (Age cat. and Gender Cat. are zero):\n\n\\(\\frac{P(Y = \\text{\"Important\"}|\\text{Woman 18-23})}{P(Y = \\text{\"Not important\"}|\\text{Woman 18-23})} = \\exp(-0.59) \\approx 0.55\\)\n\n\n\nFor a man of the same age group, this ratio equals \\(\\exp(-0.59 - 0.3881) \\approx 0.38\\)\n\n\nThe OR between a man and a woman for the “Very important” preference relative to “Not important” equals \\(\\exp(-0.813) = 0.44\\)\n\n\nThis odds is therefore more than double among women…"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#context",
    "href": "teaching/glm/slides/categorical_data.html#context",
    "title": "Models for Categorical Data",
    "section": "Context",
    "text": "Context\n\nIf the categories of \\(Y\\) follow a natural order:\n\n\nWe can obviously ignore it and use the previous nominal model: it is very general but has many parameters.\n\n\nBut we can take advantage of this structure to simplify the model (fewer parameters, easier interpretation)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#recalling-nominal-model",
    "href": "teaching/glm/slides/categorical_data.html#recalling-nominal-model",
    "title": "Models for Categorical Data",
    "section": "Recalling Nominal Model",
    "text": "Recalling Nominal Model\n\nIn consistency with the logistic model, we have focused on the “odds”\n\n\\[\\frac{P(Y = k|X = x)}{P(Y = 0|X = x)} = \\frac{p^{(k)}(x)}{p^{(0)}(x)}\\]\n\n\n\nThis OR quantifies how much the “odds” of \\(P(Y = k)\\) is modified between \\(x_1\\) and \\(x_2\\), relative to the reference category \\(Y = 0\\).\n\n\nIn the ordinal case, we will model “odds” that are easier to interpret."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#section",
    "href": "teaching/glm/slides/categorical_data.html#section",
    "title": "Models for Categorical Data",
    "section": "",
    "text": "Which Odds to Model When Categories Are Ordered?"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#other-odds-for-ordered-categories",
    "href": "teaching/glm/slides/categorical_data.html#other-odds-for-ordered-categories",
    "title": "Models for Categorical Data",
    "section": "Other Odds for Ordered Categories",
    "text": "Other Odds for Ordered Categories\n\nOdds for adjacent categories model:\n\n\\(Odds(x)=\\frac{P(Y = k|X = x)}{P(Y = k-1|X = x)}\\)\n\n\n\nOdds for continuous ratio logistic model:\n\n\\(Odds(x)=\\frac{P(Y = k|X = x)}{P(Y \\leq k-1|X = x)}\\)\n\n\n\nOdds for proportional odds model (most used):\n\n\\(Odds(x)=\\frac{P(Y \\leq k|X = x)}{P(Y &gt; k|X = x)}\\)"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#cumulative-model",
    "href": "teaching/glm/slides/categorical_data.html#cumulative-model",
    "title": "Models for Categorical Data",
    "section": "Cumulative Model",
    "text": "Cumulative Model\n\nThe idea is to construct logistic models for the binary variables \\(\\mathbf{1}_{Y \\leq k}\\), for all \\(k \\in \\{0, \\ldots, K-2\\}\\).\n\n\nThis gives in full generality the cumulative model\n\n\\[\\begin{aligned}\n\\text{logit}(\\P(Y \\leq k|X = x)) &= \\ln\\left(\\frac{\\P(Y \\leq k|X = x)}{\\P(Y &gt; k|X = x)}\\right) \\\\\n&= x^T \\beta^{(k)}\\end{aligned}\\]\n\n\n\n\\(p(K-1)\\) parameters but is different from the nominal model."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#proportional-odds-model",
    "href": "teaching/glm/slides/categorical_data.html#proportional-odds-model",
    "title": "Models for Categorical Data",
    "section": "Proportional Odds Model",
    "text": "Proportional Odds Model\n\nAssumption: the effect of regressors (except the constant) is constant regardless of the categories:\n\n\\[\\text{logit}(\\P(Y \\leq k|X = x)) = \\beta_0^{(k)} + \\beta^T X^*\\]\n\n\n\nwhere \\(X^* \\in \\mathbb{R}^{p-1}\\) denotes the vector of regressors other than the constant.\n\n\n\\((K-1) + (p-1)\\) parameters (this is much less)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#constraints-in-proportional-odds",
    "href": "teaching/glm/slides/categorical_data.html#constraints-in-proportional-odds",
    "title": "Models for Categorical Data",
    "section": "Constraints in Proportional Odds",
    "text": "Constraints in Proportional Odds\n\nSince for any \\(k\\),\n\n\n\\(\\text{logit}(P(Y \\leq k|X = x)) \\leq \\text{logit}(P(Y \\leq k+1|X = x))\\)\n\n\n\n\nthe proportional odds model must verify, for all \\(x \\in \\mathbb R^p\\):\n\n\\(\\beta_0^{(0)} \\leq \\dots \\leq \\beta_0^{(K-2)}\\)\n\n\n\nThis constraint is imposed during estimation."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds",
    "href": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds",
    "title": "Models for Categorical Data",
    "section": "Motivation for Proportional Odds",
    "text": "Motivation for Proportional Odds\n\nSuppose that the classes \\(Y = k\\) come from the discretization of a continuous latent variable \\(Z\\):\n\n\n\nfor \\(\\alpha_{-1} = -\\infty\\), \\(\\alpha_0 &lt; \\cdots &lt; \\alpha_{K-1}\\) and \\(k \\in \\{0, \\ldots, K-1\\}\\),\n\n\\[\\1\\{Y=k\\} = \\1\\{\\alpha_{k-1} \\leq Z &lt; \\alpha_k\\}\\]\n\n\n\nExample: \\(Z\\) is a grade, and \\(Y\\) the distinction level"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds-1",
    "href": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds-1",
    "title": "Models for Categorical Data",
    "section": "Motivation for Proportional Odds",
    "text": "Motivation for Proportional Odds\n\n\n\\(\\1\\{Y=k\\} = \\1\\{\\alpha_{k-1} \\leq Z &lt; \\alpha_k\\}\\)\n\n\n\nSuppose there exists a linear relationship between \\(Z\\) and the regressors \\(X\\):\n\n\\[Z = \\beta^T X + \\varepsilon\\]\n\n\n\nwhere \\(\\varepsilon\\) follows a distribution with cdf \\(F\\). Then\n\n\\[P(Y \\leq k) = F(\\alpha_k - \\beta^T X)\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds-2",
    "href": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds-2",
    "title": "Models for Categorical Data",
    "section": "Motivation for Proportional Odds",
    "text": "Motivation for Proportional Odds\n\n\n\\[P(Y \\leq k) = F(\\alpha_k - \\beta^T X)\\]\n\n\n\nThe dependence on \\(X\\) does not depend on \\(k\\).\n\n\nIf \\(F = \\text{logit}^{-1}\\), we obtain the proportional odds model.\n\n\nOther choices of \\(F\\) are possible (probit,…) but the OR become less interpretable."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#section-1",
    "href": "teaching/glm/slides/categorical_data.html#section-1",
    "title": "Models for Categorical Data",
    "section": "",
    "text": "Why “Proportional Odds”?"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#model-assumption",
    "href": "teaching/glm/slides/categorical_data.html#model-assumption",
    "title": "Models for Categorical Data",
    "section": "Model Assumption",
    "text": "Model Assumption\n\nThe proportional odds model assumes that for \\(k \\in \\{0, \\ldots, K-2\\}\\),\n\n\\[\\frac{P(Y \\leq k|X = x)}{P(Y &gt; k|X = x)} = e^{\\beta_0^{(k)} + \\beta^T X^*}\\]\n\n\n\nThis is the odds of \\(Y \\leq k\\) given \\(X=x\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#proportional-log-odds-ratio",
    "href": "teaching/glm/slides/categorical_data.html#proportional-log-odds-ratio",
    "title": "Models for Categorical Data",
    "section": "Proportional log Odds Ratio",
    "text": "Proportional log Odds Ratio\n\nThe odds-ratio of \\(Y \\leq k\\) between two individuals with regressors \\(x_1\\) and \\(x_2\\) respectively therefore equals\n\n\\[OR(x_1, x_2) = \\exp(\\beta^T(x_1^* - x_2^*))\\]\n\n\n\nThis OR does not depend on \\(k\\).\n\n\n\\(\\ln(OR(x_1, x_2))\\) is “proportional” to \\((x_1^* - x_2^*)\\), the “constant” of proportionality \\(\\beta\\) (actually a vector) being independent of \\(k\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#equality-of-slopes",
    "href": "teaching/glm/slides/categorical_data.html#equality-of-slopes",
    "title": "Models for Categorical Data",
    "section": "Equality of Slopes",
    "text": "Equality of Slopes\n\n\n\\(\\text{logit}(P(Y \\leq k|X)) = \\beta_0^{(k)} + \\beta^T X^*\\)\n\n\n\nimplies that the following \\(K-1\\) sets\n\n\\(\\{\\text{logit}(P(Y \\leq k|X=x)), ~ x \\in \\mathbb R^p\\}\\)\n\nare parallel hyperplanes.\n\nThey indeed all have the same normal vector \\(\\beta\\).\nThey differ only by the intercept constant \\(\\beta_0^{(k)}\\).\nTo validate the proportional odds model, it is appropriate to test whether this property is true."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#parallel-hyperplane-illustration",
    "href": "teaching/glm/slides/categorical_data.html#parallel-hyperplane-illustration",
    "title": "Models for Categorical Data",
    "section": "Parallel Hyperplane, Illustration",
    "text": "Parallel Hyperplane, Illustration\nEquality of slopes, or not…"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#slope-equality-test-formulation",
    "href": "teaching/glm/slides/categorical_data.html#slope-equality-test-formulation",
    "title": "Models for Categorical Data",
    "section": "Slope Equality Test, Formulation",
    "text": "Slope Equality Test, Formulation\n\nWe start from the general cumulative model\n\n\\(\\frac{P(Y \\leq k|X = x)}{P(Y &gt; k|X = x)} = e^{x^T \\beta^{(k)}}\\)\n\n\n\nWe test if the parameters (except the constant) are equal regardless of \\(k\\). Writing \\(\\beta^{(k)} = (\\beta_0^{(k)}, \\ldots, \\beta_{p-1}^{(k)})\\), \\(\\beta_0^{(k)}\\) being the constant, we test:\n\n\\[H_0: \\begin{cases}\n\\beta_1^{(0)} = \\cdots = \\beta_1^{(K-2)} \\\\\n\\vdots \\\\\n\\beta_{p-1}^{(0)} = \\cdots = \\beta_{p-1}^{(K-2)}\n\\end{cases}\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#section-2",
    "href": "teaching/glm/slides/categorical_data.html#section-2",
    "title": "Models for Categorical Data",
    "section": "",
    "text": "This can be done by a deviance test (likelihood ratio) by comparing the general cumulative model and the proportional odds model."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#inference",
    "href": "teaching/glm/slides/categorical_data.html#inference",
    "title": "Models for Categorical Data",
    "section": "Inference",
    "text": "Inference\n\nRecall: the \\((Y_i|X_i = x_i)\\) being independent and multinomially distributed, the log-likelihood is\n\n\\[L = \\sum_{i=1}^n \\sum_{k=0}^{K-1} \\mathbf{1}\\{Y_i = k\\} \\ln(p_\\beta^{(k)}(x_i))\\]\n\n\n\nFor the cumulative model and the proportional odds model:\n\nwe can deduce the form of \\(p_\\beta^{(k)}\\)\nwe then maximize \\(L\\) in \\(\\beta\\) to obtain \\(\\hat{\\beta}\\) (by numerical methods)"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#tests",
    "href": "teaching/glm/slides/categorical_data.html#tests",
    "title": "Models for Categorical Data",
    "section": "Tests",
    "text": "Tests\n\nAs usual:\n\n\nWe can compare \\(L_{\\text{mod}} = L(\\hat{\\beta})\\) with other nested models to perform a likelihood ratio test (i.e., deviance test).\n\n\n\\(\\hat{\\beta} - \\beta\\) follows asymptotically a \\(N(0, J_n(\\beta)^{-1})\\), where \\(J_n(\\beta)\\) is the negative of the Hessian of \\(L\\).\n\n\nWe can therefore perform Wald tests."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-car-equipment-study",
    "href": "teaching/glm/slides/categorical_data.html#example-car-equipment-study",
    "title": "Models for Categorical Data",
    "section": "Example: Car Equipment Study",
    "text": "Example: Car Equipment Study"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#data-table",
    "href": "teaching/glm/slides/categorical_data.html#data-table",
    "title": "Models for Categorical Data",
    "section": "Data Table",
    "text": "Data Table\n\n\n\n\nGender\nAge Category\nNot Important\nImportant\nVery Important\n\n\n\n\nFemale\n18-23\n26\n12\n7\n\n\n\n24-40\n9\n21\n15\n\n\n\n&gt;40\n5\n14\n41\n\n\nMale\n18-23\n40\n17\n8\n\n\n\n24-40\n17\n15\n12\n\n\n\n&gt;40\n8\n15\n18\n\n\n\n\n\nWe want to model the variable \\(Y =\\) “importance” (3 categories)\n\n\nThe regressors are gender (2 classes) and age (3 classes).\n\n\nWe have already modeled \\(Y\\) using a multinomial model.\n\n\nIn fact \\(Y\\) is an ordinal variable: we will exploit this."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-proportional-odds-model",
    "href": "teaching/glm/slides/categorical_data.html#example-proportional-odds-model",
    "title": "Models for Categorical Data",
    "section": "Example: Proportional Odds Model",
    "text": "Example: Proportional Odds Model\nWe estimate a proportional odds model:\nvglm(Y ~ age + sexe, family=cumulative(parallel=TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept):1\n0.04354\n0.23030\n0.189\n0.8501\n\n\n\n(Intercept):2\n1.65498\n0.25360\n6.526\n6.76e-11\n***\n\n\nage2\n-1.14710\n0.27727\n-4.137\n3.52e-05\n***\n\n\nage3\n-2.23246\n0.29042\n-7.687\n1.50e-14\n***\n\n\nsexeM\n0.57622\n0.22611\n2.548\n0.0108\n*\n\n\n\nSignif. codes: 0 ’’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nNames of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2])\nResidual deviance: 4.5321 on 7 degrees of freedom\nLog-likelihood: -25.6671 on 7 degrees of freedom"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-underlying-models",
    "href": "teaching/glm/slides/categorical_data.html#example-underlying-models",
    "title": "Models for Categorical Data",
    "section": "Example: Underlying Models",
    "text": "Example: Underlying Models\n\n\\(K-1=2\\) models (because only the intercepts differs)\n\n\nIn total: \\((K-1) + (p-1) = (3-1) + (4-1) = 5\\) parameters (instead of \\(4\\times 2=8\\))\n\nModel 1: odds between not important and (imortant or very important)\nModel 2: odds between (not important or important) and very important"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-interpretation",
    "href": "teaching/glm/slides/categorical_data.html#example-interpretation",
    "title": "Models for Categorical Data",
    "section": "Example: Interpretation",
    "text": "Example: Interpretation\n\nFor a woman aged 18 to 23 years\n\n\n\\(\\frac{P(\\text{\"Not important\"}|\\text{Woman 18-23})}{P(\\text{\"Important or very important\"}|\\text{Woman 18-23})} = e^{0.043} \\approx 1.04\\)\n\n\n\n\nFor a woman aged over 40 years\n\n\n\\(\\frac{P(\\text{\"Not important\"}|\\text{Woman &gt;40})}{P(\\text{\"Important or very important\"}|\\text{Woman &gt;40})} = e^{0.043-2.23} \\approx 0.11\\)\n\n\n\n\nThe OR \\(e^{0.57622} = 1.78\\) shows that the odds of having a lower preference is \\(1.78\\) times higher for men than for women."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-testing-slope-equality",
    "href": "teaching/glm/slides/categorical_data.html#example-testing-slope-equality",
    "title": "Models for Categorical Data",
    "section": "Example: Testing Slope Equality",
    "text": "Example: Testing Slope Equality\n\nWe test between the full cumulative and proportional odds model.\n\n\nvglm(Y ~ age + sexe, family=cumulative) # To fit with the general cumulative model, we use\nand we note the log-likelihood which equals \\(-25.3164\\).\n\n\nThat of the proportional odds model was \\(-25.6671\\).\n\n\nThe deviance test statistic therefore equals \\(2 \\times (25.6671 - 25.3164) = 0.7\\).\n\n\nWe compare to a \\(\\chi^2_{(K-2)(p-1)} = \\chi^2_3\\) distribution: there is no reason to reject \\(H_0\\) and therefore the proportional odds model is preferable to the cumulative model."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#comparison-with-nominal-model",
    "href": "teaching/glm/slides/categorical_data.html#comparison-with-nominal-model",
    "title": "Models for Categorical Data",
    "section": "Comparison with Nominal Model",
    "text": "Comparison with Nominal Model\n\nTo compare the proportional odds model with the nominal model:\n\n\nWe cannot use a deviance test because the two models are not nested.\n\n\nNevertheless, the AIC and BIC (not reported here) are in favor of the proportional odds model"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#bar-plot-of-estimated-probability",
    "href": "teaching/glm/slides/categorical_data.html#bar-plot-of-estimated-probability",
    "title": "Models for Categorical Data",
    "section": "Bar Plot of Estimated Probability",
    "text": "Bar Plot of Estimated Probability\n\nIn the proportional odds ratio model:"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html",
    "href": "teaching/hypothesis_testing/TDs/TD1.html",
    "title": "TD1: Introduction to statistical models",
    "section": "",
    "text": "We aim to determine whether ENSAI students have any preference for cats or dogs. We assume that, a priori, they have no preference on average. We ask \\(n\\) students what their preferences are, and we let \\(X\\) be the number of “cat” answers.\n\nDefine \\(H_0\\) and \\(H_1\\). Is it a one-sided (unilatéral) or two-sided (bilatéral) test ?\nWe observe \\(10\\) students and \\(X=8\\) “cat” answers. Compute the pvalue in this specific case. Interprete the result.\nWrite the expression of the pvalue in terms of \\(n\\) and \\(X\\) and \\(F\\), the cdf of \\(\\mathrm{Bin}(n,0.5)\\).\nWrite a line of code to compute the pvalue in Julia, Python or R.\nWhat is the pvalue if \\(H_1\\) is instead\n\n“Students prefer cats” or\n“Students prefer dogs”"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-1",
    "title": "TD1: Introduction to statistical models",
    "section": "",
    "text": "We aim to determine whether ENSAI students have any preference for cats or dogs. We assume that, a priori, they have no preference on average. We ask \\(n\\) students what their preferences are, and we let \\(X\\) be the number of “cat” answers.\n\nDefine \\(H_0\\) and \\(H_1\\). Is it a one-sided (unilatéral) or two-sided (bilatéral) test ?\nWe observe \\(10\\) students and \\(X=8\\) “cat” answers. Compute the pvalue in this specific case. Interprete the result.\nWrite the expression of the pvalue in terms of \\(n\\) and \\(X\\) and \\(F\\), the cdf of \\(\\mathrm{Bin}(n,0.5)\\).\nWrite a line of code to compute the pvalue in Julia, Python or R.\nWhat is the pvalue if \\(H_1\\) is instead\n\n“Students prefer cats” or\n“Students prefer dogs”"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-2",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet \\((X_1, X_2, \\ldots, X_n)\\) be iid random variables that follow distribution \\(\\mathcal E(\\lambda)\\). We want to test: \\[\nH_0: \\lambda = \\frac{1}{2} \\quad \\text{vs.} \\quad H_1: \\lambda = 1.\n\\]\n\nShow that if \\(X \\sim E(\\lambda)\\) and \\(Y \\sim \\Gamma(k, \\lambda)\\) are independent, with \\(k \\in \\mathbb{N}^*\\), then \\(X + Y \\sim \\Gamma(k+1, \\lambda)\\). We recall that the density of \\(\\Gamma(\\lambda, k)\\) is given by \\(p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\)\nDeduce that \\(S_n=\\sum_{i=1}^n X_i\\) follows a Gamma distribution \\(\\Gamma(n, \\lambda)\\).\nFor a sample of size \\(n = 10\\), what is the rejection region of \\(S_n\\) for the simple likelihood ratio test with \\(0.05\\) significance level?\nWe admit that a Gamma distribution \\(\\Gamma(n, \\frac{1}{2})\\) is a chi-squared distribution with \\(2n\\) degrees of freedom, \\(\\chi^2(2n)\\). Bonus: Show this fact for \\(n=1\\), using a polar change of variable.\nThe empirical mean is \\(\\bar{x}_{10} = 2.5\\). What can we conclude?\nRecall what a cdf is, and read the p-value on the cdf of the \\(\\chi^2(20)\\) distribution \nCompare the p-value if we use a Gaussian approximation of \\(\\sum X_i\\) with the TCL. We recall that \\(\\mathbb V(X_1) = \\frac{1}{\\lambda^2}\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-3",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet \\(X_1, X_2, \\ldots, X_n\\) be random variables drawn from a normal distribution \\(N(\\theta, 1)\\). To test \\(H_0: \\theta = 5\\) against \\(H_1: \\theta &gt; 5\\), we propose the following test:\n\\[\nT = \\mathbf 1\\{\\bar{x} &gt; 5 + u \\},\n\\]\nwhere \\(\\bar{x}\\) is the empirical mean and \\(u\\) is to be fixed.\n\n\nDerive the function \\(g:~t \\to \\mathbb P(Z \\geq t) - e^{-t^2/2}\\), where \\(Z \\sim \\mathcal N(0,1)\\)\nDeduce that \\(\\mathbb P(Z \\geq t) \\leq e^{-t^2/2}\\) for all \\(t \\geq 0\\).\n\nDeduce a value of \\(u\\) such that the type I error of this test is smaller than a given \\(\\alpha\\). Rewrite the test \\(T\\) in function of \\(\\alpha\\).\nFix \\(\\alpha = 1/e\\) (and \\(u= \\sqrt{2/n}\\)). Compute the power function."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-4",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 4",
    "text": "Exercise 4\nLet the family of Pareto distributions with known parameter \\(a\\) and unknown parameter \\(\\theta\\):\n\\[\nf(x) =\n\\begin{cases}\n\\frac{\\theta}{a} \\left( \\frac{a}{x} \\right)^{\\theta+1}, & \\text{if } x \\geq a, \\\\\n0, & \\text{if } x &lt; a.\n\\end{cases}\n\\]\n\nCompute the mean and variance of \\(X\\), if \\(X\\) follows a Pareto distribution of parameter \\(a\\) and \\(\\theta\\).\nRewrite the density in the form \\(f(x) = a(x)b(\\theta)e^{c(\\theta)d(x)}\\) and identify \\(a\\),\\(b\\),\\(c\\) and \\(d\\).\nDeduce the general form of the uniformly most powerful test \\(UMP_\\alpha\\) for \\(H_0: \\theta \\geq \\theta_0\\) vs. \\(H_1: \\theta &lt; \\theta_0\\).\nFor \\(a = 1\\), construct the test for the null hypothesis: the mean of the distribution is smaller than or equal to 2.\nWhat is the density of \\(d(X_1)\\) ? \\(d\\) is defined in Q.2\nWrite a line of code in Julia, Python or R to compute the rejection region at level \\(\\alpha=0.05\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html",
    "href": "teaching/hypothesis_testing/TDs/TD3.html",
    "title": "TD3: Goodness of Fit",
    "section": "",
    "text": "We want to test if a die is biased. it is rolled \\(1000\\) times, and the number of occurrences for each face is recorded. The data is as follows:\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nCounts\n159\n168\n167\n160\n175\n171\n\n\n\n\nFormulate the hypothesis testing problem\nCompute the expected counts under \\(H_0\\), give the degree of freedom \\(d\\) of the chi-squared test statistic and give the approximated p-value, using the cdf of \\(\\chi^2(d)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-1",
    "title": "TD3: Goodness of Fit",
    "section": "",
    "text": "We want to test if a die is biased. it is rolled \\(1000\\) times, and the number of occurrences for each face is recorded. The data is as follows:\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nCounts\n159\n168\n167\n160\n175\n171\n\n\n\n\nFormulate the hypothesis testing problem\nCompute the expected counts under \\(H_0\\), give the degree of freedom \\(d\\) of the chi-squared test statistic and give the approximated p-value, using the cdf of \\(\\chi^2(d)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-2",
    "title": "TD3: Goodness of Fit",
    "section": "Exercise 2",
    "text": "Exercise 2\nIn a survey of \\(825\\) families with \\(3\\) children, the number of boys was recorded:\n\\[\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{Number of Boys} & 0 & 1 & 2 & 3 & \\text{Total} \\\\\n\\hline\n\\text{Number of Families} & 71 & 297 & 336 & 121 & 825 \\\\\n\\hline\n\\end{array}\n\\]\nWe assume under \\(H_0\\) that the genders of children in successive births within a family are independent categorical variables and that the probability \\(p\\) of having a boy remains constant.\n\nDetermine the distribution of the number of boys in a family with 3 children as a function of \\(p\\).\nEstimate \\(p\\) using a maximum likelihood estimator.\nTest the goodness of fit to the distribution obtained in question 1."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-3",
    "title": "TD3: Goodness of Fit",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe observe X = [0, 1, 0, 0, 0, 0, 0, 0.5, 1, 1, 1, 0.7, 0.9, 1, 1, 1, 1, 0, 0.1, 0, 1] We assume that the entries of \\(X\\) are iid of distribution \\(P\\). We consider the following hypothesis testing problem:\n\\(H_0\\): \\(P= \\mathcal B(0.5)\\) (Bernoulli)\\(\\quad\\) VS \\(\\quad\\) \\(H_1\\): \\(P \\neq \\mathcal B(0.5)\\).\n\nWhat can you say about the assumptions, \\(H_0\\) and \\(H_1\\)?\nDraw on the same graph the CDF of a Bernoulli \\(0.5\\) and the empirical CDF of the observed data \\(X\\).\nApply the Kolmogorov-Smirnov Test at level \\(0.1\\). To do so, use this table.\nComment on the result."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html",
    "href": "teaching/hypothesis_testing/TDs/TD2.html",
    "title": "TD2: Gaussian Populations",
    "section": "",
    "text": "A bread manufacturing factory wants to establish control procedures with the primary goal of reducing overproduction issues, which result in losses for the factory. Here, we focus on the weights of baguettes produced by the factory, with a target weight of \\(250\\) grams. For a sample of \\(n = 30\\) baguettes, the empirical mean is \\(\\bar{X}_n = 256.3\\) and the empirical variance is \\(S^2_n = 82.1\\). A priori, the factory reaches the target weight of \\(250\\) grams. We aim to test at the level of significance \\(\\alpha = 0.01\\) whether there is an overproduction issue.\n\nIs this a one-sided (unilatéral) or two-sided (bilatéral) testing problem?\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWhat is the test statistic to use, and what is its distribution under \\(H_0\\)?\nDetermine the rejection region.\nDoes the factory have an overproduction issue?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-1",
    "title": "TD2: Gaussian Populations",
    "section": "",
    "text": "A bread manufacturing factory wants to establish control procedures with the primary goal of reducing overproduction issues, which result in losses for the factory. Here, we focus on the weights of baguettes produced by the factory, with a target weight of \\(250\\) grams. For a sample of \\(n = 30\\) baguettes, the empirical mean is \\(\\bar{X}_n = 256.3\\) and the empirical variance is \\(S^2_n = 82.1\\). A priori, the factory reaches the target weight of \\(250\\) grams. We aim to test at the level of significance \\(\\alpha = 0.01\\) whether there is an overproduction issue.\n\nIs this a one-sided (unilatéral) or two-sided (bilatéral) testing problem?\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWhat is the test statistic to use, and what is its distribution under \\(H_0\\)?\nDetermine the rejection region.\nDoes the factory have an overproduction issue?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-2",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe want to test the precision of a method for measuring blood alcohol concentration on a blood sample. Precision is defined as twice the standard deviation of the method (assumed to follow a Gaussian distribution). The reference sample is divided into \\(6\\) test tubes, which are subjected to laboratory analysis. The following blood alcohol concentrations were obtained in g/L: \\[\n1.35, \\; 1.26, \\; 1.48, \\; 1.32, \\; 1.50, \\; 1.44.\n\\]\nWe aim to test the hypothesis that the precision is less than or equal to \\(0.1 \\, \\text{g/L}\\).\n\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWrite the test statistic and give its distribution under \\(H_0\\).\nPerform the test at a significance level of \\(\\alpha = 0.05\\).\nShow that the p-value of this test lies between \\(0.001\\) and \\(0.01\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-3",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 3",
    "text": "Exercise 3\nA candidate for the European elections wants to know if their popularity differs between men and women. A survey was conducted with \\(250\\) men, of whom \\(42\\%\\) expressed support for the candidate, and \\(250\\) women, of whom \\(51\\%\\) expressed support.\n\nFormulate the hypothesis testing problem.\nAt a significance level of \\(\\alpha = 0.05\\), can we say that these values indicate a statistically significant difference in popularity?\nGive an approximation of the p-value in terms of \\(F\\) ,the CDF of \\(\\mathcal N(0,1)\\) and read it on the graph below."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-4",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 4",
    "text": "Exercise 4\nWe aim to compare the average daily durations (in hours) of home-to-work commutes in two departments, labeled \\(A\\) and \\(B\\). We randomly surveyed 26 people in \\(A\\) and 22 in \\(B\\). Let \\(X_i\\) of pepople \\(i\\) be the random variable representing the commute duration in department \\(A\\), and \\(Y_j\\) that in department \\(B\\) of people \\(j\\). We assume the samples obtained are i.i.d following a Gaussian distribution: \\[\nX_i \\sim \\mathcal{N}(\\mu_A, \\sigma_A) \\quad \\text{and} \\quad Y_j \\sim \\mathcal{N}(\\mu_B, \\sigma_B).\n\\]\nHere is a summary of the data:\n\n\n\nDepartment A\nDepartment B\n\n\n\n\n\\(n_A= 26\\)\n\\(n_B=22\\)\n\n\n\\(\\sum x_i = 535\\)\n\\(\\sum y_j = 395\\)\n\n\n\\(\\sum x_i^2 = 11400\\)\n\\(\\sum y_j^2 = 7900\\)\n\n\n\n\nFormulate the hypothesis testing problem\nTest the equality of variances at a significance level of \\(\\alpha = 0.1\\)\nTest the equality of mean commute times between the two departments at a significance level of \\(\\alpha = 0.05\\), and conclude\nGive a Gaussian approximation of the test statistic using the CLT and the LLN, and approximate the p-value using the graph of the cdf of \\(\\mathcal N(0,1)\\) given in the previous exercise"
  },
  {
    "objectID": "teaching/hypothesis_testing/glossary.html",
    "href": "teaching/hypothesis_testing/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "General\n\n\n\nEnglish\n\nincreasing function\nnondecreasing function\ninterval \\([a,b)\\)\n\n\n\n\nFrench\n\nfonction strictement croissante\nfonction croissante\nintervalle \\([a,b[\\)\n\n\n\n\n\n\nProbability/Statistics\n\n\n\nEnglish\n\nprobability distribution\nexpectation\nlikelihood\ndistribution tail\nCDF (Cumul. Distrib. Func.)\nPDF (Proba. Density Func.)\nCLT (Central Limit Theorem)\nLLN (Law of Large Numbers)\n\n\n\n\nFrench\n\nloi de probabilité\nespérance\nvraissemblance\nqueue de distribution\nFonction de Répartition\nDensité d’une distribution\nTLC (Th. de la limite centrale)\nLoi des grands nombres\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nEnglish\n\nA sample\none-sided test\ntwo-sided test\nchi-squared goodness of fit test\nchi-squared homogeneity test\n\n\n\n\nFrench\n\nUn échantillon\ntest unilatéral\ntest bilatéral\ntest d’adéquation du Khi-deux\ntest d’homogénéité du Khi-deux\n\n\n\n\n\n\nProgramming Languages (JupytR)\n\n\n\n\nJulia\nusing Distributions\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\ncdf(Binomial(n, p), x)\nquantile(Binomial(n,p), q)\ncdf(Normal(0,1), x)\nquantile(Normal(0,1), q)\n\ncdf(Chisq(n), x) # Chi-squared\ncdf(TDist(n), x) # Student\n\nn1,n2 = 5,10\ncdf(FDist(n1, n2), x) # Fisher\n\nlmbda = 2\ncdf(Gamma(n, lmbda), x) # Gamma\ncdf(Poisson(lmbda), x) # Poisson\n\n\n\nPython\nfrom scipy.stats import *\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\nbinom.cdf(x, n, p)\nbinom.ppf(q, n, p) # Quantile\nnorm.cdf(x)\nnorm.ppf(q)\n\nchi2.cdf(x, n)\nt.cdf(x, n) # Student\n\nn1,n2 = 5,10\nf.cdf(x, n1, n2) # Fisher\n\nlmbda = 2\ngamma.cdf(x,n,lmbda) # Gamma\npoisson.cdf(x, lmbda) # Poisson\n\n\n\nR\n\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\npbinom(x, n, p)\nqbinom(q, n, p)\npnorm(x)\nqnorm(q)\n\npchisq(x, n)\npt(x, n)\n\nn1,n2 = 5,10\npf(x, n1,n2)\n\nlmbda = 2\npgamma(x,n,lmbda) \nppois(x, lmbda)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Correlation Test",
    "text": "Correlation Test\n\n\n\n\n\n\nPearson’s Correlation Test\n\n\n\n\\(r =  \\frac{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline X)^2\\sum_{i=1}^n (Y_i - \\overline Y)^2}}\\)\n\\(\\psi(X,Y) = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\)\nUnder \\(H_0\\), \\(\\psi(X,Y) \\approx \\mathcal T(n-2)\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Correlation Test",
    "text": "Correlation Test\nMonte Carlo Simulation with \\(n=4\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nFor each group \\(k\\) (eg a region), we observe \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) (eg salaries).\nWe assume that the \\(X_{ik}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\).\nEx: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\) [Wooclap]\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\n\nNote\n\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\) [Wooclap]\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\\)\n\\(SS_k \\sim \\sigma^2\\chi^2(N_k-1)\\) under \\(H_0\\)\nEmpirical var of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\n\n\n\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\)\n\\(SSB \\sim \\sigma^2\\chi^2(d-1)\\) under \\(H_0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#interpretation-of-variances-in-anova",
    "href": "teaching/hypothesis_testing/slides/dependency.html#interpretation-of-variances-in-anova",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Interpretation of variances in ANOVA",
    "text": "Interpretation of variances in ANOVA\n\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#chi2-homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#chi2-homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Homogeneity Test",
    "text": "\\(\\chi^2\\) Homogeneity Test\n\n\\(d\\) different bags (or groups), each containing balls of \\(m\\) potential colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix of counts:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Homogeneity Test",
    "text": "Homogeneity Test\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown [Wooclap]\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all color \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/slides/dependency.html#example-soft-drink-preferences",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\) (proportion of Coke lovers)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#computation-of-chi2-stat",
    "href": "teaching/hypothesis_testing/slides/dependency.html#computation-of-chi2-stat",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Computation of Chi2 stat",
    "text": "Computation of Chi2 stat\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#chi2-independence-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#chi2-independence-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Independence Test",
    "text": "\\(\\chi^2\\) Independence Test\n\n\n\n\nObservations: Paired Categorical Variables \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\ne.g. \\(X_i \\in \\{\\mathrm{male}, \\mathrm{female}\\}\\), \\(Y_i \\in \\{\\mathrm{coffee}, \\mathrm{tea}\\}\\) [Wooclap]\nIdea: regroup the data into bags, e.g. \\(\\{\\mathrm{male}, \\mathrm{female}\\}\\)\nBuild the contingency table\nPerform a chi-square homogeneity test\n\n\n\n\n\n\n\nExample of contingency table:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n30\n20\n50\n\n\nTea\n28\n22\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\nExpected counts:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n29\n21\n50\n\n\nTea\n29\n21\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\n\n\n\\(N_1 \\hat p_1 = 58 \\cdot 50/100  = 29\\), Degree of freedom \\(= 1\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\nSymetrization\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t) &=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy = \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]\n\\(X\\) indep of \\(Y\\) \\(\\implies\\) \\(X-Y\\) symmetric \\(\\implies\\) \\(\\mathrm{median}(X-Y) = 0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/slides/dependency.html#dependency-problem-for-paired-data",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The median of \\((X_i - Y_i)\\) is \\(0\\) for all \\(i\\)\n\\(H_1:\\) The median of \\((X_i - Y_i)\\) is not \\(0\\) for some \\(i\\) [Wooclap]\n\n\n\n\n\n\n\n\nGenerality of \\(H_0\\)\n\n\n\nIf \\(X_i-Y_i\\) is symmetric \\(i\\), then we are under \\(H_0\\).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#section",
    "href": "teaching/hypothesis_testing/slides/dependency.html#section",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "Warning\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\))."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where [Wooclap] \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#properties-on-the-signed-ranks",
    "href": "teaching/hypothesis_testing/slides/dependency.html#properties-on-the-signed-ranks",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Properties on the Signed Ranks",
    "text": "Properties on the Signed Ranks\nUnder \\(H_0\\),\n\nThe signs \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\nProperties on the Signed Ranks\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &gt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\nif \\(H_1\\): \\(\\mathrm{median}(D_i) &gt; 0\\): left-tailed test on \\(W_-\\).\n\n\n\n\n\n\nTo generate a \\(W_-\\) under \\(H_0\\):\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#section-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#section-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "This approximation fits well the exact distribution. Monte-Carlo simulation:"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#effect-of-drug-on-blood-pressure",
    "href": "teaching/hypothesis_testing/slides/dependency.html#effect-of-drug-on-blood-pressure",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Effect of Drug on Blood Pressure",
    "text": "Effect of Drug on Blood Pressure\n\n\\(H_0\\): the drug has no effect. \\(H_1\\): it lowers the blood pressure\n\n\n\n\n\n\nPatient\n\\(X_i\\) (Before)\n\\(Y_i\\)​ (After)\n\\(D_i = X_i-Y_i\\)​\n\\(R_i\\)\n\n\n\n\n1\n150\n140\n10\n6 (+)\n\n\n2\n135\n130\n5\n5 (+)\n\n\n3\n160\n162\n-2\n2 (-)\n\n\n4\n145\n146\n-1\n1 (-)\n\n\n5\n154\n150\n4\n4 (+)\n\n\n6\n171\n160\n11\n7 (+)\n\n\n7\n141\n138\n3\n3 (+)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#numerical-appli",
    "href": "teaching/hypothesis_testing/slides/dependency.html#numerical-appli",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Numerical Appli",
    "text": "Numerical Appli\n\n\\(W_- = 1+2 = 3\\).\nFrom a simulation, we approx \\(\\mathbb P(W_-=i)\\), for \\(i \\in \\{0, 1, 2, 3, 4, 5,6\\}\\) under \\(H_0\\) by\n[0.00784066, 0.00781442, 0.00781534, 0.01563892, 0.01562184, 0.02343478]\nFrom a simulation, \\(p_{value}=\\mathbb P(W_- \\leq 3) \\approx 0.039 &lt; 0.05\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#organization",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#organization",
    "title": "Hypothesis Testing",
    "section": "Organization",
    "text": "Organization\n\n15h of lectures, 18h of TD\n\\(12^{th}\\) may: Exam (2h)\nLecture notes and slides on the website\nAbout english and programming languages\nWooclap sessions [Test]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#objective",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#objective",
    "title": "Hypothesis Testing",
    "section": "Objective",
    "text": "Objective\n\nGiven a general decision problem\n\nIntroduce precise notations to describe the pb\nformulate mathematically hypotheses \\(H_0\\) (a priori) and \\(H_1\\) (alternative)\n\nChoose a statistic adapted to the problem\nCompute this statistic and its pvalue (or an approx.)\nConclude and make a decision"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "General Principles",
    "text": "General Principles\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect Data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "Good and Bad Decisions",
    "text": "Good and Bad Decisions\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "title": "Hypothesis Testing",
    "section": "Dice Biased Toward \\(6\\)",
    "text": "Dice Biased Toward \\(6\\)\n\nObjective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-dice",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-dice",
    "title": "Hypothesis Testing",
    "section": "Fairness of Dice",
    "text": "Fairness of Dice\n\nWe observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\)\n\n\n\n\n\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_0\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_1\\) is “unlikely”)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "title": "Hypothesis Testing",
    "section": "Medical Test",
    "text": "Medical Test\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\n\n\n\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "title": "Hypothesis Testing",
    "section": "Recall of Proba",
    "text": "Recall of Proba\nConsider a probability measure \\(P\\) on \\(\\mathbb R\\).\n\nCDF (Cumulative Distribution Function): \\[x \\to P(~(-\\infty,x]~) = \\mathbb P(X \\leq x) ~~~~\\text{(if $X \\sim P$ under $\\mathbb P$)}\\]\n\n\n\nContinous Measures\n\ndensity wrp to Lebesgue: \\(\\mathbb P(X\\in[x,x+dx])=dP(x) = p(x)dx\\)\nPDF (Proba Density Function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha}) = \\alpha\\)\n\n\n\nDiscrete Measures\n\ndensity wrp to counting measure: \\(\\mathbb P(X=x) = P(\\{x\\})=p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nExponential \\(\\mathcal E(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-gammapoisson",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-gammapoisson",
    "title": "Hypothesis Testing",
    "section": "Examples Gamma/Poisson",
    "text": "Examples Gamma/Poisson\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\]\n\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\lambda\\)\n\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\n\nNumb. of tics before time \\(1\\) of clock \\(\\lambda\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_0\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]\nQuestion Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\nTest"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple\n\n\nTest Model\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_0\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\nMultiple VS Multiple"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nparametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nnon-parametric: Otherwise\n\n\nExample of Multiple VS Multiple Parametric Problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)\n\n\n\nSimple VS Multiple Non-Parametric Problems"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\n\nDecision Rule\n\n\n\nA Decision Rule or Test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\\(T(x) = 0\\) (or \\(1\\)) for all \\(x\\) is the trivial decision rule. Question: Decision Rule\n\n\n\n\n\n\n\n\n\n\n\n\nTest Statistic\n\n\n\na Test Statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter. Question: Test Statistic"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nExample of \\(\\mathcal R\\), for a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) \\not \\in [t_1, t_2]\\}:~~~~~~\\mathcal R = (-\\infty,t_1)\\cup (t_2, +\\infty)\\;\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "title": "Hypothesis Testing",
    "section": "Simple VS Simple Problem",
    "text": "Simple VS Simple Problem\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\nFor a given test \\(T\\) we define:\n\nlevel of \\(T\\): \\(\\alpha = P(T(X)=1)\\) (also: type-1 error)\npower of \\(T\\): \\(\\beta = Q(T(X)=1) = 1-Q(T(X)=0)\\) (1-\\(\\beta\\) is the type-2 error)\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta \\geq \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) ! But \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\n\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\) ?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\n\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta &gt; \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) !\nBut \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson’s Theorem",
    "text": "Neyman Pearson’s Theorem\n\n\n\n\n\n\n\n\n\n\nNeyman Pearson’s Theorem\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\n\n\n\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nWhere, if \\(X\\sim P\\), \\[ P(T^*(X)=1)=\\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\nEquivalent to Log-Likelihood Ratio Test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "title": "Hypothesis Testing",
    "section": "Example with Gaussians",
    "text": "Example with Gaussians\n\n\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\n\n\n\nLog-Likelihood Ratio Test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\nDistrib of \\(\\overline X\\) ?\n\\(t_{\\alpha}\\) ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "title": "Hypothesis Testing",
    "section": "Generalization: Exponential Families",
    "text": "Generalization: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood Ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\nLikelihood Ratio Test: (Q: Select Exp. Families) \\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "title": "Hypothesis Testing",
    "section": "Example: Radioactive Source",
    "text": "Example: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) &gt; 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) &gt; 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): Reject if \\(N \\geq 19\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#definitions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#definitions",
    "title": "Hypothesis Testing",
    "section": "Definitions",
    "text": "Definitions\n\n\n\n\\(H_0 = \\mathcal P_0=\\{P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\n\n\n\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(right-tailed: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(left-tailed: unilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(simple/multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( multiple/multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-tailed test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (right-tailed test)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\nHere, \\(\\Theta_0\\) is not necessarily a singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\n\nPivotal Test Statistic\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\n\n\n\n\n\n\nP-value: definition\n\n\nWe define \\(p_{value}(x_{\\mathrm{obs}}) =\\mathbb P(\\psi(X) \\geq x_{\\mathrm{obs}})\\) for a right-tailed test.\nFor a two-tailed test, \\(p_{value}(x_{\\mathrm{obs}}) =2\\min(\\mathbb P(\\psi(X) \\geq x_{\\mathrm{obs}}),\\mathbb P(\\psi(X) \\leq x_{\\mathrm{obs}}))\\)\n\n\n\n\n\n\n\n\n\n\n\nP-value under \\(H_0\\)\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(p_{value}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\n\n\n\n\n\nIn practice: reject if \\(p_{value}(x_{\\mathrm{obs}}) \\leq \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html",
    "href": "teaching/hypothesis_testing/annals/test_2025.html",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "",
    "text": "Duration: 2 hours, no document allowed. Special attention will be given to clarity of writing."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-1-testing-a-preference-for-renewable-or-non-renewable-energy-sources",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-1-testing-a-preference-for-renewable-or-non-renewable-energy-sources",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 1: Testing a Preference for Renewable or Non-Renewable Energy Sources",
    "text": "Exercise 1: Testing a Preference for Renewable or Non-Renewable Energy Sources\nWe aim to determine whether citizens in a region have any preference for renewable energy sources (e.g., solar, wind) or non-renewable energy sources (e.g., coal, natural gas). We assume that, a priori, there is no preference on average. We survey \\(n\\) individuals, and let \\(X\\) be the number of respondents who prefer renewable energy.\n\nQuestions:\n\nFormalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\). Indicate whether this test is one-tailed or two-tailed.\nWe survey \\(n = 100\\) individuals, and \\(X = 58\\) prefer renewable energy sources. Write the \\(p_{value}\\) in function of \\(F\\), the cdf of \\(\\text{Bin}(100, 0.5)\\) (binomial distribution with parameter \\(p = 0.5\\)).\nWrite a line of code that would compute the exact p-value in Julia, Python, or R.\nGive an approximation of the p-value using a Gaussian approximation and the graph of the cdf of \\(\\mathcal N(0,1)\\) given bellow. What do you conclude?\nRedefine the alternative hypothesis \\(H_1\\) and compute an approximated p-value if we aim to determine whether citizens have a preference for:\n\nrenewable energy sources.\nCitizens prefer non-renewable energy sources.\n\nWhat do you conclude for these two other problems?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-2-environmental-monitoring-of-river-pollution",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-2-environmental-monitoring-of-river-pollution",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 2: Environmental Monitoring of River Pollution",
    "text": "Exercise 2: Environmental Monitoring of River Pollution\nAn environmental agency is monitoring the pollution levels of a river to determine whether a nearby factory is causing an increase in harmful chemical concentration. The target concentration for a specific chemical is \\(15 \\, \\text{ppm}\\) (parts per million), which is considered safe for aquatic life. For a sample of \\(n = 20\\) water samples taken downstream from the factory, the empirical mean concentration is \\(\\bar{X}_n = 16.3 \\, \\text{ppm}\\), and the empirical variance is \\(S^2_n = 2.4 \\, \\text{ppm}^2\\).\nA priori, the river is assumed to meet the safe pollution threshold of \\(15 \\, \\text{ppm}\\).\nWe aim to test at the level of significance \\(\\alpha = 0.05\\) whether the chemical concentration downstream exceeds the safe threshold, indicating pollution from the factory.\n\nQuestions:\n\nUsing a Gaussian assumption, formalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\). Is this a one-tailed or two-tailed testing problem? Precise what the unknown parameters are.\nDefine the test statistic. What is its distribution under \\(H_0\\)?\nDetermine the rejection region. You can use a Gaussian approximation and the cdf of Exercise 1.\nWrite a line of code do compute the exact rejection threshold.\nDoes the river exhibit an increased chemical concentration that could indicate pollution from the factory?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-3-bird-migration-habitat-distribution-analysis",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-3-bird-migration-habitat-distribution-analysis",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 3: Bird Migration Habitat Distribution Analysis",
    "text": "Exercise 3: Bird Migration Habitat Distribution Analysis\nA wildlife researcher is studying the behavior of a certain species of birds that migrate to a nature reserve. The researcher has a hypothesis about how the birds distribute themselves across different types of habitats in the reserve. The expected distribution, based on historical data, is as follows:\n\nGrassland: 40%\nWetlands: 30%\nForests: 20%\nRocky Areas: 10%\n\nTo test this hypothesis, the researcher surveys 200 birds and records their habitat preferences. The observed counts are as follows:\n\n\n\nHabitat\nGrassland\nWetlands\nForests\nRocky Areas\n\n\n\n\nObserved\n90\n60\n30\n20\n\n\n\n\nQuestions:\n\nFormalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\).\nCompute the expected counts.\nCompute the chi-square statistic.\nDetermine the degree of freedom \\(df\\) of the chi-square statistic, and read the p-value on the following graph of the cdf.\nWhat do you conclude?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nEmployee Productivity Across Departments\nA company wants to evaluate whether a new management style has had a consistent effect on employee productivity across five departments. Each department has adopted a specific variation of the management style for three months, and the company has recorded the average number of tasks completed per employee during that period.\nData:\n\n\n\nDepartment\n1\n2\n3\n4\n5\n\n\n\n\nNumber of employees\n12\n10\n8\n9\n11\n\n\nAverage tasks completed\n72.4\n68.9\n75.6\n74.3\n69.7\n\n\nVariance of tasks\n8.5\n9.2\n10.1\n7.8\n9.6\n\n\n\nThe company seeks to understand whether productivity levels vary significantly across departments, indicating that the management styles might have different impacts.\nLet \\(d=5\\) be the number of departments and \\(N_{\\text{tot}} = 50\\) the total number of employees. For any department \\(j\\), we denote \\(N_k\\) the number of employees in department \\(k\\), and \\(P_{ik}\\) the number of tasks completed by employee \\(i\\) in department \\(k\\). We assume that the \\(P_{ik}\\)’s are independent and normally distributed with mean \\(\\mu_k\\) and variance \\(\\sigma^2\\).\nWe write \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDefine the hypotheses of the problem to test whether the management styles had a uniform impact on productivity.\nGive a brief interpretation of each one of the quantities \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\n\nProve the analysis of variance formula \\(V_T = V_W + V_B\\)\nCalculate \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), and \\(V_T\\).\n\nExpress the ANOVA test statistic in terms of \\(V_W\\) and \\(V_B\\).\n\nWhat are the distributions of \\(N_k V_k\\) and of \\(N_{\\mathrm{tot}}V_W\\) under \\(H_0\\)? Do they change under \\(H_1\\)?\nRecall the definition of ANOVA test statistic, and perform the ANOVA test at significance level \\(\\alpha =0.05\\). We give the \\(0.05\\) and \\(0.95\\)-quantiles of \\(\\mathcal D\\) which are approximately \\(0.18\\) and \\(2.58\\).\nConclude whether productivity differs significantly between departments."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#course-questions",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#course-questions",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Course Questions",
    "text": "Course Questions\n\nRecall the definition of a test statistic \\(\\psi\\) and a test (or decision rule) \\(T\\).\nWhat are the two types of errors that we can commit?\nFor a given test statistic \\(\\psi\\), recall the definition of the p-value in the context of a two-sided test.\nState the Neyman-Pearson theorem."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables",
    "text": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables\nNous cherchons à déterminer si les citoyens d’une région ont une quelconque préférence pour les sources d’énergie renouvelables (par exemple, solaire, éolienne) ou les sources d’énergie non renouvelables (par exemple, charbon, gaz naturel). Nous supposons que, a priori, il n’y a aucune préférence en moyenne. Nous interrogeons \\(n\\) individus, et nous notons \\(X\\) le nombre de répondants qui préfèrent les énergies renouvelables."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#questions-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#questions-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions :",
    "text": "Questions :\n\nFormalisez le problème de test d’hypothèse, et définissez \\(H_0\\) et \\(H_1\\)​. Indiquez si ce test est unilatéral ou bilatéral.\nNous interrogeons \\(n=100\\) individus, et \\(X=58\\) préfèrent les sources d’énergie renouvelables. Écrivez la p-valeur en fonction de \\(F\\), la fonction de répartition d’une distribution Binomiale Bin\\((100,0.5)\\).\nÉcrivez une ligne de code qui calculerait la p-valeur exacte en Julia, Python, ou R.\nDonnez une approximation de la p-valeur en utilisant une approximation gaussienne et le graphique de la fonction de répartition de \\(\\mathcal N(0,1)\\) fourni dans le sujet. Quelle est votre conclusion ?\nRedéfinissez l’hypothèse alternative \\(H_1\\)​ et calculez une p-valeur approximative si nous cherchons à déterminer si les citoyens ont une préférence pour :\n\nles sources d’énergie renouvelables.\nles sources d’énergie non renouvelables. Quelles sont vos conclusions pour ces deux autres problèmes ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 2 : Surveillance environnementale de la pollution fluviale",
    "text": "Exercice 2 : Surveillance environnementale de la pollution fluviale\nUne agence environnementale surveille les niveaux de pollution d’une rivière pour déterminer si une usine à proximité provoque une augmentation de la concentration de produits chimiques nocifs. La concentration cible pour un produit chimique spécifique est de \\(15 \\, \\text{ppm}\\) (parties par million), ce qui est considéré comme sûr pour la vie aquatique. Pour un échantillon de \\(n = 20\\) prélèvements d’eau effectués en aval de l’usine, la concentration moyenne empirique est \\(\\bar{X}_n = 16,3 \\, \\text{ppm}\\), et la variance empirique est \\(S^2_n = 2,4 \\, \\text{ppm}^2\\).\nA priori, on suppose que la rivière respecte le seuil de pollution sans danger de \\(15 \\, \\text{ppm}\\).\nNous visons à tester, avec un niveau de signification \\(\\alpha = 0,05\\), si la concentration chimique en aval dépasse le seuil de sécurité, indiquant une pollution provenant de l’usine.\n\nQuestions :\n\nEn utilisant une hypothèse Gaussienne, formalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\). S’agit-il d’un test unilatéral ou bilatéral ?\nDéfinissez la statistique de test. Quelle est sa distribution sous \\(H_0\\) ?\nDéterminez la zone de rejet. Vous pouvez utiliser une approximation Gaussienne et le graphe de l’exercice 1.\nEcrivez une ligne de code permettant de calculer le seuil de rejet exact.\nLa rivière présente-t-elle une concentration chimique accrue qui pourrait indiquer une pollution provenant de l’usine ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux",
    "text": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux\nUn chercheur en faune sauvage étudie le comportement d’une certaine espèce d’oiseaux qui migrent vers une réserve naturelle. Le chercheur a une hypothèse sur la façon dont les oiseaux se répartissent entre différents types d’habitats dans la réserve. La distribution attendue, basée sur des données historiques, est la suivante :\n\nPrairie : 40%\nZones humides : 30%\nForêts : 20%\nZones rocheuses : 10%\n\nPour tester cette hypothèse, le chercheur observe 200 oiseaux et enregistre leurs préférences d’habitat. Les comptages observés sont les suivants :\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé\n90\n60\n30\n20\n\n\n\n\nQuestions :\n\nFormalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\).\nCalculez les effectifs attendus.\nCalculez la statistique du chi-deux.\nDéterminez le degré de liberté \\(df\\) de la statistique du chi-deux, et lisez la p-value sur le graphique suivant de la fonction de répartition.\nQuelle est votre conclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 4",
    "text": "Exercice 4\n\nProductivité des employés entre départements\nUne entreprise souhaite évaluer si un nouveau style de management a eu un effet uniforme sur la productivité des employés dans cinq départements. Chaque département a adopté une variation spécifique du style de management pendant trois mois, et l’entreprise a enregistré le nombre moyen de tâches accomplies par employé durant cette période.\nDonnées :\n\n\n\nDépartement\n1\n2\n3\n4\n5\n\n\n\n\nNombre d’employés\n12\n10\n8\n9\n11\n\n\nMoyenne des tâches accomplies\n72,4\n68,9\n75,6\n74,3\n69,7\n\n\nVariance des tâches\n8,5\n9,2\n10,1\n7,8\n9,6\n\n\n\nL’entreprise cherche à comprendre si les niveaux de productivité varient significativement entre les départements, indiquant que les styles de management pourraient avoir des impacts différents.\nSoit \\(d=5\\) le nombre de départements et \\(N_{\\text{tot}} = 50\\) le nombre total d’employés. Pour tout département \\(j\\), nous notons \\(N_k\\) le nombre d’employés dans le département \\(k\\), et \\(P_{ik}\\) le nombre de tâches accomplies par l’employé \\(i\\) dans le département \\(k\\). Nous supposons que les \\(P_{ik}\\) sont indépendants et suivent une distribution normale de moyenne \\(\\mu_k\\) et de variance \\(\\sigma^2\\).\nNous écrivons \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDéfinissez les hypothèses du problème pour tester si les styles de management ont eu un impact uniforme sur la productivité.\nDonnez une brève interprétation de chacune des quantités \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\nDémontrez la formule d’analyse de la variance : \\(V_T = V_W + V_B\\)\nCalculez \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), et \\(V_T\\).\nExprimez la statistique de test ANOVA en termes de \\(V_W\\) et \\(V_B\\).\nQuelles sont les distributions de \\(N_k V_k\\) et de \\(N_{\\mathrm{tot}}V_W\\) sous \\(H_0\\) ? Changent-elles sous \\(H_1\\) ?\nRappelez la définition de la statistique de test ANOVA, donnez sa distribution \\(\\mathcal D\\) sous \\(H_0\\) et effectuez le test ANOVA au niveau \\(\\alpha =0,05\\). On donne les quantiles \\(0.05\\) et \\(0.95\\) de \\(\\mathcal D\\): \\(0.18\\) and \\(2.58\\). Concluez si la productivité diffère significativement entre les départements."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#questions-de-cours",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#questions-de-cours",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions de cours",
    "text": "Questions de cours\n\nRappelez la définition d’une statistique de test \\(\\psi\\) et d’un test (ou règle de décision) \\(T\\).\nQuels sont les deux types d’erreur que nous pouvons commettre ?\nPour une statistique de test \\(\\psi\\) donnée et un problème de test bilatéral, rappelez la définition de la p-valeur.\nÉnoncez le théorème de Neyman-Pearson."
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html",
    "title": "Goodness of Fit Tests",
    "section": "",
    "text": "Binomial distribution\n\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting.",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#multinomials",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#multinomials",
    "title": "Goodness of Fit Tests",
    "section": "",
    "text": "Binomial distribution\n\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting.",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit Test",
    "text": "\\chi^2 Goodness of Fit Test\n\nWe observe (X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q). This corresponds to n counts: X_1 + \\dots + X_m = n\nq = (q_1, \\dots, q_m) corresponds to probabilities of getting color 1, \\dots, m\nLet p = (p_1, \\dots, p_m) be a known vector s.t. p_1 + \\dots + p_m = 1.\nH_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\n\n\n\n\n\n\n\n\\chi^2 Goodness of Fit Test (Adéquation)\n\n\n\n\nChi-squared test statistic: \\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; , where n_i = np_i = \\mathbb E[X_i] is the expected number of counts for color i.\nWhen np_i = n_i are large, under H_0, we approximate the distribution of \\psi(X) as \\psi(X) \\sim \\chi^2(m-1)\nReject if \\psi(X) &gt; t_{1- \\alpha} (right-tail of \\chi^2(m-1))",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "title": "Goodness of Fit Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\n\n\n\n\n\n\nWe observe a bag of sweets containing n=100 sweets of m=3 different colors: red, green, and yellow.\nManufacturer: p_1= 40\\% red, p_2=35\\% green, and p_3=25\\% yellow.\nH_0: q=p (manufacturer’s claim is correct)\nH_1: q\\neq p (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\nColor\nObserved Counts\nExpected Counts\n\n\n\n\nRed\nX_1=50\nn_1=40\n\n\nGreen\nX_2=30\nn_2=35\n\n\nYellow\nX_3=20\nn_3=25\n\n\n\n\n\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\n\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878 (p_{value} \\approx 0.222)\nConclusion: do not reject H_0",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#histograms",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#histograms",
    "title": "Goodness of Fit Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nHistogram\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n\n\n\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\n\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\n\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\nwhere I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\n\n\n\n\n\n\n\n\n\nNormalization\n\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\nLaw of large numbers, Monte Carlo (informal)\n\n\n\n\nAssume that (X_1, \\dots, X_n) are iid of distrib P, and that a,b, k are fixed\nThe histogram \\mathrm{hist}(a,b,k) converges to the histogram of the density P",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit to a given distribution",
    "text": "\\chi^2 Goodness of Fit to a given distribution\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n, iid with unknown distrib P\nH_0: P = P_0, where P_0 is known\nH_1: P \\neq P_0\nIdea: under H_0, the counts in disjoint intervals I_1, I_2, \\dots, I_k follow a multinomial distribution \\mathrm{Mult}(n, (p_1, \\dots, p_{k})) where p_1 = P_0(I_1), \\dots, p_{k} = P_0(I_k)\n\n\n\n\n\n\n\n\n\n\nReduction to chi-squared test statistic\n\n\n\n\nCount the number of data (c_1, \\dots, c_k) in I_1, \\dots, I_k\nTheoretical counts nP_0(I_1) \\dots, nP_0(I_k)\nChi-squared statistic: \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_j)}\nDecide using an \\alpha-quantile of a \\chi^2(k-1) distribution\n\n\n\n\n\n\n\n\n\nCorrected \\chi^2 Test\n\n\n\n\nIf P_0 is unknown\nIn a class \\mathcal P parameterized by \\ell parameters\nThen estimate the \\ell parameters\nCompute theoretical counts with \\hat P_{\\hat \\theta}\nBut decide with \\chi^2(k - 1 - \\ell)",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Goodness of Fit Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\nH_0: X_i iid \\mathcal P(2)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n0\n1\n2 \n\\geq 3\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\n\n\n\n\n\n\nTo get 9.7, we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\gtrsim \\frac{(16-4)^2}{4} = 36\n(1-cdf(Chisq(3),36)) is very small: Reject\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nIf H_0 is X_i iid \\mathcal P(\\lambda) with unknown \\lambda\nNot \\chi^2(3) but \\chi^2(2)\n(1-cdf(Chisq(2),36)) is even smaller: (Still Reject)",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "title": "Goodness of Fit Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)}\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\n\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with y=x",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "title": "Goodness of Fit Tests",
    "section": "Kolmogorov-Smirnov Test",
    "text": "Kolmogorov-Smirnov Test\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)}\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\nEmpirical CDF: \\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf 1\\{X_i \\leq x\\}\nIdea: Max distance between empirical and true CDF\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\n\\psi(X) = \\sup_{x}|\\hat F(x) - F_0(x)|\nApprox: \\mathbb P_0(\\psi(X) &gt;c/\\sqrt{n}) \\to 2\\sum_{r=1}^{+\\infty}(-1)^{r-1}\\exp(-2c^2r^2) when n \\to +\\infty\nIn practice, use Julia, Python or R for KS Tests",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X = (X_1, \\dots, X_n), iid with distribution \\mathcal N(\\mu, \\sigma). We assume that \\mu is unknown but that \\sigma is known.\n\n\n\n\n\n\nTest problems\n\n\n\n\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\nTest Statistic:  \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\n\\psi(X) \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\nTests\n\n\n\n\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFisher’s Quote\n\n\n\nThe value for which p=0.05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.\n\n\n\n\n\n\nMultiple VS Multiple Test Problem: \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} no longer test statistic.\nIdea: replace \\sigma by its estimator  \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\nThis gives \n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\nIs \\psi(X) pivotal under H_0 ? What is its distribution ?\n\n\n\n\n\n\n\n\n\n\nChi-Squared Distribution \\chi^2(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\sum_{i=1}^k Z_i^2\nwhere the Z_i’s are iid \\mathcal N(0,1).\n\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\n\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1) when k \\to +\\infty\n\n\n\n\n\n\n\n\n\nStudent distribution \\mathcal T(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\tfrac{Z}{\\sqrt{U/k}}\nZ, U are independent and follow resp. \\mathcal N(0,1) and a \\chi^2(k)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume X_i are iid \\mathcal N(\\mu_0, \\sigma).\n\nThe test statistic \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} pivotal (indep. of \\sigma).\nIt follows a Student distribution \\mathcal T(n-1).\n\n\n\nSketch of Proof.\nRemark that, the orthogonal projection of (X_1, \\dots, X_n) on (1, \\dots, 1) is equal to \\overline X \\cdot (1, \\dots, 1). This is precisely because the empirical mean minimizes the quantity \\tfrac{1}{n}\\sum (X_i - \\overline X)^2. Hence, the two vectors \\overline X \\cdot (1, \\dots, 1) and (X_1 - \\overline X, \\dots, X_n - \\overline X) are orthogonal. From the Cochran’s theorem, Orthogonality is equivalent to independence for Gaussian random variables, and we deduce that \\overline X is independent of \\hat \\sigma^2. \n\\tag*{$\\blacksquare$}\n\nThe student tests are the same as Gaussian tests with known variance, except that we replace the quantiles of the Gaussian the quantiles of the Student distribution.\n\nThe quantiles of the Student distribution are close to the quantile of the Standard Gaussian when the degree of freedom k is large:\nThe quantiles are slightly larger when k is small. The Student Distribution has a slightly heavier tail.\n\n\n\n\n\nMultiple VS multiple test problem X=(X_1, \\dots, X_n): \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n(Student) T-test statistic: \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe X=(X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu, \\sigma). \\mu, \\sigma are unknown. \\sigma_0 is fixed.\n\n\n\n\n\n\n\n\n\n\nRight-tailed test\n\n\n\n\nH_0: \\sigma \\leq \\sigma_0, H_1: \\sigma &gt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\nq_{1-\\alpha}: quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\n\n\n\n\n\nLeft-tailed test\n\n\n\n\nH_0: \\sigma \\geq \\sigma_0, H_1: \\sigma &lt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\nq_{\\alpha}: quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X = (X_1, \\dots, X_n), iid with distribution \\mathcal N(\\mu, \\sigma). We assume that \\mu is unknown but that \\sigma is known.\n\n\n\n\n\n\nTest problems\n\n\n\n\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\nTest Statistic:  \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\n\\psi(X) \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\nTests\n\n\n\n\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Fisher’s Quote\n\n\n\nThe value for which p=0.05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Multiple VS Multiple Test Problem: \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} no longer test statistic.\nIdea: replace \\sigma by its estimator  \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\nThis gives \n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\nIs \\psi(X) pivotal under H_0 ? What is its distribution ?",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Chi-Squared Distribution \\chi^2(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\sum_{i=1}^k Z_i^2\nwhere the Z_i’s are iid \\mathcal N(0,1).\n\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\n\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1) when k \\to +\\infty\n\n\n\n\n\n\n\n\n\nStudent distribution \\mathcal T(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\tfrac{Z}{\\sqrt{U/k}}\nZ, U are independent and follow resp. \\mathcal N(0,1) and a \\chi^2(k)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume X_i are iid \\mathcal N(\\mu_0, \\sigma).\n\nThe test statistic \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} pivotal (indep. of \\sigma).\nIt follows a Student distribution \\mathcal T(n-1).\n\n\n\nSketch of Proof.\nRemark that, the orthogonal projection of (X_1, \\dots, X_n) on (1, \\dots, 1) is equal to \\overline X \\cdot (1, \\dots, 1). This is precisely because the empirical mean minimizes the quantity \\tfrac{1}{n}\\sum (X_i - \\overline X)^2. Hence, the two vectors \\overline X \\cdot (1, \\dots, 1) and (X_1 - \\overline X, \\dots, X_n - \\overline X) are orthogonal. From the Cochran’s theorem, Orthogonality is equivalent to independence for Gaussian random variables, and we deduce that \\overline X is independent of \\hat \\sigma^2. \n\\tag*{$\\blacksquare$}\n\nThe student tests are the same as Gaussian tests with known variance, except that we replace the quantiles of the Gaussian the quantiles of the Student distribution.\n\nThe quantiles of the Student distribution are close to the quantile of the Standard Gaussian when the degree of freedom k is large:\nThe quantiles are slightly larger when k is small. The Student Distribution has a slightly heavier tail.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Multiple VS multiple test problem X=(X_1, \\dots, X_n): \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n(Student) T-test statistic: \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variance-unknown-mean",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variance-unknown-mean",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X=(X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu, \\sigma). \\mu, \\sigma are unknown. \\sigma_0 is fixed.\n\n\n\n\n\n\n\n\n\n\nRight-tailed test\n\n\n\n\nH_0: \\sigma \\leq \\sigma_0, H_1: \\sigma &gt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\nq_{1-\\alpha}: quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\n\n\n\n\n\nLeft-tailed test\n\n\n\n\nH_0: \\sigma \\geq \\sigma_0, H_1: \\sigma &lt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\nq_{\\alpha}: quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1^2) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_1, \\sigma_1^2).\n\\sigma_1, \\sigma_2 are known, \\mu_1, \\mu_2 are unknown\nTest Problem: H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\nIdea: Normalize \\overline X - \\overline Y: \n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\nTwo-Tailed Test for Testing Variance: \nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\nt_{1-\\alpha/2} is the (1-\\alpha/2)-quantile of a Gaussian distribution",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown\nVariance Testing Problem: \nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\nF-Test Statistic of the Variances (ANOVA) \n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher Distribution \\mathcal F(k_1,k_2)\n\n\n\n\n(k_1, k_2): degrees of freedom\nDistribution of \\frac{U_1/k_1}{U_2/k_2}\nWhere U_1, U_2 are indep. and follow \\chi^2(k_1), \\chi^2(k_2). wiki\n\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right) when k_1,k_2 \\to +\\infty\nExample: \\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1) if Z_i \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\n\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} is independent of \\mu_1, \\mu_2, \\sigma_1, \\sigma_2. It is pivotal\nIt follow distribution \\mathcal F(n_1-1, n_2-1)\n\n\n\n\n\nTwo-tailed test:  \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown, but we know that \\sigma_1=\\sigma_2\nEquality of Mean Testing Problem: \nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\nFormally, H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}.\n\n\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\n\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\nNormalize \\overline X - \\overline Y:\n\n\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\n\n\\psi(X,Y) is pivotal because \\sigma_1 = \\sigma_2.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown\nEquality of Mean Testing Problem: \nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\nFormally, H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}.\n\n\n\n\n\n\n\nStudent Welch Test Statistic\n\n\n\n\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\n\n\\psi(X,Y) is not pivotal\nGaussian approximation: \\psi(X,Y) \\approx \\mathcal N(0,1) when n_1, n_2 \\to \\infty\nBetter approximation: Student Welch",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\n\nCLT\n\n\n\n\nLet S_n = \\sum_{i=1}^n X_i with (X_1, \\dots, X_n) iid (L^2) then  \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \nEquality when X_i’s are \\mathcal N(\\mu, \\sigma)\nRule of thumb: n \\geq 30\n\n\n\n\n\n\n\n\n\nExample: Binomials\n\n\n\n\nIf p \\in (0,1)\n\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1) when n \\to \\infty\nn should be \\gg \\frac{1}{p}\n\n\n\nGood Approx for (n=100, p=0.2)\n\nBad Approx for (n=100, p=0.01)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe X \\sim Bin(n_1, p_1) and Y \\sim Bin(n_2, p_2).\nn_1, n_2 are known but p_1, p_2 are unknown in (0,1)\nH_0: p_1 = p_2 or H_1: p_1 \\neq p_2\n\n\n\n\n\n\n\nTest Statistic\n\n\n\n \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\n\n\\hat p_1 = X/n_1, \\hat p_2 = X/n_2\n\\hat p = \\frac{X+Y}{n_1+n_2}\nIf np_1, np_2 \\gg 1: \\psi(X) \\sim \\mathcal N(0,1)\nWe reject if |\\psi(X,Y)| \\geq t_{1-\\alpha/2} (gaussian quantile)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion: “should we raise taxes on cigarettes to pay for a healthcare reform ?”\np_1, p_2: proportion of non-smokers or smokers willing to raise taxes\nH_0: p_1=p_2 or H_1: p_1 &gt; p_2\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\hat p_1 \\approx 0.58, \\hat p_2 \\approx 0.21.\n\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\n\\mathbb P(\\psi(X,Y) &gt; 8.99) = 1-cdf(Normal(0,1), 8.99)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/linear_model/TD/TD1.html",
    "href": "teaching/linear_model/TD/TD1.html",
    "title": "TD1",
    "section": "",
    "text": "Let \\(z_1, \\ldots, z_n\\) be observations of a variable \\(Z\\).\n\nDetermine the value \\(\\hat{m}\\) that minimizes the quadratic distance to the data \\(S(m) = \\sum_{i=1}^{n}(z_i - m)^2\\).\nThe quantity \\(\\hat{m}\\) actually corresponds to the ordinary least squares estimation in a linear regression model: \\(Y = X\\beta + \\epsilon\\). Specify what \\(Y\\), \\(X\\), \\(\\beta\\), and \\(\\epsilon\\) are.\nRecover the result from the first question using the general formula for the least squares estimator: \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\)."
  },
  {
    "objectID": "teaching/linear_model/TD/TD1.html#exercise-1.-empirical-mean",
    "href": "teaching/linear_model/TD/TD1.html#exercise-1.-empirical-mean",
    "title": "TD1",
    "section": "",
    "text": "Let \\(z_1, \\ldots, z_n\\) be observations of a variable \\(Z\\).\n\nDetermine the value \\(\\hat{m}\\) that minimizes the quadratic distance to the data \\(S(m) = \\sum_{i=1}^{n}(z_i - m)^2\\).\nThe quantity \\(\\hat{m}\\) actually corresponds to the ordinary least squares estimation in a linear regression model: \\(Y = X\\beta + \\epsilon\\). Specify what \\(Y\\), \\(X\\), \\(\\beta\\), and \\(\\epsilon\\) are.\nRecover the result from the first question using the general formula for the least squares estimator: \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\)."
  },
  {
    "objectID": "teaching/linear_model/TD/TD1.html#exercise-2.-recognizing-a-linear-regression-model",
    "href": "teaching/linear_model/TD/TD1.html#exercise-2.-recognizing-a-linear-regression-model",
    "title": "TD1",
    "section": "Exercise 2. Recognizing a Linear Regression Model",
    "text": "Exercise 2. Recognizing a Linear Regression Model\nAre the following models linear regression models? If not, can we apply a transformation to reduce them to one? For each linear regression model of the type \\(Y = X\\beta + \\epsilon\\), specify what \\(Y\\), \\(X\\), \\(\\beta\\), and \\(\\epsilon\\) are.\n\nWe observe \\((x_i, y_i)\\), \\(i = 1, \\ldots, n\\) theoretically linked by the relation \\(y_i = a + bx_i + \\epsilon_i\\), \\(i = 1, \\ldots, n\\), where the variables \\(\\epsilon_i\\) are centered, with variance \\(\\sigma^2\\) and uncorrelated. We wish to estimate \\(a\\) and \\(b\\).\nWe observe \\((x_i, y_i)\\), \\(i = 1, \\ldots, n\\) theoretically linked by the relation \\(y_i = a_1x_i + a_2x_i^2 + \\epsilon_i\\), \\(i = 1, \\ldots, n\\), where the variables \\(\\epsilon_i\\) are centered, with variance \\(\\sigma^2\\) and uncorrelated. We wish to estimate \\(a_1\\) and \\(a_2\\).\nFor different countries (\\(i = 1, \\ldots, n\\)), we record their production \\(P_i\\), their capital \\(K_i\\), their labor factor \\(T_i\\) which are theoretically linked by the Cobb-Douglas relation \\(P = \\alpha_1K^{\\alpha_2}T^{\\alpha_3}\\). We wish to verify this relation and estimate \\(\\alpha_1\\), \\(\\alpha_2\\), and \\(\\alpha_3\\).\nThe rate of active ingredient \\(y\\) in a medication is assumed to evolve over time \\(t\\) according to the relation \\(y = \\beta_1e^{-\\beta_2t}\\). We have measurements of \\(n\\) rates \\(y_i\\) taken at \\(n\\) time points \\(t_i\\). We wish to verify this relation and estimate \\(\\beta_1\\) and \\(\\beta_2\\).\nSame problem as above but the theoretical model between observations is written as \\(y_i = \\beta_1e^{-\\beta_2t_i} + u_i\\), \\(i = 1, \\ldots, n\\), where the variables \\(u_i\\) are centered, with variance \\(\\sigma^2\\) and uncorrelated."
  },
  {
    "objectID": "teaching/linear_model/TD/TD1.html#exercise-3.-simple-regression",
    "href": "teaching/linear_model/TD/TD1.html#exercise-3.-simple-regression",
    "title": "TD1",
    "section": "Exercise 3. Simple Regression",
    "text": "Exercise 3. Simple Regression\nConsider the simple linear regression model where we observe \\(n\\) realizations \\((x_i, y_i)\\), \\(i = 1, \\ldots, n\\) linked by the relation \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), \\(i = 1, \\ldots, n\\). We assume that the \\(x_i\\) are deterministic and that the variables \\(\\epsilon_i\\) are centered, with variance \\(\\sigma^2\\) and uncorrelated with each other.\n\nWrite the model in matrix form.\nWhat minimization problem is the least squares estimator \\(\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)\\) the solution to?\nWe can find \\(\\hat{\\beta}\\) by setting the gradient of the function to be minimized to zero. This has already been done and the solutions should be known by heart: what are they?\nRecover \\(\\hat{\\beta}\\) using the general formula \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\).\nJustify why the regression line necessarily passes through the point \\((\\bar{x}_n, \\bar{y}_n)\\).\nWe wish to predict the value \\(y_o\\) associated with the value \\(x_o\\) of a new individual, assuming that this individual follows exactly the same model as the previous \\(n\\) individuals. What is the prediction \\(\\hat{y}_o\\) of \\(y_o\\)?\nShow that the expectation of the prediction error \\(y_o - \\hat{y}_o\\) is zero.\nFor a general linear regression model, the variance of the prediction error associated with a new regressor vector \\(x\\), of dimension \\(p\\), is (see lecture notes): \\(\\sigma^2(x^T(X^TX)^{-1}x + 1)\\). Show that here this variance can be rewritten as: \\[\\sigma^2\\left(1 + \\frac{1}{n} + \\frac{(x_o - \\bar{x}_n)^2}{\\sum_{i=1}^{n}(x_i - \\bar{x}_n)^2}\\right)\\]\nDiscuss the quality of the prediction depending on whether \\(x_o\\) is close to or far from the empirical mean \\(\\bar{x}_n\\).\nWhat happens if \\(n\\) is large?"
  },
  {
    "objectID": "teaching/linear_model/TD/TD1.html#exercise-4.-other-small-questions-on-simple-linear-regression",
    "href": "teaching/linear_model/TD/TD1.html#exercise-4.-other-small-questions-on-simple-linear-regression",
    "title": "TD1",
    "section": "Exercise 4. Other Small Questions on Simple Linear Regression",
    "text": "Exercise 4. Other Small Questions on Simple Linear Regression\nConsider the simple linear regression model where we observe \\(n\\) realizations \\((x_i, y_i)\\), \\(i = 1, \\ldots, n\\) linked by the theoretical relation \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), \\(i = 1, \\ldots, n\\).\n\nWhat are the standard assumptions on the modeling errors \\(\\epsilon_i\\)?\nUnder what assumption is the model identifiable, in the sense that \\(\\beta_0\\) and \\(\\beta_1\\) are uniquely defined?\nUnder what assumption does the OLS estimator of \\(\\beta_0\\) and \\(\\beta_1\\) exist?\nDo the variables \\(y_i\\) have the same expectation?\nDoes the regression line estimated from the observations \\((x_i, y_i)\\) always pass through the point \\((\\bar{x}_n, \\bar{y}_n)\\)?\nAre the OLS estimators of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) independent?\nIs it possible to find estimators of the regression coefficients with lower variance than that of the OLS estimators?"
  },
  {
    "objectID": "teaching/linear_model/TD/TD1.html#exercise-5.-convergence-of-estimators",
    "href": "teaching/linear_model/TD/TD1.html#exercise-5.-convergence-of-estimators",
    "title": "TD1",
    "section": "Exercise 5. Convergence of Estimators",
    "text": "Exercise 5. Convergence of Estimators\nWe place ourselves as in the previous exercise in the framework of a simple regression model. We recall that the design matrix \\(X\\) and the matrix \\((X^TX)^{-1}\\) in this case are:\n\\[X = \\begin{pmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}, \\quad (X^TX)^{-1} = \\frac{1}{\\sum_{i=1}^{n}(x_i - \\bar{x}_n)^2} \\begin{pmatrix} \\frac{1}{n}\\sum_{i=1}^{n}x_i^2 & -\\bar{x}_n \\\\ -\\bar{x}_n & 1 \\end{pmatrix}\\]\nWe will examine some examples of designs, i.e., distributions of the values of \\(x_1, \\ldots, x_n\\), and verify the convergence (or not) of the OLS estimators of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in each case.\n\nRecall what the mean squared error of \\(\\hat{\\beta}\\), the OLS estimator of \\(\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\), is.\nIn this first example, we consider the case where observations occur at regular intervals and become increasingly numerous with \\(n\\). After renormalization, we assume that \\(x_i = i\\) for all \\(i = 1, \\ldots, n\\).\n\nGive the limit of the matrix \\(\\mathbb{V}(\\hat{\\beta})\\) when \\(n \\to \\infty\\).\nDeduce the asymptotic behavior in mean square of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\nSame question when observations become increasingly dense in an interval (for simplicity: the interval \\([0, 1]\\)). We assume that \\(x_i = i/n\\) for all \\(i = 1, \\ldots, n\\).\nWe consider here a case where observations are poorly dispersed: we assume that \\(x_i = 1/i\\) for all \\(i = 1, \\ldots, n\\). Thus observations concentrate at 0. What about the asymptotic behavior in mean square of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)?\nIn the previous examples, the \\(x_i\\) were deterministic. We assume here that the \\(x_i\\) are random, i.i.d., square-integrable with non-zero variance. We also assume that the \\(x_i\\) and the modeling errors \\(\\epsilon_i\\) are independent. This situation can be seen as the random equivalent of the deterministic situations treated in questions 2 and 3 (depending on whether the distribution of \\(x_i\\) is discrete or continuous).\n\nExpress \\(\\hat{\\beta} - \\beta\\) as a function of the matrix \\(X\\) and the vector \\(\\epsilon\\).\nDeduce that \\(\\hat{\\beta}\\) converges almost surely to \\(\\beta\\) when \\(n \\to \\infty\\)."
  },
  {
    "objectID": "teaching/linear_model/TD/TD1.html#exercise-6.-candy-consumption",
    "href": "teaching/linear_model/TD/TD1.html#exercise-6.-candy-consumption",
    "title": "TD1",
    "section": "Exercise 6. Candy Consumption",
    "text": "Exercise 6. Candy Consumption\nData published by the Chicago Tribune in 1993 show candy consumption in millions of pounds and population in millions of inhabitants in 17 countries in 1991. We denote \\(y_i\\) the consumption and \\(x_i\\) the population of the \\(i\\)-th country, \\(i = 1, \\ldots, 17\\). We are given the following values:\n\\[\\sum_{i=1}^{17} x_i = 751.8, \\quad \\sum_{i=1}^{17} y_i = 13683.8, \\quad \\sum_{i=1}^{17} x_i^2 = 97913.92\\]\n\\[\\sum_{i=1}^{17} y_i^2 = 36404096.44, \\quad \\sum_{i=1}^{17} x_iy_i = 1798166.66\\]\nWe wish to link candy consumption to the population of each country using a linear regression model (with intercept).\n\n\n\n\\(\\alpha \\backslash q\\)\n14\n15\n16\n17\n18\n\n\n\n\n0.01\n2.62\n2.60\n2.58\n2.57\n2.55\n\n\n0.025\n2.14\n2.13\n2.12\n2.11\n2.10\n\n\n0.05\n1.76\n1.75\n1.75\n1.74\n1.73\n\n\n0.10\n1.35\n1.34\n1.34\n1.33\n1.33\n\n\n\nTable 1: Quantiles of order \\(1 - \\alpha\\) of a Student’s t-distribution with \\(q\\) degrees of freedom, for different values of \\(\\alpha\\) and \\(q\\).\n\nWrite the equation of the proposed model for each country, specifying the assumptions made.\nGive the expressions for the OLS estimators of the slope and intercept of the model, as functions of the sums given above. Deduce their values.\nGive the expression of an unbiased estimator of the variance of the modeling error, as a function of the sums given above. Deduce its value.\nWhat is the theoretical variance of the OLS estimators? How to estimate it? Deduce an estimate of the standard deviation of each estimator (you can use the expression of \\((X^TX)^{-1}\\) recalled in Exercise 5).\nTest whether the regression slope is significantly different from 0, recalling the underlying assumptions. For the numerical application, perform a test at the 5% level using the quantiles given in Table 1.\nGive the expression for the p-value of the previous test. You are not asked to perform the numerical calculation, but at least to give an approximate value.\nSimilarly test whether the intercept is significantly different from 0, both by setting the level at 5% and by roughly evaluating the associated p-value."
  },
  {
    "objectID": "teaching/linear_model/TDh/TD6-8.html",
    "href": "teaching/linear_model/TDh/TD6-8.html",
    "title": "TD2-5",
    "section": "",
    "text": "The dataset “chdage.txt”, available in Moodle, contains data from 100 patients aged 20 to 69 years (variable age), some of whom have coronary heart disease (variable chd taking values Yes or No).\n\nImport this data under R as a data.frame. Observe the content of each variable and transform their class if necessary.\nPropose one or more graphical visualization(s) to analyze the possible link between the variables chd and age.\nIs a link apparent? What simple statistical test would confirm it?\nWe denote \\(Y = \\mathbf{1}_{chd=Yes}\\) and \\(p(x) = P(Y = 1|age = x)\\). Give the distribution of \\(Y\\) given that \\(age = x\\) as a function of \\(p(x)\\).\nBased on this distribution, give the likelihood of observations \\((y_1, \\ldots, y_n)\\) as a function of \\((p(x_1), \\ldots, p(x_n))\\) where \\(x_i\\) denotes the age of individual \\(i\\) and \\(y_i = 1\\) if the latter has coronary heart disease.\nAs a first estimation of \\(p(x)\\), we implement the following approach:\n\n\nUse the 8 age groups proposed via the agegrp variable of the dataset, which forms 8 groups of individuals associated with these classes. Calculate \\(\\bar{x}_1, \\ldots, \\bar{x}_8\\) the center of each age class.\nCalculate the proportions of \\(chd = Yes\\) in each group, denoted \\(\\hat{p}_1, \\ldots, \\hat{p}_8\\) (you can use the table and prop.table functions).\n\n\nIn order to analyze the quality of the previous estimation, transform the chd variable into a numerical variable taking the two values 0 and 1, and represent on the same graph the scatter plot of age and chd (recoded) and the estimated proportions in each class \\((\\bar{x}_k, \\hat{p}_k)\\) for \\(k = 1, \\ldots, 8\\). What virtues and what limitations does this estimation procedure have?\nWe decide to model \\(p(x)\\) using a logistic regression model with parameter \\(\\beta = (\\beta_0, \\beta_1) \\in \\mathbb{R}^2\\). What assumption does this mean on the expression of \\(p(x)\\)? Is this compatible with the previous graph? What other modeling alternative(s) could we suggest?\nWrite the log-likelihood of the logistic model as a function of \\(\\beta\\) and deduce the system that the maximum likelihood estimator \\(\\hat{\\beta}\\) must solve. Can we solve this system analytically?\nCalculate the maximum likelihood estimator using the glm function.\nRecall the theoretical definition of the odds ratio of having coronary heart disease between an individual of age \\(x_1\\) and an individual of age \\(x_2\\). What is its value for the previous model when the 2 individuals are 10 years apart (i.e. \\(x_1 = x_2 + 10\\))?\nWe are now interested in the probability ratio (and not odds ratio) of having coronary heart disease between an individual of age \\(x_1\\) and an individual of age \\(x_2\\). What is this ratio for the previous model when the 2 individuals are 10 years apart (i.e. \\(x_1 = x_2 + 10\\))? We can represent this ratio as a function of \\(x_2\\), for \\(x_2\\) taking values from 20 to 70 years.\n\nWe recall that under “good conditions”, the maximum likelihood estimator \\(\\hat{\\beta}\\) in a logistic regression model involving \\(p\\) explanatory variables and \\(n\\) individuals satisfies the following convergence in distribution, when \\(n \\to \\infty\\), \\[J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0_p, I_p)\\] where \\(J_n(\\beta)\\) is the Fisher information matrix. The latter has the expression \\[J_n(\\beta) = X'W_\\beta X\\] where \\(X\\) is the design matrix and \\(W_\\beta\\) is the diagonal matrix \\[W_\\beta = \\begin{pmatrix} p_\\beta(x_1)(1-p_\\beta(x_1)) & 0 \\\\ & \\ddots \\\\ 0 & p_\\beta(x_n)(1-p_\\beta(x_n)) \\end{pmatrix}\\]\n\nJustify that an asymptotic approximation of the distribution of \\((\\hat{\\beta} - \\beta)\\) is \\(N(0_p, J_n^{-1}(\\beta))\\).\nHow can we exploit this result to estimate the standard deviation of each coordinate of \\(\\hat{\\beta}\\)? Give the concrete approach to apply, but we do not ask to implement it numerically.\nThis estimation procedure is used by the glm function. Using its output, give an estimation of the standard deviation of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nConstruct an asymptotic 95% confidence interval for parameter \\(\\beta_1\\).\nAccording to the previous question, is parameter \\(\\beta_1\\) different from 0 at the asymptotic error threshold of 5%? What is the name of this test procedure? Give the p-value associated with this test and verify that it agrees well with the glm output.\nCalculate the deviance test statistic for significance of the GLM model (compared to the null model). Deduce the p-value and conclude at error thresholds of 10%, 5% and 1%. Compare with the results of the test performed under R using the anova function applied to the model, with the option test=“Chisq”.\nReturn to the graph made in question 7 and superimpose (in the form of a curve) the predicted values \\(\\hat{p}(x)\\) by the logistic model, calculated for a grid of values of \\(x\\) covering the range taken by the observations. You can use the predict function associated with the option type=“response”.\nStarting from the convergence in distribution of \\(\\hat{\\beta}\\) recalled above, deduce a confidence interval at the asymptotic 95% level for \\(p(x)\\). Add this confidence interval for each \\(x\\) considered to the previous graph. You can profitably exploit the option se=TRUE of the predict function in the case type=“link”.\n\n\n\n\nWe have a pair of random variables \\((X, Y)\\) where \\(Y\\) is binary and \\(X\\) takes values in \\(\\mathbb{R}^d\\). We denote \\(p = P(Y = 1)\\), \\(f_0(.)\\) the conditional density of \\(X\\) given that \\(Y = 0\\) and \\(f_1(.)\\) the conditional density of \\(X\\) given that \\(Y = 1\\). We further denote \\[h(x) = \\log \\frac{f_1(x)}{f_0(x)} + \\log \\frac{p}{1-p}, \\quad x \\in \\mathbb{R}^d\\]\n\nShow that for all \\(x\\) \\[P(Y = 1|X = x) = \\frac{1}{1 + e^{-h(x)}}\\]\nWe recall that a distribution on \\(\\mathbb{R}^d\\) belongs to the exponential family if its density can be written \\(a(x)b(\\theta)e^{\\theta'T(x)}\\), for some parameter \\(\\theta \\in \\mathbb{R}^q\\), where \\(a\\) and \\(b\\) are positive functions and \\(T: \\mathbb{R}^d \\to \\mathbb{R}^q\\) is a function called sufficient statistic. Show that if the conditional densities \\(f_0\\) and \\(f_1\\) belong to the same exponential family and differ only by the value of their associated parameter, then \\(P(Y = 1|X = x)\\) follows exactly a logistic regression model, specifying its parameter and variables.\n\n\n\n\nPreamble: Entropy is a quantity found in thermodynamics to measure the state of disorder or randomness of a system. In the same spirit, it is also found in information theory and probability to quantify the disorder or amount of randomness that a probability distribution integrates. A physical system tends to naturally evolve towards a state of maximum entropy. Following this principle, it is natural, to describe a given random experiment, to choose probability distributions that maximize entropy. This is the principle we will apply to seek to best choose \\(P(Y = 1)\\) when \\(Y\\) is binary.\nMathematically, given \\(Y\\) a binary variable and \\(p = P(Y = 1)\\), the entropy of the distribution of \\(Y\\) is \\[-p \\log(p) - (1-p) \\log(1-p)\\]\nThe entropy of a vector of independent binary variables \\(Y_1, \\ldots, Y_n\\) is simply the sum of individual entropies.\n\nWithout any source of constraint, what is the maximum entropy distribution of a binary variable?\nSuppose now that we have a sample of \\(n\\) pairs \\((Y_i, X_i)\\) where \\(X_i\\) is a random variable in \\(\\mathbb{R}^d\\). We denote \\(p_i(x_i) = P(Y_i = 1|X_i = x_i)\\), \\(i = 1, \\ldots, n\\). A priori, without using any information contained in the sample, what are the \\(p_i(x_i)\\) that maximize entropy?\nWe want to find the \\(p_i(x_i)\\) that maximize entropy while being consistent with the observations. This amounts to including constraints on the possible \\(p_i(x_i)\\). We choose the constraints: \\[\\sum_{i=1}^n y_i x_i = \\sum_{i=1}^n p_i(x_i) x_i\\] (Since \\(x_i\\) is a vector of size \\(d\\), this is indeed a system of \\(d\\) constraints).\n\nThese constraints are quite natural: we want the mean of the \\(x_i\\) of individuals in the positive group (\\(y_i = 1\\)) to coincide with the mean of the \\(x_i\\) weighted by the probability that \\(y_i\\) equals 1. In particular (for the constant variable 1), we want the proportion of \\(y_i = 1\\) to coincide with the sum of probabilities.\nFind the \\(p_i(x_i)\\) that maximize entropy while satisfying the previous constraints. You can give the solution up to an unknown (vectorial) constant.\n\nWhat relationship with logistic regression?\n\n\n\n\nWe consider the break data, available on the course Moodle page. This dataset, of size \\(n = 33\\), includes three variables relating to the state of an automobile:\n\nA variable fault that equals 1 if the car concerned experienced a breakdown, 0 otherwise;\nA variable age that gives the age of the car;\nA variable brand that gives the brand of the car.\n\nThe goal of the exercise is to model the fault variable.\n\nImport the data under R and recode the class of each variable if necessary.\nGraphically observe the possible link between fault and age on one hand, and between fault and brand on the other hand.\nWe want to implement a logistic regression model explaining the probability of having a breakdown as a function of the car’s age and its brand. Launch this modeling under R and write the mathematical formula of the obtained model. We will specify in particular the estimated model specific to cars of brand 0, then brand 1, then brand 2.\nAnalyze the overall quality of the model.\nRestart the modeling by including only the age variable in the model. Is it better?\nThe scatter plot between the age variable and the fault variable seems to suggest that breakdowns occur at the beginning of the vehicle’s life (“youth defects” or “break-in”) and at the end of life (wear breakdowns). This behavior of the breakdown probability in the shape of a parabola encourages us to try to include a quadratic term in the age variable. Make this addition to the model by also including the brand variable and analyze the results. Is the model significant?\nWhat is the odds ratio associated with brand “2” compared to brand “0” of the brand variable? Interpret this value and give a 95% confidence interval around this estimation. Is the inclusion of the brand variable in the model relevant?\n\n\n\n\nWe consider the mental data, contained in the mental.txt file available on the course Moodle page. This dataset, of size \\(n = 40\\), is extracted from a study on the mental health of adults living in Alachua County, Florida, USA. It contains three variables:\n\nA variable impair describing the mental state of the person concerned, from 1 (healthy) to 4 (in poor health),\nA variable ses that equals 1 if the person has a high socio-economic status, 0 otherwise,\nA variable life measuring the number and intensity of upheavals that the person has experienced during the last three years, from 0 (no change) to 9 (very important changes).\n\nThe goal of the exercise is to model the impair variable.\n\nImport the data under R. Is the ses variable qualitative or quantitative? Same question for the life variable. Change their class under R if needed.\nPerform a small descriptive study to identify a possible link between the impair variable and the other variables in the dataset.\nWrite mathematically the proportional cumulative logistic regression model without interaction term linking impair to ses and life, and allowing to estimate the probabilities that impair = 1, ≤ 2 and ≤ 3. How many coefficients does this model have?\nEstimate the coefficients of this model using the vglm function from the VGAM package, associated with the option family = cumulative(parallel=TRUE). Check that the number of coefficients is indeed the expected one.\nWrite the mathematical definition of the odds ratio associated with the ses variable and interpret this quantity.\nGive an asymptotic confidence interval at the 95% confidence level for the parameter linked to the ses variable.\nDeduce an asymptotic confidence interval at the 95% confidence level for the associated odds ratio.\nIs there an influence of socio-economic status on mental health status at the 5% threshold? And at the 10% threshold?\nWe want to see if a more complex model would fit the data better. Write mathematically then estimate a proportional cumulative logistic regression model with interaction term. Interpret the obtained result. Is this model significantly better, at the asymptotic error threshold of 5%?\nSame question with a cumulative logistic regression model without proportional structure, but without interaction term.\nConversely, could we propose a simpler model?\nFinally, we decide not to exploit the fact that the impair variable is ordinal, and to model it by a multinomial logistic model. Compare this approach to the previous modeling.\n\n\n\n\nThe goal of the study is to study the diversity of ants on the experimental site of Nourragues in French Guiana. 1 m² of litter was sampled in several places from 4 different forests (the plateau forest GPWT, the liana forest FLWT, the transition forest FTWT, and the Inselberg forest INWT). Each sample was weighed (Weight variable) and the number of different species present in the sample was recorded (Effectif variable). Finally, the collection conditions (humid or dry, Conditions variable) were noted to test their influence on the presence of ants.\n\nImport the data under R and transform categorical variables into factors.\nPerform a small descriptive study to identify a possible link between the number of species observed and the other available variables.\nModel by a log-linear Poisson model the Effectifs variable as a function of all available variables, including all their possible interactions. Analyze the model output.\nUsing the step function, perform a stepwise backward selection of the best sub-model of the previous model according to the AIC criterion, then according to the BIC criterion. Similarly perform a stepwise forward selection. Compare the obtained choices.\nThe inconsistency in the previous backward and forward selections suggests that there may be an alternative sub-model (not tested by these algorithms) that is even better. This encourages us to perform an exhaustive selection of the best sub-model, as proposed by the regsubsets function in linear regression. Unfortunately, the latter does not work with the Poisson model. If we wanted to implement this exhaustive selection ourselves, justify that there would be 30 sub-models (with constant) to test, counting the most general model.\nWe admit that at the end of such an exhaustive selection, the best sub-model in the sense of AIC and BIC is the one whose Weight coefficients are declined into as many crossed modalities as the Site and Conditions factors contain (i.e. 8), but which has an identical intercept for all crossed modalities of Site and Conditions. Estimate this model, calculate its AIC and BIC and compare with those of the previously selected models.\nAs an alternative, we want to try to fit a negative binomial generalized model. If we include all possible interactions, does this approach seem preferable to the Poisson model?\nAfter an exhaustive selection, we admit that the best negative binomial sub-model in the sense of AIC involves the same variables as the best Poisson model. On the other hand, for the BIC criterion, it is the model involving only Weight and Site, in which the Weight coefficient varies according to Site, but the intercept is constant. Estimate these two models and calculate their AIC and BIC.\nGiven the experts’ opinion, it seems important that the model takes into account humidity conditions. What final model should be retained?\nWrite the equation of the retained model according to the different sites and collection conditions.\nAccording to the selected model, what is the probability of observing more than 15 species on INWT type soil in dry weather, based on a soil sample that weighs 10kg? Same question if the weather is humid.\n\n\n\n\nThe crabs dataset contains the observation of 173 female horseshoe crabs. These are marine animals that resemble crabs having a horseshoe shape. For each female horseshoe crab, we record its color (coded from 1 to 4, from lightest to darkest), its width, its weight and satell: the number of satellite male horseshoe crabs (i.e. attached to the female). Color is a sign of the horseshoe crab’s age, the latter tending to darken over time. We seek to model the satell number as a function of the available variables.\n\nImplement a log-linear Poisson model and a negative binomial model. Evaluate their quality. We can in particular discuss the relevance of considering the color variable as a quantitative variable or a factor.\nImprove the modeling by taking into account zero inflation."
  },
  {
    "objectID": "teaching/linear_model/TDh/TD6-8.html#exercise-1.",
    "href": "teaching/linear_model/TDh/TD6-8.html#exercise-1.",
    "title": "TD2-5",
    "section": "",
    "text": "The dataset “chdage.txt”, available in Moodle, contains data from 100 patients aged 20 to 69 years (variable age), some of whom have coronary heart disease (variable chd taking values Yes or No).\n\nImport this data under R as a data.frame. Observe the content of each variable and transform their class if necessary.\nPropose one or more graphical visualization(s) to analyze the possible link between the variables chd and age.\nIs a link apparent? What simple statistical test would confirm it?\nWe denote \\(Y = \\mathbf{1}_{chd=Yes}\\) and \\(p(x) = P(Y = 1|age = x)\\). Give the distribution of \\(Y\\) given that \\(age = x\\) as a function of \\(p(x)\\).\nBased on this distribution, give the likelihood of observations \\((y_1, \\ldots, y_n)\\) as a function of \\((p(x_1), \\ldots, p(x_n))\\) where \\(x_i\\) denotes the age of individual \\(i\\) and \\(y_i = 1\\) if the latter has coronary heart disease.\nAs a first estimation of \\(p(x)\\), we implement the following approach:\n\n\nUse the 8 age groups proposed via the agegrp variable of the dataset, which forms 8 groups of individuals associated with these classes. Calculate \\(\\bar{x}_1, \\ldots, \\bar{x}_8\\) the center of each age class.\nCalculate the proportions of \\(chd = Yes\\) in each group, denoted \\(\\hat{p}_1, \\ldots, \\hat{p}_8\\) (you can use the table and prop.table functions).\n\n\nIn order to analyze the quality of the previous estimation, transform the chd variable into a numerical variable taking the two values 0 and 1, and represent on the same graph the scatter plot of age and chd (recoded) and the estimated proportions in each class \\((\\bar{x}_k, \\hat{p}_k)\\) for \\(k = 1, \\ldots, 8\\). What virtues and what limitations does this estimation procedure have?\nWe decide to model \\(p(x)\\) using a logistic regression model with parameter \\(\\beta = (\\beta_0, \\beta_1) \\in \\mathbb{R}^2\\). What assumption does this mean on the expression of \\(p(x)\\)? Is this compatible with the previous graph? What other modeling alternative(s) could we suggest?\nWrite the log-likelihood of the logistic model as a function of \\(\\beta\\) and deduce the system that the maximum likelihood estimator \\(\\hat{\\beta}\\) must solve. Can we solve this system analytically?\nCalculate the maximum likelihood estimator using the glm function.\nRecall the theoretical definition of the odds ratio of having coronary heart disease between an individual of age \\(x_1\\) and an individual of age \\(x_2\\). What is its value for the previous model when the 2 individuals are 10 years apart (i.e. \\(x_1 = x_2 + 10\\))?\nWe are now interested in the probability ratio (and not odds ratio) of having coronary heart disease between an individual of age \\(x_1\\) and an individual of age \\(x_2\\). What is this ratio for the previous model when the 2 individuals are 10 years apart (i.e. \\(x_1 = x_2 + 10\\))? We can represent this ratio as a function of \\(x_2\\), for \\(x_2\\) taking values from 20 to 70 years.\n\nWe recall that under “good conditions”, the maximum likelihood estimator \\(\\hat{\\beta}\\) in a logistic regression model involving \\(p\\) explanatory variables and \\(n\\) individuals satisfies the following convergence in distribution, when \\(n \\to \\infty\\), \\[J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0_p, I_p)\\] where \\(J_n(\\beta)\\) is the Fisher information matrix. The latter has the expression \\[J_n(\\beta) = X'W_\\beta X\\] where \\(X\\) is the design matrix and \\(W_\\beta\\) is the diagonal matrix \\[W_\\beta = \\begin{pmatrix} p_\\beta(x_1)(1-p_\\beta(x_1)) & 0 \\\\ & \\ddots \\\\ 0 & p_\\beta(x_n)(1-p_\\beta(x_n)) \\end{pmatrix}\\]\n\nJustify that an asymptotic approximation of the distribution of \\((\\hat{\\beta} - \\beta)\\) is \\(N(0_p, J_n^{-1}(\\beta))\\).\nHow can we exploit this result to estimate the standard deviation of each coordinate of \\(\\hat{\\beta}\\)? Give the concrete approach to apply, but we do not ask to implement it numerically.\nThis estimation procedure is used by the glm function. Using its output, give an estimation of the standard deviation of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nConstruct an asymptotic 95% confidence interval for parameter \\(\\beta_1\\).\nAccording to the previous question, is parameter \\(\\beta_1\\) different from 0 at the asymptotic error threshold of 5%? What is the name of this test procedure? Give the p-value associated with this test and verify that it agrees well with the glm output.\nCalculate the deviance test statistic for significance of the GLM model (compared to the null model). Deduce the p-value and conclude at error thresholds of 10%, 5% and 1%. Compare with the results of the test performed under R using the anova function applied to the model, with the option test=“Chisq”.\nReturn to the graph made in question 7 and superimpose (in the form of a curve) the predicted values \\(\\hat{p}(x)\\) by the logistic model, calculated for a grid of values of \\(x\\) covering the range taken by the observations. You can use the predict function associated with the option type=“response”.\nStarting from the convergence in distribution of \\(\\hat{\\beta}\\) recalled above, deduce a confidence interval at the asymptotic 95% level for \\(p(x)\\). Add this confidence interval for each \\(x\\) considered to the previous graph. You can profitably exploit the option se=TRUE of the predict function in the case type=“link”."
  },
  {
    "objectID": "teaching/linear_model/TDh/TD6-8.html#exercise-2-the-logistic-model-is-natural",
    "href": "teaching/linear_model/TDh/TD6-8.html#exercise-2-the-logistic-model-is-natural",
    "title": "TD2-5",
    "section": "",
    "text": "We have a pair of random variables \\((X, Y)\\) where \\(Y\\) is binary and \\(X\\) takes values in \\(\\mathbb{R}^d\\). We denote \\(p = P(Y = 1)\\), \\(f_0(.)\\) the conditional density of \\(X\\) given that \\(Y = 0\\) and \\(f_1(.)\\) the conditional density of \\(X\\) given that \\(Y = 1\\). We further denote \\[h(x) = \\log \\frac{f_1(x)}{f_0(x)} + \\log \\frac{p}{1-p}, \\quad x \\in \\mathbb{R}^d\\]\n\nShow that for all \\(x\\) \\[P(Y = 1|X = x) = \\frac{1}{1 + e^{-h(x)}}\\]\nWe recall that a distribution on \\(\\mathbb{R}^d\\) belongs to the exponential family if its density can be written \\(a(x)b(\\theta)e^{\\theta'T(x)}\\), for some parameter \\(\\theta \\in \\mathbb{R}^q\\), where \\(a\\) and \\(b\\) are positive functions and \\(T: \\mathbb{R}^d \\to \\mathbb{R}^q\\) is a function called sufficient statistic. Show that if the conditional densities \\(f_0\\) and \\(f_1\\) belong to the same exponential family and differ only by the value of their associated parameter, then \\(P(Y = 1|X = x)\\) follows exactly a logistic regression model, specifying its parameter and variables."
  },
  {
    "objectID": "teaching/linear_model/TDh/TD6-8.html#exercise-3-the-logistic-model-is-natural-bis",
    "href": "teaching/linear_model/TDh/TD6-8.html#exercise-3-the-logistic-model-is-natural-bis",
    "title": "TD2-5",
    "section": "",
    "text": "Preamble: Entropy is a quantity found in thermodynamics to measure the state of disorder or randomness of a system. In the same spirit, it is also found in information theory and probability to quantify the disorder or amount of randomness that a probability distribution integrates. A physical system tends to naturally evolve towards a state of maximum entropy. Following this principle, it is natural, to describe a given random experiment, to choose probability distributions that maximize entropy. This is the principle we will apply to seek to best choose \\(P(Y = 1)\\) when \\(Y\\) is binary.\nMathematically, given \\(Y\\) a binary variable and \\(p = P(Y = 1)\\), the entropy of the distribution of \\(Y\\) is \\[-p \\log(p) - (1-p) \\log(1-p)\\]\nThe entropy of a vector of independent binary variables \\(Y_1, \\ldots, Y_n\\) is simply the sum of individual entropies.\n\nWithout any source of constraint, what is the maximum entropy distribution of a binary variable?\nSuppose now that we have a sample of \\(n\\) pairs \\((Y_i, X_i)\\) where \\(X_i\\) is a random variable in \\(\\mathbb{R}^d\\). We denote \\(p_i(x_i) = P(Y_i = 1|X_i = x_i)\\), \\(i = 1, \\ldots, n\\). A priori, without using any information contained in the sample, what are the \\(p_i(x_i)\\) that maximize entropy?\nWe want to find the \\(p_i(x_i)\\) that maximize entropy while being consistent with the observations. This amounts to including constraints on the possible \\(p_i(x_i)\\). We choose the constraints: \\[\\sum_{i=1}^n y_i x_i = \\sum_{i=1}^n p_i(x_i) x_i\\] (Since \\(x_i\\) is a vector of size \\(d\\), this is indeed a system of \\(d\\) constraints).\n\nThese constraints are quite natural: we want the mean of the \\(x_i\\) of individuals in the positive group (\\(y_i = 1\\)) to coincide with the mean of the \\(x_i\\) weighted by the probability that \\(y_i\\) equals 1. In particular (for the constant variable 1), we want the proportion of \\(y_i = 1\\) to coincide with the sum of probabilities.\nFind the \\(p_i(x_i)\\) that maximize entropy while satisfying the previous constraints. You can give the solution up to an unknown (vectorial) constant.\n\nWhat relationship with logistic regression?"
  },
  {
    "objectID": "teaching/linear_model/TDh/TD6-8.html#exercise-4-break-data",
    "href": "teaching/linear_model/TDh/TD6-8.html#exercise-4-break-data",
    "title": "TD2-5",
    "section": "",
    "text": "We consider the break data, available on the course Moodle page. This dataset, of size \\(n = 33\\), includes three variables relating to the state of an automobile:\n\nA variable fault that equals 1 if the car concerned experienced a breakdown, 0 otherwise;\nA variable age that gives the age of the car;\nA variable brand that gives the brand of the car.\n\nThe goal of the exercise is to model the fault variable.\n\nImport the data under R and recode the class of each variable if necessary.\nGraphically observe the possible link between fault and age on one hand, and between fault and brand on the other hand.\nWe want to implement a logistic regression model explaining the probability of having a breakdown as a function of the car’s age and its brand. Launch this modeling under R and write the mathematical formula of the obtained model. We will specify in particular the estimated model specific to cars of brand 0, then brand 1, then brand 2.\nAnalyze the overall quality of the model.\nRestart the modeling by including only the age variable in the model. Is it better?\nThe scatter plot between the age variable and the fault variable seems to suggest that breakdowns occur at the beginning of the vehicle’s life (“youth defects” or “break-in”) and at the end of life (wear breakdowns). This behavior of the breakdown probability in the shape of a parabola encourages us to try to include a quadratic term in the age variable. Make this addition to the model by also including the brand variable and analyze the results. Is the model significant?\nWhat is the odds ratio associated with brand “2” compared to brand “0” of the brand variable? Interpret this value and give a 95% confidence interval around this estimation. Is the inclusion of the brand variable in the model relevant?"
  },
  {
    "objectID": "teaching/linear_model/TDh/TD6-8.html#exercise-5-mental-data",
    "href": "teaching/linear_model/TDh/TD6-8.html#exercise-5-mental-data",
    "title": "TD2-5",
    "section": "",
    "text": "We consider the mental data, contained in the mental.txt file available on the course Moodle page. This dataset, of size \\(n = 40\\), is extracted from a study on the mental health of adults living in Alachua County, Florida, USA. It contains three variables:\n\nA variable impair describing the mental state of the person concerned, from 1 (healthy) to 4 (in poor health),\nA variable ses that equals 1 if the person has a high socio-economic status, 0 otherwise,\nA variable life measuring the number and intensity of upheavals that the person has experienced during the last three years, from 0 (no change) to 9 (very important changes).\n\nThe goal of the exercise is to model the impair variable.\n\nImport the data under R. Is the ses variable qualitative or quantitative? Same question for the life variable. Change their class under R if needed.\nPerform a small descriptive study to identify a possible link between the impair variable and the other variables in the dataset.\nWrite mathematically the proportional cumulative logistic regression model without interaction term linking impair to ses and life, and allowing to estimate the probabilities that impair = 1, ≤ 2 and ≤ 3. How many coefficients does this model have?\nEstimate the coefficients of this model using the vglm function from the VGAM package, associated with the option family = cumulative(parallel=TRUE). Check that the number of coefficients is indeed the expected one.\nWrite the mathematical definition of the odds ratio associated with the ses variable and interpret this quantity.\nGive an asymptotic confidence interval at the 95% confidence level for the parameter linked to the ses variable.\nDeduce an asymptotic confidence interval at the 95% confidence level for the associated odds ratio.\nIs there an influence of socio-economic status on mental health status at the 5% threshold? And at the 10% threshold?\nWe want to see if a more complex model would fit the data better. Write mathematically then estimate a proportional cumulative logistic regression model with interaction term. Interpret the obtained result. Is this model significantly better, at the asymptotic error threshold of 5%?\nSame question with a cumulative logistic regression model without proportional structure, but without interaction term.\nConversely, could we propose a simpler model?\nFinally, we decide not to exploit the fact that the impair variable is ordinal, and to model it by a multinomial logistic model. Compare this approach to the previous modeling."
  },
  {
    "objectID": "teaching/linear_model/TDh/TD6-8.html#exercise-6-ants-data",
    "href": "teaching/linear_model/TDh/TD6-8.html#exercise-6-ants-data",
    "title": "TD2-5",
    "section": "",
    "text": "The goal of the study is to study the diversity of ants on the experimental site of Nourragues in French Guiana. 1 m² of litter was sampled in several places from 4 different forests (the plateau forest GPWT, the liana forest FLWT, the transition forest FTWT, and the Inselberg forest INWT). Each sample was weighed (Weight variable) and the number of different species present in the sample was recorded (Effectif variable). Finally, the collection conditions (humid or dry, Conditions variable) were noted to test their influence on the presence of ants.\n\nImport the data under R and transform categorical variables into factors.\nPerform a small descriptive study to identify a possible link between the number of species observed and the other available variables.\nModel by a log-linear Poisson model the Effectifs variable as a function of all available variables, including all their possible interactions. Analyze the model output.\nUsing the step function, perform a stepwise backward selection of the best sub-model of the previous model according to the AIC criterion, then according to the BIC criterion. Similarly perform a stepwise forward selection. Compare the obtained choices.\nThe inconsistency in the previous backward and forward selections suggests that there may be an alternative sub-model (not tested by these algorithms) that is even better. This encourages us to perform an exhaustive selection of the best sub-model, as proposed by the regsubsets function in linear regression. Unfortunately, the latter does not work with the Poisson model. If we wanted to implement this exhaustive selection ourselves, justify that there would be 30 sub-models (with constant) to test, counting the most general model.\nWe admit that at the end of such an exhaustive selection, the best sub-model in the sense of AIC and BIC is the one whose Weight coefficients are declined into as many crossed modalities as the Site and Conditions factors contain (i.e. 8), but which has an identical intercept for all crossed modalities of Site and Conditions. Estimate this model, calculate its AIC and BIC and compare with those of the previously selected models.\nAs an alternative, we want to try to fit a negative binomial generalized model. If we include all possible interactions, does this approach seem preferable to the Poisson model?\nAfter an exhaustive selection, we admit that the best negative binomial sub-model in the sense of AIC involves the same variables as the best Poisson model. On the other hand, for the BIC criterion, it is the model involving only Weight and Site, in which the Weight coefficient varies according to Site, but the intercept is constant. Estimate these two models and calculate their AIC and BIC.\nGiven the experts’ opinion, it seems important that the model takes into account humidity conditions. What final model should be retained?\nWrite the equation of the retained model according to the different sites and collection conditions.\nAccording to the selected model, what is the probability of observing more than 15 species on INWT type soil in dry weather, based on a soil sample that weighs 10kg? Same question if the weather is humid."
  },
  {
    "objectID": "teaching/linear_model/TDh/TD6-8.html#exercise-7-horseshoe-crabs-data",
    "href": "teaching/linear_model/TDh/TD6-8.html#exercise-7-horseshoe-crabs-data",
    "title": "TD2-5",
    "section": "",
    "text": "The crabs dataset contains the observation of 173 female horseshoe crabs. These are marine animals that resemble crabs having a horseshoe shape. For each female horseshoe crab, we record its color (coded from 1 to 4, from lightest to darkest), its width, its weight and satell: the number of satellite male horseshoe crabs (i.e. attached to the female). Color is a sign of the horseshoe crab’s age, the latter tending to darken over time. We seek to model the satell number as a function of the available variables.\n\nImplement a log-linear Poisson model and a negative binomial model. Evaluate their quality. We can in particular discuss the relevance of considering the color variable as a quantitative variable or a factor.\nImprove the modeling by taking into account zero inflation."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-lectures",
    "href": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-lectures",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Link with Previous Lectures",
    "text": "Link with Previous Lectures\n\nPreviously, we considered that:\n\nResponse variable \\(Y\\) is quantitative\nExplanatory variables \\(X^{(j)}\\) are quantitative\n\n\n\nWe still assume \\(Y\\) is quantitative, but explanatory variables can be qualitative and/or quantitative."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#terminology",
    "href": "teaching/linear_model/slides/anova_ancova.html#terminology",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Terminology",
    "text": "Terminology\n\nANOVA (Analysis of Variance): All explanatory variables \\(X^{(j)}\\) are qualitative\nANCOVA (Analysis of Covariance): Explanatory variables mix both quantitative and qualitative variables\n\n\nWe’ll see that these situations reduce to the case of the previous chapter."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#context-notations",
    "href": "teaching/linear_model/slides/anova_ancova.html#context-notations",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Context, Notations",
    "text": "Context, Notations\n\nWe seek to explain \\(Y\\) using a single qualitative variable \\(X\\).\nWe observe \\((Y, X)\\), and define\n\n\n\n\n\\[N_i = \\sum_{k=1}^n \\mathbf 1\\{X_k=i\\} \\and \\overline Y_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\mathbf 1\\{X_k=i\\}\\]\n\n\n\n\nTotal mean:\n\\(\\overline Y = \\frac{1}{N}\\sum_{k=1}^n Y_k = \\frac{1}{N}\\sum_{i=1}^I N_i \\overline Y_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#setting",
    "href": "teaching/linear_model/slides/anova_ancova.html#setting",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Setting",
    "text": "Setting\n\nFor \\(k = 1, \\ldots, n\\)\n\n\\[Y_k = \\sum_{i=1}^I \\mu_i \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\]\n\nwhere \\(\\E[\\varepsilon_k]=0\\), \\(\\Cov(\\varepsilon_k,\\varepsilon_l)=\\sigma^2\\1\\{k=l\\}\\)\n\n\\(Y_k\\) is random and has expectation \\(\\mu_i\\) if \\(X_k=A_i\\)\n\\(\\Var(Y_k)=\\sigma^2\\) is the same regardless of modality \\(A_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#problem",
    "href": "teaching/linear_model/slides/anova_ancova.html#problem",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Problem",
    "text": "Problem\n\nDo we have \\(\\mu_1 = \\mu_2 \\ldots = \\mu_I\\)? (Does factor \\(A\\) influences \\(Y\\)?)\nDoes a \\(\\mu_i\\) has more influence on \\(Y\\)?\nHow do we estimate the \\(\\mu_i\\)’s?"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#one-hot-encoding",
    "href": "teaching/linear_model/slides/anova_ancova.html#one-hot-encoding",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "One Hot Encoding",
    "text": "One Hot Encoding\n\nWe use one-hot encoding: \\(X_{ki}=\\1\\{X_k=A_i\\}\\)\n\n\nThat is, for individual \\(k\\), \\(X_{k\\cdot} = (\\1\\{X_k=A_1\\}, \\dots, \\1\\{X_k=A_I\\}) \\in \\{0,1\\}^I\\)\n\n\nOr, using previous notations, \\(X = (X^{(1)}, \\dots, X^{(I)})\\) where\n\\[X^{(i)}= \\begin{pmatrix}\nX^{(i)}_1 \\\\\n\\vdots  \\\\\nX^{(i)}_n\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#in-other-words",
    "href": "teaching/linear_model/slides/anova_ancova.html#in-other-words",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "In Other Words",
    "text": "In Other Words\n\nIf there are \\(3\\) categories e.g. blue, orange, green, we replace the column \\(X\\) by \\(3\\) columns\nExample with \\(I=3\\) categories and \\(n=5\\) individuals \\[\n\\begin{pmatrix}\n\\color{blue}{\\mathrm{blue}} \\\\\n\\mathrm{green} \\\\\n\\color{blue}{\\mathrm{blue}} \\\\\n\\mathrm{orange} \\\\\n\\mathrm{orange} \\\\\n\\end{pmatrix} \\quad  \\text{becomes} \\quad\nX=\\begin{pmatrix}\n\\color{blue}{\\mathrm{1}} & \\mathrm{0} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{0} & \\mathrm{1}\\\\\n\\color{blue}{\\mathrm{1}} & \\mathrm{0} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{1} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{1} & \\mathrm{0}\\\\\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-matrix-form",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-matrix-form",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model, Matrix Form",
    "text": "Model, Matrix Form\n\nWe rewrite the model \\(Y_k = \\sum_{i=1}^I \\mu_i \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\) as\n\n\\[Y = X\\mu + \\varepsilon\\]\n\n\nHere, \\(X_{k\\cdot} = (\\1\\{X_k=A_1\\}, \\dots, \\1\\{X_k=A_I\\}) \\in \\{0,1\\}^I\\)\nThere is no constant in this model"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-with-constant",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-with-constant",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model with Constant",
    "text": "Model with Constant\n\nIf we want to model the constant (intercept) and \\(I\\) modalities, we assume\n\n\\[Y_k = m+\\sum_{i=2}^I \\alpha_i \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\]\n\n\n\nEquivalently,\n\n\\(Y = m \\mathbf{1} + \\sum_{i=2}^I \\alpha_i\\mathbf{1}_{A_i}+\\varepsilon\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#colinearity-issues-and-constraints",
    "href": "teaching/linear_model/slides/anova_ancova.html#colinearity-issues-and-constraints",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Colinearity issues and constraints",
    "text": "Colinearity issues and constraints\n\n\\(Y_k = m+\\sum_{i=2}^I \\alpha_i \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\)\n\n\n\n\\(\\sum_{i=1}^I \\mathbf{1}_{A_i} = \\mathbf{1}\\)\n\n\n\nWe have \\(I+1\\) parameters instead of \\(I\\). We have to set one constraint:\n\n\\(m=0\\) (model without intercept). Here, \\(\\alpha_i= \\mu_i\\)\n\\(\\alpha_1 =0\\). In this case, we set \\(\\alpha_i = \\mu_i-\\mu_1\\) (default in R)."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#in-r",
    "href": "teaching/linear_model/slides/anova_ancova.html#in-r",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "In R",
    "text": "In R\n\n\nwith intercept (default)\n\nlm(Y~A) #with intercept\n\n\nInterpretation: gives expectation \\(\\E[Y_k] = \\mu_1\\) and coefficients \\(\\alpha_i= \\mu_i - \\mu_1\\)\n \n\n\n\n\nwithout intercept\n\nlm(Y~A-1) #without intercept\n\n\nInterpretation: gives coefficients \\(\\alpha_i= \\mu_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#estimation-of-mu",
    "href": "teaching/linear_model/slides/anova_ancova.html#estimation-of-mu",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Estimation of \\(\\mu\\)",
    "text": "Estimation of \\(\\mu\\)\n\nWhichever the model we choose (with constant or not), estimation of \\(\\mu_i=\\E[Y_k|X_k=A_i]\\) is the same. Same for \\(\\Var(\\varepsilon_k)=\\sigma^2\\).\n\n\n\n\n\nProposition\n\n\nIn category \\(i\\), OLS estimation of \\(\\mu_i\\) leads to:\n\n\\(\\hat{\\mu}_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\1\\{X_k=A_i\\} = \\overline{Y}_i\\)\n\nAn unbiased estimator of \\(\\sigma^2\\) is\n\n\\(\\hat{\\sigma}^2 = \\frac{1}{n-I} \\sum_{k=1}^{n}\\sum_{i=1}^I (Y_{k} - \\overline{Y}_i)^2\\1\\{X_k=A_i\\}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#idea-of-proof-projections",
    "href": "teaching/linear_model/slides/anova_ancova.html#idea-of-proof-projections",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Idea of Proof: Projections",
    "text": "Idea of Proof: Projections\n\nWe write \\(\\mathbf 1_i = (\\1\\{X_1=A_i\\}, \\dots, \\1\\{X_n=A_i\\})^T\\), and \\(E = span(\\mathbf 1_1, \\dots, \\mathbf 1_I)\\)\n\n\nThen, the OLS is exactly\n\\(\\hat \\mu = \\mathrm{argmin}_{\\mu' \\in E}\\|Y - \\mu'\\|_2^2 = P_E Y\\)\n\n\nFor \\(\\hat \\sigma\\), \\(\\mathbb E[((I-P_E)\\varepsilon)^T(I-P_E)\\varepsilon]= \\sigma^2 (n-I)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-for-factor-effect-anova-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-for-factor-effect-anova-test",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing for Factor Effect (ANOVA Test)",
    "text": "Testing for Factor Effect (ANOVA Test)\n\nWe want to test \\(H_0: \\mu_1 = \\cdots = \\mu_I\\).\nThis is a linear constraints test! (See previous chapters)\n\n\n\n\n\nProposition\n\n\nIf \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: \\mu_1 = \\cdots = \\mu_I\\):\n\\[F = \\frac{SSB/(I-1)}{SSW/(n-I)} \\sim F(I-1, n-I)\\]\n\n\\(SSB = \\sum_{i=1}^I N_i (\\overline{Y}_i - \\overline{Y})^2\\)\n\\(SSW = \\sum_{i=1}^I \\sum_{k=1}^{n} (Y_{k} - \\overline{Y}_i)^2\\1\\{X_k=A_i\\}\\)\n\nCritical region at level \\(\\alpha\\): \\(CR_\\alpha = \\{F &gt; f_{I-1,n-I}(1-\\alpha)\\}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#idea-of-proof",
    "href": "teaching/linear_model/slides/anova_ancova.html#idea-of-proof",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Idea of proof",
    "text": "Idea of proof\nUsing the previous projection \\(P_E = \\sum_{i=1}^I \\frac{1}{N_i}\\mathbf{1}_i\\mathbf{1}_i^T\\), we have the decomposition\n\n\\[Y - P_0Y = Y-P_E Y + P_E Y - P_0 Y\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-chapter",
    "href": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-chapter",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Link with previous chapter",
    "text": "Link with previous chapter\nIn \\(\\mu_1= \\ldots = \\mu_I\\), there are \\(I-1\\) constraints to test.\n\n\\[F = \\frac{n-I}{I-1} \\cdot \\frac{SSR_c - SSR}{SSR}\\]\n\n\nWe show that\n\n\\(SSR = SSW\\)\n\\(SSR_c = SST = \\sum_{k=1}^n(Y_k - \\overline Y)^2\\)\n\\(SST = SSB + SSW\\).\n\n\n\nIn R: anova(lm(Y~A))"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#factor-significance-analysis-of-variance-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#factor-significance-analysis-of-variance-test",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Factor Significance: Analysis of Variance Test",
    "text": "Factor Significance: Analysis of Variance Test\n\n\n\n\nWarning\n\n\nThe previous analysis of variance test tests equality of means between modalities not equality of variances\n\n\n\n\n\nIt is valid under the assumption \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\).\n\nGaussian assumption: Not critical if \\(n\\) is large\nHomoscedasticity: Important assumption"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#homoscedasticity-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#homoscedasticity-tests",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Homoscedasticity Tests",
    "text": "Homoscedasticity Tests\n\nHow to test equality of variances in each modality:\n\nLevene test\nBartlett test\n\n\n\nIn R: leveneTest or bartlett.test from car library"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis-multiple-testing-problem",
    "href": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis-multiple-testing-problem",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Post-hoc Analysis: Multiple Testing Problem",
    "text": "Post-hoc Analysis: Multiple Testing Problem\n\nIf factor \\(A\\) is significant, we want to know more:\nwhich modality(ies) differs from others?\n\n\nWe want to perform all tests:\n\n\\[H_{0}^{i,j}: \\mu_i = \\mu_j \\quad \\text{vs} \\quad H_{1}^{i,j}: \\mu_i \\neq \\mu_j\\]\n\n\n\nfor all \\(i \\neq j\\) in \\(\\{1, \\ldots, I\\}\\), corresponding to \\(I(I-1)/2\\) tests."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-naive-approach",
    "href": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-naive-approach",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Multiple Testing: Naive Approach",
    "text": "Multiple Testing: Naive Approach\n\nPerform all Student’s t-tests for mean comparison (1 constraint), each at level \\(\\alpha\\).\nProblem: Given the number of tests, this would lead to many false positives.\n\n\nFalse positives are a well-known problem in multiple testing.\n\n\nSolution: Apply a correction to the decision rule, e.g.\n\nBonferroni correction\nBenjamini-Hochberg correction\n\n\n\nFor one-way ANOVA: Tukey’s test addresses the problem."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-tukeys-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-tukeys-test",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Multiple Testing: Tukey’s Test",
    "text": "Multiple Testing: Tukey’s Test\n\n\n\\[Q = \\max_{(i,j)} \\frac{|\\overline{Y}_i - \\overline{Y}_j|}{\\hat{\\sigma}\\sqrt{\\frac{1}{N_i} + \\frac{1}{N_j}}}\\]\n\n\n\n\n\n\nDistribution of Tukey’s Test Statistic\n\n\nUnder \\(H_0: \\mu_1 = \\cdots = \\mu_I\\) and assuming \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\):\n\\[Q \\sim Q_{I,n-I}\\]\nwhere \\(Q_{I,n-I}\\) denotes the Tukey distribution with \\((I, n-I)\\) degrees of freedom.\nNote: This is exact if all \\(n_i\\) are equal, otherwise the distribution is approximately Tukey."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#individual-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#individual-tests",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Individual Tests",
    "text": "Individual Tests\nTo test each \\(H_0^{i,j}: \\mu_i = \\mu_j\\), we use the critical regions:\n\n\n\n\\[CR_\\alpha^{i,j} = \\left\\{|\\overline{Y}_i - \\overline{Y}_j| &gt; \\frac{\\hat{\\sigma}}{\\sqrt{2}} \\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}} \\cdot Q_{I,n-I}(1-\\alpha)\\right\\}\\]\n\n\nwhere \\(Q_{I,n-I}(1-\\alpha)\\) denotes the \\((1-\\alpha)\\) quantile of a tukey distribution \\(Q(I,n-I)\\) of degrees \\((I, n-I)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#key-properties-of-tukeys-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#key-properties-of-tukeys-test",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Key Properties of Tukey’s Test",
    "text": "Key Properties of Tukey’s Test\nThe form of the previous \\(CR_\\alpha^{i,j}\\) ensures that: \\[\\mathbb{P}_{\\mu_1 = \\cdots = \\mu_I}\\left(\\bigcup_{(i,j)} \\{Y \\in CR_\\alpha^{i,j}\\}\\right) = \\alpha\\]\nInterpretation: If all null hypotheses \\(H_0^{i,j}\\) are true (\\(\\mu_1 = \\cdots = \\mu_I\\)), then the probability of concluding at least one \\(H_1^{i,j}\\) equals \\(\\alpha\\).\n\nThis is the simultaneous Type I error rate."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#comparison-with-students-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#comparison-with-students-tests",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Comparison with Student’s tests",
    "text": "Comparison with Student’s tests\n\nIndividual Student’s t-tests \\(T^{i,j}\\) at level \\(\\alpha\\): only guarantee \\(\\mathbb{P}_{\\mu_i = \\mu_j}(T^{i,j}=1) = \\alpha\\)\nWhen cumulated: \\(\\mathbb{P}_{\\mu_1 = \\cdots = \\mu_I}\\left[\\cup_{(i,j)}\\{T^{i,j}=1\\}\\right] \\approx 1\\) → false positives\n\n\nAdvantage\nWith Tukey’s test, two significantly different means are truly different, not just due to false positives.\nIn R: TukeyHSD"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-of-r-output",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-of-r-output",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Example of R output",
    "text": "Example of R output\n\nHomoscedasticity Test:\nleveneTest(Loss∼Exercise)\nWe get\nLevene’s Test for Homogeneity of Variance (center = median)\n      Df   F value  Pr(&gt;F)\ngroup 3    0.6527   0.584\n      68\n\n\nHomoscedasticity is ok. ANOVA?\nreg=lm(Loss∼Exercise) # Exercise has 4 categories\nanova(reg)\n\n\nWe get\nResponse: Loss\n         Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nExercise   3 712.56  237.519  20.657 1.269e-09 ***\nResiduals 68 781.89   11.498"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#results-interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#results-interpretation",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Results Interpretation",
    "text": "Results Interpretation\n\nThe means are significantly different. We read in particular:\n\n\\(I - 1 = 3\\), \\(n - I = 68\\)\n\\(SSB = 712.56\\), \\(SSW = 781.89\\)\n\\(F = \\frac{SSB/(I-1)}{SSW/(n-I)} = 20.657\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#r-example-post-hoc-analysis",
    "href": "teaching/linear_model/slides/anova_ancova.html#r-example-post-hoc-analysis",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "R Example: Post-hoc Analysis",
    "text": "R Example: Post-hoc Analysis\n\nWe finish by analyzing the mean differences more precisely:\n\n\nTukeyHSD(aov(Loss~Exercise))\n\n\nExercise\n     diff        lwr        upr     p adj\n2-1  7.1666667   4.1897551 10.1435782 0.0000001\n3-1  3.8888889   0.9119773  6.8658005 0.0053823\n4-1 -0.6111111  -3.5880227  2.3658005 0.9487355\n3-2 -3.2777778  -6.2546894 -0.3008662 0.0252761\n4-2 -7.7777778 -10.7546894 -4.8008662 0.0000000\n4-3 -4.5000000  -7.4769116 -1.5230884 0.0009537\n\n\nConclusion: All differences are significant, except between exercise 4 and exercise 1."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#setting-1",
    "href": "teaching/linear_model/slides/anova_ancova.html#setting-1",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Setting",
    "text": "Setting\n\nWe observe \\(Y\\) and explanatory variables \\(X^{(1)}, X^{(2)}\\)\n\n\\(X^{(1)}_k \\in \\{A_1, \\dots, A_I\\}\\) (\\(I\\) modalities)\n\\(X^{(2)}_k \\in \\{B_1, \\dots, B_J\\}\\) (\\(J\\) modalities)\n\\(N_{ij} = \\sum_{k=1}^n \\1\\{X^{(1)}_k \\in A_i\\}\\1\\{X_k^{(2)} \\in B_j\\}\\) individuals in modality \\(A_i\\) and \\(B_j\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model",
    "href": "teaching/linear_model/slides/anova_ancova.html#model",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model",
    "text": "Model\n\n\n\n\\[\\begin{aligned}\nY_k &= m + \\alpha_i\\sum_{i=1}^I\\1\\{X^{(1)}_k \\in A_i\\} +\\beta_i\\sum_{j=1}^J\\1\\{X^{(2)}_k \\in B_j\\} \\\\\n&+ \\gamma_{ij}\\sum_{i=1}^I\\sum_{j=1}^J\\1\\{X^{(1)}_k \\in A_i\\}\\1\\{X^{(2)}_k \\in B_i\\} + \\varepsilon_k\n\\end{aligned}\\]\n\n\n\n\nIn other words, in modality \\(A_i\\) and \\(B_j\\),\n\n\n\n\\(Y_k =m+\\alpha_i + \\beta_j + \\gamma_{ij}+ \\varepsilon_k\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#interpretation",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Interpretation",
    "text": "Interpretation\n\nThis is a model of the type \\(Y_k = \\mu_{ij} + \\varepsilon_k\\).\n\n\nIn modalities \\((i,j)\\), \\(\\E[Y_k] = \\mu_{ij} = m + \\alpha_i + \\beta_j + \\gamma_{ij}\\)\n\n\\(m\\): the average effect of \\(Y\\) (without considering \\(A\\) and \\(B\\))\n\\(\\alpha_i = \\mu_{i.} - m\\): the marginal effect due to \\(A\\)\n\\(\\beta_j = \\mu_{.j} - m\\): the marginal effect due to \\(B\\)\n\n\\(\\gamma_{ij} = \\mu_{ij} - m - \\alpha_i - \\beta_j\\): the remaining effect, due to interaction between \\(A\\) and \\(B\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-1",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-1",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Example 1",
    "text": "Example 1\n\n\\(Y\\): employee satisfaction\n\\(A\\): schedule type (flexible or fixed)\n\\(B\\): training level (basic or advanced)\n\n\nWe can imagine:\n\nEffect due to \\(A\\): satisfaction is greater with flexible schedules\nEffect due to \\(B\\): satisfaction is higher with advanced training\nNo particular interaction between A and B"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-2",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-2",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Example 2",
    "text": "Example 2\n\n\\(Y\\): plant yield\n\\(A\\): fertilizer type (1 or 2)\n\\(B\\): water quantity (low, medium, high)\nWe can imagine:\n\nEffect due to \\(A\\): yield differs according to fertilizer used\nEffect due to \\(B\\): yield is better when there is lots of water\nInteraction: fertilizer 1 is better with water, and vice versa for fertilizer 2\n\n\n\nMaybe interaction is so strong that the effect due to A seems absent!"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#constraints-on-the-parameters",
    "href": "teaching/linear_model/slides/anova_ancova.html#constraints-on-the-parameters",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Constraints on the Parameters",
    "text": "Constraints on the Parameters\n\nIn modalities \\((i,j)\\), \\(\\E[Y_k] = \\mu_{ij} = m + \\alpha_i + \\beta_j + \\gamma_{ij}\\)\n\n\nInitial two-factor ANOVA problem: \\(I \\times J\\) parameters \\(\\mu_{ij}\\).\n\n\nNow: \\(1 + I + J + IJ\\) parameters (\\(m\\), \\(\\alpha_i\\), \\(\\beta_j\\), \\(\\gamma_{ij}\\)).\n\n\nTherefore, we need \\(1 + I + J\\) constraints for identifiability:\n\n\n\n\\(\\sum_{i=1}^{I} \\alpha_i = 0 \\and \\sum_{j=1}^{J} \\beta_j = 0\\)\n\n\n\n\n\\(\\sum_{i=1}^{I} \\gamma_{ij} = 0 \\and \\sum_{j=1}^{J} \\gamma_{ij} = 0\\)\n\n\n\nThese constraints ensure model identifiability by removing the redundant parameters that cause multicollinearity issues."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#implementation-in-r",
    "href": "teaching/linear_model/slides/anova_ancova.html#implementation-in-r",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Implementation in R",
    "text": "Implementation in R\n\nThe complete model (with interaction) is launched with the command:\nlm(Y ~ A + B + A:B) ## or equivalently, lm(Y ~ A*B)\n\n\nWith these constraints, the parameter interpretation is as follows:\nIntercept: \\(m = \\mu_{11}\\)\nMain effect A: \\(\\alpha_i = \\mu_{i1} - \\mu_{11}\\)\nMain effect B: \\(\\beta_j = \\mu_{1j} - \\mu_{11}\\)\nInteraction: \\(\\gamma_{ij} = \\mu_{ij} - \\mu_{i1} - \\mu_{1j} + \\mu_{11}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#important-note-on-interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#important-note-on-interpretation",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Important Note on Interpretation",
    "text": "Important Note on Interpretation\n\nFrom\nlm(Y ~ A + B + A:B) ## or equivalently, lm(Y ~ A*B)\n\n\nIntercept: \\(m = \\mu_{11}\\)\nMain effect A: \\(\\alpha_i = \\mu_{i1} - \\mu_{11}\\)\nMain effect B: \\(\\beta_j = \\mu_{1j} - \\mu_{11}\\)\nInteraction: \\(\\gamma_{ij} = \\mu_{ij} - \\mu_{i1} - \\mu_{1j} + \\mu_{11}\\)\n\n\n\n\n\n\nBe Cautious on the Interpretation of the Coefficients\n\n\nTo impose the constraints from the previous approach (sum-to-zero constraints):\nlm(Y ~ A*B, contrasts = list(A = contr.sum, B = contr.sum))"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#estimation",
    "href": "teaching/linear_model/slides/anova_ancova.html#estimation",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Estimation",
    "text": "Estimation\n\nThe choice of constraints does not affect the estimation of the expectation of \\(Y\\) in each crossed modality \\(A_i \\cap B_j\\).\n\n\n\n\n\nProposition\n\n\nWhatever the linear constraints chosen, the OLS leads to, for all \\(i = 1, \\ldots, I\\), \\(j = 1, \\ldots, J\\) and \\(k = 1, \\ldots, N_{ij}\\), if \\(X^{(1)}_{k}=A_i\\) and \\(X_k^{(2)}=B_j\\):\n\\[\\widehat{Y}_{k} = \\overline{Y}_{ij}:= \\frac{1}{N_{ij}} \\sum_{k=1}^{n} Y_{k}\\1\\{X_k^{(1)}=A_i \\and X_k^{(2)}=B_j\\}\\]\nand to the estimation of the residual variance:\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - IJ} \\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^n (Y_{k} - \\overline{Y}_{ij})^2\\1\\{X_k^{(1)}=A_i \\and X_k^{(2)}=B_j\\}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#significance-tests-for-effects",
    "href": "teaching/linear_model/slides/anova_ancova.html#significance-tests-for-effects",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Significance Tests for Effects",
    "text": "Significance Tests for Effects\n\n\nIs the effect due to the interaction between A and B significant?\nIs the marginal effect due to A significant?\nIs the marginal effect due to B significant?\n\n\n\nFirst, in the additive model with interaction, \\(Y_{ijk} = m + \\alpha_i + \\beta_j + \\varepsilon_{ijk}\\)\n\n\nDo we have \\(\\gamma_{ij} = 0\\) for all \\(i, j\\)?"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#interpretation-on-plots",
    "href": "teaching/linear_model/slides/anova_ancova.html#interpretation-on-plots",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Interpretation on Plots:",
    "text": "Interpretation on Plots:\n\nPlot \\(\\overline Y_{ij}\\) in function of modalities \\((i,j)\\)\n\n\n\nwithout interactions, lines should be almost parallel\n\n\n\ninteraction.plot(A,B,Y)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#plot-presence-of-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#plot-presence-of-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Plot: Presence of Interaction",
    "text": "Plot: Presence of Interaction\nLines cross in presence of interaction"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#significance-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#significance-tests",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Significance Tests",
    "text": "Significance Tests\n\nWe want to test for the presence of an interaction:\n\n\\[H_0^{(AB)}: \\gamma_{ij} = 0 \\text{ for all } i, j\\]\n\n\n\nIf we conclude \\(H_0^{(AB)}\\) (accept the null hypothesis), we then want to test the marginal effects:\n\n\\[H_0^{(A)}: \\alpha_i = 0 \\text{ for all } i \\and H_0^{(B)}: \\beta_j = 0 \\text{ for all } j\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#presence-of-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#presence-of-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Presence of Interaction",
    "text": "Presence of Interaction\n\n\n\\[H_0^{(AB)}: \\gamma_{ij} = 0 \\text{ for all } i, j\\]\n\n\n\n\n\n\nWarning\n\n\nIf we reject \\(H_0^{(AB)}\\), it makes no sense to test whether A or B have an effect: they have one through their interaction.\n\n\n\n\n\nThese tests reduce to constraint tests in the regression model."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#balanced-design-analysis-of-variance",
    "href": "teaching/linear_model/slides/anova_ancova.html#balanced-design-analysis-of-variance",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Balanced Design Analysis of Variance",
    "text": "Balanced Design Analysis of Variance\n\nWe assume that “the design is balanced”: this means that \\(N_{ij}:=N\\) does not depend on \\(i\\) or \\(j\\). (Otherwise, everything becomes complicated).\n\n\nIn this case, we have the analysis of variance formula:"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#anova-formula",
    "href": "teaching/linear_model/slides/anova_ancova.html#anova-formula",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "ANOVA Formula",
    "text": "ANOVA Formula\n\n\n\n\\[S_T^2 = S_A^2 + S_B^2 + S_{AB}^2 + S_R^2\\]\n\n\n\\(S_T^2 = \\sum_{k=1}^{n} (Y_{k} - \\overline{Y})^2\\): total sum of squares\n\\(S_A^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{i.} - \\overline{Y})^2\\): \\(S^2_{between}\\) in the case of one-factor ANOVA where the factor is \\(A\\)\n\\(S_B^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{.j} - \\overline{Y})^2\\): \\(S^2_{between}\\) in the case of one-factor ANOVA where the factor is \\(B\\)\n\\(S_{AB}^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{ij} - \\overline{Y}_{i.} - \\overline{Y}_{.j} + \\overline{Y})^2\\) quantifies the interaction\n\\(S_R^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{N_{ij}} (Y_{k} - \\overline{Y}_{ij})^2\\1\\{X^{(1)}_k=A_i \\and X^{(2)}_k=B_j\\}\\): \\(S_{within}\\) in one-factor ANOVA"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing the Interaction",
    "text": "Testing the Interaction\n\nFor testing the interaction, \\(H_0^{(AB)}: \\gamma_{ij} = 0\\) for all \\(i, j\\)\n\n\nWe the linear constraint test statistic \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\gamma_{ij}=0\\) for all \\((i,j)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-interaction-1",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-interaction-1",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing the Interaction",
    "text": "Testing the Interaction\n\n\n\\[F^{(AB)} = \\frac{S_{AB}^2/(I-1)(J-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2 I_n)\\), \\(F^{(AB)} \\sim \\mathcal F((I-1)(J-1), n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-a",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-a",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing the Effect of \\(A\\)",
    "text": "Testing the Effect of \\(A\\)\n\nFor testing the main effect of \\(A\\), \\(H_0^{(A)}: \\alpha_i = 0\\) for all \\(i\\):\nWe use \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\alpha_i=0\\) for all \\(i\\).\n\n\n\n\\[F^{(A)} = \\frac{S_A^2/(I-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\), \\(F^{(A)} \\sim \\mathcal{F}(I-1, n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-b",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-b",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing the Effect of \\(B\\)",
    "text": "Testing the Effect of \\(B\\)\n\nFor testing the main effect of \\(B\\), \\(H_0^{(B)}: \\beta_j = 0\\) for all \\(j\\):\nWe use \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\beta_j=0\\) for all \\(j\\).\n\n\n\n\\[F^{(B)} = \\frac{S_B^2/(J-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\), \\(F^{(B)} \\sim \\mathcal{F}(J-1, n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#r-outputs",
    "href": "teaching/linear_model/slides/anova_ancova.html#r-outputs",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "R outputs",
    "text": "R outputs\n\nIn software, these tests are summarized in a table as shown below.\nIn R: anova(lm(Y ~ A*B))\n\n\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\n\\(A\\)\n\\(I-1\\)\n\\(S_A^2\\)\n\\(S_A^2/(I-1)\\)\n\\(F^{(A)}\\)\n…\n\n\n\\(B\\)\n\\(J-1\\)\n\\(S_B^2\\)\n\\(S_B^2/(J-1)\\)\n\\(F^{(B)}\\)\n…\n\n\n\\(A:B\\)\n\\((I-1)(J-1)\\)\n\\(S_{AB}^2\\)\n\\(S_{AB}^2/((I-1)(J-1))\\)\n\\(F^{(AB)}\\)\n…\n\n\nResiduals\n\\(n-IJ\\)\n\\(S_R^2\\)\n\\(S_R^2/(n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#assumptions",
    "href": "teaching/linear_model/slides/anova_ancova.html#assumptions",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nFisher tests are based on the assumption \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\)\n\nNormality is not critical, but homoscedasticity is\nWe can test equality of variances in each modality of A (or B), or in each crossed modality if the \\(n_{ij}\\) are sufficiently large\nThis can be done with Levene’s test or Bartlett’s test"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#practical-procedure",
    "href": "teaching/linear_model/slides/anova_ancova.html#practical-procedure",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Practical Procedure",
    "text": "Practical Procedure\n\nTest equality of variances\nForm the ANOVA table (independent of chosen constraints)\nIf the \\(AB\\) interaction is significant: don’t change anything\nIf the interaction is not significant: analyze the marginal effects of A and B\n\n\nIf they are significant: the model is additive: lm(Y ~ A + B)\nOtherwise: we can remove \\(A\\) (or \\(B\\)) from the model"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis",
    "href": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Post-Hoc Analysis",
    "text": "Post-Hoc Analysis\nOnce effects are identified: perform post-hoc analysis by examining differences between (crossed) modalities more closely, using Tukey’s test as in one-factor ANOVA"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#principle-and-limits",
    "href": "teaching/linear_model/slides/anova_ancova.html#principle-and-limits",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Principle and Limits",
    "text": "Principle and Limits\n\nWe seek to explain \\(Y\\) using \\(k\\) qualitative variables \\(A=X^{(1)}\\), \\(B=X^{(2)}\\), \\(C=X^{(3)}\\), …\n\n\nWe can assume that \\(\\E[Y_k]\\) depends on each factor and their interactions:\n\nTwo-way interactions: \\(AB\\), \\(BC\\), \\(AC\\), etc.\nThree-way interactions: \\(ABC\\), etc.\nHigher-order interactions: and potentially more\n\n\n\n\n\n\nWarning\n\n\nThis results in \\(2^k - 1\\) possible effects for \\(k\\) factors."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-approach",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-approach",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing Approach",
    "text": "Testing Approach\n\nEach effect can be tested using linear constraint tests. However, this approach presents several challenges:\n\nMultiple testing burden: Too many tests to perform\nSample size limitations: Risk of insufficient sample sizes in each crossed modality\n\n\n\nIn practice, choices are made to include only a limited number of effects and interactions in the analysis."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-components",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-components",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model Components",
    "text": "Model Components\n\nMost general situation: we seek to explain Y using both quantitative and qualitative variables.\n\n\nQuantitative variable effects: Through each regression coefficient \\(\\beta_j\\) associated with each variable\n\n\nFactor effects and interactions: As in ANOVA analysis\n\nMain effects of factors (quali. var.)\nInteractions between factors\n\n\n\nMixed interactions: Effects of interactions between factors and quantitative variables"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-of-mixed-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-of-mixed-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Example of Mixed Interaction",
    "text": "Example of Mixed Interaction\n\n\\(A \\in \\{\\text{Labrador}, \\text{Chihuaha}\\}\\)\n\\(X^{(1)}\\): size of the dog\nThere is clearly a mixed interraction between factor \\(A\\) and \\(X^{(1)}\\)!"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#statistical-testing",
    "href": "teaching/linear_model/slides/anova_ancova.html#statistical-testing",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Statistical Testing",
    "text": "Statistical Testing\n\nEach effect can be tested using a Fisher test.\nObviously, choices must be made regarding which effects and interactions to include in the model."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-without-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-without-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model Without Interaction",
    "text": "Model Without Interaction\n\n\\(Y\\): quantitative response variable\n\n\n\\(X\\): quantitative variable,\n\\(Z\\): factor with \\(I\\) modalities \\(\\{A_1, \\dots, A_I\\}\\)\n\n\nlm(Y~X+Z) estimates the model without interaction, where, for each individual \\(k = 1, \\ldots, n\\):\n\n\\[Y_k = m + \\beta X_k + \\sum_{i=2}^{I} \\alpha_i \\mathbf{1}\\{Z_k=A_i\\} + \\varepsilon_k\\]\n\nConstraint \\(\\alpha_1 = 0\\) is adopted to make the model identifiable."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-with-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-with-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model With Interaction",
    "text": "Model With Interaction\n\nlm(Y~X+A+X:A) or lm(Y~X*A) estimates the model with interaction:\n\n\n\n\\[\n\\begin{aligned}\nY_k &= m + \\beta X_k + \\sum_{i=2}^{I} \\beta_i X_k \\mathbf{1}\\{Z_k=A_i\\} \\\\\n&+ \\sum_{i=2}^{I} \\alpha_i \\mathbf{1}\\{Z_k=A_i\\} + \\varepsilon_k\n\\end{aligned}\n\\]\n\n\n\nKey Insight: In the interaction model, the coefficient \\(\\beta\\) associated with \\(X\\) varies according to the modality \\(A_i\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#pythagorean-decomposition",
    "href": "teaching/linear_model/slides/validation.html#pythagorean-decomposition",
    "title": "Validation",
    "section": "Pythagorean Decomposition",
    "text": "Pythagorean Decomposition\n\nLet \\(\\mathbf{1}\\) be the constant column vector in \\(\\mathbb R^{n\\times 1}\\).\n\n\nIf \\(\\mathbf{1} \\in [X]\\) (eg if we consider an intercept) \\[ \\underbrace{\\|Y-\\overline Y \\1\\|^2}_{SST} = \\underbrace{\\|Y-\\widehat Y\\|^2}_{SSR}+\\underbrace{\\|\\widehat Y-\\overline Y \\1\\|^2}_{SSE}\\]\nIn the general case,\n\n\n\\[\\|Y\\|^2 = \\|Y-\\widehat Y\\|^2 + \\|\\widehat Y\\|^2\\]\nGood model if sum of squares of residuals \\(SSR \\ll 1\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#r2",
    "href": "teaching/linear_model/slides/validation.html#r2",
    "title": "Validation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\in [X]\\)\n\n\n\\[R^2 = \\frac{SSE}{SST} = 1-\\frac{SSR}{SST}\\]\n\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\not \\in [X]\\)\n\n\n\\[R^2 = \\frac{\\|\\widehat Y\\|^2}{\\|Y\\|^2} = 1 - \\frac{SSR}{\\|Y\\|^2}\\]\n\n\n\n\n\n\\(0 \\leq R^2 \\leq 1\\). Better model if \\(R^2\\) close to \\(1\\)\nTwo definitions of \\(R^2\\) when \\(\\1 \\in [X]\\) or not\nIn simple linear regression \\((Y_i = \\beta_1+\\beta_2X_i+\\varepsilon_i)\\): \\(R^2 = \\hat \\rho^2\\) is the square empirical correlation between \\(Y\\) and \\(X\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#adjusted-r2",
    "href": "teaching/linear_model/slides/validation.html#adjusted-r2",
    "title": "Validation",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nMain flaw of \\(R^2\\): adding a new variables decreases \\(R^2\\) (because \\([X]\\) is a bigger projection space)\n\n\n\n\n\n\n\n\\(R_a^2\\) if \\(\\1 \\in [X]\\)\n\n\n\\[R^2_a = 1-\\frac{n-1}{n-p}\\frac{SSR}{SST}\\]\n\n\n\n\n\n\n\n\\(R_a^2\\) if \\(\\1 \\not \\in [X]\\)\n\n\n\\[R^2_a = 1 - \\frac{n}{n-p}\\frac{SSR}{\\|Y\\|^2}\\]\n\n\n\n\n\n\nWith a new variable, \\(SSR\\) decreases but \\(p \\to p+1\\)\n\n\n\\(R_a^2\\)​ only decreases when adding a new variable if that variable significantly reduces the residual sum of squares."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#r-output-interpretation",
    "href": "teaching/linear_model/slides/validation.html#r-output-interpretation",
    "title": "Validation",
    "section": "R output interpretation",
    "text": "R output interpretation\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nHere, \\(R^2=0.9353\\) and \\(R^2_a=0.9331\\).\n\n\\(\\approx 93\\%\\) of the variability is explained by the model."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linear-constraints",
    "href": "teaching/linear_model/slides/validation.html#linear-constraints",
    "title": "Validation",
    "section": "Linear Constraints",
    "text": "Linear Constraints\n\nWe want to test q linear constraints on the coefficient vector \\(\\beta \\in \\mathbb R^p\\).\n\n\nThis is formulated as:\n\n\\[H_0: R\\beta = 0 \\quad \\text{vs} \\quad H_1: R\\beta \\neq 0\\]\n\nwhere R is a \\((q × p)\\) constraint matrix encoding the restrictions, with \\(q \\leq p\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#common-test-types",
    "href": "teaching/linear_model/slides/validation.html#common-test-types",
    "title": "Validation",
    "section": "Common Test Types",
    "text": "Common Test Types\n\n\\(H_0: R\\beta = 0 \\quad \\text{vs} \\quad H_1: R\\beta \\neq 0\\)\n\n\nStudent’s t-test: is variable \\(j\\) significant?\n\\(H_0: \\beta_j = 0\\) vs \\(H_1: \\beta_j \\neq 0\\)\nGlobal F-test: is any variable significant? Identity matrix excluding intercept\n\\(H_0: \\beta_2 = \\cdots = \\beta_p = 0\\) vs \\(H_1: \\exists j \\in \\{2,\\ldots,p\\}\\) s.t. \\(\\beta_j \\neq 0\\)\nNested model test: are q variables jointly significant?\n\\(H_0: \\beta_{p-q+1} = \\cdots = \\beta_p = 0\\) vs \\(H_1\\): the contrary"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#key-applications",
    "href": "teaching/linear_model/slides/validation.html#key-applications",
    "title": "Validation",
    "section": "Key Applications",
    "text": "Key Applications\n\nIndividual significance: Testing if a single predictor matters\nOverall model significance: Testing if the model explains anything beyond the intercept\n\nVariable subset significance: Testing if a group of variables contributes to the model"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#fisher-test",
    "href": "teaching/linear_model/slides/validation.html#fisher-test",
    "title": "Validation",
    "section": "Fisher Test",
    "text": "Fisher Test\n\n\n\nTheorem\n\n\n\n\\(SSR\\): sum of squares of residuals in the unconstrained regression model\n\\(SSR_c\\): sum of squares of residuals in the constrained regression model, i.e., in the sub-model satisfying \\(R\\beta = 0\\)\n\n\nIf \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R)=q\\) and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{{SSR}_c - {SSR}}{{SSR}} \\sim F(q, n-p)\\]\nwhere \\(F(q, n-p)\\) denotes the Fisher distribution with \\((q, n-p)\\) degrees of freedom. Elements of proof"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#rejection-region",
    "href": "teaching/linear_model/slides/validation.html#rejection-region",
    "title": "Validation",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nKey argument: \\(SSR_c - SSR\\) is equal to \\(\\|(P_{V_0^\\perp}-P_{[X]^\\perp})Y\\|^2\\), where \\(V_0=X(Ker(R))\\) and \\(\\text{dim}(V_0)=q\\).\n\n\nTherefore, the critical region at significance level \\(\\alpha\\) for testing \\(H_0: R\\beta = 0\\) against \\(H_1: R\\beta \\neq 0\\) is:\n\n\\[RC_\\alpha = \\{F &gt; f_{q,n-p}(1-\\alpha)\\}\\]\n\nwhere \\(f_{q,n-p}(1-\\alpha)\\) denotes the \\((1-\\alpha)\\)-quantile of an \\(F(q, n-p)\\) distribution."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-1-student-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-1-student-test",
    "title": "Validation",
    "section": "Particular Case 1: Student Test",
    "text": "Particular Case 1: Student Test\n\nFix some variable \\(j\\) and consider\n\n\\(H_0: \\beta_j=0\\) VS \\(H_1:\\beta_j\\neq 0\\)\n\n\n\nOnly one constraint: \\(q=1\\), so that\n\n\n\\[ F = (n-p) \\frac{SSR_c-SSR}{SSR} \\sim \\mathcal F(1,n-p) \\sim \\mathcal T^2(n-p)\\]\n\n\n\n\nIn fact, Here \\(F = \\big(\\tfrac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}}\\big)^2\\) so \\(F\\) is the squared student test statistic presented before"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-2-global-fisher-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-2-global-fisher-test",
    "title": "Validation",
    "section": "Particular Case 2: Global Fisher Test",
    "text": "Particular Case 2: Global Fisher Test\n\n\n\\(H_0: \\beta_2= \\dots = \\beta_p=0\\) VS \\(H_1\\): contrary\n\n\\(q = p-1\\) in this case\n\n\n\n\\[F = \\frac{n-p}{p-1}\\frac{SSE}{SSR} = \\frac{n-p}{p-1}\\frac{R^2}{1-R^2} \\sim \\mathcal F(p-1, n-p)\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-3-nested-fisher-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-3-nested-fisher-test",
    "title": "Validation",
    "section": "Particular Case 3: Nested Fisher Test",
    "text": "Particular Case 3: Nested Fisher Test\n\n\n\\(H_0: \\beta_{p-q+1}= \\dots = \\beta_p=0\\) VS \\(H_1\\): contrary\n\n\n\n\n\\[F = \\frac{n-p}{q}\\frac{SSR_c - SSR}{SSR} \\sim \\mathcal F(q, n-p)\\]\n\n\n\nInterpretation:\nIf \\(F \\geq f_{q,n-p}(1-\\alpha)\\) (\\(1-\\alpha\\)-quantile of Fisher dist.) then constraints are not satisfied. We do not accept the submodel with respect to larger model."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#example-of-r-output",
    "href": "teaching/linear_model/slides/validation.html#example-of-r-output",
    "title": "Validation",
    "section": "Example of R Output",
    "text": "Example of R Output\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\nHere, Fisher global significance test statistic is \\(F=419.4\\), \\(q=1\\) and \\(n-p=29\\) (\\(n=31\\)). pvalue is negligible\n\n\nHere, \\(q=1\\) and Global Fisher test is a Student Test."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#model-assumptions",
    "href": "teaching/linear_model/slides/validation.html#model-assumptions",
    "title": "Validation",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nThe linear regression model relies on the following key assumptions:\n\nModel specification: \\(Y = X\\beta + \\varepsilon\\) (linear relationship)\nFull rank design: \\(\\text{rank}(X) = p\\) (no perfect multicollinearity)\nZero mean errors: \\(\\mathbb{E}(\\varepsilon) = 0\\)\nHomoscedastic errors: \\(\\text{Var}(\\varepsilon) = \\sigma^2 I_n\\) (constant variance and uncorrelated errors)\n\n\nNow: diagnostic tools to verify each assumption and remedial strategies when they fail."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linearity-assumption",
    "href": "teaching/linear_model/slides/validation.html#linearity-assumption",
    "title": "Validation",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\nThe linearity assumption \\(\\mathbb{E}(Y) = X\\beta\\) is the fundamental hypothesis of linear regression.\nDiagnostic Tools:\n\nPre-modeling: Scatter plots \\((X^{(j)}, Y)\\) and empirical correlations for each predictor\nPost-modeling: Residual analysis - non-linearity manifests as patterns in \\(\\hat{\\varepsilon}\\)\nAdvanced: Partial residual plots (not covered here)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linearity-assumption-1",
    "href": "teaching/linear_model/slides/validation.html#linearity-assumption-1",
    "title": "Validation",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\nThe linearity assumption \\(\\mathbb{E}(Y) = X\\beta\\) is the fundamental hypothesis of linear regression.\nRemedial Strategies:\n\nTransformations: Apply transformations to \\(Y\\) and/or predictors \\(X^{(j)}\\) to achieve linearity\nAlternative models: If transformations fail, consider nonlinear regression models"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#full-rank-assumption",
    "href": "teaching/linear_model/slides/validation.html#full-rank-assumption",
    "title": "Validation",
    "section": "Full Rank Assumption",
    "text": "Full Rank Assumption\nThe condition \\(\\text{rank}(X) = p\\) ensures no predictor \\(X^{(j)}\\) is a linear combination of others.\n\nWhy it matters:\n\nIdentifiability: Without full rank, \\(\\beta\\) is not uniquely defined\nEstimation: \\((X^TX)\\) becomes non-invertible, making \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\) undefined\nInfinitely many solutions satisfy \\(X^TX\\hat{\\beta} = X^TY\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#full-rank-assumption-1",
    "href": "teaching/linear_model/slides/validation.html#full-rank-assumption-1",
    "title": "Validation",
    "section": "Full Rank Assumption",
    "text": "Full Rank Assumption\nThe condition \\(\\text{rank}(X) = p\\) ensures no predictor \\(X^{(j)}\\) is a linear combination of others.\nIn practice:\n\nPerfect collinearity is rare, so \\(\\text{rank}(X) = p\\) usually holds\nNear-collinearity is the real concern - when predictors are “almost” linearly dependent"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#near-collinearity-issues",
    "href": "teaching/linear_model/slides/validation.html#near-collinearity-issues",
    "title": "Validation",
    "section": "Near-Collinearity Issues",
    "text": "Near-Collinearity Issues\nWhen a variable is highly correlated with others (correlation close to but not exactly \\(\\pm 1\\)):\n\nMathematical consequences:\n\n\\(X^TX\\) remains invertible, but its smallest eigenvalue approaches zero\n\\((X^TX)^{-1}\\) becomes numerically unstable"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#near-collinearity-issues-1",
    "href": "teaching/linear_model/slides/validation.html#near-collinearity-issues-1",
    "title": "Validation",
    "section": "Near-Collinearity Issues",
    "text": "Near-Collinearity Issues\n\nStatistical implications:\n\nInstability: Adding/removing a single observation can drastically change \\((X^TX)^{-1}\\)\nUnreliable estimates: \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\) becomes highly unstable\nInflated variance: \\(\\text{Var}(\\hat{\\beta}) = \\sigma^2(X^TX)^{-1}\\) becomes very large\n\n\n\nThis is undesirable from a statistical point of view"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#detecting-collinearity-vif",
    "href": "teaching/linear_model/slides/validation.html#detecting-collinearity-vif",
    "title": "Validation",
    "section": "Detecting Collinearity: VIF",
    "text": "Detecting Collinearity: VIF\nCompute the VIF (Variance Inflation Factor) for each \\(X^{(j)}\\):\n\nRegress \\(X^{(j)}\\) on all other \\(X^{(k)}\\) (where \\(k \\neq j\\))\nCompute \\(R_j^2\\) from this regression\nCalculate: \\(\\text{VIF}_j = \\frac{1}{1 - R_j^2}\\)\n\n\nProperties:\n\n\\(\\text{VIF}_j \\geq 1\\) always\nHigh VIF indicates collinearity. Common threshold: \\(\\text{VIF}_j \\geq 5\\). In R: vif() from car package"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#remedies-for-multicollinearity",
    "href": "teaching/linear_model/slides/validation.html#remedies-for-multicollinearity",
    "title": "Validation",
    "section": "Remedies for Multicollinearity",
    "text": "Remedies for Multicollinearity\n\nVariable removal: Drop variables with high VIF\nPreferably remove those least correlated with \\(Y\\)\nPenalized regression: Ridge, LASSO, or elastic net methods (not covered here)\n\n\nImportant Distinction on Multicollinearity\n\nParameter estimation: Multicollinearity severely affects \\(\\hat{\\beta}\\) reliability\nPrediction: Not problematic: \\(\\hat{Y}\\) remains well-defined and stable since projection on \\([X]\\) is still unique"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#analysis-of-the-residuals",
    "href": "teaching/linear_model/slides/validation.html#analysis-of-the-residuals",
    "title": "Validation",
    "section": "Analysis of the Residuals",
    "text": "Analysis of the Residuals\n\nRecall that \\(\\hat \\varepsilon = Y - \\widehat Y = P_{[X]^{\\perp}} \\varepsilon\\)\n\n\nResidual Properties\n\n\\(\\E(\\hat{\\varepsilon}) = 0\\)\n\\(\\Var(\\hat{\\varepsilon}) = \\sigma^2P_{[X]}^{\\perp}\\)\n\\(\\text{Cov}(\\hat{\\varepsilon}, \\hat{Y}) = 0\\)\nIf \\(\\1 \\in [X]\\): \\(\\bar{\\hat{\\varepsilon}} = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#diagnostic-tools",
    "href": "teaching/linear_model/slides/validation.html#diagnostic-tools",
    "title": "Validation",
    "section": "Diagnostic Tools",
    "text": "Diagnostic Tools\n\nGraphical Assessment: Visual evaluation of model quality\nHomoscedasticity Test: Test: \\(\\Var(\\varepsilon_i) = \\sigma^2\\) for all \\(i\\) (constant variance)\nNon-correlation Test:\nTest: \\(\\Var(\\varepsilon)\\) is diagonal (uncorrelated errors)\nNormality Test: Examine normality of residuals"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#residual-vs.-fitted-plot",
    "href": "teaching/linear_model/slides/validation.html#residual-vs.-fitted-plot",
    "title": "Validation",
    "section": "Residual vs. Fitted Plot",
    "text": "Residual vs. Fitted Plot\n\nThe scatter plot between \\(\\hat{Y}\\) and \\(\\hat{\\varepsilon}\\) is informative.\nSince \\(\\text{Cov}(\\hat{\\varepsilon}, \\hat{Y}) = 0\\), no structure should appear.\nIf patterns emerge, this may indicate violations of:\n\nLinearity assumption\nHomoscedasticity assumption\n\nNon-correlation assumption\nOr a combination of these…"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#homosced.-test-breusch-pagan",
    "href": "teaching/linear_model/slides/validation.html#homosced.-test-breusch-pagan",
    "title": "Validation",
    "section": "Homosced. Test (Breusch-Pagan)",
    "text": "Homosced. Test (Breusch-Pagan)\n\nWe want to test whether \\(\\Var(\\varepsilon_i)=\\sigma^2, ~~\\forall i\\)\n\n\nPrinciple: Assume \\(\\varepsilon_i\\) has variance \\(\\sigma_i^2 = \\sigma^2 + z_i^T\\gamma\\) where:\n\n\\(z_i\\) is a \\(k\\)-vector of variables that might explain heteroscedasticity (known)\nDefault in R: \\(z_i = (X_i^{(1)}, \\ldots, X_i^{(p)})\\), so \\(k = p\\)\n\\(\\gamma\\) is an unknown \\(k\\)-dimensional parameter\n\n\n\n\\(H_0: \\gamma = 0\\) (homosced.) VS \\(H_1: \\gamma \\neq 0\\) (heterosced.)\n\n\nR function: bptest from lmtest library"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#consequences-of-heteroscedasticity",
    "href": "teaching/linear_model/slides/validation.html#consequences-of-heteroscedasticity",
    "title": "Validation",
    "section": "Consequences of Heteroscedasticity",
    "text": "Consequences of Heteroscedasticity\n\nWhat happens:\n\nOLS estimation of \\(\\beta\\) is no longer optimal but remains consistent\nInference tools (tests, confidence intervals) become invalid because they rely on \\(\\hat{\\sigma}^2\\) estimation"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#solutions-for-heteroscedasticity",
    "href": "teaching/linear_model/slides/validation.html#solutions-for-heteroscedasticity",
    "title": "Validation",
    "section": "Solutions for Heteroscedasticity",
    "text": "Solutions for Heteroscedasticity\n\nTransformation: Transform \\(Y\\) (e.g., log transformation) to stabilize variance\nModeling: Model heteroscedasticity explicitly and account for it in estimation\nGLS: Use Generalized Least Squares (not detailed here)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#non-correlation-test",
    "href": "teaching/linear_model/slides/validation.html#non-correlation-test",
    "title": "Validation",
    "section": "3. Non-correlation Test",
    "text": "3. Non-correlation Test\n\nPurpose: Test if \\(\\Var(\\varepsilon)\\) is diagonal (uncorrelated errors)\n\n\nCorrelation between \\(\\varepsilon_i\\) often occurs with temporal data (index \\(i\\) represents time)\n\n\nAuto-correlation Model of order \\(r\\):\n\n\\[\\varepsilon_i = \\rho_1 \\varepsilon_{i-1} + \\dots + \\rho_r \\varepsilon_{i-r} + \\eta_i\\] where \\(\\eta_i \\sim \\text{iid } N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#tests-for-correlation",
    "href": "teaching/linear_model/slides/validation.html#tests-for-correlation",
    "title": "Validation",
    "section": "Tests for Correlation",
    "text": "Tests for Correlation\n\nIn the auto-correlation model \\(\\varepsilon_i = \\rho_1 \\varepsilon_{i-1} + \\dots + \\rho_r \\varepsilon_{i-r} + \\eta_i\\):\n\n\nDurbin-Watson Test (for \\(r = 1\\) only):\n\n\\(H_0: \\rho_1 = 0\\) VS \\(H_1: \\rho_1 \\neq 0\\)\nR function: dwtest from lmtest\n\n\n\nBreusch-Godfrey Test (for any \\(r\\)):\n\n\\(H_0: \\rho_1 = \\cdots = \\rho_r = 0\\) VS \\(H_1:\\) at least one \\(\\rho_j \\neq 0\\)\nUser chooses order \\(r\\) (default \\(r = 1\\))\nR function: bgtest from lmtest"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#consequences-of-auto-correlation",
    "href": "teaching/linear_model/slides/validation.html#consequences-of-auto-correlation",
    "title": "Validation",
    "section": "Consequences of Auto-correlation",
    "text": "Consequences of Auto-correlation\n\nOLS estimation of \\(\\beta\\) is no longer optimal but remains consistent\nInference tools (tests, confidence intervals) become invalid\n\n\nSolutions:\n\nGLS modeling: Model the dependence structure (complex, risky if wrong)\nModel improvement: Exploit the dependence to enhance the model\nEx: Explain \\(Y_i\\) using \\(Y_{i-1}\\) in addition to \\((X_i^{(1)}, \\dots, X_i^{(p)})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#normality-test",
    "href": "teaching/linear_model/slides/validation.html#normality-test",
    "title": "Validation",
    "section": "4. Normality Test",
    "text": "4. Normality Test\n\nPurpose: Examine normality of residuals \\(\\hat \\varepsilon\\)\nReminder on Normality Assumption\n\nNot essential when \\(n\\) is large\nAll tests remain asymptotically valid\nOnly prediction intervals truly require normality\n\n\n\nPerharps the least important assumption, but we can still examine it since\nIf \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\) then \\(\\hat{\\varepsilon} \\sim N(0, \\sigma^2 P_{[X]}^{\\perp})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#diagnostic-tools-for-normality-of-hat-varepsilon",
    "href": "teaching/linear_model/slides/validation.html#diagnostic-tools-for-normality-of-hat-varepsilon",
    "title": "Validation",
    "section": "Diagnostic Tools for Normality of \\(\\hat \\varepsilon\\)",
    "text": "Diagnostic Tools for Normality of \\(\\hat \\varepsilon\\)\n\nQ-Q Plot (Henry’s line):\n\nPlot theoretical vs. sample quantiles of \\(\\hat{\\varepsilon}\\)\nR function: qqnorm\n\n\n\nShapiro-Wilk, \\(\\chi^2\\) or KS Tests:\n\nFormal test of normality for \\(\\hat{\\varepsilon}\\)\n\\(H_0\\): residuals are normally distributed\n\\(H_1\\): residuals are not normally distributed\nR function: shapiro.test"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#outlier-analysis",
    "href": "teaching/linear_model/slides/validation.html#outlier-analysis",
    "title": "Validation",
    "section": "Outlier Analysis",
    "text": "Outlier Analysis\n\nAn individual is atypical when:\n\nPoorly explained by the model, and/or\nHeavily influences coefficient estimation\n\n\n\nIdentify these individuals to:\n\nUnderstand the reason for this particularity\nPotentially modify the model accordingly\n\nPotentially exclude the individual from the study"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#poorly-explained-individuals",
    "href": "teaching/linear_model/slides/validation.html#poorly-explained-individuals",
    "title": "Validation",
    "section": "Poorly Explained Individuals",
    "text": "Poorly Explained Individuals\n\nIndividual \\(i\\) is poorly explained if its residual \\(\\hat{\\varepsilon}_i\\) is “abnormally” large.\n\n\nHow to quantify “abnormally”?\nLet \\(h_{ij}\\) be elements of matrix \\(P_{[X]}\\) (hat matrix).\nFor a Gaussian model: \\(\\hat{\\varepsilon}_i \\sim N(0, (1-h_{ii})\\sigma^2)\\)\n\n\nStandardized Residuals\n\n\\[t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#poorly-explained-individuals-1",
    "href": "teaching/linear_model/slides/validation.html#poorly-explained-individuals-1",
    "title": "Validation",
    "section": "Poorly Explained Individuals",
    "text": "Poorly Explained Individuals\n\n\n\\[h_{ij} = (P_{[X]})_{ij} \\and t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\]\n\n\n\nWe expect \\(t_i \\sim St(n-p)\\) (not strictly true since \\(\\hat{\\varepsilon}_i \\not\\perp \\hat{\\sigma}^2\\))\n\n\n\nDefinition\n\n\nIndividual \\(i\\) is considered poorly explained by the model if: \\[|t_i| &gt; t_{n-p}(1-\\alpha/2)\\] for predetermined \\(\\alpha\\), typically \\(\\alpha = 0.05\\), giving \\(t_{n-p}(1-\\alpha/2) \\approx 2\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#leverage-points",
    "href": "teaching/linear_model/slides/validation.html#leverage-points",
    "title": "Validation",
    "section": "Leverage Points",
    "text": "Leverage Points\n\nA point is influential if it contributes significantly to \\(\\hat{\\beta}\\) estimation.\n\n\nLeverage value: \\(h_{ii}\\) corresponds to the weight of \\(Y_i\\) on its own estimation \\(\\hat{Y}_i\\)\n\n\nWe know that: \\(\\sum_{i=1}^n h_{ii} = \\text{tr}(P_{[X]}) = p\\)\n\n\nTherefore, on average: \\(h_{ii} \\approx p/n\\)\n\n\n\n\n\nDefinition\n\n\nIndividual \\(i\\) is called a leverage point if \\(h_{ii} \\gg p/n\\)\nTypically: \\(h_{ii} &gt; 2p/n\\) or \\(h_{ii} &gt; 3p/n\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#outlier-analysis-cooks-distance",
    "href": "teaching/linear_model/slides/validation.html#outlier-analysis-cooks-distance",
    "title": "Validation",
    "section": "Outlier Analysis: Cook’s Distance",
    "text": "Outlier Analysis: Cook’s Distance\n\nCook’s Distance\nQuantifies the influence of individual \\(i\\) on \\(\\hat{Y}\\):\n\n\\[C_i = \\frac{\\|\\hat{Y} - \\hat{Y}_{(-i)}\\|^2}{p\\hat{\\sigma}^2}\\]\n\nwhere \\(\\hat{Y}_{(-i)} = X\\hat{\\beta}_{(-i)}\\) with \\(\\hat{\\beta}_{(-i)}\\): estimation of \\(\\beta\\) without individual \\(i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#cooks-distance-alternative-formula",
    "href": "teaching/linear_model/slides/validation.html#cooks-distance-alternative-formula",
    "title": "Validation",
    "section": "Cook’s Distance, Alternative Formula",
    "text": "Cook’s Distance, Alternative Formula\n\n\n\\[C_i = \\frac{1}{p} \\cdot \\frac{h_{ii}}{1-h_{ii}} \\cdot t_i^2,\\]\n\nwhere \\(t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\).\nThis formula shows that Cook’s distance \\(C_i\\) combines:\n\nAberrant effect of individual (through \\(t_i\\))\nLeverage effect (through \\(h_{ii}\\))\n\n\n\nR functions: cooks.distance and last plot of plot.lm"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#next",
    "href": "teaching/linear_model/slides/validation.html#next",
    "title": "Validation",
    "section": "Next",
    "text": "Next\nSelection of Models"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#ordinary-least-square-estimator-ols",
    "href": "teaching/linear_model/slides/inference.html#ordinary-least-square-estimator-ols",
    "title": "Inference",
    "section": "Ordinary Least Square Estimator (OLS)",
    "text": "Ordinary Least Square Estimator (OLS)\n\nWe observe \\(Y \\in \\mathbb R^{n\\times 1}\\) and \\(X \\in \\mathbb R^{n \\times p}\\).\n\n\nWhat is the best estimator \\(\\hat \\beta\\) of \\(\\beta\\) such that \\(Y = X\\beta + \\varepsilon\\)?\n\n\nOrdinary Least Square (OLS) Estimator:\n\\(\\newcommand{\\argmin}{\\mathrm{argmin}}\\)\n\n\\(\\hat \\beta = \\argmin_{\\beta'}\\|Y-X\\beta'\\|^2\\)\n\nHere, \\(\\|Y-X\\beta'\\|^2 = \\sum_{i=1}^n(Y_i-\\beta_1X_{i}^{(1)}- \\dots - \\beta_pX_i^{(p)})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#formula-for-hat-beta",
    "href": "teaching/linear_model/slides/inference.html#formula-for-hat-beta",
    "title": "Inference",
    "section": "Formula for \\(\\hat \\beta\\)",
    "text": "Formula for \\(\\hat \\beta\\)\n\nIf \\(\\mathrm{rk}(X) = p\\), it holds that\n\n\\[\\hat \\beta = (X^TX)^{-1}X^TY\\]\n\n\n\nproof"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\n\nLet \\([X]\\) be the subspace of \\(\\mathbb R^p\\) generated by columns of \\(X\\):\n\n\n\\([X]=\\mathrm{Im}(X)=\\mathrm{Span}(X^{(1)},\\dots, X^{(p)})=\\{X\\alpha, \\alpha \\in \\mathbb R^p\\}\\)\n\n\n\n\nThen,\n\n\\(X\\hat \\beta = X(X^TX)^{-1}X^TY\\)\n\nis the projection of \\(Y\\) on \\([X]\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-1",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-1",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\n\n\n\\[P_{[X]} = X(X^TX)^{-1}X^T \\and \\widehat{Y} = P_{[X]}Y = X\\hat \\beta\\]\n\n\n\nCheck that \\(P_{[X]}\\) is the orthogonal projector on \\([X]\\)\n\n\nThat is \\(P_{[X]}^2 = P_{[X]}\\), \\(P_{[X]}=P_{[X]}^T\\) and \\(\\mathrm{Im} (P_{[X]}) = [X]\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#pythagorean-decomposition-of-y",
    "href": "teaching/linear_model/slides/inference.html#pythagorean-decomposition-of-y",
    "title": "Inference",
    "section": "Pythagorean Decomposition of \\(Y\\)",
    "text": "Pythagorean Decomposition of \\(Y\\)\n\nWe can decompose\n\n\\[Y = \\underbrace{P_{[X]}Y}_{\\widehat Y} + \\underbrace{(I-P_{[X]})Y}_{\"residuals\"}\\]\n\n\n\nNotice that \\(I-P_{[X]}= P_{[X]^{\\perp}}\\) is the orthogonal projection on \\([X]^{\\perp} = \\{\\alpha\\in \\mathbb R^n:~ X^T\\alpha =0\\}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#illustration",
    "href": "teaching/linear_model/slides/inference.html#illustration",
    "title": "Inference",
    "section": "Illustration",
    "text": "Illustration\n\n(image: elements of statistical learning. In yellow: \\([X]\\) with \\(p=2\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-on-hat-beta",
    "href": "teaching/linear_model/slides/inference.html#properties-on-hat-beta",
    "title": "Inference",
    "section": "Properties on \\(\\hat \\beta\\)",
    "text": "Properties on \\(\\hat \\beta\\)\n\n\n\n\nExpectation and variance\n\n\nAssume that \\(rk(X)=p\\), \\(\\mathbb E[\\varepsilon] = 0\\) and \\(\\mathbb V(\\varepsilon) = \\sigma^2I_n\\). Then,\n\n\\(\\hat \\beta = (X^TX)^{-1}X^T Y\\) is a linear estimator\n\\(\\mathbb E[\\hat \\beta] = \\beta\\) unbiased estimator\n\\(\\mathbb V(\\hat \\beta) = \\sigma^2(X^TX)^{-1}\\)\n\n\n\n\n\n\nProof"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#gauss-markov-theorem",
    "href": "teaching/linear_model/slides/inference.html#gauss-markov-theorem",
    "title": "Inference",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\n\n\n\nGauss-Markov Theorem\n\n\nUnder the same assumptions, if \\(\\tilde \\beta\\) is another linear and unbiased estimator then \\[\\mathbb V(\\hat \\beta) \\preceq \\mathbb V(\\tilde \\beta),\\]\nwhere \\(A\\preceq B\\) means that \\(B-A\\) is a symmetric positive semidefinite matrix\n\n\n\n\n\nProof"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#residuals",
    "href": "teaching/linear_model/slides/inference.html#residuals",
    "title": "Inference",
    "section": "Residuals",
    "text": "Residuals\n\nWe define the residuals \\(\\hat \\varepsilon\\) as\n\n\\[\n\\hat \\varepsilon = Y- \\hat Y = Y-X\\hat \\beta\n\\]\n\n\n\nIt can be computed from the data.\n\n\nIt is the orthogonal projection of \\(Y\\) on \\([X]^{\\perp}\\):\n\n\n\n\\[Y = \\underbrace{P_{[X]}Y}_{\\widehat Y} + \\underbrace{(I-P_{[X]})Y}_{\\hat \\varepsilon=\"residuals\"}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#residuals-1",
    "href": "teaching/linear_model/slides/inference.html#residuals-1",
    "title": "Inference",
    "section": "Residuals",
    "text": "Residuals\n\n\n\\[\n\\begin{aligned}\nY &= X\\beta + \\varepsilon & \\quad \\text{(model)} \\\\\nY &= \\widehat Y + \\hat \\varepsilon & \\quad \\text{(estimation)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-on-residuals",
    "href": "teaching/linear_model/slides/inference.html#properties-on-residuals",
    "title": "Inference",
    "section": "Properties on residuals",
    "text": "Properties on residuals\n\n\n\n\nExpectation and Variance of \\(\\hat\\varepsilon\\)\n\n\nIf \\(rk(X)=p\\), \\(\\mathbb E[\\varepsilon] = 0\\) and \\(\\mathbb V(\\varepsilon)=\\sigma^2I_n\\), then\n\n\\(\\mathbb E[\\hat\\varepsilon]=0\\)\n\\(\\mathbb V(\\hat \\varepsilon) = \\sigma^2P_{[X]^\\perp} = \\sigma^2(I_n - X(X^TX)^{-1}X^T)\\)\n\n\n\n\n\n\nRemark: if a constant vector is in \\([X]\\), e.g. \\(\\forall i,X_i^{(1)}=1\\), then \\(\\hat \\varepsilon \\perp \\mathbf 1\\) and\n\\[\n\\overline{\\hat\\varepsilon} = \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i = 0 \\and \\overline{\\widehat Y} = \\overline Y\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#estimation-of-sigma2-1",
    "href": "teaching/linear_model/slides/inference.html#estimation-of-sigma2-1",
    "title": "Inference",
    "section": "Estimation of \\(\\sigma^2\\)",
    "text": "Estimation of \\(\\sigma^2\\)\n\nRecall that in the model \\(Y=X\\beta + \\varepsilon\\), both \\(\\beta\\) and \\(\\sigma^2=\\mathbb E[\\varepsilon_i^2]\\) are unknown (\\(p+1\\) parameters)\n\n\n\n\\[\n\\hat\\varepsilon = Y- \\hat Y = P_{[X]^\\perp}\\varepsilon \\and dim([X]^{\\perp}) = n-p\n\\]\n\n\n\nWe estimate \\(\\sigma^2\\) with\n\n\\[\\hat \\sigma^2 = \\frac{1}{n-p}\\sum_{i=1}^n \\hat \\varepsilon_i^2\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-of-hat-sigma2",
    "href": "teaching/linear_model/slides/inference.html#properties-of-hat-sigma2",
    "title": "Inference",
    "section": "Properties of \\(\\hat \\sigma^2\\)",
    "text": "Properties of \\(\\hat \\sigma^2\\)\n\n\n\n\nProposition\n\n\nIf \\(rk(X)=p\\), \\(\\E[\\varepsilon]=0\\) and \\(\\Var(\\varepsilon)= \\sigma^2I_n\\), then\n\\[\\hat \\sigma^2 = \\frac{1}{n-p}\\sum_{i=1}^n \\hat \\varepsilon_i^2=\\frac{\\mathrm{SSR}}{n-p}\\]\nis an unbiased estimator of \\(\\sigma^2\\). If moreover the \\(\\varepsilon_i\\)’s are iid, then \\(\\hat \\sigma^2\\) is a consistent estimator. Proof\n\n\n\n\nunbiased: \\(\\E[\\hat \\sigma^2]= \\sigma^2\\)\nconsistent: \\(\\hat \\sigma^2 \\to \\sigma^2\\) in \\(L^2\\) as \\(n\\to +\\infty\\)\nSSR: Sum of squared residuals"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#gaussian-model",
    "href": "teaching/linear_model/slides/inference.html#gaussian-model",
    "title": "Inference",
    "section": "Gaussian Model",
    "text": "Gaussian Model\n\nUntil then, we assumed that \\(Y=X\\beta+\\varepsilon\\), where \\(\\E[\\varepsilon]=0\\) and \\(\\Var(\\varepsilon)= \\sigma^2I_n\\).\n\n\nThe \\(\\varepsilon_i\\) are uncorrelated but there can be dependency\n\n\nNo assumption was made on the distribution of \\(\\varepsilon\\)\n\n\nNow (Gaussian Model):\n\n\\[\\varepsilon \\sim \\mathcal N(0, \\sigma^2I_n) \\quad \\text{i.e.} \\quad Y \\sim \\mathcal N(X\\beta, \\sigma^2I_n)\\]\n\n\n\nEquivalently we assume that the \\(\\varepsilon_i\\)’s are iid \\(\\mathcal N(0, \\sigma^2)\\).\n\n\nIn this simpler model, we can do maximum likelihood estimation (MLE)!"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#maximum-likelihood-estimation",
    "href": "teaching/linear_model/slides/inference.html#maximum-likelihood-estimation",
    "title": "Inference",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\n\n\n\nMLE\n\n\nLet \\(\\hat \\beta_{MLE}\\) and \\(\\hat \\sigma_{MLE}^2\\) be the MLE of \\(\\beta\\) and \\(\\sigma^2\\), respectively.\n\n\\(\\hat{\\beta}_{MLE} = \\hat{\\beta}\\) et \\(\\hat{\\sigma}^2_{MLE} = \\frac{SCR}{n} = \\frac{n-p}{n} \\hat{\\sigma}^2\\).\n\\(\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\\).\n\\(\\frac{n-p}{\\sigma^2} \\hat{\\sigma}^2 = \\frac{n}{\\sigma^2} \\hat{\\sigma}^2_{MLE} \\sim \\chi^2(n - p)\\).\n\\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent\n\n\n\n\n\n\nProof"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#efficient-estimator",
    "href": "teaching/linear_model/slides/inference.html#efficient-estimator",
    "title": "Inference",
    "section": "Efficient Estimator",
    "text": "Efficient Estimator\n\n\n\n\nTheorem\n\n\nIn the Gaussian Model, \\(\\hat \\beta\\) is an efficient estimator of \\(\\hat \\beta\\). This means that \\[\n\\Var(\\hat \\beta) \\preceq \\Var(\\tilde \\beta)\\; ,\n\\] for any unbiased estimator \\(\\tilde \\beta\\). See Proof\n\n\n\n\nThis is stronger than Gauss-Markov\nBetter than any unbiased \\(\\tilde \\beta\\), not only linear \\(\\tilde  \\beta\\). \\(\\hat \\beta\\) is “BUE” (Best Unbiased Estimator)\nIf \\(n\\) is large, most of the result in Gaussian case remains valid."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#pivotal-distribution-in-gaussian-case",
    "href": "teaching/linear_model/slides/inference.html#pivotal-distribution-in-gaussian-case",
    "title": "Inference",
    "section": "Pivotal Distribution in Gaussian Case",
    "text": "Pivotal Distribution in Gaussian Case\n\nRecall that \\(\\hat \\sigma^2 = \\frac{1}{n-p}\\|\\hat \\varepsilon\\|^2\\) and \\(\\hat \\beta \\sim \\mathcal N(\\beta, \\sigma^2(X^TX)^{-1})\\)\n\n\n\n\n\nProperty\n\n\nIn the Gaussian model,\n\\[ \\frac{\\hat \\beta_j - \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) \\]\n(Student Distribution of degree \\(n-p\\), \\((X^TX)^{-1}_{jj}\\) is the \\(j^{th}\\) element of the matrix \\((X^TX)^{-1}\\))\n\n\n\n\n\n\n\\(\\mathbb V(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\\) implies that \\(\\mathbb V(\\hat{\\beta}_j) = \\sigma^2 (X^T X)^{-1}_{jj}\\). \\(\\hat{\\sigma}^2_{\\hat{\\beta}_j}:=\\hat\\sigma^2 (X^T X)^{-1}\\) is an estimator of \\(\\mathbb V(\\hat{\\beta}_j)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#student-signifactivity-test",
    "href": "teaching/linear_model/slides/inference.html#student-signifactivity-test",
    "title": "Inference",
    "section": "Student Signifactivity Test",
    "text": "Student Signifactivity Test\n\nWe observe \\(Y = X\\beta+\\varepsilon\\), where \\(\\beta \\in \\mathbb R^p\\) is unknown.\n\n\nWe want to test whether the \\(j^{th}\\) feature \\(X^{(j)}\\) is significant in the LM, that is:\n\n\\(H_0: \\beta_j =0 \\VS H_1: \\beta_j \\neq 0\\).\n\nWe use the test statistic\n\n\n\\[\\psi_j(X,Y)= \\frac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}} = \\frac{\\hat \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) ~~\\text{(under $H_0$)}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#student-signifactivity-test-1",
    "href": "teaching/linear_model/slides/inference.html#student-signifactivity-test-1",
    "title": "Inference",
    "section": "Student Signifactivity Test",
    "text": "Student Signifactivity Test\n\n\n\n\\[\\psi_j(X,Y)= \\frac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}} = \\frac{\\hat \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) ~~\\text{(under $H_0$)}\\]\n\n\n\n\nWe reject if \\(|\\psi(X,Y)| \\geq t_{1-\\alpha/2}\\), where \\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of \\(\\mathcal T(n-p)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#p-value-of-student-test",
    "href": "teaching/linear_model/slides/inference.html#p-value-of-student-test",
    "title": "Inference",
    "section": "P-value of Student Test",
    "text": "P-value of Student Test\n\n\n\\[p_{value}=2\\min\\left(F\\left(\\frac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}}\\right), 1-F\\left(\\frac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}}\\right)\\right)\\]\n\nwhere \\(F\\) is the cdf of \\(\\mathcal T(n-p)\\).\n\n\nIf we get \\(\\beta_j=0\\), we can remove \\(X^{(j)}\\) from the model."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#confidence-interval",
    "href": "teaching/linear_model/slides/inference.html#confidence-interval",
    "title": "Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\nConfidence interval with proba \\(1-\\alpha\\) around \\(\\beta_j\\):\n\n\\[CI_{1-\\alpha} = [\\hat \\beta_j \\pm t\\hat \\sigma_{\\hat \\beta_j}]\\]\n\n\nHere, \\(t\\) is the \\(1-\\alpha/2\\) quantile of \\(\\mathcal T(n-p)\\)\nRecall that \\(\\hat \\sigma_{\\hat \\beta_j} = \\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}\\)\n\n\n\nWe check that\n\\[\n\\P(\\beta \\in CI_{1-\\alpha}) = 1- \\alpha\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#remarks",
    "href": "teaching/linear_model/slides/inference.html#remarks",
    "title": "Inference",
    "section": "Remarks",
    "text": "Remarks\n\nThis is what is computed on \\(R\\)\nThis is valid in Gaussian case, but is also true when \\(n\\) is large for other distributions"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-setting",
    "href": "teaching/linear_model/slides/inference.html#prediction-setting",
    "title": "Inference",
    "section": "Prediction Setting",
    "text": "Prediction Setting\n\nConsider the LM \\(Y = X\\beta + \\varepsilon\\).\n\n\nFrom previous slides, we estimate \\((\\beta, \\sigma)\\) with \\((\\hat \\beta, \\hat \\sigma)\\) from observations \\(Y\\) and matrix \\(X=(X^{(1)}, \\dots, X^{(p)})\\).\n\n\nWe observe a new individual \\(o\\), with unknown \\(Y_o\\) and vector \\(X_o=X_{o,\\cdot}=(X^{(1)}_o, \\dots, X^{(p)}_o)\\), and independent noise \\(\\varepsilon_o\\).\n\n\nwith this definition, \\(X_o\\) is a row vector and\n\n\\[Y_o = X_{o}\\beta + \\varepsilon_o = \\beta_1X^{(1)}_o + \\dots + \\beta_p X^{(p)}_o+\\varepsilon_o\\]\n\nHere, \\(\\mathbb E[\\varepsilon_o] = 0\\) and \\(\\mathbb V(\\varepsilon_o)=\\sigma^2\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-1",
    "href": "teaching/linear_model/slides/inference.html#prediction-1",
    "title": "Inference",
    "section": "Prediction",
    "text": "Prediction\n\nWe want predict \\(Y_o\\) (unknown) from \\(X_o\\) (known). Natural predictor:\n\n\\(\\hat Y_o = X_o \\hat \\beta\\)\n\n\n\nPrediction error \\(Y_o - \\hat Y_o\\) decomposes in two terms:\n\n\\[Y_o - \\hat Y_o = \\underbrace{X_o(\\beta - \\hat \\beta)}_{\\text{Estimation error}} + \\underbrace{\\varepsilon_o}_{\\text{Random error}}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#error-decomposition",
    "href": "teaching/linear_model/slides/inference.html#error-decomposition",
    "title": "Inference",
    "section": "Error Decomposition",
    "text": "Error Decomposition\n\n\n\n\\[Y_o - \\hat Y_o = \\underbrace{X_o(\\beta - \\hat \\beta)}_{\\text{Estimation error}} + \\underbrace{\\varepsilon_o}_{\\text{Random error}}\\]\n\n\n\n\n\\(\\E(Y_o - \\hat Y_o)\\): prediction error is \\(0\\) on average\n\n\n\\[\\begin{aligned}\n\\Var(Y_o - \\hat Y_o) &= \\color{blue}{ \\Var(X_o(\\beta - \\hat \\beta))} + \\color{red}{\\Var(\\varepsilon_o)} \\\\\n&=\\color{blue}{\\sigma^2X_o(X^TX)^{-1}X_o} + \\color{red}{\\sigma^2} \\\\\n\\end{aligned}\\]\n\n\n\\(\\color{blue}{\\Var(X_o(\\beta - \\hat \\beta))\\to 0}\\) when \\(n \\to +\\infty\\) but \\(\\color{red}{\\Var(\\varepsilon_o) =\\sigma^2}\\)\n\n\nEstimation error is negligible when \\(n \\to +\\infty\\) but random error is incompressible."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-interval",
    "href": "teaching/linear_model/slides/inference.html#prediction-interval",
    "title": "Inference",
    "section": "Prediction Interval",
    "text": "Prediction Interval\n\nIn the Gaussian model, \\(Y_o - \\hat Y_o \\sim \\mathcal N(0,  \\sigma^2X_o(X^TX)^{-1}X_o^T+\\sigma^2)\\).\n\n\nSince \\(\\hat \\sigma\\) is indep of \\(\\hat Y_o\\) (check this with projections!),\n\n\n\\[\\frac{Y_o - \\hat Y_o}{\\hat \\sigma\\sqrt{X_o(X^TX)^{-1}X_o^T+1}} \\sim \\mathcal T(n-p)\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-interval-1",
    "href": "teaching/linear_model/slides/inference.html#prediction-interval-1",
    "title": "Inference",
    "section": "Prediction Interval",
    "text": "Prediction Interval\n\nWe deduce the prediction interval\n\n\n\\(PI_{1-\\alpha}(Y_o)=\\left[\\hat Y_o \\pm t\\sqrt{\\color{blue}{\\hat \\sigma^2X_o(X^TX)^{-1}X_o^T} + \\color{red}{\\hat \\sigma^2}}\\right]\\)\n\n\nwhere \\(t\\) is the \\(1-\\alpha/2\\) quantile of \\(\\mathcal T(n-p)\\).\n\n\nWe have \\(\\P(Y_o \\in PI_{1-\\alpha})= 1-\\alpha\\)\n\n\nIf we only want to estimate \\(\\mathbb E[Y_o]=X_o \\beta\\), (point on the hyperplane), we get the confidence interval\n\n\n\\(CI_{1-\\alpha}(X_o\\beta)=\\left[\\hat Y_o \\pm \\sqrt{\\color{blue}{\\hat \\sigma^2X_o(X^TX)^{-1}X_o^T}}\\right]\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#example",
    "href": "teaching/linear_model/slides/inference.html#example",
    "title": "Inference",
    "section": "Example",
    "text": "Example\nFor 31 trees, we record their volume and their girth."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#model",
    "href": "teaching/linear_model/slides/inference.html#model",
    "title": "Inference",
    "section": "Model",
    "text": "Model\n\nVolume = \\(\\beta_1\\) + \\(\\beta_2\\) Girth + \\(\\varepsilon\\)\n# In R\nreg=lm(Volume ~ Girth, data=trees)\nsummary(reg)\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#results",
    "href": "teaching/linear_model/slides/inference.html#results",
    "title": "Inference",
    "section": "Results",
    "text": "Results\n\nEstimation:\n\n\n\\[\\hat \\beta_1=-36.9 \\and \\hat \\beta_2=5.07\\] \\[\\hat \\sigma_{\\hat \\beta_1} =3.4 \\and \\hat \\sigma_{\\hat \\beta_2} =0.25\\]\n\n\n\n\nStatistics:\n\n\n\\[\\frac{\\hat \\beta_1}{\\hat \\sigma_{\\hat \\beta_1}}=-10.98 \\and \\frac{\\hat \\beta_2}{\\hat \\sigma_{\\hat \\beta_2}}=20.48\\]\n\n\n\n\n\\(Pr(&gt;|t|)\\): pvalues of the student tests. Last row: \\(\\hat \\sigma=4.25\\) and df."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#illustration-1",
    "href": "teaching/linear_model/slides/inference.html#illustration-1",
    "title": "Inference",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#r-code",
    "href": "teaching/linear_model/slides/inference.html#r-code",
    "title": "Inference",
    "section": "R code",
    "text": "R code\nrequire(stats); require(graphics)\npairs(trees, main = \"trees data\")\ntrees[,c(\"Girth\", \"Volume\")]\nreg &lt;- lm(Volume ~ Girth, data = trees)\n# Create sequence of x values for smooth curves\nx_seq &lt;- seq(min(trees$Girth), max(trees$Girth), length.out = 100)\n\n# Calculate confidence and prediction intervals\nconf_int &lt;- predict(reg, newdata = data.frame(Girth = x_seq), \n                   interval = \"confidence\", level = 0.95)\npred_int &lt;- predict(reg, newdata = data.frame(Girth = x_seq), \n                   interval = \"prediction\", level = 0.95)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#next",
    "href": "teaching/linear_model/slides/inference.html#next",
    "title": "Inference",
    "section": "Next",
    "text": "Next\nValidation"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html",
    "href": "teaching/linear_model/notes/cramer-rao.html",
    "title": "Cramér-Rao Bound",
    "section": "",
    "text": "AI was used to assist with the formatting and writing of the proofs on this page."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#setup",
    "href": "teaching/linear_model/notes/cramer-rao.html#setup",
    "title": "Cramér-Rao Bound",
    "section": "Setup",
    "text": "Setup\nLet:\n\n\\(\\beta \\in \\mathbb R^p\\) be a vector of parameters\n\\(X=(X_1, \\dots, X_n) \\in \\mathbb R^n\\) be observations with joint pdf \\(f\\)\n\\(\\tilde \\beta\\) be an unbiased estimator of \\(\\beta\\), so \\(\\mathbb E[\\tilde\\beta]= \\mathbb E_{X\\sim f}[\\tilde\\beta] = \\beta\\)\n\\(s(x; \\beta) = \\nabla_{\\beta} \\log f(x; \\beta)\\) be the derivative of the log-likelihood"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#key-definitions",
    "href": "teaching/linear_model/notes/cramer-rao.html#key-definitions",
    "title": "Cramér-Rao Bound",
    "section": "Key Definitions",
    "text": "Key Definitions\nThe Fisher Information Matrix is: \\[I(\\beta) = E[s(x; \\beta)s(x; \\beta)^T]\\]\nUnder regularity conditions, this equals: \\[I(\\beta) = -E\\left[\\frac{\\partial^2 \\log f(x; \\beta)}{\\partial \\beta \\partial \\beta^T}\\right]\\]\n\n\n\n\n\n\nCramér-Rao (vector version)\n\n\n\nIn this context, it holds that \\[[I(\\beta)]^{-1} \\preceq \\mathbb V(\\tilde \\beta) \\; .\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\\(I(\\beta)\\) and \\(\\mathbb V(\\tilde \\beta)\\) are matrices\n\\(I(\\beta)\\) does not depend on the estimator, unlike \\(\\mathbb V(\\tilde \\beta)\\)."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#matrix-cauchy-schwarz-inequality",
    "href": "teaching/linear_model/notes/cramer-rao.html#matrix-cauchy-schwarz-inequality",
    "title": "Cramér-Rao Bound",
    "section": "Matrix Cauchy-Schwarz Inequality",
    "text": "Matrix Cauchy-Schwarz Inequality\nFor random vectors \\(U \\in \\mathbb R^p\\) and \\(V \\in \\mathbb R^q\\), the covariance satisfies: \\[\\text{Cov}(U, V)^T [\\text{Var}(V)]^{-1} \\text{Cov}(U, V) \\preceq \\text{Var}(U)\\]\nwhere \\(A\\preceq B\\) means \\(B-A\\) is positive semidefinite."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#proof-of-cramér-rao-bound",
    "href": "teaching/linear_model/notes/cramer-rao.html#proof-of-cramér-rao-bound",
    "title": "Cramér-Rao Bound",
    "section": "Proof of Cramér-Rao Bound",
    "text": "Proof of Cramér-Rao Bound\nSince \\(\\tilde \\beta\\) is unbiased, \\(\\mathbb E[\\tilde\\beta]\\) = \\(\\beta\\). Differentiating both sides with respect to \\(\\beta\\): \\[\\frac{\\partial}{\\partial \\beta} \\int \\tilde \\beta(x) f(x; \\beta) dx = I_p\\]\nwhere \\(I_p\\) is the p×p identity matrix.\nBy interchanging differentiation and integration (under regularity conditions): \\[\\int \\tilde \\beta(x) \\big(\\nabla_{\\beta}f(x; \\beta)\\big)^T dx = I_p\\]\nUsing the identity \\(\\nabla_{\\beta} f(x;\\beta)= f(x;\\beta)\\cdot\\nabla_{\\beta} \\log(f(x;\\beta))\\): \\[\\int \\tilde \\beta(x) f(x; \\beta) s(x; \\beta)^T dx = I_p\\]\nThis gives us: \\[E[\\tilde \\beta s^T] = I_p\\]\nSince \\(\\mathbb E[s]=0\\) (under regularity conditions), we have: \\[\\text{Cov}(\\tilde \\beta, s) = E[\\tilde \\beta s^T] - \\mathbb E[\\tilde\\beta]E[s]^T = I_p\\]\nApply the matrix Cauchy-Schwarz inequality with \\(U=\\tilde \\beta\\) and \\(V = s\\): \\[\\text{Cov}(\\tilde \\beta, s)^T [\\text{Var}(s)]^{-1} \\text{Cov}(\\tilde \\beta, s) \\preceq \\text{Var}(\\tilde \\beta)\\]\nSubstituting our results:\n\n\\(\\mathrm{Cov}(\\tilde \\beta, s) = I_p\\)\n\\(\\mathbb V(s) = I(\\beta)\\) (the Fisher Information Matrix)\n\nWe get: \\[I_p^T [I(\\beta)]^{-1} I_p \\preceq \\mathbb V(\\tilde \\beta)\\]\nSimplifying: \\[[I(\\beta)]^{-1} \\preceq \\text{Var}(\\tilde \\beta)\\]\nThis is the Cramér-Rao Lower Bound for vector parameters."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#interpretation",
    "href": "teaching/linear_model/notes/cramer-rao.html#interpretation",
    "title": "Cramér-Rao Bound",
    "section": "Interpretation",
    "text": "Interpretation\n\nFor any unbiased estimator \\(\\tilde \\beta\\) of \\(\\beta\\), its covariance matrix is bounded below by the inverse of the Fisher Information Matrix.\nFor a scalar function \\(c^T \\beta\\), we have: \\(\\mathbb V(c^T \\tilde \\beta) \\succeq c^T[I(\\beta)]^{-1}c\\)\n\nThis generalizes the scalar Cramér-Rao bound to the multivariate case using matrix inequalities."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#theorem",
    "href": "teaching/linear_model/notes/cramer-rao.html#theorem",
    "title": "Cramér-Rao Bound",
    "section": "Theorem",
    "text": "Theorem\nFor random vectors \\(U\\in \\mathbb R^p\\) and \\(V \\in \\mathbb R^q\\) with finite second moments, if \\(\\mathbb V(V)\\) is invertible, then: \\[\\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\text{Cov}(V, U) \\preceq \\text{Var}(U)\\]"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#proof",
    "href": "teaching/linear_model/notes/cramer-rao.html#proof",
    "title": "Cramér-Rao Bound",
    "section": "Proof",
    "text": "Proof\nFor any matrix \\(A \\in \\mathbb R^{p \\times q}\\), consider: \\[\\text{Var}(U - AV) = \\text{Var}(U) - A\\text{Cov}(V, U) - \\text{Cov}(U, V)A^T + A\\text{Var}(V)A^T\\]\nTo minimize this quadratic form in A, take the derivative and set to zero: \\[\\frac{\\partial}{\\partial A} = -2\\text{Cov}(U, V) + 2A\\text{Var}(V) = 0\\]\nSolving gives: \\[A^* = \\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\]\nAt this minimum: \\[\\text{Var}(U - A^*V) = \\text{Var}(U) - \\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\text{Cov}(V, U) \\succeq 0\\]\nThis gives the generalized CS inequality."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#connection-to-scalar-case",
    "href": "teaching/linear_model/notes/cramer-rao.html#connection-to-scalar-case",
    "title": "Cramér-Rao Bound",
    "section": "Connection to Scalar Case",
    "text": "Connection to Scalar Case\nWhen U and V are scalars, this reduces to: \\[\\frac{[\\text{Cov}(U,V)]^2}{\\text{Var}(V)} \\leq \\text{Var}(U)\\]\nWhich is equivalent to the familiar form: \\[[\\text{Cov}(U,V)]^2 \\leq \\text{Var}(U)\\text{Var}(V)\\]\nThe matrix version generalizes this to higher dimensions using positive semidefiniteness instead of simple inequality."
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html",
    "href": "teaching/linear_model/lectures/validation.html",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "If \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R) = q\\), and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR} \\sim F(q, n-p)\\]\nwhere:\n\n\\(SSR\\): sum of squares of residuals in the unconstrained model\n\\(SSR_c\\): sum of squares of residuals in the constrained model satisfying \\(R\\beta = 0\\)\n\n\n\n\nDefine\n\n\\(V_0 = \\{X\\beta : R\\beta = 0\\} = X(\\text{Ker}(R))\\) (constrained subspace)\n\nSince \\(\\text{rank}(X) = p\\) and \\(\\text{rank}(R) = q\\), it holds that \\(\\dim(\\text{Ker}(R)) = p - q\\).\nSince \\(X\\) has full column rank, the \\(\\beta \\mapsto X\\beta\\) is injective Therefore: \\(\\dim(V_0) = \\dim(X(\\text{Ker}(R))) = \\dim(\\text{Ker}(R)) = p - q\\).\nWe have that\n\n\\(SSR = \\|P_{[X]^\\perp} Y\\|^2\\)\n\\(SSR_c = \\|P_{V_0^\\perp}Y\\|^2\\)\n\nLet us now decompose\n\\[\nY = P_{[X]^\\perp} Y + (P_{V_0^\\perp} - P_{[X]^\\perp})Y\n\\]\nThe first term correspond to the residuals, and let us now analyse the second term. For this we use the following useful Lemma.\n\n\n\n\n\n\nUseful Lemma\n\n\n\nIf \\(E\\) \\(F\\) are two subspace of \\(\\mathbb R^n\\) such that \\(E \\subset F\\), and if \\(P_E\\), \\(P_F\\) are the orthogonal projectors on \\(E\\) and \\(F\\), then\n\\(P_EP_F = P_FP_E = P_E\\).\n\n\nHence, projecting the above decomposition on \\(V_0^\\perp\\), we get\n\n\\[\nP_{V_0^\\perp}Y = P_{[X]^\\perp} Y + (P_{V_0^\\perp} - P_{[X]^\\perp})Y\n\\]\n\nHere, \\(V_0 \\subset [X]\\) so that \\([X]^\\perp \\subset V_0^{\\perp}\\), and we check that\n\\((P_{V_0^\\perp} - P_{[X]^\\perp})^2 = (P_{V_0^\\perp} - P_{[X]^\\perp})= (P_{V_0^\\perp} - P_{[X]^\\perp})^T\\)\nso that \\(P_{V_0^\\perp} - P_{[X]^\\perp}\\) is an orthogonal projector on a subspace of dimension \\(tr(V_0^\\perp) - tr(P_{[X]^\\perp}) = n-(p-q)-(n-p) = q\\)\nSince \\(P_{[X]^\\perp}(P_{V_0^\\perp} - P_{[X]^\\perp}) = P_{[X]^\\perp}P_{[X]^\\perp}-P_{[X]^\\perp}P_{[X]^\\perp} = 0\\), we also have that \\(P_{[X]^\\perp} Y\\) and \\((P_{V_0^\\perp} - P_{[X]^\\perp})Y\\) are orthogonal.\nHence, by the Cochran Theorem, \\(P_{[X]^\\perp} Y\\) and \\((P_{V_0^\\perp} - P_{[X]^\\perp})Y\\) are indpendent and\n\n\\(SSR=\\|P_{[X]^\\perp} Y\\|^2 = \\|P_{[X]^\\perp} \\varepsilon\\|^2\\) has distribution \\(\\chi^2(n-p)\\)\nUnder \\(H_0\\), \\(X\\beta \\in V_0\\) so that \\(SSR_c=\\|P_{V_0^\\perp}Y\\|^2 = P_{V_0^\\perp}(X\\beta + \\varepsilon) = P_{V_0^\\perp}(\\varepsilon)\\)\nHence, sill under \\(H_0\\), \\(SSR_c -SSR = \\|(P_{V_0^\\perp} - P_{[X]^\\perp}) \\varepsilon\\|^2\\) has distribution \\(\\chi^2(q)\\)\n\\(SSR\\) and \\(SSR_c - SSR\\) are independent since they are projections on orthogonal subspaces of \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2I_n)\\)"
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html#theorem",
    "href": "teaching/linear_model/lectures/validation.html#theorem",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "If \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R) = q\\), and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR} \\sim F(q, n-p)\\]\nwhere:\n\n\\(SSR\\): sum of squares of residuals in the unconstrained model\n\\(SSR_c\\): sum of squares of residuals in the constrained model satisfying \\(R\\beta = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html#proof",
    "href": "teaching/linear_model/lectures/validation.html#proof",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "Define\n\n\\(V_0 = \\{X\\beta : R\\beta = 0\\} = X(\\text{Ker}(R))\\) (constrained subspace)\n\nSince \\(\\text{rank}(X) = p\\) and \\(\\text{rank}(R) = q\\), it holds that \\(\\dim(\\text{Ker}(R)) = p - q\\).\nSince \\(X\\) has full column rank, the \\(\\beta \\mapsto X\\beta\\) is injective Therefore: \\(\\dim(V_0) = \\dim(X(\\text{Ker}(R))) = \\dim(\\text{Ker}(R)) = p - q\\).\nWe have that\n\n\\(SSR = \\|P_{[X]^\\perp} Y\\|^2\\)\n\\(SSR_c = \\|P_{V_0^\\perp}Y\\|^2\\)\n\nLet us now decompose\n\\[\nY = P_{[X]^\\perp} Y + (P_{V_0^\\perp} - P_{[X]^\\perp})Y\n\\]\nThe first term correspond to the residuals, and let us now analyse the second term. For this we use the following useful Lemma.\n\n\n\n\n\n\nUseful Lemma\n\n\n\nIf \\(E\\) \\(F\\) are two subspace of \\(\\mathbb R^n\\) such that \\(E \\subset F\\), and if \\(P_E\\), \\(P_F\\) are the orthogonal projectors on \\(E\\) and \\(F\\), then\n\\(P_EP_F = P_FP_E = P_E\\).\n\n\nHence, projecting the above decomposition on \\(V_0^\\perp\\), we get\n\n\\[\nP_{V_0^\\perp}Y = P_{[X]^\\perp} Y + (P_{V_0^\\perp} - P_{[X]^\\perp})Y\n\\]\n\nHere, \\(V_0 \\subset [X]\\) so that \\([X]^\\perp \\subset V_0^{\\perp}\\), and we check that\n\\((P_{V_0^\\perp} - P_{[X]^\\perp})^2 = (P_{V_0^\\perp} - P_{[X]^\\perp})= (P_{V_0^\\perp} - P_{[X]^\\perp})^T\\)\nso that \\(P_{V_0^\\perp} - P_{[X]^\\perp}\\) is an orthogonal projector on a subspace of dimension \\(tr(V_0^\\perp) - tr(P_{[X]^\\perp}) = n-(p-q)-(n-p) = q\\)\nSince \\(P_{[X]^\\perp}(P_{V_0^\\perp} - P_{[X]^\\perp}) = P_{[X]^\\perp}P_{[X]^\\perp}-P_{[X]^\\perp}P_{[X]^\\perp} = 0\\), we also have that \\(P_{[X]^\\perp} Y\\) and \\((P_{V_0^\\perp} - P_{[X]^\\perp})Y\\) are orthogonal.\nHence, by the Cochran Theorem, \\(P_{[X]^\\perp} Y\\) and \\((P_{V_0^\\perp} - P_{[X]^\\perp})Y\\) are indpendent and\n\n\\(SSR=\\|P_{[X]^\\perp} Y\\|^2 = \\|P_{[X]^\\perp} \\varepsilon\\|^2\\) has distribution \\(\\chi^2(n-p)\\)\nUnder \\(H_0\\), \\(X\\beta \\in V_0\\) so that \\(SSR_c=\\|P_{V_0^\\perp}Y\\|^2 = P_{V_0^\\perp}(X\\beta + \\varepsilon) = P_{V_0^\\perp}(\\varepsilon)\\)\nHence, sill under \\(H_0\\), \\(SSR_c -SSR = \\|(P_{V_0^\\perp} - P_{[X]^\\perp}) \\varepsilon\\|^2\\) has distribution \\(\\chi^2(q)\\)\n\\(SSR\\) and \\(SSR_c - SSR\\) are independent since they are projections on orthogonal subspaces of \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2I_n)\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#introduction",
    "href": "research/presentations/crowdsourcing/presentation_long.html#introduction",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Introduction",
    "text": "Introduction\n\nIt’s a great privilege to address you today and present my latest research on crowdsourcing problems.\nFor context, I conducted this research primarily at ENS Lyon as a postdoctoral researcher, building on my previous PhD work in crowdsourcing.\nImagine a group of workers assigned binary classification tasks.\nThey must provide binary responses: YES or NO.\nThis scenario applies to many practical situations\n\nworkers might be paid to perform image classification, text moderation, sentiment analysis, or data verification tasks.\n\n\n\nWorkers are given binary tasks to which they have to give a response: YES or NO\nExamples\n\nImage Classification: “Does this image contain a dog?”\nText Moderation: “Is this comment toxic or offensive?”\nSentiment Analysis: “Does this review express positive sentiment?”\nData Verification: “Is this information factually correct?”"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#objectives",
    "href": "research/presentations/crowdsourcing/presentation_long.html#objectives",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Objectives",
    "text": "Objectives\n\nGiven their responses, three natural objectives emerge\n\nwhat are the actual true labels?\nCan we compare workers to identify which perform better or worse?\nHow well do workers perform on a given task?\n\nI will touch on all these issues in my talk. However, my primary focus will be on the main question of recovering the true labels.\n\n\nGiven their responses, we have 3 objectives\n\nrecover the true label\nrank the workers\nestimate their abilities\n\n\n\nGiven \\(n\\) workers and \\(d\\) binary tasks\n\n\nA proportion \\(\\lambda\\) of observations\n\n\nMain Quetion: How can we accurately recover the labels?"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#this-talk",
    "href": "research/presentations/crowdsourcing/presentation_long.html#this-talk",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "This Talk",
    "text": "This Talk\n\nMy talk will break down this problem into three parts.\nI’ll begin by introducing the non-parametric isotonic model, since it offers a framework for addressing all three problems.\nNext, I’ll review existing methods, as I believe understanding these approaches is essential to grasp the key insights behind the method I introduce in my paper.\nThe final section will focus on my contribution - the new method I’ve developed and some of the key ideas that emerge from it.\n\n\nIntroducing the non-parametric isotonic model\nPresenting already existing algorithms\nIterative Spectral Voting (ISV) algo and Key Insights"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#illustration",
    "href": "research/presentations/crowdsourcing/presentation_long.html#illustration",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration",
    "text": "Illustration\n\nLet me start with a concrete example to illustrate the type of data we’re working with.\nConsider binary tasks where labels are either \\(-1\\) or \\(+1\\). If we have \\(9\\) tasks, there is a vector \\(x^*\\) of true labels with length \\(9\\).\nWe observe a matrix \\(Y\\) where each entry \\((i,k)\\) represents the response of worker \\(i\\) to task \\(k\\).\nWe model partial observations with rate \\(\\lambda \\in [0,1]\\) — when a worker doesn’t respond to a particular task, we simply put \\(0\\) in that matrix position.\n\n\nTwo binary tasks with labels in \\(\\{\\color{green}{-1},\\color{blue}{+1}\\}\\)\nUnknown vector \\(x^*\\) of labels with \\(d=9\\) tasks \\(x^*= (\\color{blue}{+1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1})\\)\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1}\n\\end{array}\\right)\n\\]\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{red}{0} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0}\n\\end{array}\\right)\n\\]\n\n\n\n\nRate of observations: \\(\\color{red}{\\lambda= 0.72}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#observation-model",
    "href": "research/presentations/crowdsourcing/presentation_long.html#observation-model",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nThe underlying statistical model can be described as follows. Given a worker \\(i\\) and a task \\(k\\), we observe their response \\(Y_{ik}\\), and we assume this relation:\n\n\\(x^*\\) is the vector of unknown true labels\n\\(M_{ik} \\in [0,1]\\) represents the unknown ability of worker \\(i\\) on task \\(k\\). For the best experts or easiest tasks, we have \\(M_{ik} = 1\\)\n\\(E_{ik}\\) is standard sub-Gaussian noise\n\\(B_{ik}\\) is a \\(0\\)-\\(1\\) coefficient that indicates whether we observe \\((i,k)\\) or not\n\n\n\nGiven workers \\(i\\in\\{1, \\dots, n\\}\\) and tasks \\(k\\in\\{1, \\dots, d\\}\\)\n\n\nWe observe \\(Y_{ik} \\in \\{-1,0,1\\}\\)\n\n\\[\nY_{ik} = B_{ik}(M_{ik}x_k^* + E_{ik}) \\enspace\n\\]\n\n\n\\(x_k^*\\in \\{-1,1\\}\\) true response of task \\(k\\)\n\\(M_{ik} \\in [0,1]\\) represents the unknown ability of worker \\(i\\) to task \\(k\\)\n\\(E_{ik}\\) are independent and \\(1\\)-subGaussian noise\n\\(B_{ik}\\) are iid Bernoulli \\(\\lambda \\in [0,1]\\). \\(=1\\) if \\((i,k)\\) is observed"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#alernative-formulation",
    "href": "research/presentations/crowdsourcing/presentation_long.html#alernative-formulation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Alernative Formulation",
    "text": "Alernative Formulation\n\nIn matrix form, we can rewrite the model as follows […] where \\(\\odot\\) is the coordinate-wise product between matrices.\nThis is equivalent to what I said before, except here \\(M\\), \\(E\\), and \\(B\\) are matrices. \\(B\\) can be thought of as a mask matrix.\n\n\nWe observe\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\nwhere \\(\\odot\\) is the Hadamard product.\n\n\n\n\\(M \\in [0,1]^{n \\times d}\\) is the ability matrix\n\\(x^*\\) is the vector of unknown labels\n\\(E\\) is a matrix of independent noise\n\\(B\\) is a Bernoulli “mask” matrix, modelling partial observations"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#bernoulli-observation-submodel",
    "href": "research/presentations/crowdsourcing/presentation_long.html#bernoulli-observation-submodel",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Bernoulli Observation Submodel",
    "text": "Bernoulli Observation Submodel\n\nAn important example is the Bernoulli Observation Model.\nThis is the general model I just presented, and this is the Bernoulli Model, introduced by Shah et al. in 2020.\n\nIt assumes that each worker has probability \\((1+M_{ik})/2\\) of being correct on task \\(k\\)\nWe observe each response with probability \\(\\lambda\\)\n\nThis is a submodel because Bernoulli variables are indeed \\(1\\)-subgaussian.\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\n\n\n\nBernoulli submodel (Shah et al., 2020)\n\n\n\\[\n\\begin{aligned}\\label{eq:bernoulli_model}\n    Y_{ik} = \\begin{cases}\n        x^*_k \\text{ with proba } \\lambda\\left(\\frac{1+M_{ik}}{2}\\right)\n        \\\\\n        -x^*_k \\text{ with proba } \\lambda\\left(\\frac{1-M_{ik}}{2}\\right)\\\\\n        0 \\text{ with proba } 1-\\lambda\n        \\end{cases}\n\\end{aligned}\n\\]\n\n\n\n\n\n\\(\\frac{1+M_{ik}}{2}\\) is the proba that \\(i\\) answers correctly to task \\(k\\).\n\n\n\\(\\lambda \\in[0,1]\\) is the probability of observing worker/task pair \\((i,k)\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#shape-constraints",
    "href": "research/presentations/crowdsourcing/presentation_long.html#shape-constraints",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nUntil now, I have’nt assumed anything on the ability matrix \\(M\\) except that it has coef. in \\([0,1]\\).\nFrom now on, we assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) of its rows, that is,\nit has increasing columns, up to an unknown permutation \\(\\pi^*\\)\n\n\n\\(M \\in [0,1]^{n \\times d}\\) represents the ability matrix of worker/task pairs\nWe assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) , that is\n\n\n\n\\[\nM_{\\pi^*}=\\left(\\begin{array}{ccccccccc}\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8} \\\\\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6}\n\\end{array}\\right)\n\\]\n\n\n\\[\nM_{\\phantom{{\\pi^*}}}=\\left(\\begin{array}{ccccccccc}\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6} \\\\\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#shape-constraints-1",
    "href": "research/presentations/crowdsourcing/presentation_long.html#shape-constraints-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nMore formally, the isotonicity constraint can be written as this set of inequalities.\nIt means that the rows of \\(M\\) are uniformly increasing.\nFor any two workers \\(i\\) and \\(j\\), either \\(i\\) or \\(j\\) is uniformly better than the other one.\n\n\nFor all \\(k = 1, \\dots, d\\),\n\n\n\n\\[\nM_{\\pi^*(1), k}\\geq \\dots \\geq M_{\\pi^*(n),k}\n\\]\n\n\nIt means that the rows are uniformly increasing\nA worker \\(i\\) is better on average than \\(j\\), if it is better on all tasks on average"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#illustration-when-x-is-known",
    "href": "research/presentations/crowdsourcing/presentation_long.html#illustration-when-x-is-known",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration when \\(x^*\\) is Known",
    "text": "Illustration when \\(x^*\\) is Known\n\nLet me illustrate the isotonicity assumption with this picture. This is an isotonic matrix represented in a scale of grays. This is a permuted isotonic matrix, and this is what we might observe.\nNow I’m mostly interested in the statistical limit for finding labels, that is, where the noise is of the same scale as the coefficients of \\(M\\), and when the picture becomes more blurry."
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#square-norm-loss-vs-hamming-loss",
    "href": "research/presentations/crowdsourcing/presentation_long.html#square-norm-loss-vs-hamming-loss",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Square Norm Loss VS Hamming Loss",
    "text": "Square Norm Loss VS Hamming Loss\n\nNow let move on to the definition of the Square norm loss for finding \\(x^*\\).\nTo motivate its definition, let me recall that the Hamming loss consists simply in summing up all the mistakes we made in the estimation of the true labels.\nFor the square norm loss, we rather take the frobenius norm of \\(M\\), restricted to the column corresponding to tasks on which we did a mistake estimating \\(x^*_k\\).\n\nIf workers are bad (\\(M\\) close to \\(0\\)), square norm loss is small but Hamming Loss can be of order \\(d\\)\nThe square norm loss is better for my purpose, because it evaluates the quality of the estimator \\(\\hat x\\) instead of the performance of the workers!\n\n\n\n\n\n\nHamming Loss: \\[ \\sum_{k=1}^d \\mathbf 1\\{\\hat x_k \\neq x_k^*\\}\\]\n\n\n\n\n\nSquare Norm Loss: \\[ \\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\]\n\n\n\n\nIf workers are bad, \\(M\\) is small but Hamming Loss is large\nIf \\(M \\sim 0\\), Hamming loss \\(\\sim d\\)\nSquare norm loss evaluates the quality of \\(\\hat x\\) rather than workers"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#crowdsourcing-problems",
    "href": "research/presentations/crowdsourcing/presentation_long.html#crowdsourcing-problems",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\nA good feature of the isotonic model is that in addition to recovering labels, we can define two other objectives: ranking the workers and estimating their abilities. Each objective corresponds to a similar squre norm loss.\nThese three objectives are closely connected in practice.\n\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\n\nObjectives\n\n\nRecovering the labels (\\(x^*\\))\n\n\nRanking the workers (\\(\\pi^*\\))\n\n\nEstimating their abilities (\\(M\\))\n\n\n\nLosses\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\)\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2\\)\n\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M - \\hat M\\|_F^2\\)\n\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#crowdsourcing-problems-1",
    "href": "research/presentations/crowdsourcing/presentation_long.html#crowdsourcing-problems-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\nObjectives\nRecovering the labels (\\(x^*\\))\n\nRanking the workers (\\(\\pi^*\\))\n\nEstimating their abilities (\\(M\\))\n\nRisks\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#minimax-risk-for-recovering-labels",
    "href": "research/presentations/crowdsourcing/presentation_long.html#minimax-risk-for-recovering-labels",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "MiniMax Risk for Recovering Labels",
    "text": "MiniMax Risk for Recovering Labels\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\n\nmaximize squre norm loss on all \\(M\\), \\(\\pi^*\\), \\(x^*\\) such that \\(M_{\\pi^*}\\) is isotonic\nminimize on all estimator \\(\\hat x\\)\nSimilarly, we can define minimax ranking risk and estimation risks"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#short-story",
    "href": "research/presentations/crowdsourcing/presentation_long.html#short-story",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Short Story",
    "text": "Short Story\n\n(Shah et al., 2020): recovering \\(x^*\\) optimally using a least square method, conjectured NP hard (\\(x^*\\) unknown, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic).\n\n\n(Mao et al., 2020): estimating abilities \\(M\\) of workers optimally with least square method (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Liu & Moitra, 2020): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): improve state of the art poly. time (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Pilliat et al., 2024): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): achieves rates of Liu & Moitra (2020) without bi-isotonic assumption (\\(x^*\\) known, \\(M_{\\pi^*}\\) isotonic)\n\n\nThis paper: recovering \\(x^*\\), ranking \\(\\pi^*\\) and estimating abilities \\(M\\) in poly. time when \\(n=d\\) (\\(x^*\\) unknown, \\(M_{\\pi^*}\\) isotonic)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#main-results",
    "href": "research/presentations/crowdsourcing/presentation_long.html#main-results",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Results",
    "text": "Main Results\n\n\n\n\nTheorem\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), there exists a poly. time method \\(\\hat x\\) achieving \\(\\mathcal R^*_{\\mathrm{reco}}\\) up to polylog factors, i.e. \\[\n\\mathcal R_{\\mathrm{reco}}(n,d,\\lambda, \\hat x) \\lesssim \\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\enspace .\n\\]\nMoreover, up to polylogs,\n\\[\n\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\asymp \\frac{d}{\\lambda} \\enspace .\n\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#majority-vote",
    "href": "research/presentations/crowdsourcing/presentation_long.html#majority-vote",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Majority Vote",
    "text": "Majority Vote\n\n\\[ \\hat x^{(maj)}_k = \\mathrm{sign} \\left( \\sum_{i=1}^n Y_{ik} \\right) \\enspace .\\]\n\n\n\n\n\nMax risk of majority vote\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n, d, \\lambda, \\hat x^{(maj)}) \\asymp \\tfrac{d \\sqrt{n}}{\\lambda}\\]\n\n\n\n\n\nWorst case (\\(\\lambda=1\\)): \\(M \\asymp \\frac{1}{\\sqrt{n}}(\\mathbf 1_{n\\times d})\\)\n\n\nIn this case, \\(\\hat x^{(maj)}\\) is not much better than random labelling and \\(\\|M\\mathrm{diag}(\\hat x \\neq x^*)\\|_F^2 \\asymp d\\sqrt{n}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#obi-wan-shah2020permutation",
    "href": "research/presentations/crowdsourcing/presentation_long.html#obi-wan-shah2020permutation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "OBI-WAN (Shah et al., 2020)",
    "text": "OBI-WAN (Shah et al., 2020)\n\nIdea: Population term \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T\\) is independent of \\(x^*\\)\n\n\nPCA Step:\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]\n\n\n\n(Shah et al., 2020) sort \\(|\\hat v|\\) to get a partial ranking\n\n\nAggregation: Majority vote on \\(k\\) top experts according to \\(|\\hat v|\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#result-on-obi-wan-method",
    "href": "research/presentations/crowdsourcing/presentation_long.html#result-on-obi-wan-method",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Result on Obi-Wan Method",
    "text": "Result on Obi-Wan Method\n\n\n\n\n\nTheorem 1 (Shah et al., 2020)\n\n\nIn submodel where \\(\\mathrm{rk}(M)= 1\\), \\(\\hat x^{(\\mathrm{Obi-Wan})}\\) achieves minimax risk up to polylogs: \\[\\mathbb E\\|M\\mathrm{diag}(\\hat x^{\\text{Obi-Wan}} \\neq x^*)\\|_F^2 \\lesssim \\frac{d}{\\lambda} \\quad \\textbf{(minimax)}\\]\n\n\n\n\n\n\n\n\n\n\nTheorem 2 (Shah et al., 2020)\n\n\nIn the model where \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic up to polylogs: \\[\\mathbb E\\|M\\mathrm{diag}(\\hat x^{\\text{Obi-Wan}} \\neq x^*)\\|_F^2 \\lesssim \\frac{\\sqrt{n}d}{\\lambda} \\quad \\textbf{(not minimax)}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#pca-step",
    "href": "research/presentations/crowdsourcing/presentation_long.html#pca-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "PCA Step",
    "text": "PCA Step\n\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y^{(1)}\\|^2 \\quad \\text{and} \\quad \\tilde v = \\hat v \\land \\sqrt{\\lambda / T}\\]\n\n\n\nMain idea: if \\(M\\) is isotonic, then up to a polylog\n\n\\[\\|MM^T\\|_{\\mathrm{op}} \\gtrsim \\|M\\|_F^2\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#voting-step",
    "href": "research/presentations/crowdsourcing/presentation_long.html#voting-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Voting Step",
    "text": "Voting Step\n\nDefine the weighted vote vector\n\n\\[\\hat w = \\tilde v^T Y^{(2)}\\]\n\n\n\nDefine the estimated label as\n\n\\[\\hat x_k^{(1)} = \\mathrm{sign}(\\hat w_k)\\mathbf{1}\\bigg\\{|\\hat w_k| \\gg \\sqrt{\\sum_{i=1}^n \\tilde v_i B_{ik}^{(2)}}\\bigg\\}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#iterate",
    "href": "research/presentations/crowdsourcing/presentation_long.html#iterate",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Iterate",
    "text": "Iterate\n\nKeep certain labels: if \\(\\hat x^{(t)}\\neq 0\\), set \\(\\hat x^{(t+1)}= \\hat x^{(t)}\\).\n\n\nRestrict columns of \\(Y\\) to uncertain labels \\(\\hat x^{(t)}=0\\)\n\n\nRepeat PCA Step + Voting Step on \\(Y \\mathrm{diag}(\\hat x^{(t)} \\neq 0)\\) a polylogarithmic number of times.\n\n\nOutput last estimator \\(\\hat x^{(T)}\\)\n\\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#proof-idea",
    "href": "research/presentations/crowdsourcing/presentation_long.html#proof-idea",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Proof Idea",
    "text": "Proof Idea\n\nLet \\(M(t) = M\\mathrm{diag}(\\hat x^{(t-1)} = 0)\\)\n\n\nWhile \\(M(t) \\gg d/\\lambda\\), we prove that\n\n\n\\[\\|\\tilde v^TM(t)\\|_2^2 \\gtrsim \\|M\\|_F^2 \\and \\|\\tilde v^TM(t+1)\\|_2^2 \\lesssim d/\\lambda\\]\n\n\n\n\nBy Pythagoeran Theorem, we have\n\n\n\\[\\|M(t)\\|_F^2 - \\|M(t+1)\\|_F^2 \\geq \\|\\tilde v M(t)\\|_2^2 - \\|\\tilde v^TM(t+1)\\|_2^2\\]\n\n\n\n\nThis leads to exponential decay of \\(\\|M(t)\\|_F^2\\) until \\(M(t) \\leq d/\\lambda\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#summary",
    "href": "research/presentations/crowdsourcing/presentation_long.html#summary",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Summary",
    "text": "Summary\n\nThe core of my presentation can be captured in three key points.\n\nFirst, the isotonic model is a very flexible, non-parametric framework for describing crowdsourcing data and tackling tasks such as recovering labels and ranking workers.\nSecondly, ISV method is computationally feasible and achieves minimax rates in most interesting regimes.\nLastly, and this is a surprising result: not knowing the true labels does not make the problem of ranking workers any harder from a statistical perspective\n\n\n\nNon parametric isotonic model very flexible in crowdsourcing problems\nMinimax and polynomial time method ISV for recovering labels, ranking and estimating abiliti (at least when \\(n=d\\))\nNot knowing the labels is not harder than knowing them for ranking workers"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#main-insights",
    "href": "research/presentations/crowdsourcing/presentation_long.html#main-insights",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Insights",
    "text": "Main Insights\n\nSpectral method because \\(\\|M\\mathrm{diag}(x^*)(M\\mathrm{diag}(x^*))^T\\|_{\\mathrm{op}}=\\|MM^T\\|_{\\mathrm{op}}\\) does not depend on \\(x^*\\)\nIterate to reduce remaining square norm loss \\(\\|M\\mathrm{diag}(x^* \\neq \\hat x^{(t)})\\|_F^2\\)\nBecause \\(\\|M\\|^2_{\\mathrm{op}}\\gtrsim\\|M\\|_F^2\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#whats-next",
    "href": "research/presentations/crowdsourcing/presentation_long.html#whats-next",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "What’s Next?",
    "text": "What’s Next?\n\n\nCan we do better if we are allowed to select worker task pairs \\((i,k)\\) based on past information?"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problem-with-binary-tasks",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problem-with-binary-tasks",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problem with Binary Tasks",
    "text": "Crowdsourcing Problem with Binary Tasks\n\nImage Classification: “Does this image contain a dog?”\nText Moderation: “Is this comment toxic or offensive?”\nSentiment Analysis: “Does this review express positive sentiment?”\nData Verification: “Is this information factually correct?”"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#main-question",
    "href": "research/crowdsourcing/presentation.html#main-question",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Question",
    "text": "Main Question\n\nGiven \\(n\\) workers and \\(d\\) binary tasks\n\n\nA proportion \\(\\lambda\\) of observations\n\n\nHow can we accurately recover the labels?"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#illustration",
    "href": "research/crowdsourcing/presentation.html#illustration",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration",
    "text": "Illustration\n\nTwo binary tasks with labels in \\(\\{\\color{green}{-1},\\color{blue}{+1}\\}\\)\nUnknown vector \\(x^*\\) of labels with \\(d=9\\) tasks \\(x^*= (\\color{blue}{+1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1})\\)\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1}\n\\end{array}\\right)\n\\]\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{red}{0} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0}\n\\end{array}\\right)\n\\]\n\n\n\n\nRate of observations: \\(\\color{red}{\\lambda= 0.72}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#observation-model",
    "href": "research/crowdsourcing/presentation.html#observation-model",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nGiven workers \\(i\\in\\{1, \\dots, n\\}\\) and tasks \\(k\\in\\{1, \\dots, d\\}\\)\n\n\nWe observe\n\n\\[\nY_{ik} = B_{ik}(M_{ik}x_k^* + E_{ik}) \\enspace\n\\]\n\n\nWhere:\n\n\\(M_{ik} \\in [0,1]\\)\n\\(E_{ik}\\) are independent and \\(1\\)-subGaussian\n\\(B_{ik}\\) are iid Bernoulli \\(\\lambda \\in [0,1]\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#observation-model-1",
    "href": "research/crowdsourcing/presentation.html#observation-model-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nWe observe\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\nwhere \\(\\odot\\) is the Hadamard product.\n\n\\(M \\in [0,1]^{n \\times d}\\) is the ability matrix\n\\(x^*\\) is the vector of unknown labels\n\\(E\\) is a matrix of independent noise\n\\(B\\) is a Bernoulli “mask” matrix, modelling partial observations"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#bernoulli-observation-submodel",
    "href": "research/crowdsourcing/presentation.html#bernoulli-observation-submodel",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Bernoulli Observation SubModel",
    "text": "Bernoulli Observation SubModel\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\n\n\n\nBernoulli submodel (Shah et al., 2020)\n\n\n\\[\n\\begin{aligned}\\label{eq:bernoulli_model}\n    Y_{ik} = \\begin{cases}\n        x^*_k \\text{ with proba } \\lambda\\left(\\frac{1+M_{ik}}{2}\\right)\n        \\\\\n        -x^*_k \\text{ with proba } \\lambda\\left(\\frac{1-M_{ik}}{2}\\right)\\\\\n        0 \\text{ with proba } 1-\\lambda\n        \\end{cases}\n\\end{aligned}\n\\]\n\n\n\n\n\n\\(\\frac{1+M_{ik}}{2}\\) is the proba that \\(i\\) answers correctly to task \\(k\\).\n\n\n\\(\\lambda\\) is the probability of observing worker/task pair \\((i,k)\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#shape-constraints",
    "href": "research/crowdsourcing/presentation.html#shape-constraints",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\n\\(M \\in [0,1]^{n \\times d}\\) represents the ability matrix of worker/task pairs\nWe assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) , that is\n\n\n\n\\[\nM_{\\pi^*}=\\left(\\begin{array}{ccccccccc}\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8} \\\\\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6}\n\\end{array}\\right)\n\\]\n\n\n\\[\nM_{\\phantom{{\\pi^*}}}=\\left(\\begin{array}{ccccccccc}\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6} \\\\\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#shape-constraints-1",
    "href": "research/crowdsourcing/presentation.html#shape-constraints-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nFor all \\(k = 1, \\dots, d\\), \\[\nM_{\\pi^*(1), k}\\geq \\dots \\geq M_{\\pi^*(n),k}\n\\]\nIt means that the rows are uniformly increasing\nA worker \\(i\\) is better on average than \\(j\\), if it is better on all tasks on average"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#illustration-when-x-is-known",
    "href": "research/crowdsourcing/presentation.html#illustration-when-x-is-known",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration when \\(x^*\\) is Known",
    "text": "Illustration when \\(x^*\\) is Known"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problems",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problems",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\n\nObjectives\n\n\nRecovering the labels\n\n\nRanking the workers\n\n\nEstimating their abilities\n\n\n\nLosses\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\)\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2\\)\n\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M - \\hat M\\|_F^2\\)\n\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\nObjectives\nRecovering the labels\n\nRanking the workers\n\nEstimating their abilities\n\nRisks\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "href": "research/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Square Norm Loss VS Hamming Loss",
    "text": "Square Norm Loss VS Hamming Loss\n\n\n\n\nHamming Loss: \\[ \\sum_{k=1}^d \\mathbf 1\\{\\hat x_k \\neq x_k^*\\}\\]\n\n\n\n\n\nSquare Norm Loss: \\[ \\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\]\n\n\n\n\nIf workers are bad, \\(M\\) is small but Hamming Loss is large\nIf \\(M \\sim 0\\), Hamming loss \\(\\sim d\\)\nSquare norm loss evaluates the quality of \\(\\hat x\\) rather than workers"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#other-parametric-models",
    "href": "research/crowdsourcing/presentation.html#other-parametric-models",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Other Parametric Models",
    "text": "Other Parametric Models\n\nDS: \\(M_{ik} = q_i\\) (Dawid & Skene, 1979), (Shah et al., 2020)\n\nThe abilities are independent of the tasks\n\nBTL: \\(M_{ik}=\\phi(a_i-b_k)\\) (Bradley & Terry, 1952)\n\n\\(a_i\\): abilities of the workers\n\\(b_i\\): difficulties of the tasks\n\n\n\nThese parametric models often fail to fit data well"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#non-parametric-models",
    "href": "research/crowdsourcing/presentation.html#non-parametric-models",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Non-Parametric Models",
    "text": "Non-Parametric Models\n\nknown labels \\(x^*\\), \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic (Mao et al., 2020) (Liu & Moitra, 2020)\nknown labels \\(x^*\\), \\(M_{\\pi^*}\\) is isotonic (Flammarion et al., 2019) (Pilliat et al., 2024)\nunknown labels \\(x^*\\), \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic (Shah et al., 2020)\nunknown labels \\(x^*\\), \\(M_{\\pi^*}\\) is isotonic (this paper)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#risks-recap",
    "href": "research/crowdsourcing/presentation.html#risks-recap",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Risks Recap",
    "text": "Risks Recap\n\n\n\n\nObjectives\n\n\nRecovering the labels\n\nRanking the workers\n\nEstimating their abilities\n\nRisks\n\n\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#minimax-recovering-risk",
    "href": "research/crowdsourcing/presentation.html#minimax-recovering-risk",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "MiniMax Recovering Risk",
    "text": "MiniMax Recovering Risk\n\n\n\n\nMax risk for recovering labels\n\n\n\\[\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\nmaximize on all \\(M\\), \\(\\pi^*\\), \\(x^*\\) such that \\(M_{\\pi^*}\\) is isotonic\n\n\n\n\n\n\n\n\n\nMiniMax risk for recovering labels\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\nminimize on all estimator \\(\\hat x\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#minimax-risks",
    "href": "research/crowdsourcing/presentation.html#minimax-risks",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Minimax Risks",
    "text": "Minimax Risks\n\n\n\n\nRecovering labels\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\n\n\n\n\n\n\n\nRanking workers\n\n\n\\[\\mathcal R^*_{\\mathrm{rk}}(n,d,\\lambda)=\\min_{\\hat \\pi}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\]\n\n\n\n\n\n\n\n\nEstimating abilities\n\n\n\\[\\mathcal R^*_{\\mathrm{est}}(n,d,\\lambda)=\\min_{\\hat M}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M- \\hat M\\|_F^2]\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#short-story",
    "href": "research/crowdsourcing/presentation.html#short-story",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Short Story",
    "text": "Short Story\n\n(Shah et al., 2020): recovering \\(x^*\\) optimally using a least square method, conjectured NP hard (\\(x^*\\) unknown, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic).\n\n\n(Mao et al., 2020): estimating abilities \\(M\\) of workers optimally with least square method (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Liu & Moitra, 2020): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): improve state of the art poly. time (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Pilliat et al., 2024): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): achieves rates of Liu & Moitra (2020) without bi-isotonic assumption (\\(x^*\\) known, \\(M_{\\pi^*}\\) isotonic)\n\n\nThis paper: recovering \\(x^*\\), ranking \\(\\pi^*\\) and estimating abilities \\(M\\) in poly. time when \\(n=d\\) (\\(x^*\\) unknown, \\(M_{\\pi^*}\\) isotonic)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#main-results",
    "href": "research/crowdsourcing/presentation.html#main-results",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Results",
    "text": "Main Results\n\n\n\n\nOptimal poly. time method\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), there exists a poly. time method \\(\\hat x\\) achieving \\(\\mathcal R^*_{\\mathrm{reco}}\\) up to polylog factors, i.e. \\[\n\\mathcal R_{\\mathrm{reco}}(n,d,\\lambda, \\hat x) \\lesssim \\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)\n\\]\n\n\n\n\n\n\n\n\nMinimax Risk\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), up to polylogs, \\[\n\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\asymp \\frac{d}{\\lambda}\n\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#majority-vote",
    "href": "research/crowdsourcing/presentation.html#majority-vote",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Majority Vote",
    "text": "Majority Vote\n\n\\[ \\hat x^{(maj)}_k = \\mathrm{sign} \\left( \\sum_{i=1}^n Y_{ik} \\right) \\enspace .\\]\n\n\n\n\n\nMax risk of majority vote\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n, d, \\lambda, \\hat x^{(maj)}) \\asymp \\tfrac{d \\sqrt{n}}{\\lambda}\\]\n\n\n\n\n\nWorst case (\\(\\lambda=1\\)): \\(M \\asymp \\frac{1}{\\sqrt{n}}(\\mathbf 1_{n\\times d})\\)\n\n\nIn this case, \\(\\hat x^{(maj)}\\) is no better than random labelling and \\(\\|M\\mathrm{diag}(\\hat x \\neq x^*)\\|_F^2 \\asymp d\\sqrt{n}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#least-square-conjectured-np-hard",
    "href": "research/crowdsourcing/presentation.html#least-square-conjectured-np-hard",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Least square (conjectured NP Hard)",
    "text": "Least square (conjectured NP Hard)\n\nMinimize\n\n\\[\\|Y- \\lambda M' \\mathrm{diag}(x)\\|_F^2\\]\n\n\nover all \\(x \\in \\{-1, 1\\}^d\\)\nand \\(M'\\) isotonic, up to a permutation \\(\\pi^*\\)\n\n\n\nThe set of isotonic matrices is convex…\n\n\n\nBut not isotonic matrices up to a permutation \\(\\pi^*\\)\n\n\nIt is minimax optimal (Shah et al., 2020)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "href": "research/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "OBI-WAN (Shah et al., 2020)",
    "text": "OBI-WAN (Shah et al., 2020)\n\nIdea: Population term \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T\\) is independent of \\(x^*\\)\n\n\nPCA Step:\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]\n\n\n\n(Shah et al., 2020) sort \\(|\\hat v|\\) to get a partial ranking\n\n\nAggregation: Majority vote on \\(k\\) top experts according to \\(|\\hat v|\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "href": "research/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Result on Obi-Wan Method",
    "text": "Result on Obi-Wan Method\n\n\n\n\n\nTheorem 1 (Shah et al., 2020)\n\n\nIn submodel where \\(\\mathrm{rk}(M)= 1\\), \\(\\hat x^{(\\mathrm{Obi-Wan})}\\) achieves minimax risk up to polylogs: \\[\\mathcal R(n,d, \\lambda, \\hat x^{(\\mathrm{Obi-Wan})}) \\lesssim \\frac{d}{\\lambda} \\quad \\textbf{(minimax)}\\]\n\n\n\n\n\n\n\n\n\n\nTheorem 2 (Shah et al., 2020)\n\n\nIn the model where \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic up to polylogs: \\[\\mathcal R(n,d, \\lambda, \\hat x^{(\\mathrm{Obi-Wan})}) \\lesssim \\frac{\\sqrt{n}d}{\\lambda} \\quad \\textbf{(not minimax)}\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#subsampling",
    "href": "research/crowdsourcing/presentation.html#subsampling",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Subsampling",
    "text": "Subsampling\n\nLet \\(T \\geq 1\\). We generate \\(T\\) samples \\((Y^{(1)}, \\dots, Y^{(T)})\\) from \\(Y\\).\n\n\nPut \\(Y_{ik}\\) uniformly at random into one of the \\(Y^{(s)}\\).\n\n\\(Y_{ik}^{(s)}= 0\\) for all \\(s\\) except one, which is \\(Y_{ik}\\)\nThe \\((Y^{(s)})\\)’s are not independent!\nTechnical trick: condition on the sampling scheme"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#pca-step",
    "href": "research/crowdsourcing/presentation.html#pca-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "PCA Step",
    "text": "PCA Step\n\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y^{(1)}\\|^2 \\quad \\text{and} \\quad \\tilde v = \\hat v \\land \\sqrt{\\lambda / T}\\]\n\n\n\nMain idea: if \\(M\\) is isotonic, then up to a polylog\n\n\\[\\|MM^T\\|_{\\mathrm{op}} \\gtrsim \\|M\\|_F^2\\]\n\n\n\nIdea for the proof: if \\(\\|M\\|_F^2 \\gg \\frac{d}{\\lambda}\\), then \\(\\|\\tilde v^T M\\| \\gtrsim \\|M\\|_F^2\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#voting-step",
    "href": "research/crowdsourcing/presentation.html#voting-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Voting Step",
    "text": "Voting Step\n\nDefine the weighted vote vector\n\n\\[\\hat w = \\tilde v^T Y^{(2)}\\]\n\n\n\nDefine the estimated label as\n\n\\[\\hat x_k^{(1)} = \\mathrm{sign}(\\hat w_k)\\mathbf{1}\\bigg\\{|\\hat w_k| \\gg \\sqrt{\\sum_{i=1}^n \\tilde v_i B_{ik}^{(2)}}\\bigg\\}\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#iterate",
    "href": "research/crowdsourcing/presentation.html#iterate",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Iterate",
    "text": "Iterate\n\nRepeat PCA Step + Voting Step on \\(Y \\mathrm{diag}(\\hat x \\neq 0)\\) a polylogarithmic number of times.\n\n\nWe get \\(\\hat x^{(1)}, \\hat x^{(2)}, \\dots, \\hat x^{(T)}\\)\n\n\nOutput \\(\\hat x^{(T)}\\)\n\\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#proof-idea",
    "href": "research/crowdsourcing/presentation.html#proof-idea",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Proof Idea",
    "text": "Proof Idea\n\nLet \\(M(t) = M\\mathrm{diag}(x^{(t-1)} = 0)\\)\n\n\nWhile \\(M(t) \\gg d/\\lambda\\), we prove that\n\n\\[\\|\\tilde v^TM(t)\\|_2^2 \\gtrsim \\|M\\|_F^2 \\and \\|\\tilde v^TM(t+1)\\|_2^2 \\lesssim d/\\lambda\\]\n\n\n\nBy Pythagoeran Theorem, we have\n\n\\[\\|M(t)\\|_F^2 - \\|M(t+1)\\|_F^2 \\geq \\|\\tilde v M(t)\\|_2^2 - \\|\\tilde v^TM(t+1)\\|_2^2\\]\n\n\n\nThis leads to exponential decay of \\(\\|M(t)\\|_F^2\\) until \\(M(t) \\leq d/\\lambda\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#synthetic-data",
    "href": "research/crowdsourcing/presentation.html#synthetic-data",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Synthetic Data",
    "text": "Synthetic Data\n\n\n\n\n\n\n\n\n\n \n\nBlack: \\(M_{ik}=0\\)\nBlue: \\(M_{ik} = h\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#next",
    "href": "teaching/glm/slides/Introduction_glm.html#next",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Next",
    "text": "Next\nlogistic model"
  },
  {
    "objectID": "teaching/glm/speaker_notes/logistic_model.html",
    "href": "teaching/glm/speaker_notes/logistic_model.html",
    "title": "Context",
    "section": "",
    "text": "Context\nLet’s now focus specifically on the binary case and work through it in detail.\nY is a binary variable—Y_k is in {0,1}.\nX equals (X^(1) through X^(p)) are our p explanatory variables or regressors.\nY given X equals x follows a Bernoulli distribution with parameter p(x), which equals the probability that Y equals 1 given X equals x.\nWe model p(x) as g-inverse of x-transpose-beta.\nWhere g-inverse is a strictly increasing function with values in [0,1]. This ensures our probabilities are valid.\nWe’ll begin by discussing how to choose g-inverse—or equivalently, the link function g.\n\n\n\nExample - Coronary Heart Disease\nLet’s work through a concrete example to see how this works in practice.\nWe have data on the presence of coronary heart disease—chd—as a function of age. So Y equals chd, taking values 0 or 1, and X equals age.\nWe want to estimate p(x)— the probability that a given individual with characteristics x suffers from coronary heart desease\nHere’s a scatter plot of the raw data. Each point is either 0 or 1 in functino of age. As you can see, it’s hard to discern a pattern directly.\n\n\nExample - Simple Idea\nA simple first approach to understand the relationship.\nWe can group the x values by age class, then calculate the proportion of chd equals 1 in each class containing x.\nNow we see something clearer! The proportion increases with age. But this approach is crude and doesn’t give us a smooth model.\n\n\n\nGLM Approach\nThe GLM approach gives us a smoother model.\nWe want to model p(x)—the probability that chd equals 1 given X equals x—as g-inverse of (beta_0 plus beta_1 times x).\nThis is a simple model with just an intercept and a slope. The age effect is captured by beta_1.\nWe need g-inverse to have values in [0,1] to ensure valid probabilities.\n\n\n\nExample - The Logit Model\nLet’s try the logit link—the most common choice.\ng-inverse(t) equals e^t over (1 plus e^t), which means g(t) equals the natural log of t over (1 minus t)—the logit function.\nHere are the results of g-inverse of (beta-hat_0 plus beta-hat_1 times x), obtained by maximum likelihood estimation. Notice how smooth the curve is, and how it respects the bounds—staying between 0 and 1.\n\n\n\nExample - The Probit Model\nWhat if we use a different link function?\nIf Phi is the CDF of a standard normal distribution, we take g-inverse(t) equals Phi(t).\nHere are the results using the probit model with g-inverse of (beta-hat_0 plus beta-hat_1 times x), again obtained by MLE. Very similar to the logit model!\n\n\n\nExample - Model cloglog\nLet’s try one more: the complementary log-log model.\ng-inverse(t) equals 1 minus e to the negative e^t.\nHere are the results. Notice this curve is not symmetric—it rises differently on the left versus the right compared to logit and probit.\n\n\n\nExample - Comparison of the Three Models\nLet’s compare all three models side by side.\nLogit and probit give approximately the same result—the curves are nearly identical.\nThe cloglog differs slightly and is not symmetric. It approaches 1 much faster than it appraoches 0\nHere they are overlaid. For this data, the choice doesn’t matter much, but in principle there are differences.\n\n\n\nWhich Link Function to Choose for Binary Case?\nBy default, we favor the logit model. This is the main choice for most applications.\nHowever, there are exceptions. We might choose something else - such as the probit model, complementary log-log, or log-log - when there’s a good reason to do so.\nIn the next few slides, we’ll return to the three usual choices and justify why logit is typically preferred.\n\n\n\nSlide 2: Details on the Probit Model\nThe probit model has a specific theoretical justification. It’s appropriate when the binary variable Y given X equals x comes from thresholding a Gaussian latent variable Z of x.\nMathematically, Y given X equals x is the indicator that Z of x is greater than or equal to some threshold tau.\nThe key assumption is that Z of x follows a normal distribution with mean x transpose beta and variance sigma squared.\nLet’s see what this implies for probabilities. If Phi is the cumulative distribution function of a standard normal distribution - that’s a normal with mean zero and variance one - then the probability that Y equals 1 given X equals x can be expressed as the probability that Z of x is at least tau. This equals Phi of the quantity x transpose beta minus tau, all divided by sigma.\nThis formulation shows how the probit model naturally arises from a latent variable framework.\n\n\n\nExamples in Probit Model\nLet me give you two concrete examples where the probit model makes intuitive sense.\nFirst example: Y represents a purchase decision - whether someone buys a product or not. Here, Z of x quantifies the utility or satisfaction the person would get from the good. When this utility exceeds a threshold, they make the purchase.\nSecond example: Y is a declared psychological state, such as whether someone reports being happy or depressed. In this case, Z of x represents a latent, unobserved measure of personal satisfaction. The binary outcome we observe is just whether this latent satisfaction crosses a certain threshold.\nIn both cases, the idea of an underlying continuous variable being thresholded to create a binary outcome is very natural.\n\n\n\nProbit vs Logit: Current Trends\nThe probit model remains relatively popular among econometricians. There’s a tradition of using it in economics research.\nHowever, the general trend is that it’s increasingly being replaced by the logistic model across many fields.\nWhy is this happening? The logit model has many advantages that probit does not have. Let me highlight two key ones:\nFirst, interpretation of results. The logit model allows us to work with odds ratios, which are very intuitive to interpret.\nSecond, explicit formulas. The logit model has closed-form expressions that make computation and interpretation easier.\nFrom a theoretical perspective, there’s also good justification for this trend. The cumulative distribution function of the probit model is actually quite close to the CDF of the logit model, so we’re not losing much by switching.\n\n\n\nRemarks on cloglog Model\nNow let’s turn to the complementary log-log, or cloglog, model.\nFor cloglog, the link function g of t is the natural log of negative log of one minus t. The inverse link is therefore g inverse of t equals one minus e to the negative e to the t.\nAn important property of this link function is that it’s not symmetric. Specifically, g of t does not equal negative g of one minus t. What this means in practice is that p of x approaches zero slowly, but approaches one very rapidly.\nIf the opposite behavior is true in your data - if probabilities approach one slowly but zero rapidly - you should use the log-log model instead, where g of t equals negative natural log of negative natural log of t.\nThe cloglog model is particularly useful in survival models, such as Cox model where this kind of loglog appears naturally.\n\n\n\nDetails on the Logit Model\nLet me give you three compelling reasons why the logit model is our preferred choice for binary regression.\nFirst, it provides a highly valued interpretation tool: odds ratios. These allow us to communicate results in an intuitive way that’s meaningful across many fields, from medicine to social sciences to marketing.\nSecond, it’s more practical from a theoretical point of view. The mathematical properties make it easier to work with, and we have good theoretical foundations for why it arises naturally.\nThird, the logit model is the natural model in many situations. As we’ll see in the next slide, it emerges automatically under common distributional assumptions.\n\n\n\nTheoretical Motivation of Logit\nNow let’s see a beautiful theoretical result that explains why logit is so natural.\nWe will show in exercises that there’s an important connection between the logit model and Gaussian distributions.\nHere’s the setup: If the two groups of individuals associated with Y equals 0 and Y equals 1 have a Gaussian distribution of X with different means - that is, for m_0 not equal to m_1, we have X given Y equals 0 follows a normal distribution with mean m_0 and covariance Sigma, and X given Y equals 1 follows a normal distribution with mean m_1 and the same covariance Sigma - then the probability that Y equals 1 given X equals x automatically follows a logistic model.\nThis is a powerful result! It tells us that whenever we have two normally distributed groups that differ in their means but share the same covariance structure, the logit model emerges naturally.\nAnd there’s more. The previous result remains true for any exponential family instead normal distributions. So this property of the logit model is quite general - it’s not limited to just Gaussian distributions.\nThis theoretical foundation helps explain why the logit model appears so frequently in practice.\n\n\nSummary for Binary Variables\nLet’s summarize our discussion about modeling binary variables.\nIf Y is a binary variable, then Y given X equals x follows a Bernoulli distribution with parameter p of x.\nIn a GLM model for Y, we set p of x equals g inverse of x transpose beta, where g is the link function. Now, which link function should we choose?\nBy default, we use the logit function, which is the most natural choice for all the reasons we’ve discussed: odds ratio interpretation, theoretical justification, and mathematical convenience.\nPossibly, we might use probit if we have good reasons to justify it - for example, if there’s a clear latent variable story. But keep in mind that the results will typically be very similar to logit, so unless you have a compelling reason, logit is simpler.\nWe might use cloglog, or its mirror image loglog, if we have good reasons to justify it. The main reasons would be: first, strong asymmetry in p of x - probabilities that approach zero and one at very different rates - and second, a connection with a Cox model when dealing with discretized survival data.\nIn the following sections, we will focus on the logit model, since it’s the default choice and the most widely applicable.\nThis completes our journey through link function selection for binary outcomes. The key takeaway is: start with logit, and only consider alternatives when you have a specific, justifiable reason to do so.\n\n\nOutline\nNow that we’ve established why the logit model is our preferred choice, let’s see how to work with it in practice.\nWe’ll cover four essential aspects: First, how to interpret the model and understand what the coefficients tell us. Second, how to estimate beta from a dataset. Third, how to evaluate the quality of our estimation. And fourth, how to exploit the model to make predictions and perform classification.\nLet’s begin with model interpretation.\n\n\n\nSection Title: Model Interpretation\n\n\nInterpretation of the Logistic Model\nLet’s start with the mathematical form of the logistic model.\nIf x is a vector with p components - x superscript 1 through x superscript p - then our model is:\np of x equals logit inverse of x transpose beta, which equals e to the x transpose beta, divided by 1 plus e to the x transpose beta.\nWhat does this tell us about how each variable affects the probability?\nFirst key point: p of x is increasing with x^{(j)} if beta j is positive, and decreasing otherwise. So the sign of the coefficient tells us the direction of the effect.\nSecond key point: The larger the absolute value of beta j, the stronger the discriminatory power of regressor X j. In other words, when the coefficient has a large magnitude, a small variation in x j can cause a large variation in p of x.\n\n\n\nShape of logit function\nLet me show you what this relationship looks like visually.\nThe shape of the function that maps x superscript j to p of x has this characteristic S-curve, or sigmoid shape. You can see how the probability transitions smoothly from near zero to near one as the predictor increases. The steepness of this S-curve in the middle region is determined by the magnitude of beta j - larger coefficients create steeper transitions.\nThis S-shaped relationship is one of the appealing features of the logistic model - it naturally constrains probabilities to lie between zero and one, and it has this smooth, interpretable transition region.\n\n\n\nSlide 13: Example: BMI Study\nLet’s look at a concrete example to make this more tangible. This is a BMI study with French data - BMI is called IMC in French.\nFor each of 5,300 patients, we observe several variables:\nOur outcome variable Y is binary: it equals 1 if BMI is greater than 35, and 0 otherwise. So we’re trying to predict obesity.\nOur predictor variables include: AGE in years, DBP which is diastolic blood pressure - that’s the low pressure measurement, SEXE indicating male or female, ACTIV which equals 1 if the person engages in intense sports activity and 0 otherwise, WALK which equals 1 if they walk or cycle to work and 0 otherwise, and MARITAL which is marital status with 6 categories: married, widowed, divorced, separated, single, or cohabiting.\n\n\n\nModel Definition\nOur goal is to model the probability that Y equals 1 given X, where X groups all the predictor variables we just mentioned, excluding Y of course.\nIn R, implementing this is straightforward. We use the glm function - that’s generalized linear model - with the family equals binomial option. This tells R we’re doing logistic regression.\nThe syntax would be: glm of Y tilde AGE plus DBP plus SEXE plus ACTIV plus WALK plus MARITAL, with family equals binomial.\nThe tilde notation means “is modeled by” and the plus signs indicate we’re including all these variables as predictors.\n\n\n\nModel Results\nNow let’s look at what the model produces. I’ll walk through the key findings without reading every number.\nThe output gives us estimated coefficients, standard errors, z-values for hypothesis tests, and p-values indicating statistical significance.\nLooking at the significant variables - those marked with asterisks - we see several interesting patterns:\nDBP, diastolic blood pressure, has a positive coefficient and is highly significant. Higher blood pressure is associated with higher probability of obesity.\nBeing female - SEXE FEMME - has a positive significant coefficient, indicating women in this sample have higher probability of high BMI.\nWALK1 and ACTIV1 both have negative coefficients and are highly significant. This makes intuitive sense: people who walk or cycle to work, and people who engage in intense physical activity, have lower probability of high BMI.\nAGE appears not to be significant in this simple linear form.\nThe interpretation is similar to that of a linear regression model, except we’re modeling the log-odds rather than the outcome directly.\nLooking at the marital status variables - MARITAL 2 through 6 - none of them are statistically significant. This suggests we want to remove the MARITAL variable from the model to simplify it.\n\n\n\nSlide 16: Model Results with AGE squared\nPerhaps AGE has a non-linear effect. Let’s try adding AGE squared to the model.\nIn R, we write: glm of Y tilde AGE plus I of AGE squared - the I function tells R to compute AGE squared as is - plus DBP plus SEXE plus WALK plus ACTIV, with family equals binomial.\nNow look at what happens.\nBoth AGE and AGE squared are now highly significant! The positive coefficient on AGE and negative coefficient on AGE squared suggests an inverted U-shape relationship: the probability of high BMI increases with age initially, then decreases at older ages.\nAll our other key variables remain significant with similar interpretations.\nLet me show you how to use this model for prediction.\nFor someone for which WALK1 equals 0 and ACTIV1 equals 0 - so they don’t walk to work and don’t do intense sports - the probability that Y equals 1 given their age and blood pressure is: logit inverse of negative 3.95 plus 0.064 times AGE minus 0.00068 times AGE squared plus 0.0122 times DBP.\nNow, for someone for which WALK1 equals 0 and ACTIV1 equals 1 - they do engage in intense sports activity - we get the same formula but we subtract 0.657. That’s the coefficient of ACTIV1 in red. This person has a lower probability of high BMI, all else being equal, and we can quantify exactly how much lower.\nThis demonstrates how we can use the fitted model to compute predicted probabilities for individuals with different covariate profiles."
  },
  {
    "objectID": "teaching/glm/speaker_notes/Introduction.html",
    "href": "teaching/glm/speaker_notes/Introduction.html",
    "title": "Introduction to Generalized Linear Models",
    "section": "",
    "text": "Introduction to Generalized Linear Models\nWelcome! In our previous sessions, we studied the linear regression model, where Y was a quantitative response variable that could take any real value.\nToday, we’re starting the second part of this course: Generalized Linear Models, or GLMs.\nThe major shift is this: we’ll now assume that Y is qualitative—it takes discrete or categorical values rather than continuous ones.\nAs we’ll see, the standard linear model doesn’t work for these types of variables. We need a more flexible framework—that’s what GLMs provide.\nOver the next sessions, we’ll learn how to properly model these qualitative responses while maintaining the same goals: understanding relationships between variables and making predictions.\nLet’s begin by understanding why the linear model breaks down for qualitative responses.\n\n\nWhen Linearity is Reasonable\nSo, when does the linear assumption makes sense ?\nIn linear regression, we assume Y is a linear function of our regressors.\nThis means Y can be ANY real number—negative, positive, whatever. No restrictions.\nWhen does this make sense?\nWhen Y given X is Gaussian. Normal distributions span the entire real line, so any value for E(Y|X) makes sense.\nOr when Y given X follows any other “nice” continuous distribution on the real line. That is Y does not look like to a discrete distrib too much.\n\n\n\nWhen it is Not\nBut this assumption fails for certain types of response variables.\nEspecially when Y is qualitative or discrete.\nExamples: Binary responses: If Y represents disease presence (0 or 1), can E(Y|X) equal 1.5? Or -0.3? No—we need values between 0 and 1.\nCategorical responses: If Y is transportation choice (car, bus, or bike), it’s not a numeric scale that can take any real value.\nCount data: If Y is the number of traffic accident per month, it can’t be negative. Linear regression might give E(Y|X) = -2.\nBottom line: If Y can’t reasonably take any real number, linear regression isn’t appropriate. We’ll see alternative approaches like logistic regression later in the course.\n\n\nKey Differences by Response Type\nNow let’s be more precise about what changes depending on the type of response variable.\nIn all situations—whether Y is continuous, binary, categorical, or count data—our objective is the same: link Y to our regressors X through modeling E(Y|X).\nHowever, E(Y|X) has different interpretations depending on what Y represents.\nAnd here’s the key: in all these cases, the linear model E(Y|X) equals X-transpose-beta is inappropriate. We need something different.\n\n\n\nObjectives of the GLM\nSo what do we do instead?\nWe model E(Y|X) differently using generalized linear models—or GLMs.\nThe good news is that our objectives remain the same as in linear regression.\nWe still want to understand the individual effect of a given regressor, holding all other variables constant.\nWe want to understand relationships between our regressors and the response.\nAnd we want to forecast outcomes for new observations.\nGLMs give us a flexible framework to achieve these goals when linear regression isn’t appropriate.\n\n\n\nThree Fundamental Cases\nLet’s outline the three fundamental cases we’ll study in detail.\nBinary responses—when Y takes only values 0 or 1. Think disease presence/absence, success/failure, yes/no outcomes.\nCategorical responses—when Y can be one of several categories: A1 through Ak. Think transportation choice, treatment selection, or any general qualitative variable.\nCount data—when Y is in the natural numbers. Think number of events, hospital visits, or any situation where you’re counting occurrences.\nFor each case, we’ll see how to properly model E(Y|X) and interpret the results. These will form the building blocks of generalized linear models.\n\n\nCase 1 - Binary Case\nLet’s dive into the first fundamental case: binary responses.\nWithout loss of generality, we’ll work with Y taking values 0 or 1.\nIf Y models membership in some category A—for example, whether someone has a disease—this is equivalent to studying the indicator variable Y.\nThe key insight is that the distribution of Y given X equals x is entirely determined by p(x), which equals the probability that Y equals 1 given X equals x.\nOnce we know p(x), we automatically know the probability that Y equals 0 given X equals x—it’s just 1 minus p(x).\nSo Y given X equals x follows a Bernoulli distribution with parameter p(x).\nThis means the conditional expectation E(Y|X = x) equals p(x). So modeling the conditional expectation is the same as modeling the probability.\nHere’s the crucial constraint: p(x) must be between 0 and 1: it’s a probability.\n\n\n\nModelling p(x)\nSo how do we model p(x)?\nRemember, E(Y|X = x) equals P(Y = 1|X = x) equals p(x), and this must lie in the interval 0 to 1.\nThe naive approach would be to set p(x) equals x-transpose-beta, just like in linear regression. But this doesn’t work! Nothing guarantees that x-transpose-beta stays between 0 and 1.\nInstead, we propose a model of the form: p(x) equals f of x-transpose-beta.\nHere, f is a function that maps from the real line to the interval 0 to 1. Think of it as a transformation that takes any real number—which could be x-transpose-beta—and squashes it into a probability.\nThis gives us a coherent model. The linear combination x-transpose-beta can be any real number, but f transforms it to stay between 0 and 1. And we still only need to estimate beta—the same number of parameters as linear regression.\nThe question is: what function f should we use? We’ll explore specific choices like the logistic function next.\n\n\n1: Case 2 - Categorical Y (Part 1)\nNow let’s move to the second case: categorical responses with more than two categories.\nIf Y represents membership in k different classes—A1 through Ak—its distribution is determined by k probabilities: p_j(x) equals the probability that Y is in class Aj given X equals x, for j equals 1 through k.\nFor instance, if Y is transportation choice—car, bus, or bike—we have three probabilities that sum to one.\nThe key constraint is that these probabilities must sum to 1. And notice: if k equals 2, we have only two categories, which reduces to the binary case.\n\n\n\nSlide 2: Case 2 - Categorical Y (Part 2)\nLet’s write this more formally using indicator variables.\nWe can represent Y as a vector of indicators: Y equals (indicator of A1, through indicator of Ak). This follows a multinomial distribution.\nThe conditional expectation E(Y|X = x) is then a vector with components p_1(x) through p_k(x)—each representing the probability of being in that category.\n\n\n\n3: Case 2 - Model for Categorical Y\nHow do we model this?\nHere’s a key simplification: to model E(Y|X = x), we only need to model k minus 1 probabilities—p_1(x) through p_(k-1)(x)—because the last probability p_k(x) is just 1 minus the sum of all the others.\nJust like in the binary case, we can propose: p_j(x) equals f of x-transpose-beta_j, for j equals 1 through k minus 1.\nWhat is the number of parameter in this model ? \\((k-1)p\\)\nThis is more complex than the binary case, but the principle is the same: use a transformation function to keep probabilities between 0 and 1.\n\n\nCase 3:\nFor count variables, we impose that the function \\(f\\) is nonnegative.\nNotice that while Y represents an integer here, the expectation of \\(Y\\) can be fractional.\nThink of poisson distribution of parameter 2.3\n\n\nModel Formulation\nLet g be a strictly monotonic function—meaning it’s always increasing or always decreasing. We call this the link function.\nA generalized linear model establishes a relationship of this type: g of E(Y|X = x) equals x-transpose-beta.\nThis is exactly what we proposed earlier! The linear combination x-transpose-beta can be any real number, but then g-inverse transforms it to the appropriate range for E(Y|X).\n\n\nRemarks on Model\nIn the previous slides, we denoted g-inverse as f. It’s the same concept, just different notation.\nWe generally assume that the distribution of Y given X belongs to an exponential family. That includes Bernoulli, multinomial, Poisson, and many others.\nThis is mainly because it allows us to compute the likelihood function easily.\n\n\nGeneral Objectives\nWhat are we trying to accomplish with a GLM?\nThe primary goal is to estimate beta in R^p—our vector of regression coefficients.\nWe use n independent observations of (Y, X), and we estimate using maximum likelihood estimation. The key is that the distribution of Y given X is known up to \\(\\beta\\).\nImportant: the link function g is NOT estimated. We don’t learn it from the data. Instead, we choose it based on the nature of our response variable.\nJust like in linear regression, we have inference and diagnostic tools available—hypothesis tests, confidence intervals, residual analysis, and model selection procedures.\n\n\nRemark on the Intercept\nA quick technical note about how we handle the intercept.\nAs in linear regression, among the regressors X^(1) through X^(p), we often assume that X^(1) equals 1 for all observations. This is to account for the presence of a constant term—the intercept.\n\n\nExample 1 - Linear Regression Model\nLet’s star to see how linear regression fits into the GLM framework.\nWe recover linear regression by taking the identity link function: g(t) equals t. So we’re not transforming anything.\nThen E(Y|X = x) equals x-transpose-beta itself. No transformation.\nIn the standard Gaussian linear model, Y given X follows a normal distribution with mean X-beta and variance sigma-squared.\nSo linear regression is actually a special case of GLM models! It’s what you get when you choose the identity for link function and assume a Gaussian distribution.\n\n\n\nExample 2 - Binary Case (Part 1)\nNow let’s look at the binary case, which is more interesting.\nThe link function g-inverse of x-transpose-beta stays in the interval 0 to 1.\nSo we have Y given X following a Bernoulli distribution with parameter g-inverse of X-transpose-beta. What should we choose for g-inverse?\nAny CDF of a continuous distribution on the real line works! A CDF maps the real line to [0,1], which is exactly what we need.\nThe standard choice for g-inverse is the CDF of a logistic distribution: g-inverse(t) equals e^t over (1 plus e^t).\nThis means g(t) equals the natural log of t over (1 minus t)—this is called the logit function.\nThis leads to the logistic model, which is the most important model in this chapter. We’ll spend some time on it.\n\n\nExample 3 - Count Data\nFinally, let’s look at count data.\nFor count data, we use the link function g(t) equals natural log of t, so g-inverse(t) equals e^t.\nThis gives E(Y|X) = exponential x-transpose-beta. This ensures the expected value is always positive, which makes sense for counts.\nSecond point - Highlight yellow: For the distribution of Y given X, defined on the natural numbers, we often assume a Poisson distribution—which is in the exponential family.\nThird point - Full specification: In this context, Y given X follows a Poisson distribution with parameter exponential X-transpose-beta.\n\n\n\nSummary\nLet’s step back and see the big picture.\nThere are 2 choices to make when setting up a GLM model: First, the distribution of Y given X. Second, the link function g that defines E(Y|X) equals g-inverse of X-transpose-beta.\nThese two choices are linked! The distribution you choose for Y given X guides which link function makes sense.\n\n\n\nThe 3 Common Cases\nTo summarize, here are the three most common cases you’ll encounter, with their standard choices.\nWhen Y is in {0,1}, Y given X follows a Bernoulli distribution, and by default we use g equals logit. We’ll see why this is the natural choice.\nWhen Y is in {A1 through Ak}, Y given X follows a multinomial distribution, and by default we again use g equals logit.\nWhen Y is in the natural numbers, Y given X typically follows a Poisson distribution—though sometimes negative binomial—and by default we use g equals natural log."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#context",
    "href": "teaching/glm/slides/logistic_model.draft.html#context",
    "title": "The Logistic Model",
    "section": "Context",
    "text": "Context\n\nLet’s now focus specifically on the binary case and work through it in detail.\nY is a binary variable—Y_k is in {0,1}.\nX equals (X^(1) through X^(p)) are our p explanatory variables or regressors.\nY given X equals x follows a Bernoulli distribution with parameter p(x), which equals the probability that Y equals 1 given X equals x.\nWe model p(x) as g-inverse of x-transpose-beta.\nWhere g-inverse is a strictly increasing function with values in [0,1]. This ensures our probabilities are valid.\nWe’ll begin by discussing how to choose g-inverse—or equivalently, the link function g.\n\n\\(Y\\) is a binary variable \\(Y_k \\in \\{0,1\\}\\)\n. . .\n\\(X = (X^{(1)}, \\ldots, X^{(p)})\\) are \\(p\\) regressors\n. . .\n\\(Y|X = x\\) follows a Bernoulli distribution with parameter \\(p(x) = P(Y = 1|X = x)\\). Model:\n. . .\n\n\\[p(x) = g^{-1}(x^T\\beta)\\]\n\nwhere \\(g^{-1}\\) is a strictly increasing function with values in \\([0, 1]\\)\n. . .\nApproach: We begin by discussing the choice of \\(g^{-1}\\) (or \\(g\\))"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-coronary-heart-disease",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-coronary-heart-disease",
    "title": "The Logistic Model",
    "section": "Example: Coronary Heart Disease",
    "text": "Example: Coronary Heart Disease\n\nLet’s work through a concrete example to see how this works in practice.\nWe have data on the presence of coronary heart disease—chd—as a function of age. So Y equals chd, taking values 0 or 1, and X equals age.\nWe want to estimate p(x)— the probability that a given individual with characteristics x suffers from coronary heart desease\nHere’s a scatter plot of the raw data. Each point is either 0 or 1 in functino of age. As you can see, it’s hard to discern a pattern directly.\n\n. . .\nData Description: Presence of chd as a function of age \\(Y = \\text{chd} \\in \\{0,1\\}\\), \\(X = \\text{age}\\)\n. . .\nWe want to estimate \\(p(x) = \\E(Y|X = x) = \\P(\\text{chd} = 1|X = x)\\) for all \\(x\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-simple-idea",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-simple-idea",
    "title": "The Logistic Model",
    "section": "Example: Simple Idea",
    "text": "Example: Simple Idea\n\nA simple first approach to understand the relationship.\nWe can group the x values by age class, then calculate the proportion of chd equals 1 in each class containing x.\nNow we see something clearer! The proportion increases with age. But this approach is crude and doesn’t give us a smooth model.\n\n\nGroup the \\(x\\) values by age class\nCalculate the proportion of \\(chd = 1\\) in the class containing \\(x\\)\n\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-glm-approach",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-glm-approach",
    "title": "The Logistic Model",
    "section": "Example: GLM Approach",
    "text": "Example: GLM Approach\n\nThe GLM approach gives us a smoother model.\nWe want to model p(x)—the probability that chd equals 1 given X equals x—as g-inverse of (beta_0 plus beta_1 times x).\nThis is a simple model with just an intercept and a slope. The age effect is captured by beta_1.\nWe need g-inverse to have values in [0,1] to ensure valid probabilities.\n\n. . .\nObjective: We want to model \\(p(x) = P(\\text{chd} = 1|X = x)\\) by: \\[p(x) = g^{-1}(\\beta_0 + \\beta_1 x)\\]\n. . .\nConstraint: We need \\(g^{-1}\\) to have values in \\([0, 1]\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-the-logit-model",
    "title": "The Logistic Model",
    "section": "Example: The Logit Model",
    "text": "Example: The Logit Model\n\nLet’s try the logit link—the most common choice.\ng-inverse(t) equals e^t over (1 plus e^t), which means g(t) equals the natural log of t over (1 minus t)—the logit function.\nHere are the results of g-inverse of (beta-hat_0 plus beta-hat_1 times x), obtained by maximum likelihood estimation. Notice how smooth the curve is, and how it respects the bounds—staying between 0 and 1.\n\n. . .\n\n\n\\[g^{-1}(t) = \\frac{e^t}{1 + e^t} \\quad \\text{i.e.} \\quad g(t) = \\ln\\left(\\frac{t}{1-t}\\right) = \\text{logit}(t)\\]\n\n\n. . .\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-the-probit-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-the-probit-model",
    "title": "The Logistic Model",
    "section": "Example: The Probit Model",
    "text": "Example: The Probit Model\n\nWhat if we use a different link function?\nIf Phi is the CDF of a standard normal distribution, we take g-inverse(t) equals Phi(t).\nHere are the results using the probit model with g-inverse of (beta-hat_0 plus beta-hat_1 times x), again obtained by MLE. Very similar to the logit model!\n\n. . .\nIf \\(\\Phi\\) is the CDF of a \\(\\mathcal{N}(0, 1)\\) distribution, we take \\(g^{-1}(t) = \\Phi(t)\\)\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-model-cloglog",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-model-cloglog",
    "title": "The Logistic Model",
    "section": "Example: Model cloglog",
    "text": "Example: Model cloglog\n\nLet’s try one more: the complementary log-log model.\ng-inverse(t) equals 1 minus e to the negative e^t.\nHere are the results. Notice this curve is not symmetric—it rises differently on the left versus the right compared to logit and probit.\n\n. . .\n\n\\[g^{-1}(t) = 1 - e^{-e^t}\\]\n\n. . .\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-comparison-of-the-three-models",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-comparison-of-the-three-models",
    "title": "The Logistic Model",
    "section": "Example: Comparison of the Three Models",
    "text": "Example: Comparison of the Three Models\n\nLet’s compare all three models side by side.\nLogit and probit give approximately the same result—the curves are nearly identical.\nThe cloglog differs slightly and is not symmetric. It approaches 1 much faster than it appraoches 0\nHere they are overlaid. For this data, the choice doesn’t matter much, but in principle there are differences.\n\n. . .\nlogit and probit give approximately the same result cloglog differs slightly and is not “symmetric”"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#which-link-function-to-choose-for-binary-case",
    "href": "teaching/glm/slides/logistic_model.draft.html#which-link-function-to-choose-for-binary-case",
    "title": "The Logistic Model",
    "section": "Which Link Function to Choose for Binary Case?",
    "text": "Which Link Function to Choose for Binary Case?\n\nBy default, we favor the logit model. This is the main choice for most applications.\nHowever, there are exceptions. We might choose something else - such as the probit model, complementary log-log, or log-log - when there’s a good reason to do so.\nIn the next few slides, we’ll return to the three usual choices and justify why logit is typically preferred.\n\n. . .\nQuestion: Which link function to choose in practice when \\(Y\\) is binary?\n. . .\nDefault choice: By default, we favor the logit model\n. . .\nExceptions: Unless there is a good reason to choose something else (probit model, complementary log-log, or log-log)\n. . .\nNext steps: We return to the 3 usual choices to justify this preference"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#details-on-the-probit-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#details-on-the-probit-model",
    "title": "The Logistic Model",
    "section": "Details on the Probit Model",
    "text": "Details on the Probit Model\n\nThe probit model has a specific theoretical justification. It’s appropriate when the binary variable Y given X equals x comes from thresholding a Gaussian latent variable Z of x.\nMathematically, Y given X equals x is the indicator that Z of x is greater than or equal to some threshold tau.\nThe key assumption is that Z of x follows a normal distribution with mean x transpose beta and variance sigma squared.\nLet’s see what this implies for probabilities. If Phi is the cumulative distribution function of a standard normal distribution - that’s a normal with mean zero and variance one - then the probability that Y equals 1 given X equals x can be expressed as the probability that Z of x is at least tau. This equals Phi of the quantity x transpose beta minus tau, all divided by sigma.\nThis formulation shows how the probit model naturally arises from a latent variable framework.\n\n. . .\nThe probit model is justified when the binary variable \\(Y|X = x\\) comes from thresholding a Gaussian latent variable \\(Z(x)\\):\n\n\\[(Y|X = x) = \\mathbf{1}_{Z(x) \\geq \\tau}\\]\n\nwhere \\(Z(x) \\sim \\mathcal{N}(x^T\\beta, \\sigma^2)\\)\n. . .\nIf \\(\\Phi\\) is the CDF of a \\(\\mathcal N(0,1)\\) \\[P(Y = 1|X = x) = P(Z(x) \\geq \\tau) = \\Phi\\left(\\frac{x^T\\beta - \\tau}{\\sigma}\\right)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#examples-in-probit-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#examples-in-probit-model",
    "title": "The Logistic Model",
    "section": "Examples in Probit Model",
    "text": "Examples in Probit Model\n\nLet me give you two concrete examples where the probit model makes intuitive sense.\nFirst example: Y represents a purchase decision - whether someone buys a product or not. Here, Z of x quantifies the utility or satisfaction the person would get from the good. When this utility exceeds a threshold, they make the purchase.\nSecond example: Y is a declared psychological state, such as whether someone reports being happy or depressed. In this case, Z of x represents a latent, unobserved measure of personal satisfaction. The binary outcome we observe is just whether this latent satisfaction crosses a certain threshold.\nIn both cases, the idea of an underlying continuous variable being thresholded to create a binary outcome is very natural.\n\n\n\\(Y\\) represents a purchase decision, and \\(Z(x)\\) quantifies the utility of the good\n\\(Y\\) is a declared psychological state (happiness, depression) and \\(Z(x)\\) is a latent, unobserved measure of personal satisfaction"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#probit-vs-logit-current-trends",
    "href": "teaching/glm/slides/logistic_model.draft.html#probit-vs-logit-current-trends",
    "title": "The Logistic Model",
    "section": "Probit vs Logit: Current Trends",
    "text": "Probit vs Logit: Current Trends\n\nThe probit model remains relatively popular among econometricians. There’s a tradition of using it in economics research.\nHowever, the general trend is that it’s increasingly being replaced by the logistic model across many fields.\nWhy is this happening? The logit model has many advantages that probit does not have. Let me highlight two key ones:\nFirst, interpretation of results. The logit model allows us to work with odds ratios, which are very intuitive to interpret.\nSecond, explicit formulas. The logit model has closed-form expressions that make computation and interpretation easier.\nFrom a theoretical perspective, there’s also good justification for this trend. The cumulative distribution function of the probit model is actually quite close to the CDF of the logit model, so we’re not losing much by switching.\n\n. . .\nEconometricians: The probit model remains relatively popular among econometricians…\n. . .\nGeneral trend: but it tends to be replaced by the logistic model\n. . .\nAdvantages of logit: The logit model has many advantages that probit does not have:\n\nInterpretation of results\nExplicit formulas\n\n. . .\nTheoretical justification: CDF of probit close to CDF of logit"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#remarks-on-cloglog-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#remarks-on-cloglog-model",
    "title": "The Logistic Model",
    "section": "Remarks on cloglog Model",
    "text": "Remarks on cloglog Model\n\nNow let’s turn to the complementary log-log, or cloglog, model.\nFor cloglog, the link function g of t is the natural log of negative log of one minus t. The inverse link is therefore g inverse of t equals one minus e to the negative e to the t.\nAn important property of this link function is that it’s not symmetric. Specifically, g of t does not equal negative g of one minus t. What this means in practice is that p of x approaches zero slowly, but approaches one very rapidly.\nIf the opposite behavior is true in your data - if probabilities approach one slowly but zero rapidly - you should use the log-log model instead, where g of t equals negative natural log of negative natural log of t.\nThe cloglog model is particularly useful in survival models, such as Cox model where this kind of loglog appears naturally.\n\n. . .\nThe modeling approach is \\(p(x) = g^{-1}(x^T\\beta)\\) with\n\n\\[g(t) = \\ln(-\\ln(1-t)) \\quad \\text{i.e.} \\quad g^{-1}(t) = 1 - e^{-e^t}\\]\n\n. . .\nNot symmetric in the sense that \\(g(t) \\neq -g(1-t)\\).\n\\(p(x)\\) approaches \\(0\\) slowly but \\(1\\) very rapidly\n. . .\nIf the opposite is true: take \\(g(t) = -\\ln(-\\ln(t))\\) (loglog model)\n. . .\nUseful in survival models (e.g. Cox)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#details-on-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#details-on-the-logit-model",
    "title": "The Logistic Model",
    "section": "Details on the Logit Model",
    "text": "Details on the Logit Model\n\nLet me give you three compelling reasons why the logit model is our preferred choice for binary regression.\nFirst, it provides a highly valued interpretation tool: odds ratios. These allow us to communicate results in an intuitive way that’s meaningful across many fields, from medicine to social sciences to marketing.\nSecond, it’s more practical from a theoretical point of view. The mathematical properties make it easier to work with, and we have good theoretical foundations for why it arises naturally.\nThird, the logit model is the natural model in many situations. As we’ll see in the next slide, it emerges automatically under common distributional assumptions.\n\n\nHighly valued interpretation tool: odds-ratios.\nMore “practical” from a theoretical point of view.\nNatural model in many situations."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#theoretical-motivation-of-logit",
    "href": "teaching/glm/slides/logistic_model.draft.html#theoretical-motivation-of-logit",
    "title": "The Logistic Model",
    "section": "Theoretical Motivation of Logit",
    "text": "Theoretical Motivation of Logit\n\nNow let’s see a beautiful theoretical result that explains why logit is so natural.\nWe will show in exercises that there’s an important connection between the logit model and Gaussian distributions.\nHere’s the setup: If the two groups of individuals associated with Y equals 0 and Y equals 1 have a Gaussian distribution of X with different means - that is, for m_0 not equal to m_1, we have X given Y equals 0 follows a normal distribution with mean m_0 and covariance Sigma, and X given Y equals 1 follows a normal distribution with mean m_1 and the same covariance Sigma - then the probability that Y equals 1 given X equals x automatically follows a logistic model.\nThis is a powerful result! It tells us that whenever we have two normally distributed groups that differ in their means but share the same covariance structure, the logit model emerges naturally.\nAnd there’s more. The previous result remains true for any exponential family instead normal distributions. So this property of the logit model is quite general - it’s not limited to just Gaussian distributions.\nThis theoretical foundation helps explain why the logit model appears so frequently in practice.\n\n. . .\nWe will show in exercises that:\nIf the two groups of individuals associated with \\(Y = 0\\) and \\(Y = 1\\) have a Gaussian distribution of \\(X\\) with different means, i.e. for \\(m_0 \\neq m_1\\),\n\n\\(X|(Y = 0) \\sim \\mathcal N(m_0, \\Sigma) \\and X|(Y = 1) \\sim \\mathcal N(m_1, \\Sigma)\\)\n\nthen \\(\\mathbb P(Y = 1|X = x)\\) follows a logistic model.\n. . .\nThe previous result remains true for any distribution from the exponential family instead of \\(\\mathcal N\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#summary-for-binary-variables",
    "href": "teaching/glm/slides/logistic_model.draft.html#summary-for-binary-variables",
    "title": "The Logistic Model",
    "section": "Summary for Binary Variables:",
    "text": "Summary for Binary Variables:\n\nLet’s summarize our discussion about modeling binary variables.\nIf Y is a binary variable, then Y given X equals x follows a Bernoulli distribution with parameter p of x.\nIn a GLM model for Y, we set p of x equals g inverse of x transpose beta, where g is the link function. Now, which link function should we choose?\nBy default, we use the logit function, which is the most natural choice for all the reasons we’ve discussed: odds ratio interpretation, theoretical justification, and mathematical convenience.\nPossibly, we might use probit if we have good reasons to justify it - for example, if there’s a clear latent variable story. But keep in mind that the results will typically be very similar to logit, so unless you have a compelling reason, logit is simpler.\nWe might use cloglog, or its mirror image loglog, if we have good reasons to justify it. The main reasons would be: first, strong asymmetry in p of x - probabilities that approach zero and one at very different rates - and second, a connection with a Cox model when dealing with discretized survival data.\nIn the following sections, we will focus on the logit model, since it’s the default choice and the most widely applicable.\nThis completes our journey through link function selection for binary outcomes. The key takeaway is: start with logit, and only consider alternatives when you have a specific, justifiable reason to do so.\n\n. . .\nIf \\(Y\\) is a binary variable, \\((Y|X = x) \\sim \\mathcal B(p(x))\\).\nIn a GLM model for \\(Y\\), we set \\(p(x) = g^{-1}(x^T\\beta)\\) where \\(g\\) is:\n\nby default the logit function, which is the most natural;\npossibly probit if we have good reasons to justify it (but the results will be similar to logit);\ncloglog (or loglog) if we have good reasons to justify it (strong asymmetry of \\(p(x)\\), connection with a Cox model).\n\n. . .\nIn the following, we will focus on the logit model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#outline",
    "href": "teaching/glm/slides/logistic_model.draft.html#outline",
    "title": "The Logistic Model",
    "section": "Outline",
    "text": "Outline\n\nNow that we’ve established why the logit model is our preferred choice, let’s see how to work with it in practice.\nWe’ll cover four essential aspects: First, how to interpret the model and understand what the coefficients tell us. Second, how to estimate beta from a dataset. Third, how to evaluate the quality of our estimation. And fourth, how to exploit the model to make predictions and perform classification.\nLet’s begin with model interpretation.\n\n\ninterpret the model,\nestimate \\(\\beta\\) from a dataset,\nevaluate the quality of estimation,\nexploit it to make predictions/classification."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#interpretation-of-the-logistic-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#interpretation-of-the-logistic-model",
    "title": "The Logistic Model",
    "section": "Interpretation of the Logistic Model",
    "text": "Interpretation of the Logistic Model\n\nLet’s start with the mathematical form of the logistic model.\nIf x is a vector with p components - x superscript 1 through x superscript p - then our model is:\np of x equals logit inverse of x transpose beta, which equals e to the x transpose beta, divided by 1 plus e to the x transpose beta.\nWhat does this tell us about how each variable affects the probability?\nFirst key point: p of x is increasing with x^{(j)} if beta j is positive, and decreasing otherwise. So the sign of the coefficient tells us the direction of the effect.\nSecond key point: The larger the absolute value of beta j, the stronger the discriminatory power of regressor X j. In other words, when the coefficient has a large magnitude, a small variation in x j can cause a large variation in p of x.\n\nIf \\(x=(x^{(1)}, \\dots, x^{(p)}) \\in \\mathbb R^{p \\times 1}\\)\n\n\\[p(x) = \\text{logit}^{-1}(x^T\\beta) = \\frac{e^{x^T\\beta}}{1 + e^{x^T\\beta}}\\]\n\n\n\\(x^{(j)} \\to p(x)\\) is increasing if \\(\\beta_j &gt; 0\\), decreasing otherwise.\nThe larger \\(|\\beta_j|\\) is, the stronger the discriminatory power of regressor \\(X^{(j)}\\) (a small variation in \\(x^{(j)}\\) can cause a large variation in \\(p(x)\\))."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#shape-of-logit-function",
    "href": "teaching/glm/slides/logistic_model.draft.html#shape-of-logit-function",
    "title": "The Logistic Model",
    "section": "Shape of logit function",
    "text": "Shape of logit function\n\nLet me show you what this relationship looks like visually.\nThe shape of the function that maps x superscript j to p of x has this characteristic S-curve, or sigmoid shape. You can see how the probability transitions smoothly from near zero to near one as the predictor increases. The steepness of this S-curve in the middle region is determined by the magnitude of beta j - larger coefficients create steeper transitions.\nThis S-shaped relationship is one of the appealing features of the logistic model - it naturally constrains probabilities to lie between zero and one, and it has this smooth, interpretable transition region.\n\n. . .\n\n\n\nshape of \\(x^{(j)} \\to p(x)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-bmi-study-french-imc",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-bmi-study-french-imc",
    "title": "The Logistic Model",
    "section": "Example: BMI Study (French: IMC)",
    "text": "Example: BMI Study (French: IMC)\n\nLet’s look at a concrete example to make this more tangible. This is a BMI study with French data - BMI is called IMC in French.\nFor each of 5,300 patients, we observe several variables:\nOur outcome variable Y is binary: it equals 1 if BMI is greater than 35, and 0 otherwise. So we’re trying to predict obesity.\nOur predictor variables include: AGE in years, DBP which is diastolic blood pressure - that’s the low pressure measurement, SEXE indicating male or female, ACTIV which equals 1 if the person engages in intense sports activity and 0 otherwise, WALK which equals 1 if they walk or cycle to work and 0 otherwise, and MARITAL which is marital status with 6 categories: married, widowed, divorced, separated, single, or cohabiting.\n\nFor each of the 5300 patients, we observe:\n\n\\(Y\\): 1 if BMI &gt; 35, 0 otherwise\nAGE\nDBP: low pressure (diastolic)\nSEXE: male or female\nACTIV: 1 if intense sports activity, 0 otherwise\nWALK: 1 if walking or cycling to work, 0 otherwise\nMARITAL: marital status (6 categories: married, widowed, divorced, separated, single or cohabiting)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#model-definition",
    "href": "teaching/glm/slides/logistic_model.draft.html#model-definition",
    "title": "The Logistic Model",
    "section": "Model Definition",
    "text": "Model Definition\n\nOur goal is to model the probability that Y equals 1 given X, where X groups all the predictor variables we just mentioned, excluding Y of course.\nIn R, implementing this is straightforward. We use the glm function - that’s generalized linear model - with the family equals binomial option. This tells R we’re doing logistic regression.\nThe syntax would be: glm of Y tilde AGE plus DBP plus SEXE plus ACTIV plus WALK plus MARITAL, with family equals binomial.\nThe tilde notation means “is modeled by” and the plus signs indicate we’re including all these variables as predictors.\n\n. . .\nWe seek to model \\(P(Y = 1|X)\\) where \\(X\\) groups the previous variables (excluding \\(Y\\)).\n. . .\nIn R, we use the glm function with the family=binomial option.\nglm(Y ~ AGE + DBP + SEXE + ACTIV + WALK + MARITAL, family=binomial)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#model-results",
    "href": "teaching/glm/slides/logistic_model.draft.html#model-results",
    "title": "The Logistic Model",
    "section": "Model Results",
    "text": "Model Results\n\nNow let’s look at what the model produces.\nThe output gives us estimated coefficients, as in linear regression\nLooking at the significant variables - those marked with asterisks - we see several interesting patterns:\nDBP, diastolic blood pressure, has a positive coefficient and is highly significant. Higher blood pressure is associated with higher probability of obesity.\nBeing female - SEXE FEMME - has a positive significant coefficient, indicating women in this sample have higher probability of high BMI.\nWALK1 and ACTIV1 both have negative coefficients and are highly significant. This makes intuitive sense: people who walk or cycle to work, and people who engage in intense physical activity, have lower probability of high BMI.\nAGE appears not to be significant in this simple linear form.\nLooking at the marital status variables - MARITAL 2 through 6 - none of them are statistically significant. This suggests we want to remove the MARITAL variable from the model to simplify it.\n\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-2.810240\n0.294316\n-9.548\n&lt; 2e-16\n***\n\n\nAGE\n-0.004407\n0.002717\n-1.622\n0.105\n\n\n\nDBP\n0.017581\n0.003283\n5.356\n8.53e-08\n***\n\n\nSEXEFEMME\n0.544916\n0.081261\n6.706\n2.00e-11\n***\n\n\nWALK1\n-0.409344\n0.095972\n-4.265\n2.00e-05\n***\n\n\nACTIV1\n-0.789734\n0.126653\n-6.235\n4.51e-10\n***\n\n\nMARITAL2\n0.070132\n0.149638\n0.469\n0.639\n\n\n\nMARITAL3\n-0.071318\n0.127510\n-0.559\n0.576\n\n\n\nMARITAL4\n0.188228\n0.206598\n0.911\n0.362\n\n\n\nMARITAL5\n0.070613\n0.115928\n0.609\n0.542\n\n\n\nMARITAL6\n-0.150165\n0.157687\n-0.952\n0.341\n\n\n\n\n\n. . .\nThe interpretation is similar to that of a linear regression model.\n. . .\nWe want to remove the MARITAL variable from the model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#model-results-with-age2",
    "href": "teaching/glm/slides/logistic_model.draft.html#model-results-with-age2",
    "title": "The Logistic Model",
    "section": "Model Results with \\(AGE^2\\)",
    "text": "Model Results with \\(AGE^2\\)\n\nPerhaps AGE has a non-linear effect. Let’s try adding AGE squared to the model.\nIn R, we write: glm of Y tilde AGE plus I of AGE squared\nNow look at what happens.\nBoth AGE and AGE squared are now highly significant! The positive coefficient on AGE and negative coefficient on AGE squared suggests an inverted U-shape relationship: the probability of high BMI increases with age initially, then decreases at older ages.\nAll our other key variables remain significant with similar interpretations.\nFor someone for which WALK1 equals 0 and ACTIV1 equals 0 - so they don’t walk to work and don’t do intense sports - the probability that Y equals 1 given their age and blood pressure is: logit inverse of negative 3.95 plus 0.064 times AGE minus 0.00068 times AGE squared plus 0.0122 times DBP.\nNow, for someone for which WALK1 equals 0 and ACTIV1 equals 1 - they do engage in intense sports activity - we get the same formula but we subtract 0.657. That’s the coefficient of ACTIV1 in red. This person has a lower probability of high BMI, all else being equal, and we can quantify exactly how much lower.\nThis demonstrates how we can use the fitted model to compute predicted probabilities for individuals with different covariate profiles.\n\n. . .\nglm(Y ~ AGE + I(AGE^2) + DBP + SEXE + WALK + ACTIV, family=binomial)\nGives\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\nFor someone for which WALK1=0 and ACTIV1=0:\n\n\\(P(Y = 1|\\text{AGE}, \\text{DBP}) = \\text{logit}^{-1}(-3.95 + 0.064 \\times \\text{AGE} - 0.00068 \\times \\text{AGE}^2 + 0.0122 \\times \\text{DBP})\\)\n\n. . .\nFor someone for which WALK1=0 and ACTIV1=1:\n\n\\(P(Y = 1|\\text{AGE}, \\text{DBP}) = \\text{logit}^{-1}(-3.95 + 0.064 \\times \\text{AGE} - 0.00068 \\times \\text{AGE}^2 + 0.0122 \\times \\text{DBP} \\color{red}{ - 0.657})\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#odds",
    "href": "teaching/glm/slides/logistic_model.draft.html#odds",
    "title": "The Logistic Model",
    "section": "Odds",
    "text": "Odds\n\nLet’s look at how odds relate to probability. When we know the probability pp of an event, its odds are simply p/(1−p)p/(1−p). For example, if the probability of winning is 0.75, then the odds are 0.75/0.25=30.75/0.25=3. That means the event is three times as likely to happen than not to happen.\n​\nIn betting, when you hear “odds 3 to 1,” it means for every 3 people who bet on event A, there’s 1 person betting against it. That also means, if you pick a random bettor, there’s a 3 out of 4 chance they chose A, giving you a probability p=3/4p=3/4 and for not-A, 1/41/4.\n\n\n\\[\\text{odds} = \\frac{p}{1-p}\\]\n\n. . .\nBetting interpretation for example, \\(3\\) to \\(1\\) means that for \\(3\\) people betting on \\(A\\), \\(1\\) person bets on \\(B\\).\n. . .\nSo a randomly chosen bettor has a probability of \\(p=3/4\\) of betting on \\(A\\) and \\(1-p=1/4\\) on betting on \\(B\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#odds-given-xx",
    "href": "teaching/glm/slides/logistic_model.draft.html#odds-given-xx",
    "title": "The Logistic Model",
    "section": "Odds given \\(X=x\\)",
    "text": "Odds given \\(X=x\\)\n\n​\nJust like before, if we consider the probability of an event given some condition—say, the odds of Y=1Y=1 given X=xX=x—we use p(x)p(x) to denote the probability. The odds in this case are odds(x)=p(x)1−p(x)odds(x)=1−p(x)p(x), where p(x)=P(Y=1∣X=x)p(x)=P(Y=1∣X=x) If \\(p\\) is the probability of an event \\(A\\), then its odds are:\n\n. . .\nSimilarly, the odds of obtaining \\(Y = 1\\) given \\(X = x\\) is:\n\n\\[\\text{odds}(x) = \\frac{p(x)}{1 - p(x)}\\]\n\n. . .\nwhere \\(p(x) = P(Y = 1|X = x)\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#odds-ratio",
    "href": "teaching/glm/slides/logistic_model.draft.html#odds-ratio",
    "title": "The Logistic Model",
    "section": "Odds Ratio",
    "text": "Odds Ratio\n\nNow let’s talk about the odds ratio, which compares odds between two different individuals or groups.\nIf we have two people with characteristics x1x1 and x2x2, the odds ratio tells us how the odds compare between them. It’s calculated as OR(x1,x2)=odds(x1)odds(x2)OR(x1,x2)=odds(x2)odds(x1), which expands to that double fraction you see on the slide.\nHere’s something really important though: don’t confuse odds ratio with probability ratio. They’re not the same thing.\nThe odds ratio is a ratio of odds, not probabilities. So if someone has odds of 3 and another person has odds of 1.5, the odds ratio is 2. But that doesn’t mean one person has twice the probability of the other.\nThere’s only one exception to watch out for: when both probabilities are very small—say, less than 0.1. In that case, 1−p(x1)1−p(x1) is approximately 1, and 1−p(x2)1−p(x2) is also approximately 1. So the denominators basically cancel out, and the odds ratio becomes pretty close to the probability ratio. But this only works for rare events.\nOtherwise, always remember: odds ratios and probability ratios are different beasts, so don’t treat them the same way.\n\n. . .\nIf two individuals have characteristics \\(x_1\\) and \\(x_2\\) respectively, we call the odds ratio between \\(x_1\\) and \\(x_2\\):\n\\[OR(x_1, x_2) = \\frac{\\text{odds}(x_1)}{\\text{odds}(x_2)} = \\frac{\\frac{p(x_1)}{1-p(x_1)}}{\\frac{p(x_2)}{1-p(x_2)}}\\]\n. . .\n\n\n\n\n\n\nWarning\n\n\n\nDO NOT CONFUSE ODDS RATIO WITH PROBABILITY RATIO\nOnly possible exception: if \\(p(x_1)\\) and \\(p(x_2)\\) are very small because then \\(1 - p(x_1) \\approx 1\\) and \\(1 - p(x_2) \\approx 1\\), so that \\(OR(x_1, x_2) \\approx p(x_1)/p(x_2)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#link-with-proba.-ratio",
    "href": "teaching/glm/slides/logistic_model.draft.html#link-with-proba.-ratio",
    "title": "The Logistic Model",
    "section": "Link with Proba. Ratio",
    "text": "Link with Proba. Ratio\n\n\n\n. . .\nHowever, it remains that:\n\\[\\begin{aligned}\nOR(x_1, x_2) &gt; 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} &gt; 1\\\\\nOR(x_1, x_2) &lt; 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} &lt; 1\\\\\nOR(x_1, x_2) = 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} = 1\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#other-property-of-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.draft.html#other-property-of-odds-ratio",
    "title": "The Logistic Model",
    "section": "Other Property of Odds Ratio",
    "text": "Other Property of Odds Ratio\n. . .\n\\(OR(x_1, x_2)\\) accentuates the differences compared to \\(p(x_1)/p(x_2)\\):\n\\[\\begin{aligned}\nOR(x_1, x_2) &gt; \\frac{p(x_1)}{p(x_2)} &\\text{ when } OR(x_1, x_2) &gt; 1\\\\\nOR(x_1, x_2) &lt; \\frac{p(x_1)}{p(x_2)} &\\text{ when } OR(x_1, x_2) &lt; 1\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#examples-using-odds-ratios",
    "href": "teaching/glm/slides/logistic_model.draft.html#examples-using-odds-ratios",
    "title": "The Logistic Model",
    "section": "Examples Using Odds Ratios",
    "text": "Examples Using Odds Ratios\n. . .\nA logistic regression is most often used to compare the behavior of two individuals with respect to the variable of interest.\n. . .\nExamples:\n\nprobability of purchase depending on whether or not one has been the subject of a personalized promotion;\nfor a given vehicle, probability of experiencing a breakdown according to age;\nprobability of recovery according to the treatment used;"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#odds-ratio-in-logistic-regression",
    "href": "teaching/glm/slides/logistic_model.draft.html#odds-ratio-in-logistic-regression",
    "title": "The Logistic Model",
    "section": "Odds Ratio in Logistic Regression",
    "text": "Odds Ratio in Logistic Regression\n. . .\nIt holds that \\(\\text{odds}(x) = \\frac{p(x)}{1 - p(x)} = \\exp(x^T \\beta)\\)\nHence,\n\n\\(OR(x_1, x_2) = \\frac{\\text{odds}(x_1)}{\\text{odds}(x_2)} = \\exp((x_1 - x_2)^T \\beta)\\)\n\n. . .\nIf the two individuals differ only by regressor \\(j\\), then\n\\(OR(x_1, x_2) = \\exp(\\beta_j (x_1^{(j)} - x_2^{(j)}))\\)\n. . .\nIf regressor \\(j\\) is binary (\\(x_1^{(j)} = 1\\) while \\(x_2^{(j)} = 0\\)):\n\\(OR(x_1, x_2) = \\exp(\\beta_j)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#key-summary-statement",
    "href": "teaching/glm/slides/logistic_model.draft.html#key-summary-statement",
    "title": "The Logistic Model",
    "section": "Key Summary Statement",
    "text": "Key Summary Statement\n. . .\nIn a logistic regression model, \\(\\beta_j\\) is interpreted as the logarithm of the odds-ratio between two individuals differing by a quantity of \\(1\\) on regressor \\(j\\), all else being equal.\n. . .\nIn brief: \\(\\exp(\\beta_j) = OR(x^{(j)} + 1, x^{(j)})\\)\nIf regressor \\(j\\) is binary (absence or presence of a certain characteristic):\n. . .\n\\(\\exp(\\beta_j)\\) is simply the OR between the presence or absence of this characteristic, all else being equal."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-1-intense-sports-activity",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-1-intense-sports-activity",
    "title": "The Logistic Model",
    "section": "Example 1: Intense Sports Activity",
    "text": "Example 1: Intense Sports Activity\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\n. . .\nThe Odds Ratio corresponding to practicing or not practicing intense sports activity is, all else being equal:\n. . .\n\\(\\exp(-0.657) \\approx 0.52\\)\n. . .\nThe odds of obesity occurrence therefore decrease by half for individuals practicing intense sports activity.\n(The odds, not the probability!)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-2-diastolic-pressure",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-2-diastolic-pressure",
    "title": "The Logistic Model",
    "section": "Example 2: Diastolic Pressure",
    "text": "Example 2: Diastolic Pressure\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\nThe OR for a diastolic pressure difference of \\(+20\\) is:\n. . .\n\\(\\exp(0.0121546 \\times 20) \\approx 1.28\\)\n. . .\nThe odds of obesity occurrence therefore increase by \\(28\\%\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#the-framework",
    "href": "teaching/glm/slides/logistic_model.draft.html#the-framework",
    "title": "The Logistic Model",
    "section": "The Framework",
    "text": "The Framework\n. . .\nWe observe \\(n\\) i.i.d. realizations \\((Y_i, X_i)\\) where \\(Y_i \\in \\{0, 1\\}\\) and \\(X_i \\in \\mathbb{R}^p\\).\n. . .\nWe denote \\(p(x_i) = P(Y_i = 1|X_i = x_i)\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#the-logistic-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#the-logistic-model",
    "title": "The Logistic Model",
    "section": "The Logistic Model",
    "text": "The Logistic Model\n. . .\nWe assume the logistic model: for all \\(i\\),\n\n\\[p(x_i) = \\text{logit}^{-1}(x_i^T \\beta) = \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\]\n\n. . .\nwhere \\(\\beta = (\\beta_1, \\ldots, \\beta_p)^T\\) and \\(x_i^T \\beta = \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#parameter-estimation",
    "href": "teaching/glm/slides/logistic_model.draft.html#parameter-estimation",
    "title": "The Logistic Model",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n. . .\n\n\\[p(x_i) = \\text{logit}^{-1}(x_i^T \\beta) = \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\]\n\nWe will estimate \\(\\beta\\) by maximizing the likelihood.\n. . .\nWe will denote \\(p_\\beta(x_i)\\) to emphasize the dependence of \\(p(x_i)\\) on \\(\\beta\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#likelihood-calculation",
    "href": "teaching/glm/slides/logistic_model.draft.html#likelihood-calculation",
    "title": "The Logistic Model",
    "section": "Likelihood Calculation",
    "text": "Likelihood Calculation\n. . .\nFor all \\(i\\), \\(Y_i|(X_i = x_i)\\) follows the distribution \\(B(p_\\beta(x_i))\\). Therefore\n\n\n\\[P(Y_i = y_i|X_i = x_i) = p_\\beta(x_i)^{y_i}(1 - p_\\beta(x_i))^{1-y_i}\\]\n\n\n. . .\nfor all \\(y_i \\in \\{0, 1\\}\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#likelihood",
    "href": "teaching/glm/slides/logistic_model.draft.html#likelihood",
    "title": "The Logistic Model",
    "section": "Likelihood",
    "text": "Likelihood\n. . .\nBy independence, we obtain the likelihood\n\n\\[\\ell(\\beta, y_1, \\ldots, y_n, x_1, \\ldots, x_n) = \\prod_{i=1}^n p_\\beta(x_i)^{y_i}(1 - p_\\beta(x_i))^{1-y_i}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#log-likelihood",
    "href": "teaching/glm/slides/logistic_model.draft.html#log-likelihood",
    "title": "The Logistic Model",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n. . .\nTaking the log and replacing \\(p(x_i)\\) by its expression, we obtain the log-likelihood:\n\n\n\\[\\begin{aligned}\nL(\\beta, y_1, \\ldots, y_n, x_1, \\ldots, x_n) &= \\ln(\\ell) \\\\\n&=\\sum_{i=1}^n \\left[y_i x_i^T \\beta - \\ln(1 + e^{x_i^T \\beta})\\right]\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#mle-calculation",
    "href": "teaching/glm/slides/logistic_model.draft.html#mle-calculation",
    "title": "The Logistic Model",
    "section": "MLE Calculation",
    "text": "MLE Calculation\n. . .\nThe MLE \\(\\hat{\\beta}\\), if it exists, cancels the gradient of \\(L\\) with respect to \\(\\beta\\). This gradient equals\n\n\\[\\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^n x_i \\left(y_i - \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\right)\\]\n\nWe therefore need to solve a system of \\(p\\) equations with \\(p\\) unknowns.\n. . .\nBut the solution is not explicit: we resort to numerical methods (Newto-Raphso algo)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#remarks",
    "href": "teaching/glm/slides/logistic_model.draft.html#remarks",
    "title": "The Logistic Model",
    "section": "Remarks",
    "text": "Remarks\n. . .\nThis is a classic situation when using advanced statistical models: we often resort to optimization algorithms.\n. . .\nDoes the solution exist? Is it unique?"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#mle-uniqueness",
    "href": "teaching/glm/slides/logistic_model.draft.html#mle-uniqueness",
    "title": "The Logistic Model",
    "section": "MLE Uniqueness",
    "text": "MLE Uniqueness\n. . .\nLet \\(X\\) be the design matrix \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\)\n. . .\n\n\n\n\n\n\nProposition\n\n\n\nIf \\(\\text{rank}(X) = p\\), then the MLE, if it exists, is unique."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#proof-of-mle-uniqueness",
    "href": "teaching/glm/slides/logistic_model.draft.html#proof-of-mle-uniqueness",
    "title": "The Logistic Model",
    "section": "Proof of MLE Uniqueness",
    "text": "Proof of MLE Uniqueness\n. . .\nIt suffices to show that \\(L\\) is strictly concave in \\(\\beta\\).\n. . .\nHessian Matrix of \\(L\\):\n\\[\\frac{\\partial^2 L}{\\partial \\beta^2} = -\\sum_{i=1}^n p_\\beta(x_i)(1 - p_\\beta(x_i)) x_i x_i^T\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#hessian-properties",
    "href": "teaching/glm/slides/logistic_model.draft.html#hessian-properties",
    "title": "The Logistic Model",
    "section": "Hessian Properties",
    "text": "Hessian Properties\n. . .\nIt is negative semi-definite. Moreover, for all \\(u \\in \\mathbb{R}^p\\),\n\\[\\begin{aligned}\nu^T \\frac{\\partial^2 L}{\\partial \\beta^2} u = 0 &\\Leftrightarrow u^T x_i x_i^T u = 0 \\text{ for all } i\\\\\n&\\Leftrightarrow u^T x_i = 0 \\text{ for all } i\\\\\n&\\Leftrightarrow Xu = 0\\\\\n&\\Leftrightarrow u = 0\n\\end{aligned}\\]\nsince \\(\\text{rank}(X) = p\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#conclusion",
    "href": "teaching/glm/slides/logistic_model.draft.html#conclusion",
    "title": "The Logistic Model",
    "section": "Conclusion",
    "text": "Conclusion\n. . .\nThus, for all \\(u \\neq 0\\),\n\n\\[u^T \\frac{\\partial^2 L}{\\partial \\beta^2} u &lt; 0\\]\n\nThe Hessian matrix is negative definite and therefore \\(L\\) is strictly concave,\nThe MLE is unique"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#about-mle-existence",
    "href": "teaching/glm/slides/logistic_model.draft.html#about-mle-existence",
    "title": "The Logistic Model",
    "section": "About MLE Existence",
    "text": "About MLE Existence\n. . .\nAlthough \\(L\\) is strictly concave, its maximum can occur at infinity (think of the \\(\\ln\\) function), in which case \\(\\hat{\\beta}\\) does not exist.\n. . .\nThis occurs if there is non-overlap, i.e., separation by a hyperplane of the \\(x_i\\) for which \\(y_i = 0\\) and those for which \\(y_i = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#non-overlap-situation",
    "href": "teaching/glm/slides/logistic_model.draft.html#non-overlap-situation",
    "title": "The Logistic Model",
    "section": "Non-Overlap Situation",
    "text": "Non-Overlap Situation\n. . .\nMathematically, there is non-overlap if there exists \\(\\alpha \\in \\mathbb{R}^p\\) such that\n\\[\\begin{cases}\n\\text{for all } i \\text{ such that } y_i = 0, & \\alpha^T x_i \\geq 0 \\\\\n\\text{for all } i \\text{ such that } y_i = 1, & \\alpha^T x_i \\leq 0\n\\end{cases}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#illustration-of-non-overlap-situation",
    "href": "teaching/glm/slides/logistic_model.draft.html#illustration-of-non-overlap-situation",
    "title": "The Logistic Model",
    "section": "Illustration of Non-Overlap Situation",
    "text": "Illustration of Non-Overlap Situation"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#non-overlap-and-existence",
    "href": "teaching/glm/slides/logistic_model.draft.html#non-overlap-and-existence",
    "title": "The Logistic Model",
    "section": "Non-Overlap and Existence",
    "text": "Non-Overlap and Existence\n. . .\n\n\n\n\n\n\nProposition (admitted)\n\n\n\nIn case of non-overlap, the estimator \\(\\hat{\\beta}\\) does not exist, in the sense that \\(L(\\beta)\\) is maximal when \\(\\|\\beta\\| \\to \\infty\\) (in one or several directions).\n\n\n. . .\nFor all \\(x\\), \\(\\hat{p}(x) = \\in \\{0,1\\}\\), depending on the position of \\(x\\) relative to the separating hyperplane.\n. . .\nNevertheless, there is a “dead zone” in the middle of the \\(2\\) point clouds, because the separating hyperplane is not necessarily unique."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#beyond-the-dead-zone",
    "href": "teaching/glm/slides/logistic_model.draft.html#beyond-the-dead-zone",
    "title": "The Logistic Model",
    "section": "Beyond the Dead Zone",
    "text": "Beyond the Dead Zone\n. . .\nBeyond this dead zone, classification is very simple (\\(0\\) or \\(1\\)).\n. . .\nBut no interpretation of the model is possible (the OR are worth \\(0\\) or \\(+\\infty\\))."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#existence-and-uniqueness-of-mle",
    "href": "teaching/glm/slides/logistic_model.draft.html#existence-and-uniqueness-of-mle",
    "title": "The Logistic Model",
    "section": "Existence and Uniqueness of MLE",
    "text": "Existence and Uniqueness of MLE\n. . .\nWe say there is overlap when no hyperplane can separate the red points from the blue points.\n. . .\n\n\n\n\n\n\nProposition (admitted)\n\n\n\nIf \\(\\text{rank}(X) = p\\) and there is overlap, then the MLE exists and is unique.\n\n\nUnder these conditions, we can therefore search for the MLE using the Newton-Raphson algorithm.\n\nthe maximum exists,\nthe function to optimize is strictly concave and there is therefore no local maximum, only a global maximum."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#fisher-information-recall",
    "href": "teaching/glm/slides/logistic_model.draft.html#fisher-information-recall",
    "title": "The Logistic Model",
    "section": "Fisher Information (Recall)",
    "text": "Fisher Information (Recall)\n. . .\nLet \\(X\\) be the design matrix (whose rows are the vectors \\(x_i\\)).\n. . .\nLet \\(J_n(\\beta)\\) be the Fisher information matrix:\n\n\\[J_n(\\beta) = -E\\left[\\frac{\\partial^2 L}{\\partial \\beta^2}(\\beta) \\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#asymptotic-efficiency",
    "href": "teaching/glm/slides/logistic_model.draft.html#asymptotic-efficiency",
    "title": "The Logistic Model",
    "section": "Asymptotic Efficiency",
    "text": "Asymptotic Efficiency\n\n\n\n\n\n\nProposition (admitted)\n\n\n\nIn the logistic regression model, if\n\nthe distribution of the regressors \\((X_1, \\ldots, X_p)\\) has compact support,\n\\(\\text{rank}(X) = p\\),\nthe smallest eigenvalue of \\(X^T X\\) tends to infinity with \\(n\\),\n\nthen\n\nthe maximum likelihood estimator \\(\\hat{\\beta}\\) is consistent;\n\\(J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\)\n\nwhere \\(I_p\\) is the identity matrix of size \\(p\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#comments",
    "href": "teaching/glm/slides/logistic_model.draft.html#comments",
    "title": "The Logistic Model",
    "section": "Comments",
    "text": "Comments\n. . .\nUnder these conditions, the MLE therefore exists for sufficiently large \\(n\\). In fact, there is necessarily overlap when \\(n\\) is large.\n. . .\nIt is asymptotically efficient (= minimal asymptotic variance)\n. . .\nThe Fisher information matrix \\(J_n(\\beta)\\) can be calculated\n. . .\nWe will be able to rely on asymptotic normality to perform tests and construct confidence intervals"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#comparison-with-linear-regression",
    "href": "teaching/glm/slides/logistic_model.draft.html#comparison-with-linear-regression",
    "title": "The Logistic Model",
    "section": "Comparison with Linear Regression",
    "text": "Comparison with Linear Regression\n. . .\nThe formula for \\(\\hat{\\beta}\\) is explicit: \\(\\hat{\\beta} = (X^T X)^{-1} X^T Y\\);\n. . .\nIts expectation and variance are explicit;\n. . .\nIn the Gaussian model (\\(Y|X\\) Gaussian), the distribution of \\(\\hat{\\beta}\\) is explicit, which allows constructing exact tests (Student, Fisher).\n. . .\nIf the model is not Gaussian, these tests are valid asymptotically."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#comparison-with-linear-regression-1",
    "href": "teaching/glm/slides/logistic_model.draft.html#comparison-with-linear-regression-1",
    "title": "The Logistic Model",
    "section": "Comparison with Linear Regression",
    "text": "Comparison with Linear Regression\n. . .\nNo explicit formula for \\(\\hat{\\beta}\\), the solution is obtained numerically;\n. . .\nWe know neither the bias nor the variance of \\(\\hat{\\beta}\\);\n. . .\nThe distribution of \\(Y|X\\) is simple (a Bernoulli), but we don’t know the distribution of \\(\\hat{\\beta}\\).\n. . .\nWe only know its asymptotic distribution.\n. . .\nWe’ll do asymptotic tests!"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#asymptotic-framework",
    "href": "teaching/glm/slides/logistic_model.draft.html#asymptotic-framework",
    "title": "The Logistic Model",
    "section": "Asymptotic Framework",
    "text": "Asymptotic Framework\n. . .\nUnder “good conditions”,\n\n\\[J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]\n\nwhere \\(J_n(\\beta)\\) is the Fisher information matrix.\n. . .\nTo build asymptotic tests, we need to understand \\(J_n(\\beta)\\) and be able to estimate it."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#computation-of-j_nbeta",
    "href": "teaching/glm/slides/logistic_model.draft.html#computation-of-j_nbeta",
    "title": "The Logistic Model",
    "section": "Computation of \\(J_n(\\beta)\\)",
    "text": "Computation of \\(J_n(\\beta)\\)\n. . .\n\n\\[J_n(\\beta) = -E\\left[\\frac{\\partial^2 L}{\\partial \\beta^2}(\\beta) \\bigg| X\\right]\\]\n\nwhere \\(L\\) is the log-likelihood of the model.\n. . .\nFrom the proof of existence of MLE,\n\n\\[J_n(\\beta) = \\sum_{i=1}^n p_\\beta(x_i)(1 - p_\\beta(x_i)) x_i x_i^T\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#equivalent-form",
    "href": "teaching/glm/slides/logistic_model.draft.html#equivalent-form",
    "title": "The Logistic Model",
    "section": "Equivalent Form",
    "text": "Equivalent Form\n. . .\nWe can write equivalently\n\n\\[J_n(\\beta) = X^T W_\\beta X\\]\n\nwhere \\(X\\) is the design matrix and \\(W_\\beta\\) is the diagonal matrix\n. . .\n\n\n\\[W_\\beta = \\begin{pmatrix}\np_\\beta(x_1)(1 - p_\\beta(x_1)) & 0 & \\cdots & 0 \\\\\n0 & p_\\beta(x_2)(1 - p_\\beta(x_2)) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & p_\\beta(x_n)(1 - p_\\beta(x_n))\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#estimation",
    "href": "teaching/glm/slides/logistic_model.draft.html#estimation",
    "title": "The Logistic Model",
    "section": "Estimation",
    "text": "Estimation\n. . .\nTo estimate \\(J_n(\\beta)\\), we simply replace \\(\\beta\\) by the MLE \\(\\hat{\\beta}\\)\n. . .\nUnder the same regularity assumptions, we can show that\n\n\\[J_n(\\hat{\\beta})^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#estimated-variance-of-hat-beta_j",
    "href": "teaching/glm/slides/logistic_model.draft.html#estimated-variance-of-hat-beta_j",
    "title": "The Logistic Model",
    "section": "Estimated Variance of \\(\\hat \\beta_j\\)",
    "text": "Estimated Variance of \\(\\hat \\beta_j\\)\n. . .\n\n\\[J_n(\\hat{\\beta})^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]\n\n. . .\nDenoting \\(\\hat{\\sigma}_j^2\\) the \\(j\\)-th diagonal element of \\(J_n(\\hat{\\beta})^{-1}\\), we obtain (admitted):\n\n\\[\\frac{\\hat{\\beta}_j - \\beta_j}{\\hat{\\sigma}_j} \\xrightarrow{L} N(0, 1)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#confidence-interval",
    "href": "teaching/glm/slides/logistic_model.draft.html#confidence-interval",
    "title": "The Logistic Model",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n. . .\nWe deduce a confidence interval for \\(\\beta_j\\), at asymptotic confidence level \\(1 - \\alpha\\):\n\n\\[\\text{CI}_{1-\\alpha}(\\beta_j) = \\left[\\hat{\\beta}_j - q_{1-\\alpha/2}\\hat{\\sigma}_j \\,;\\, \\hat{\\beta}_j + q_{1-\\alpha/2}\\hat{\\sigma}_j\\right]\\]\n\nwhere \\(q(1 - \\alpha/2)\\) is the quantile of order \\(1 - \\alpha/2\\) of the \\(N(0, 1)\\) distribution.\n. . .\nWe verify that we have \\(\\P(\\beta_j \\in \\text{CI}_{1-\\alpha}(\\beta_j)) \\to 1 - \\alpha\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#significance-test-for-one-coefficient",
    "href": "teaching/glm/slides/logistic_model.draft.html#significance-test-for-one-coefficient",
    "title": "The Logistic Model",
    "section": "Significance Test for One Coefficient",
    "text": "Significance Test for One Coefficient\n. . .\nWe want to test \\(H_0: \\beta_j = 0\\) against \\(H_1: \\beta_j \\neq 0\\).\nUnder \\(H_0\\) we know that \\(\\frac{\\hat{\\beta}_j}{\\hat{\\sigma}_j} \\xrightarrow{L} N(0, 1)\\)\nWe deduce a critical region at asymptotic level \\(\\alpha\\):\n\n\\[\\mathcal R_\\alpha = \\left\\{\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha/2}\\right\\}\\]\n\n. . .\nIndeed \\(P_{H_0}(\\mathcal R_\\alpha) \\to \\alpha\\).\nThis test is called the Wald test. (As any other test that relies on asymptotic normality)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#p-value",
    "href": "teaching/glm/slides/logistic_model.draft.html#p-value",
    "title": "The Logistic Model",
    "section": "P-value",
    "text": "P-value\n. . .\nDenoting \\(\\Phi\\) the cdf of the \\(\\mathcal N(0, 1)\\) distribution, the p-value of the test equals\n\n\\[p\\text{-value} = 2\\left(1 - \\Phi\\left(\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j}\\right)\\right)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-in-r",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-in-r",
    "title": "The Logistic Model",
    "section": "Example in R",
    "text": "Example in R\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\n. . .\nEach columns corresponds resp. to\n\n\n\\(\\hat \\beta_j\\)\n\\(\\hat \\sigma_j\\)\n\\(\\hat \\beta_j/\\hat \\sigma_j\\) (z-score)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#estimation-of-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.draft.html#estimation-of-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Estimation of an Odds-Ratio",
    "text": "Estimation of an Odds-Ratio\n. . .\nWe consider two individuals \\(1\\) and \\(2\\) who differ only by their regressor \\(j\\). Then,\n\n\\[OR(x_1^{(j)}, x_2^{(j)}) = e^{\\beta_j(x_1^{(j)} - x_2^{(j)})}\\]\n\n. . .\nDo we have \\(OR(x_1^{(j)}, x_2^{(j)})\\approx 1\\)?\n. . .\nThe estimation of \\(OR(x_1^{(j)}, x_2^{(j)})\\) is simply\n\n\\[\\widehat{OR}(x_1^{(j)}, x_2^{(j)}) = e^{\\hat{\\beta}_j(x_1^{(j)} - x_2^{(j)})}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#important-example",
    "href": "teaching/glm/slides/logistic_model.draft.html#important-example",
    "title": "The Logistic Model",
    "section": "Important Example",
    "text": "Important Example\n. . .\nIf regressor \\(j\\) is binary with \\(x_1^{(j)} = 1\\) and \\(x_2^{(j)} = 0\\), then\n\n\\[\\widehat{OR}(x_1^{(j)}, x_2^{(j)}) = e^{\\hat{\\beta}_j}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#asymptotic-ci-for-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.draft.html#asymptotic-ci-for-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Asymptotic CI for an Odds-Ratio",
    "text": "Asymptotic CI for an Odds-Ratio\n. . .\nWe have seen that an asymptotic CI at confidence level \\(1 - \\alpha\\) for \\({\\beta}_j\\) is\n\n\\[\\text{CI}_{1-\\alpha}(\\beta_j) = \\left[\\hat{\\beta}_j - q_{1-\\alpha/2}\\hat{\\sigma}_j \\,;\\, \\hat{\\beta}_j + q_{1-\\alpha/2}\\hat{\\sigma}_j\\right]= [l,r]\\]\n\n. . .\nIf \\(x_1^{(j)} &gt; x_2^{(j)}\\), an asymptotic CI for \\(OR(x_1^{(j)}, x_2^{(j)})= e^{\\beta_j(x^{(j)}_1 - x^{(j)}_2)}\\) at level \\(1 - \\alpha\\) is\n\n\\[\\text{CI}_{1-\\alpha}(OR(x_1^{(j)}, x_2^{(j)})) = \\left[e^{l(x_1^{(j)} - x_2^{(j)})}, e^{r(x_1^{(j)} - x_2^{(j)})}\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#significance-test-for-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.draft.html#significance-test-for-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Significance Test for an Odds-Ratio",
    "text": "Significance Test for an Odds-Ratio\n. . .\nWe generally want to compare \\(OR(x_1^{(j)}, x_2^{(j)})\\) to \\(1\\).\n\n\\[OR(x_1^{(j)}, x_2^{(j)}) = 1 \\Leftrightarrow e^{\\beta_j(x_1^{(j)} - x_2^{(j)})} = 1 \\Leftrightarrow \\beta_j = 0\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#two-sided-test",
    "href": "teaching/glm/slides/logistic_model.draft.html#two-sided-test",
    "title": "The Logistic Model",
    "section": "Two-Sided Test",
    "text": "Two-Sided Test\n. . .\n\n\\(H_0: OR(x_1^{(j)}, x_2^{(j)}) = 1\\) VS \\(H_1: OR(x_1^{(j)}, x_2^{(j)}) \\neq 1\\)\n\n. . .\namounts to testing \\(H_0: \\beta_j = 0\\) against \\(H_1: \\beta_j \\neq 0\\). The Rejection region at level \\(\\alpha\\) is\n. . .\n\n\\[\\mathcal R_\\alpha = \\left\\{\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha/2}\\right\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#one-sided-tests",
    "href": "teaching/glm/slides/logistic_model.draft.html#one-sided-tests",
    "title": "The Logistic Model",
    "section": "One-Sided Tests",
    "text": "One-Sided Tests\n. . .\nNevertheless, for ORs, we often prefer one-sided tests.\n. . .\n\n\\(H_0: OR(x_1^{(j)}, x_2^{(j)}) = 1\\) VS \\(H_1: OR(x_1^{(j)}, x_2^{(j)}) &gt; 1\\)\n\n. . .\nIf \\(x_1^{(j)} &gt; x_2^{(j)}\\). Since \\(OR(x_1^{(j)}, x_2^{(j)}) \\geq 1 \\Leftrightarrow \\beta_j \\geq 0\\) rejection region at level \\(\\alpha\\) is\n\n\\[\\mathcal R_{\\alpha}=\\left\\{\\frac{\\hat{\\beta}_j}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha}\\right\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#saturated-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#saturated-model",
    "title": "The Logistic Model",
    "section": "Saturated Model",
    "text": "Saturated Model\n. . .\nSuppose we have \\(n\\) observations \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\) (categorical)\n. . .\nHere \\(X_k\\) can represent the vector \\(X_k = (X^{(1)}_k, \\dots, X^{(p)}_k)^T\\).\n. . .\nAssume that indivudal are iid, with \\(\\mathbb P(Y=1|X_k=x) = p(x)\\).\n. . .\nHow do we estimate \\(p(x)\\)?"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#saturated-estimator",
    "href": "teaching/glm/slides/logistic_model.draft.html#saturated-estimator",
    "title": "The Logistic Model",
    "section": "Saturated Estimator",
    "text": "Saturated Estimator\n. . .\nSuppose we have \\(n\\) observations \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\)\n. . .\n\\(n(x) = |\\{k:~ X_k = x\\}|\\) (number of indiv. \\(k\\) s.t. \\(X_k=x\\))\n. . .\n\\(n_1(x) = |\\{k:~ X_k = x ~~\\text{and}~~Y_k=1\\}|\\)\n. . .\nThe saturated model is one that estimates \\(p(x)\\), for an observed \\(x\\), by\n\n\\[\\hat{p}_{\\text{sat}}(x) = \\frac{n_1(x)}{n(x)}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#remark",
    "href": "teaching/glm/slides/logistic_model.draft.html#remark",
    "title": "The Logistic Model",
    "section": "Remark",
    "text": "Remark\n. . .\n\n\\[\\hat{p}_{\\text{sat}}(x) = \\frac{n_1(x)}{n(x)}\\]\n\nIf all observations are distinct, i.e., each observed \\(x\\) is only for a single individual, then for an observed \\(x\\):\n. . .\n\\(n(x) = 1\\), \\(n_1(x) \\in \\{0,1\\}\\), and \\(\\hat{p}_{\\text{sat}}(x) = 0\\) or \\(1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#remarks-1",
    "href": "teaching/glm/slides/logistic_model.draft.html#remarks-1",
    "title": "The Logistic Model",
    "section": "Remarks",
    "text": "Remarks\n. . .\nThe saturated model is the simplest model to imagine.\n. . .\nIt fits the data perfectly.\n. . .\nHowever, it has no explanatory power (effect of regressors on \\(Y\\)?).\n. . .\nAnd it says nothing about \\(p(x)\\) if \\(x\\) is not observed.\n. . .\nIt will serves as a reference for fit"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#likelihood-of-saturated-estimator",
    "href": "teaching/glm/slides/logistic_model.draft.html#likelihood-of-saturated-estimator",
    "title": "The Logistic Model",
    "section": "Likelihood of Saturated Estimator",
    "text": "Likelihood of Saturated Estimator\n. . .\nFor the saturated model with probabilities \\(p(x)\\), the Log-likelihood is:\n\n\n\\[L(y_1, \\ldots, y_n|x_1, \\ldots, x_n) = \\sum_{i=1}^n y_i \\ln(p(x_i)) + (1 - y_i) \\ln(1 - p(x_i))\\]\n\n\n. . .\nThe saturated model minimizes this likelihood, and we denote\n\n\n\\[L_{\\text{sat}} = \\sum_{i=1}^n y_i \\ln(\\hat p_{\\text{sat}}(x_i)) + (1 - y_i) \\ln(1 - \\hat p_{\\text{sat}}(x_i))\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#case-1-distinct-observations",
    "href": "teaching/glm/slides/logistic_model.draft.html#case-1-distinct-observations",
    "title": "The Logistic Model",
    "section": "Case 1: Distinct Observations",
    "text": "Case 1: Distinct Observations\n. . .\nIf all observations \\(x_i\\) are distinct, we have \\(\\hat{p}_{\\text{sat}}(x_i) = y_i\\) with \\(y_i \\in \\{0, 1\\}\\). We thus have\n\n\\[L_{\\text{sat}} = 0\\]\n\n. . .\nThe saturated estimator has highest possible log-likelihood: it fits the data perfectly (too well)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#case-2-non-distinct-observations",
    "href": "teaching/glm/slides/logistic_model.draft.html#case-2-non-distinct-observations",
    "title": "The Logistic Model",
    "section": "Case 2: Non-Distinct Observations",
    "text": "Case 2: Non-Distinct Observations\n. . .\nIf the observations \\(x_i\\) are not distinct, we obtain\n\n\n\\[L_{\\text{sat}} = \\sum_x \\left[n_1(x) \\ln\\left(\\frac{n_1(x)}{n(x)}\\right) + (n(x) - n_1(x)) \\ln\\left(1 - \\frac{n_1(x)}{n(x)}\\right)\\right]\\]\n\n\nwhere the sum runs over the set of values \\(x\\) taken by the \\(x_i\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#deviance-as-a-substitute-for-ssr",
    "href": "teaching/glm/slides/logistic_model.draft.html#deviance-as-a-substitute-for-ssr",
    "title": "The Logistic Model",
    "section": "Deviance as a Substitute for SSR",
    "text": "Deviance as a Substitute for SSR"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#definition",
    "href": "teaching/glm/slides/logistic_model.draft.html#definition",
    "title": "The Logistic Model",
    "section": "Definition",
    "text": "Definition\n\nThe deviance of a model measures how much this model deviates from the saturated model (the ideal model in terms of likelihood).\n\n\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\]\n\nwhere \\(L_{\\text{mod}}\\) denotes the log-likelihood for the model parameters.\n\n\nWe always have \\(D \\geq 0\\).\n\n\nIf all observations are distinct, \\(L_{\\text{sat}} = 0\\) therefore \\(D = -2L_{\\text{mod}}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#role-of-deviance-and-computation-in-r",
    "href": "teaching/glm/slides/logistic_model.draft.html#role-of-deviance-and-computation-in-r",
    "title": "The Logistic Model",
    "section": "Role of Deviance and Computation in R",
    "text": "Role of Deviance and Computation in R\n. . .\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\]\n\n. . .\nDeviance plays the role of the SSR of a linear model: the higher the deviance, the less well the model is fitted to the data.\n. . .\nIn R, The returned deviance is \\(-2L_{\\text{mod}}\\): the term \\(L_{\\text{sat}}\\) is therefore omitted."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#linear-constraint-test",
    "href": "teaching/glm/slides/logistic_model.draft.html#linear-constraint-test",
    "title": "The Logistic Model",
    "section": "Linear Constraint Test",
    "text": "Linear Constraint Test\n. . .\nAs in linear regression, we would like to test \\(H_0: R\\beta = 0\\) VS \\(H_1: R\\beta \\neq 0\\)\nwhere \\(R\\) is a constraint matrix of size \\((q, p)\\) of full rank.\n. . .\nFor recall, depending on the choice of \\(R\\) this allows:\n\ntesting the minimum: is there at least one relevant regressor?\ncomparing nested models\nexamining the collective significance of a family of regressors"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#linear-constraint-testing-test-procedures",
    "href": "teaching/glm/slides/logistic_model.draft.html#linear-constraint-testing-test-procedures",
    "title": "The Logistic Model",
    "section": "Linear Constraint Testing: Test Procedures",
    "text": "Linear Constraint Testing: Test Procedures"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#available-procedures",
    "href": "teaching/glm/slides/logistic_model.draft.html#available-procedures",
    "title": "The Logistic Model",
    "section": "Available Procedures",
    "text": "Available Procedures\n. . .\nIn GLM, several test procedures address the problem.\nThe Wald test, based on the asymptotic normality of \\(\\hat{\\beta}\\), which generalizes the one seen for testing \\(\\beta_j = 0\\) against \\(\\beta_j \\neq 0\\).\n. . .\nThe likelihood ratio test, called in this context the deviance test.\n. . .\nThe score test, based on the behavior of the gradient of the log-likelihood at the critical point.\n. . .\nThe most used is the deviance test."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#the-deviance-test-or-likelihood-ratio-test",
    "href": "teaching/glm/slides/logistic_model.draft.html#the-deviance-test-or-likelihood-ratio-test",
    "title": "The Logistic Model",
    "section": "The Deviance Test (or Likelihood Ratio Test)",
    "text": "The Deviance Test (or Likelihood Ratio Test)\n. . .\nTo test \\(H_0: R\\beta = 0\\) against \\(H_1: R\\beta \\neq 0\\), the principle of the test is as follows:\n. . .\nWe calculate the MLE in each model to obtain \\(\\hat{\\beta}\\) in the complete model and \\(\\hat{\\beta}_{H_0}\\) in the constrained model.\n. . .\nLogic: If \\(H_0\\) is true, the constrained model should be as “likely” as the complete model, so \\(L(\\hat{\\beta})\\) and \\(L(\\hat{\\beta}_{H_0})\\) should be similar."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#deviance-test-statistic",
    "href": "teaching/glm/slides/logistic_model.draft.html#deviance-test-statistic",
    "title": "The Logistic Model",
    "section": "Deviance Test Statistic",
    "text": "Deviance Test Statistic\n. . .\nThe test statistic is the difference of deviances:\n\n\\[D_{H_0} - D_{H_1} = 2\\left(L(\\hat{\\beta}) - L(\\hat{\\beta}_{H_0})\\right)\\]\n\n. . .\nUnder \\(H_0\\), denoting \\(q\\) the number of constraints, we have the convergence (admitted):\n\n\\[D_{H_0} - D_{H_1} = 2\\left(L(\\hat{\\beta}) - L(\\hat{\\beta}_{H_0})\\right) \\xrightarrow{L} \\chi^2_q\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#rejection-region-and-p-value",
    "href": "teaching/glm/slides/logistic_model.draft.html#rejection-region-and-p-value",
    "title": "The Logistic Model",
    "section": "Rejection Region and P-value",
    "text": "Rejection Region and P-value\n. . .\nThe asymp. rejection region at asymptotic level \\(\\alpha\\) is therefore\n\n\\[\\mathcal R_\\alpha = \\{D_{H_0} - D_{H_1} &gt; \\chi^2_{q,1-\\alpha}\\}\\]\n\nwhere \\(\\chi^2_{q,1-\\alpha}\\): quantile \\(1 - \\alpha\\) of a \\(\\chi^2_q\\) distribution.\n. . .\nThe p-value equals\n\n\\[p\\text{-value} = 1 - F(D_{H_0} - D_{H_1})\\]\n\nwhere \\(F\\) is the cdf of a \\(\\chi^2_q\\) distribution."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#special-case-1-significance-test",
    "href": "teaching/glm/slides/logistic_model.draft.html#special-case-1-significance-test",
    "title": "The Logistic Model",
    "section": "Special Case 1: Significance Test",
    "text": "Special Case 1: Significance Test\n. . .\nWe want to test if a model (having a constant) is significant\n. . .\nWe therefore test \\(H_0\\): all its coefficients are zero except the constant. This corresponds to the special case\n\n\\(R = [0 | I_{p-1}]\\).\n\n. . .\nWe compare the deviance of the model to the null deviance \\(D_0\\), corresponding to a model that contains only the constant."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#test-statistic",
    "href": "teaching/glm/slides/logistic_model.draft.html#test-statistic",
    "title": "The Logistic Model",
    "section": "Test Statistic",
    "text": "Test Statistic\n. . .\nThe test statistic is \\(D_0 - D\\). Under \\(H_0\\), when \\(n \\to \\infty\\):\n\n\\[D_0 - D \\sim \\chi^2_{p-1}\\]\n\n. . .\nThe model is therefore significant (relative to the null model) if the sample is in the critical region of asymptotic level \\(\\alpha\\):\n\n\\[\\mathcal R_\\alpha = \\{D_0 - D &gt; \\chi^2_{p-1,1-\\alpha/2}\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#special-case-2-nested-models",
    "href": "teaching/glm/slides/logistic_model.draft.html#special-case-2-nested-models",
    "title": "The Logistic Model",
    "section": "Special Case 2: Nested Models",
    "text": "Special Case 2: Nested Models\n. . .\nSuppose that model \\(1\\) (with deviance \\(D_1\\)) is a sub-model of model \\(2\\) (with deviance \\(D_2\\))\n. . .\nModel \\(1\\) is therefore obtained from model \\(2\\), with parameter \\(\\beta\\), via a constraint of the type \\(R\\beta = 0\\) where \\(R\\) is a \\((q, p)\\) matrix.\n. . .\nUnder \\(H_0: R\\beta = 0\\), we have asymptotically \\(D_1 - D_2 \\sim \\chi^2_q\\)\n. . .\nHence the asymptotic test: .\n\n\\(\\mathcal R_\\alpha = \\{D_1 - D_2 &gt; \\chi^2_{q,1 - \\alpha}\\}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#aic-and-bic-criteria",
    "href": "teaching/glm/slides/logistic_model.draft.html#aic-and-bic-criteria",
    "title": "The Logistic Model",
    "section": "AIC and BIC Criteria",
    "text": "AIC and BIC Criteria\n. . .\nThe AIC and BIC criteria are defined similarly to linear regression, i.e.\n\n\\(\\text{AIC} = -2L_{\\text{mod}} + 2p\\)\n\n\n\\(\\text{BIC} = -2L_{\\text{mod}} + \\ln(n)p\\)\n\nwhere \\(L_{\\text{mod}}\\) is the log-likelihood of the estimated model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#aic-and-bic-criteria-1",
    "href": "teaching/glm/slides/logistic_model.draft.html#aic-and-bic-criteria-1",
    "title": "The Logistic Model",
    "section": "AIC and BIC Criteria",
    "text": "AIC and BIC Criteria\n. . .\nIf we ignore saturated likelihood and set \\(L_{\\text{sat}}=0\\),\n\n\\(\\text{AIC} = D + 2p\\)\n\n\n\\(\\text{BIC} = D + \\ln(n)p\\)\n\nIn practice, we choose the model having the minimal AIC or BIC\n. . .\nAs in linear regression, we can use automatic selection procedures (backward, forward, etc)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-obesity-study-in-the-us",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-obesity-study-in-the-us",
    "title": "The Logistic Model",
    "section": "Example: Obesity Study in the US",
    "text": "Example: Obesity Study in the US\n\nmodel=glm(Y~AGE+DBP+SEXE+ACTIV+WALK+MARITAL, family=binomial)\nsummary(model)\n\n\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4459.5\\)\non \\(5290\\)\n\n\nAIC\n\\(4481.5\\)\n\n\n\n\n\n\n\nThe model deviance is therefore \\(D = 4459.5\\).\nSignificance Test: We compare \\(D\\) to the null deviance \\(D_0 = 4610.8\\): \\(D_0 - D = 151.3\\). The p-value of the test equals \\(1 - F_{10}(151.3) \\approx 0\\) where \\(F_{10}\\) is the cdf of a \\(\\chi^2_{10}\\).\n\n\nThe model is significant."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-significance-test-of-marital",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-significance-test-of-marital",
    "title": "The Logistic Model",
    "section": "Example: Significance Test of MARITAL",
    "text": "Example: Significance Test of MARITAL\n. . .\nmodel=glm(Y~AGE+DBP+SEXE+ACTIV+WALK, family=binomial) # We want to test if `MARITAL` is significant\nsummary(model)\n. . .\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4462.7\\)\non \\(5295\\)\n\n\nAIC\n\\(4474.7\\)\n\n\n\n\n\n. . .\nThe deviance is now \\(D_2 = 4462.7\\). To compare with the previous model, we calculate: \\(D_2 - D = 3.2\\).\n. . .\nThe p-value of the test equals \\(1 - F_5(3.2) \\approx 0.67\\), where \\(F_5\\): cdf of a \\(\\chi^2_5\\).\nWe therefore accept \\(H_0\\): the coefficients related to MARITAL are zero. (Also confirmed with AIC)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-significance-test-of-age2",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-significance-test-of-age2",
    "title": "The Logistic Model",
    "section": "Example: Significance Test of AGE\\(~^2\\)",
    "text": "Example: Significance Test of AGE\\(~^2\\)\n. . .\nmodel=glm(Y∼AGE+I(AGE2 )+DBP+SEXE+WALK+ACTIV, family=binomial) # We add AGE^2\nsummary(model)\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4439.5\\)\non \\(5294\\)\n\n\nAIC\n\\(4453.5\\)\n\n\n\n\n\n. . .\nThe deviance test with the previous model has p-value \\(1 - F_1(4462.7 - 4439.5) = 1 - F_1(23.2) \\approx 10^{-6}\\)\n. . .\nThis model is therefore preferable, (confirmed with AIC).\n. . .\nHowever, we cannot compare this model with the first one by deviance test because they are not nested.\n. . .\nWe can however compare their AIC: this model is preferable."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#setup-for-prevision",
    "href": "teaching/glm/slides/logistic_model.draft.html#setup-for-prevision",
    "title": "The Logistic Model",
    "section": "Setup for Prevision",
    "text": "Setup for Prevision\n\nSuppose we are interested in a new individual for whom\n\nwe know their characteristics \\(x \\in \\mathbb{R}^p\\),\nwe do not know their \\(Y\\).\n\n\n\nWe want to predict \\(Y\\) for this new individual."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#probability-estimation",
    "href": "teaching/glm/slides/logistic_model.draft.html#probability-estimation",
    "title": "The Logistic Model",
    "section": "Probability Estimation",
    "text": "Probability Estimation\n\nIf we have fitted a logistic regression model, we can estimate\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nby\n\n\\[p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta}) = \\frac{e^{x^T \\hat{\\beta}}}{1 + e^{x^T \\hat{\\beta}}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#what-we-will-see",
    "href": "teaching/glm/slides/logistic_model.draft.html#what-we-will-see",
    "title": "The Logistic Model",
    "section": "What We Will See",
    "text": "What We Will See\n\nWe will see:\n\nhow to construct a confidence interval around this estimation;\nhow to exploit this estimation to classify the individual in category \\(Y = 0\\) or \\(Y = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#asymptotic-distribution",
    "href": "teaching/glm/slides/logistic_model.draft.html#asymptotic-distribution",
    "title": "The Logistic Model",
    "section": "Asymptotic Distribution",
    "text": "Asymptotic Distribution\n\nWe know that when \\(n \\to \\infty\\):\n\n\\[\\hat{\\beta} \\sim N(\\beta, (X^T W_{\\hat{\\beta}} X)^{-1})\\]\n\nHence, when \\(n \\to \\infty\\), for any \\(x \\in \\mathbb R^p\\),\n\n\\[x^T \\hat{\\beta} \\sim N(x^T \\beta, x^T (X^T W_{\\hat{\\beta}} X)^{-1} x)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#ci-for-linear-predictor",
    "href": "teaching/glm/slides/logistic_model.draft.html#ci-for-linear-predictor",
    "title": "The Logistic Model",
    "section": "CI for Linear Predictor",
    "text": "CI for Linear Predictor\n. . .\nWe deduce that when \\(n \\to +\\infty\\), \\(x^T \\hat{\\beta} \\sim N(x^T \\beta, x^T (X^T W_{\\hat{\\beta}} X)^{-1} x)\\), and the asymptotic CI of \\(x^T\\beta\\):\n. . .\n\n\\[\\text{CI}_{1-\\alpha}(x^T \\beta) = \\left[x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#setting-and-objective",
    "href": "teaching/glm/slides/logistic_model.draft.html#setting-and-objective",
    "title": "The Logistic Model",
    "section": "Setting and Objective",
    "text": "Setting and Objective\n. . .\nSuppose we are interested in a new individual for whom\n\nwe know their characteristics \\(x \\in \\mathbb{R}^p\\),\nwe do not know their \\(Y\\).\n\n. . .\nWe want to predict \\(Y\\) for this new individual."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#recall-in-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.draft.html#recall-in-the-logit-model",
    "title": "The Logistic Model",
    "section": "Recall in the Logit Model",
    "text": "Recall in the Logit Model\n. . .\nIf we have fitted a logistic regression model, we can estimate\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nby\n. . .\n\n\\[p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta}) = \\frac{e^{x^T \\hat{\\beta}}}{1 + e^{x^T \\hat{\\beta}}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#outline-for-prediction",
    "href": "teaching/glm/slides/logistic_model.draft.html#outline-for-prediction",
    "title": "The Logistic Model",
    "section": "Outline for Prediction",
    "text": "Outline for Prediction\n. . .\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nWe will see:\n\nhow to construct a confidence interval around the estimation \\(p_{\\hat{\\beta}}(x)\\);\nhow to exploit this estimation to classify the new individual in category \\(Y = 0\\) or \\(Y = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#ci-asymptotic-distribution-of-p_betax",
    "href": "teaching/glm/slides/logistic_model.draft.html#ci-asymptotic-distribution-of-p_betax",
    "title": "The Logistic Model",
    "section": "CI: Asymptotic Distribution of \\(p_\\beta(x)\\)",
    "text": "CI: Asymptotic Distribution of \\(p_\\beta(x)\\)\n. . .\nWe know that when \\(n \\to \\infty\\):\n\n\\[\\hat{\\beta} \\sim N(\\beta, (X^T W_{\\hat{\\beta}} X)^{-1})\\]\n\n\n\\(X=(X^{(1)}, \\dots, X^{(p)})\\) is the \\(n \\times p\\) design matrix\n\\(W_{\\hat{\\beta}}\\) is the \\(n \\times n\\) diagonal matrix with coefs \\(p_{\\hat \\beta}(x_i)(1-p_{\\hat \\beta}(x_i))\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#ci-for-linear-predictor-1",
    "href": "teaching/glm/slides/logistic_model.draft.html#ci-for-linear-predictor-1",
    "title": "The Logistic Model",
    "section": "CI for Linear Predictor",
    "text": "CI for Linear Predictor\n\nWe deduce that when \\(n \\to +\\infty\\), \\(x^T \\hat{\\beta} \\sim N(x^T \\beta, x^T (X^T W_{\\hat{\\beta}} X)^{-1} x)\\), and the asymptotic CI of \\(x^T\\beta\\):\n\n\n\n\\[\\text{CI}_{1-\\alpha}(x^T \\beta) = \\left[x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#ci-for-probability-p_betax",
    "href": "teaching/glm/slides/logistic_model.draft.html#ci-for-probability-p_betax",
    "title": "The Logistic Model",
    "section": "CI for Probability \\(p_{\\beta}(x)\\)",
    "text": "CI for Probability \\(p_{\\beta}(x)\\)\n. . .\nSince \\(p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta})\\), we have therefore by application of the increasing function \\(\\text{logit}^{-1}\\), the CI at asymptotic level \\(1 - \\alpha\\):\n. . .\n\n\n\\[\\text{CI}_{1-\\alpha}(p_\\beta(x)) = \\left[\\text{logit}^{-1}\\left(x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right)\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#classification-1",
    "href": "teaching/glm/slides/logistic_model.draft.html#classification-1",
    "title": "The Logistic Model",
    "section": "Classification",
    "text": "Classification\n\nWe have estimated \\(p_\\beta(x) = P(Y = 1|X = x)\\) by \\(p_{\\hat{\\beta}}(x)\\).\n\n\nFor a threshold to choose \\(s \\in [0, 1]\\), we use the rule:\n\n\\[\\begin{cases}\n\\text{if } p_{\\hat{\\beta}}(x) &gt; s, & \\hat{Y} = 1 \\\\\n\\text{if } p_{\\hat{\\beta}}(x) &lt; s, & \\hat{Y} = 0\n\\end{cases}\\]\n\n\n\nThe “natural” choice of threshold is \\(s = 0.5\\) but this choice can be optimized."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#evaluation-of-classification-quality",
    "href": "teaching/glm/slides/logistic_model.draft.html#evaluation-of-classification-quality",
    "title": "The Logistic Model",
    "section": "Evaluation of Classification Quality",
    "text": "Evaluation of Classification Quality\n. . .\nWe proceed by cross-validation:\n. . .\nUsing a train sample, predict \\(Y\\) on a test sample and form the confusion matrix.\n. . .\n\n\n\n\n\\(Y = 0\\)\n\\(Y = 1\\)\n\n\n\n\n\\(\\hat{Y} = 0\\)\nTN\nFN\n\n\n\\(\\hat{Y} = 1\\)\nFP\nTP\n\n\n\nReading: T: true, F:False, N: Negative, P: Positive.\n. . .\nFP: false positives: number of individuals who were classified positive who were actually negative"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#evaluation-of-classification-quality-1",
    "href": "teaching/glm/slides/logistic_model.draft.html#evaluation-of-classification-quality-1",
    "title": "The Logistic Model",
    "section": "Evaluation of Classification Quality",
    "text": "Evaluation of Classification Quality\n. . .\nThe ideal is to have a confusion matrix that is as diagonal as possible.\n. . .\nWe generally seek to maximize the following indicators:\n. . .\n\n\nThe sensitivity (or recall, or true positive rate) estimates \\(\\P(\\hat{Y} = 1|Y = 1)\\) by\n\n\\[\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\]\n\n\nThe specificity (or selectivity, or true negative rate) estimates \\(\\P(\\hat{Y} = 0|Y = 0)\\)\n\n\\[\\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\\]\n\n\n\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#other-indicators",
    "href": "teaching/glm/slides/logistic_model.draft.html#other-indicators",
    "title": "The Logistic Model",
    "section": "Other Indicators",
    "text": "Other Indicators\n. . .\nThe precision (or positive predictive value) estimates \\(\\P(Y = 1|\\hat{Y} = 1)\\) by\n\n\\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\)\n\n. . .\nThe \\(F\\)-score is the harmonic mean between sensitivity and precision:\n\n\\[F_1 = 2 \\frac{\\text{precision} \\times \\text{sensitivity}}{\\text{precision} + \\text{sensitivity}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#choice-of-threshold-s",
    "href": "teaching/glm/slides/logistic_model.draft.html#choice-of-threshold-s",
    "title": "The Logistic Model",
    "section": "Choice of Threshold \\(s\\)",
    "text": "Choice of Threshold \\(s\\)\n. . .\nFor each threshold \\(s\\), from a test sample:\n\nwe can form the confusion matrix\ncalculate scores (sensitivity, \\(F\\)-score, etc.)\n\n. . .\nWe finally choose the optimal threshold \\(s\\), according to the chosen score."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#choosing-the-score",
    "href": "teaching/glm/slides/logistic_model.draft.html#choosing-the-score",
    "title": "The Logistic Model",
    "section": "Choosing the score",
    "text": "Choosing the score\n\nIt depends on the context of the study\nIt can be much more serious to wrongly predict \\(\\hat{Y} = 0\\) than \\(\\hat{Y} = 1\\)\n\n. . .\n\\(\\hat Y=1\\) (treatment) while the patient is not ill (\\(Y = 0\\))\n. . .\n\\(\\hat Y=0\\) (no treatment) while the patient has a serious illness (\\(Y=1\\))"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#roc-curve",
    "href": "teaching/glm/slides/logistic_model.draft.html#roc-curve",
    "title": "The Logistic Model",
    "section": "ROC Curve",
    "text": "ROC Curve\n. . .\nWe can also plot the ROC curve (TP rate as a function of FP rate for \\(s \\in [0, 1]\\)):\n\n\n\\(ROC:~~\\mathrm{sensitiv.} = \\frac{TP}{TP+FN} = F\\left(\\frac{FP}{FP+TN}\\right) = F(1-\\mathrm{specific.})\\)\n\n\n. . .\nThe AUC (area under the curve) is a quality indicator of the model (\\(0 \\leq \\text{AUC} \\leq 1\\)).\n. . .\nOr equivalently, the Gini index: \\(2 \\times \\text{AUC} - 1\\).\n. . .\nUse: compare \\(2\\) models by plotting the 2 ROC curves."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#roc-curve-illustration",
    "href": "teaching/glm/slides/logistic_model.draft.html#roc-curve-illustration",
    "title": "The Logistic Model",
    "section": "ROC Curve Illustration",
    "text": "ROC Curve Illustration\n. . ."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#recalling-the-linear-model",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#recalling-the-linear-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Recalling the Linear Model",
    "text": "Recalling the Linear Model\n\nWelcome! In our previous sessions, we studied the linear regression model, where Y was a quantitative response variable that could take any real value.\nToday, we’re starting the second part of this course: Generalized Linear Models, or GLMs.\nThe major shift is this: we’ll now assume that Y is qualitative—it takes discrete or categorical values rather than continuous ones.\nAs we’ll see, the standard linear model doesn’t work for these types of variables. We need a more flexible framework—that’s what GLMs provide.\nOver the next sessions, we’ll learn how to properly model these qualitative responses while maintaining the same goals: understanding relationships between variables and making predictions.\nLet’s begin by understanding why the linear model breaks down for qualitative responses.\n\n\nWe observe \\(Y = (Y_1, \\dots, Y_n)\\) and \\(X = (X^{(1)} , . . . , X^{(p)}) \\in \\mathbb R^{n \\times p}\\),\n\n\nIn the Linear Model, We assume that for some unknown \\(\\beta\\), \\(\\sigma^2\\) where \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2 I_n)\\),\n\n\\[Y = X\\beta + \\varepsilon\\]\n\n\n\nThe hypothesis can be written in the form \\(\\mathbb E[Y|X] = X\\beta\\)\n\n\nThe OLS estimator of \\(\\beta\\) is \\(\\hat \\beta= (X^TX)^{-1}X^TY\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#when-this-linearity-is-reasonable",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#when-this-linearity-is-reasonable",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When this Linearity is Reasonable",
    "text": "When this Linearity is Reasonable\n\nSo, when does the linear assumption makes sense ?\nIn linear regression, we assume Y is a linear function of our regressors.\nThis means Y can be ANY real number—negative, positive, whatever. No restrictions.\nWhen does this make sense?\nWhen Y given X is Gaussian. Normal distributions span the entire real line, so any value for E(Y|X) makes sense.\nOr when Y given X follows any other “nice” continuous distribution on the real line. That is Y does not look like to a discrete distrib too much.\n\n\nThe hypothesis \\(\\E(Y|X) = X^T\\beta\\) in linear regression models implies that \\(\\E(Y|X)\\) can take any real value.\n\n\nThis is not a restriction when:\n\n\\(Y|X\\) follows a Gaussian distribution\n\\(Y|X\\) follows any other “nice” continuous distribution on \\(\\mathbb{R}\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#when-it-is-not",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#when-it-is-not",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When it is not",
    "text": "When it is not\n\nBut this assumption fails for certain types of response variables.\nEspecially when Y is qualitative or discrete.\n\n\nThe linear assumption is inappropriate for certain variables \\(Y\\), particularly when \\(Y\\) is qualitative or discrete."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#when-it-is-not-1",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#when-it-is-not-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When it is not",
    "text": "When it is not\n\nExamples: Binary responses: If Y represents disease presence (0 or 1), can E(Y|X) equal 1.5? Or -0.3? No—we need values between 0 and 1.\nCategorical responses: If Y is transportation choice (car, bus, or bike), it’s not a numeric scale that can take any real value.\nCount data: If Y is the number of traffic accident per month, it can’t be negative. Linear regression might give E(Y|X) = -2.\nBottom line: If Y can’t reasonably take any real number, linear regression isn’t appropriate. We’ll see alternative approaches like logistic regression.\n\n\nBinary outcomes (\\(Y = 0\\) or \\(1\\)):\n\ndisease presence\n\n\n\nCategorical outcomes (\\(Y \\in \\{A_1, \\ldots, A_k\\}\\)):\n\nTransportation choice: \\(\\{\\) Car, Bus, Bike \\(\\}\\)\n\n\n\nCount data (\\(Y \\in \\mathbb{N}\\)):\n\nNumber of traffic accident per month"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#key-differences-by-response-type",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#key-differences-by-response-type",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Key Differences by Response Type",
    "text": "Key Differences by Response Type\n\nNow let’s be more precise about what changes depending on the type of response variable.\nIn all situations—whether Y is continuous, binary, categorical, or count data—our objective is the same: link Y to our regressors X through modeling E(Y|X).\nHowever, E(Y|X) has different interpretations depending on what Y represents –\nbinary / categorical / count data\nAnd here’s the key: in all these cases, the linear model E(Y|X) equals X-transpose-beta is inappropriate. We need something different.\n\n\nIn all examples, the objective remains to link \\(Y\\) to \\(X = (X^{(1)}, \\ldots, X^{(p)})\\) through modeling \\(\\E(Y|X)\\).\n\n\nHowever, \\(\\E(Y|X)\\) has different interpretations depending on the situation:\n\nBinary \\(Y\\): \\(Y = 0\\) or \\(1\\)\nCategorical \\(Y\\): \\(Y \\in \\{A_1, \\ldots, A_k\\}\\)\n\nCount data: \\(Y \\in \\mathbb{N}\\)\n\n\n\nIn all these cases, the linear model \\(\\E(Y|X) = X^T\\beta\\) is inappropriate."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#objectives-of-the-glm-1",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#objectives-of-the-glm-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Objectives of the GLM",
    "text": "Objectives of the GLM\n\nSo what do we do instead?\nWe model E(Y|X) differently using generalized linear models—or GLMs.\nThe good news is that our objectives remain the same as in linear regression.\nWe still want to understand the individual effect of a given regressor, holding all other variables constant.\nWe want to understand relationships between our regressors and the response.\nAnd we want to forecast outcomes for new observations.\nGLMs give us a flexible framework to achieve these goals when linear regression isn’t appropriate.\n\n\nWe model \\(\\E(Y|X)\\) differently using generalized linear models.\nAs in linear regression, we focus on:\n\nSpecific effects: The individual effect of a given regressor, (all other things being equal)\nExplanation: Understanding relationships\nPrediction: Forecasting outcomes"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#three-fundamental-cases",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#three-fundamental-cases",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Three Fundamental Cases",
    "text": "Three Fundamental Cases\n\nLet’s outline the three fundamental cases we’ll study in detail.\nBinary responses—when Y takes only values 0 or 1. Think disease presence/absence, success/failure, yes/no outcomes.\nCategorical responses—when Y can be one of several categories: A1 through Ak. Think transportation choice, treatment selection, or any general qualitative variable.\nCount data—when Y is in the natural numbers. Think number of events, hospital visits, or any situation where you’re counting occurrences.\nFor each case, we’ll see how to properly model E(Y|X) and interpret the results. These will form the building blocks of generalized linear models.\n\n\nWe detail the modeling challenges for \\(\\E(Y|X)\\) in three fundamental cases:\n\nCase 1: Binary: \\(Y\\) is binary (takes values 0 or 1)\nCase 2: Categorical: \\(Y \\in \\{A_1, \\ldots, A_k\\}\\) (general qualitative variable)\nCase 3: Count: \\(Y \\in \\mathbb{N}\\) (count variable)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#case-1-binary-case",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#case-1-binary-case",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 1: Binary Case",
    "text": "Case 1: Binary Case\n\nLet’s dive into the first fundamental case: binary responses.\nWithout loss of generality, we’ll work with Y taking values 0 or 1.\nIf Y models membership in some category A—for example, whether someone has a disease—this is equivalent to studying the indicator variable Y.\nThe key insight is that the distribution of Y given X equals x is entirely determined by p(x), which equals the probability that Y equals 1 given X equals x.\nOnce we know p(x), we automatically know the probability that Y equals 0 given X equals x—it’s just 1 minus p(x).\nSo Y given X equals x follows a Bernoulli distribution with parameter p(x).\nThis means the conditional expectation E(Y|X = x) equals p(x). So modeling the conditional expectation is the same as modeling the probability.\nHere’s the crucial constraint: p(x) must be between 0 and 1: it’s a probability.\n\n\nWithout loss of generality, \\(Y \\in \\{0, 1\\}\\)\n\n\nIf \\(Y\\) models membership in a category \\(A\\), this is equivalent to studying the variable \\(Y = \\mathbf{1}_A\\)\n\n\nThe distribution of \\(Y\\) given \\(X = x\\) is entirely determined by \\(p(x) = P(Y = 1|X = x)\\)\n\n\nWe deduce \\(P(Y = 0|X = x) = 1 - p(x)\\)\n\n\n\\(Y|X = x\\) follows a Bernoulli distribution with parameter \\(p(x)\\)\n\n\n\\(\\E(Y|X = x) = p(x)\\)\n\n\nkey constraint: \\(p(x) \\in [0, 1]\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#modelling-px",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#modelling-px",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Modelling \\(p(x)\\)",
    "text": "Modelling \\(p(x)\\)\n\nSo how do we model p(x)?\nRemember, E(Y|X = x) equals P(Y = 1|X = x) equals p(x), and this must lie in the interval 0 to 1.\nThe naive approach would be to set p(x) equals x-transpose-beta, just like in linear regression. But this doesn’t work! Nothing guarantees that x-transpose-beta stays between 0 and 1.\nInstead, we propose a model of the form: p(x) equals f of x-transpose-beta.\nHere, f is a function that maps from the real line to the interval 0 to 1. Think of it as a transformation that takes any real number—which could be x-transpose-beta—and squashes it into a probability.\nThis gives us a coherent model. The linear combination x-transpose-beta can be any real number, but f transforms it to stay between 0 and 1. And we still only need to estimate beta—the same number of parameters as linear regression.\nThe question is: what function f should we use? We’ll explore specific choices like the logistic function next.\n\n\n\n\\[E(Y|X = x) = P(Y = 1|X = x) = p(x) \\in [0, 1]\\]\n\n\n\nWhat NOT to do: \\(p(x) = x^T\\beta\\) (for some \\(\\beta \\in \\mathbb{R}^p\\) to be estimated)\n\n\nProposed approach: We can propose a model of the type:\n\n\\[p(x) = f(x^T\\beta)\\]\n\nwhere \\(f\\) is a function from \\(\\mathbb{R}\\) to \\([0, 1]\\)\n\n\nBenefits: Coherent model that depends only on \\(\\beta\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#case-2-categorical-y",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#case-2-categorical-y",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Categorical \\(Y\\)",
    "text": "Case 2: Categorical \\(Y\\)\n\nNow let’s move to the second case: categorical responses with more than two categories.\nIf Y represents membership in k different classes—A1 through Ak—its distribution is determined by k probabilities: p_j(x) equals the probability that Y is in class Aj given X equals x, for j equals 1 through k.\nFor instance, if Y is transportation choice—car, bus, or bike—we have three probabilities that sum to one.\nThe key constraint is that these probabilities must sum to 1. And notice: if k equals 2, we have only two categories, which reduces to the binary case.\n\n\nIf \\(Y\\) represents membership in \\(k\\) different classes \\(A_1, \\ldots, A_k\\), its distribution is determined by the probabilities:\n\n\\[p_j(x) = P(Y \\in A_j|X = x), \\quad \\text{for } j = 1, \\ldots, k\\]\n\n\n\nConstraint: \\(\\sum_{j=1}^{k} p_j(x) = 1\\) (If \\(k = 2\\), this reduces to the previous case)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#case-2-categorical-y-1",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#case-2-categorical-y-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Categorical \\(Y\\)",
    "text": "Case 2: Categorical \\(Y\\)\n\nWe can represent Y as a vector of indicators: Y equals (indicator of A1, through indicator of Ak). This follows a multinomial distribution.\nThe conditional expectation E(Y|X = x) is then a vector with components p_1(x) through p_k(x)—each representing the probability of being in that category.\n\n\n\\(Y = (\\mathbf{1}_{A_1}, \\ldots, \\mathbf{1}_{A_k})\\) follows a multinomial distribution and:\n\n\\[\\E(Y|X = x) = \\begin{pmatrix} p_1(x) \\\\ \\vdots \\\\ p_k(x) \\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#case-2-model-for-categorical-y",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#case-2-model-for-categorical-y",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Model for Categorical \\(Y\\)",
    "text": "Case 2: Model for Categorical \\(Y\\)\n\nHow do we model this?\nHere’s a key simplification: to model E(Y|X = x), we only need to model k minus 1 probabilities—p_1(x) through p_(k-1)(x)—because the last probability p_k(x) is just 1 minus the sum of all the others.\nJust like in the binary case, we can propose: p_j(x) equals f of x-transpose-beta_j, for j equals 1 through k minus 1.\nWhat is the number of parameter in this model ? \\((k-1)p\\)\nThis is more complex than the binary case, but the principle is the same: use a transformation function to keep probabilities between 0 and 1.\n\n\nTo model \\(\\E(Y|X = x)\\), it suffices to model \\(p_1(x), \\ldots, p_{k-1}(x)\\) since \\(p_k(x) = 1 - \\sum_{j=1}^{k-1} p_j(x)\\)\n\n\nProposed model: As in the binary case, we can propose:\n\n\\[p_j(x) = f(x^T\\beta_j), \\quad j = 1, \\ldots, k-1\\]\n\nwhere \\(f: \\mathbb{R} \\to [0,1]\\)\n\n\nParameters: There will be \\(k-1\\) unknown parameters to estimate, each in \\(\\mathbb{R}^p\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#case-3-count-y---non-negative-integer-values",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#case-3-count-y---non-negative-integer-values",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 3: Count Y - Non-negative Integer Values",
    "text": "Case 3: Count Y - Non-negative Integer Values\n\nFor count variables, we impose that the function \\(f\\) is nonnegative.\nNotice that while Y represents an integer here, the expectation of \\(Y\\) can be fractional.\nThink of poisson distribution of parameter 2.3\n\n\nIf \\(Y\\) takes integer values, we have for all \\(x\\), \\(E(Y|X = x) \\geq 0\\)\n\n\nCoherent choice: A coherent approach is:\n\n\\[\\E(Y|X = x) = f(x^T\\beta)\\]\n\nwhere \\(f\\) is a function from \\(\\mathbb{R}\\) to \\([0, +\\infty)\\)\n\n\nExample of possible choice for f: The exponential function"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#model-formulation",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#model-formulation",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Model Formulation",
    "text": "Model Formulation\n\nLet g be a strictly monotonic function—meaning it’s always increasing or always decreasing. We call this the link function.\nA generalized linear model establishes a relationship of this type: g of E(Y|X = x) equals x-transpose-beta.\nThis is exactly what we proposed earlier! The linear combination x-transpose-beta can be any real number, but then g-inverse transforms it to the appropriate range for E(Y|X).\n\n\nLet \\(g\\) be a strictly monotonic function, called the link function\n\n\nA generalized linear model (GLM) establishes a relationship of the type:\n\n\\[g(\\E(Y|X = x)) = x^T\\beta\\]\n\nEquivalently,\n\n\\[\\E(Y|X = x) = g^{-1}(x^T\\beta)\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#remarks-on-model",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#remarks-on-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Remarks on Model",
    "text": "Remarks on Model\n\nIn the previous slides, we denoted g-inverse as f. It’s the same concept, just different notation.\nWe generally assume that the distribution of Y given X belongs to an exponential family. That includes Bernoulli, multinomial, Poisson, and many others.\nThis is mainly because it allows us to compute the likelihood function easily.\n\n\nIn the previous examples, \\(g^{-1}\\) was denoted \\(f\\)\nWe generally assume that the distribution \\(Y|X\\) belongs to the exponential family, with unknown parameter \\(\\beta\\)\nThis allows us to compute the likelihood and facilitates inference"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#remark-on-the-intercept",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#remark-on-the-intercept",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Remark on the Intercept",
    "text": "Remark on the Intercept\n\nA quick technical note about how we handle the intercept.\nAs in linear regression, among the regressors X^(1) through X^(p), we often assume that X^(1) equals 1 for all observations. This is to account for the presence of a constant term—the intercept.\n\n\nAmong the explanatory variables \\(X^{(1)}, \\ldots, X^{(p)}\\), we often assume that \\(X^{(1)}=\\1\\) to account for the presence of a constant. Thus: \\[X\\beta = \\beta_1 X^{(1)} + \\cdots + \\beta_p X^{(p)}\\]\n\n\nAlternative notation: Sometimes indexed differently to write \\(\\beta_0 + \\beta_1 X^{(1)} + \\cdots + \\beta_p X^{(p)}\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#example-1-linear-regression-model",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#example-1-linear-regression-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 1: Linear Regression Model",
    "text": "Example 1: Linear Regression Model\n\nLet’s star to see how linear regression fits into the GLM framework.\nWe recover linear regression by taking the identity link function: g(t) equals t. So we’re not transforming anything.\nThen E(Y|X = x) equals x-transpose-beta itself. No transformation.\nIn the standard Gaussian linear model, Y given X follows a normal distribution with mean X-beta and variance sigma-squared.\nSo linear regression is actually a special case of GLM models! It’s what you get when you choose the identity for link function and assume a Gaussian distribution.\n\n\nLink function: We recover linear regression by taking the identity link function \\(g(t) = t\\)\n\n\nExpected value: Then:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) = x^T\\beta\\]\n\n\n\nIn the Gaussian linear model:\n\n\\[Y|X \\sim \\mathcal{N}(X\\beta, \\sigma^2)\\]\n\n\n\nLinear regression is therefore a special case of GLM models!"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#example-2-binary-case-y-in-0-1",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#example-2-binary-case-y-in-0-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)",
    "text": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)\n\nNow let’s look at the binary case, which is more interesting.\nThe link function g-inverse of x-transpose-beta stays in the interval 0 to 1.\nSo we have Y given X following a Bernoulli distribution with parameter g-inverse of X-transpose-beta. What should we choose for g-inverse?\n\n\nLink function requirement: The link function \\(g\\) must satisfy:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) \\in [0, 1]\\]\n\n\n\nSince \\(Y\\in \\{0,1\\}\\), \\(Y|X\\) follows a Bernoulli distribution\n\n\\[Y|X \\sim \\mathcal{B}(g^{-1}(X^T\\beta))\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#example-2-binary-case-y-in-0-1-1",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#example-2-binary-case-y-in-0-1-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)",
    "text": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)\n\nAny CDF of a continuous distribution on the real line works! A CDF maps the real line to [0,1], which is exactly what we need.\nThe standard choice for g-inverse is the CDF of a logistic distribution: g-inverse(t) equals e^t over (1 plus e^t).\nThis means g(t) equals the natural log of t over (1 minus t)—this is called the logit function.\nThis leads to the logistic model, which is the most important model in this chapter. We’ll spend some time on it.\n\n\n\n\\[Y|X \\sim \\mathcal{B}(g^{-1}(X^T\\beta))\\]\n\nPossible choices for \\(g^{-1}\\): A CDF of a continuous distrib. on \\(\\mathbb{R}\\)\n\n\nStandard choice for \\(g^{-1}\\): The CDF of a logistic distribution:\n\n\\[g^{-1}(t) = \\frac{e^t}{1 + e^t} \\quad \\text{i.e.} \\quad g(t) = \\ln\\left(\\frac{t}{1-t}\\right) = \\text{logit}(t)\\]\n\n\n\nThis leads to the logistic model, the most important model in this chapter"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#example-3-count-data-yin-mathbb-n",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#example-3-count-data-yin-mathbb-n",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 3: Count Data \\(Y\\in \\mathbb N\\)",
    "text": "Example 3: Count Data \\(Y\\in \\mathbb N\\)\n\nFinally, let’s look at count data.\nFor count data, we use the link function g(t) equals natural log of t, so g-inverse(t) equals e^t.\nThis gives E(Y|X) = exponential x-transpose-beta. This ensures the expected value is always positive, which makes sense for counts.\nSecond point - Highlight yellow: For the distribution of Y given X, defined on the natural numbers, we often assume a Poisson distribution—which is in the exponential family.\nThird point - Full specification: In this context, Y given X follows a Poisson distribution with parameter exponential X-transpose-beta.\n\n\nLink function: \\(g(t) = \\ln(t)\\), \\(g^{-1}(t) = e^t\\) gives:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) = e^{x^T\\beta}\\]\n\n\n\nFor the distribution of \\(Y|X\\), defined on \\(\\mathbb{N}\\), we often assume it follows a Poisson distribution (exp. familily)\n\n\nIn this context:\n\n\\[Y|X \\sim \\mathcal{P}(e^{X^T\\beta})\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#summary",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#summary",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Summary",
    "text": "Summary\n\nLet’s step back and see the big picture.\nThere are 2 choices to make when setting up a GLM model: First, the distribution of Y given X. Second, the link function g that defines E(Y|X) equals g-inverse of X-transpose-beta.\nThese two choices are linked! The distribution you choose for Y given X guides which link function makes sense.\n\nThere are 2 choices to make when setting up a GLM model:\n\nThe distribution of \\(Y|X\\)\nThe link function \\(g\\) defining \\(E(Y|X) = g^{-1}(X^T\\beta)\\)\n\n\nKey insight: The second choice is linked to the first"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#the-3-common-cases",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#the-3-common-cases",
    "title": "Introduction to the Generalized Linear Model",
    "section": "The 3 Common Cases",
    "text": "The 3 Common Cases\n\nTo summarize, here are the three most common cases you’ll encounter, with their standard choices.\nWhen Y is in {0,1}, Y given X follows a Bernoulli distribution, and by default we use g equals logit. We’ll see why this is the natural choice.\nWhen Y is in {A1 through Ak}, Y given X follows a multinomial distribution, and by default we again use g equals logit.\nWhen Y is in the natural numbers, Y given X typically follows a Poisson distribution—though sometimes negative binomial—and by default we use g equals natural log.\n\n\nBinary (\\(Y \\in \\{0, 1\\}\\)):\n→ \\(Y|X\\): it’s a Bernoulli distribution\n→ By default \\(g = \\text{logit}\\) (see later)\n\n\nMulti-category (\\(Y \\in \\{A_1, \\ldots, A_k\\}\\)):\n→ \\(Y|X\\): it’s a multinomial distribution\n→ By default \\(g = \\text{logit}\\)\n\n\nCount (\\(Y \\in \\mathbb{N}\\)):\n→ \\(Y|X\\): Poisson (often) or negative binomial\n→ Choice of \\(g\\): by default \\(g = \\ln\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.draft.html#next",
    "href": "teaching/glm/slides/Introduction_glm.draft.html#next",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Next",
    "text": "Next\nlogistic model"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#example-obesity-study",
    "href": "teaching/glm/slides/logistic_model.draft.html#example-obesity-study",
    "title": "The Logistic Model",
    "section": "Example: Obesity Study",
    "text": "Example: Obesity Study\n. . .\nmodel=glm(Y~AGE+DBP+SEXE+ACTIV+WALK+MARITAL, family=binomial)\nsummary(model)\n. . .\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4459.5\\)\non \\(5290\\)\n\n\nAIC\n\\(4481.5\\)\n\n\n\n\n\n. . .\nThe model deviance is therefore \\(D = 4459.5\\).\nSignificance Test: We compare \\(D\\) to the null deviance \\(D_0 = 4610.8\\): \\(D_0 - D = 151.3\\). The p-value of the test equals \\(1 - \\chi^2_{10}(151.3) \\approx 0\\) where \\(\\chi^2_{10}\\) is the cdf of a \\(\\chi^2_{10}\\).\n. . .\nThe model is significant."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#classification",
    "href": "teaching/glm/slides/logistic_model.draft.html#classification",
    "title": "The Logistic Model",
    "section": "Classification",
    "text": "Classification\n. . .\nWe have estimated \\(p_\\beta(x) = P(Y = 1|X = x)\\) by \\(p_{\\hat{\\beta}}(x)\\).\n. . .\nFor a threshold to choose \\(s \\in [0, 1]\\), we use the rule:\n\n\\[\\begin{cases}\n\\text{if } p_{\\hat{\\beta}}(x) &gt; s, & \\hat{Y} = 1 \\\\\n\\text{if } p_{\\hat{\\beta}}(x) &lt; s, & \\hat{Y} = 0\n\\end{cases}\\]\n\n. . .\nThe “natural” choice of threshold is \\(s = 0.5\\) but this choice can be optimized."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html#deviance-1",
    "href": "teaching/glm/slides/logistic_model.draft.html#deviance-1",
    "title": "The Logistic Model",
    "section": "Deviance",
    "text": "Deviance\n. . .\nThe deviance of a model measures how much this model deviates from the saturated model (the ideal model in terms of likelihood).\n. . .\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\]\n\nwhere \\(L_{\\text{mod}}\\) denotes the log-likelihood for the model parameters.\n. . .\nWe always have \\(D \\geq 0\\).\n. . .\nIf all observations are distinct, \\(L_{\\text{sat}} = 0\\) therefore \\(D = -2L_{\\text{mod}}\\)"
  },
  {
    "objectID": "teaching/linear_model/TD/TD6-8.html",
    "href": "teaching/linear_model/TD/TD6-8.html",
    "title": "TD2-5",
    "section": "",
    "text": "The dataset “chdage.txt”, available in Moodle, contains data from 100 patients aged 20 to 69 years (variable age), some of whom have coronary heart disease (variable chd taking values Yes or No).\n\nImport this data into R as a data.frame. Observe the content of each variable and transform their class if necessary.\nPropose one or more graphical visualization(s) to analyze the potential relationship between the variables chd and age.\nIs a relationship apparent? What simple statistical test would confirm it?\nLet \\(Y = \\mathbb{1}_{\\text{chd}=\\text{Yes}}\\) and \\(p(x) = \\mathbb{P}(Y = 1 \\mid \\text{age} = x)\\). Give the distribution of \\(Y\\) given that \\(\\text{age} = x\\) as a function of \\(p(x)\\).\nBased on this distribution, give the likelihood of the observations \\((y_1, \\ldots, y_n)\\) as a function of \\((p(x_1), \\ldots, p(x_n))\\) where \\(x_i\\) denotes the age of individual \\(i\\) and \\(y_i = 1\\) if the latter has coronary heart disease.\nAs a first estimate of \\(p(x)\\), we implement the following approach:\n\nUse the 8 age groups proposed via the variable agegrp in the dataset, which forms 8 groups of individuals associated with these classes. Calculate \\(\\bar{x}_1, \\ldots, \\bar{x}_8\\), the midpoint of each age class.\nCalculate the proportions of chd = Yes in each group, denoted \\(\\hat{p}_1, \\ldots, \\hat{p}_8\\) (one can use the functions table and prop.table).\n\nTo analyze the quality of the previous estimate, transform the variable chd into a numerical variable taking the two values 0 and 1, and represent on the same graph the scatter plot of age and chd (recoded) and the estimated proportions in each class \\((\\bar{x}_k, \\hat{p}_k)\\) for \\(k = 1, \\ldots, 8\\).\nWhat are the virtues and limitations of this estimation procedure?\nWe decide to model \\(p(x)\\) using a logistic regression model with parameter \\(\\beta = (\\beta_0, \\beta_1) \\in \\mathbb{R}^2\\). What hypothesis does this imply about the expression of \\(p(x)\\)? Is this compatible with the previous graph? What other modeling alternative(s) could we suggest?\nWrite the log-likelihood of the logistic model as a function of \\(\\beta\\) and deduce the system that the maximum likelihood estimator \\(\\hat{\\beta}\\) must solve. Can this system be solved analytically?\nCalculate the maximum likelihood estimator using the glm function.\nRecall the theoretical definition of the odds ratio of having coronary heart disease between an individual of age \\(x_1\\) and an individual of age \\(x_2\\). What is its value for the previous model when the 2 individuals are 10 years apart (that is, \\(x_1 = x_2 + 10\\))?\nWe are now interested in the probability ratio (not the odds ratio) of having coronary heart disease between an individual of age \\(x_1\\) and an individual of age \\(x_2\\). What is this ratio for the previous model when the 2 individuals are 10 years apart (that is, \\(x_1 = x_2 + 10\\))? We can represent this ratio as a function of \\(x_2\\), for \\(x_2\\) taking values from 20 to 70 years.\n\nWe recall that under “good conditions”, the maximum likelihood estimator \\(\\hat{\\beta}\\) in a logistic regression model with \\(p\\) explanatory variables and \\(n\\) individuals satisfies the following convergence in distribution, when \\(n \\to \\infty\\),\n\\[J_n(\\beta)^{1/2} (\\hat{\\beta} - \\beta) \\xrightarrow{\\mathcal{L}} \\mathcal{N}(0_p, I_p),\\]\nwhere \\(J_n(\\beta)\\) is the Fisher information matrix. The latter has the expression\n\\[J_n(\\beta) = X^T W_\\beta X\\]\nwhere \\(X\\) is the design matrix and \\(W_\\beta\\) is the diagonal matrix\n\\[W_\\beta = \\begin{pmatrix}\np_\\beta(x_1)(1 - p_\\beta(x_1)) & 0 & & \\\\\n0 & \\ddots & & \\\\\n& & \\ddots & 0 \\\\\n& & 0 & p_\\beta(x_n)(1 - p_\\beta(x_n))\n\\end{pmatrix}.\\]\n\nJustify that an asymptotic approximation of the distribution of \\((\\hat{\\beta} - \\beta)\\) is \\(\\mathcal{N}(0_p, J_n^{-1}(\\beta))\\).\nHow can we exploit this result to estimate the standard deviation of each coordinate of \\(\\hat{\\beta}\\)? Give the concrete procedure to apply, but it is not required to implement it numerically.\nThis estimation procedure is used by the glm function. Using its output, give an estimate of the standard deviation of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nConstruct an asymptotic 95% confidence interval for the parameter \\(\\beta_1\\).\nAccording to the previous question, is the parameter \\(\\beta_1\\) different from 0 at the 5% asymptotic error level? What is the name of this test procedure? Give the p-value associated with this test and verify that it agrees with the output of glm.\nCalculate the deviance test statistic for the significance of the GLM model (relative to the null model). Deduce the p-value and conclude at the 10%, 5%, and 1% error levels. Compare with the results of the test performed in R using the anova function applied to the model, with the option test=\"Chisq\".\nRepeat the graph from question 7 and superimpose (in the form of a curve) the predicted values \\(\\hat{p}(x)\\) by the logistic model, calculated for a grid of values of \\(x\\) covering the range taken by the observations. One can use the predict function associated with the option type=\"response\".\nStarting from the convergence in distribution of \\(\\hat{\\beta}\\) recalled above, deduce a confidence interval at the 95% asymptotic level for \\(p(x)\\). Add this confidence interval for each \\(x\\) considered to the previous graph. One can profitably exploit the option se=TRUE of the predict function in the case type=\"link\".\n\n\n\n\nWe have a pair of random variables \\((X, Y)\\) where \\(Y\\) is binary and \\(X\\) takes values in \\(\\mathbb{R}^d\\). We denote \\(p = \\mathbb{P}(Y = 1)\\), \\(f_0(\\cdot)\\) the conditional density of \\(X\\) given that \\(Y = 0\\) and \\(f_1(\\cdot)\\) the conditional density of \\(X\\) given that \\(Y = 1\\). We further denote\n\\[h(x) = \\log \\frac{f_1(x)}{f_0(x)} + \\log \\frac{p}{1 - p}, \\quad x \\in \\mathbb{R}^d.\\]\n\nShow that for all \\(x\\)\n\n\\[\\mathbb{P}(Y = 1 \\mid X = x) = \\frac{1}{1 + e^{-h(x)}}.\\]\n\nRecall that a distribution on \\(\\mathbb{R}^d\\) belongs to the exponential family if its density can be written as \\(a(x)b(\\theta)e^{\\theta^T T(x)}\\), for some parameter \\(\\theta \\in \\mathbb{R}^q\\), where \\(a\\) and \\(b\\) are positive functions and \\(T : \\mathbb{R}^d \\to \\mathbb{R}^q\\) is a function called the sufficient statistic.\nShow that if the conditional densities \\(f_0\\) and \\(f_1\\) belong to the same exponential family and differ only in the value of their associated parameter, then \\(\\mathbb{P}(Y = 1 \\mid X = x)\\) follows exactly a logistic regression model, specifying the parameter and variables.\n\n\n\n\nPreamble: Entropy is a quantity found in thermodynamics to measure the state of disorder or randomness of a system. In the same spirit, it is also found in information theory and probability to quantify the disorder or amount of randomness that a probability distribution incorporates. A physical system tends to naturally evolve towards a state of maximum entropy. Following this principle, it is natural, to describe a given random experiment, to choose probability distributions that maximize entropy. This is the principle we will apply to seek to best choose \\(\\mathbb{P}(Y = 1)\\) when \\(Y\\) is binary.\nMathematically, given \\(Y\\) a binary variable and \\(p = \\mathbb{P}(Y = 1)\\), the entropy of the distribution of \\(Y\\) equals\n\\[-p \\log(p) - (1 - p) \\log(1 - p).\\]\nThe entropy of a vector of independent binary variables \\(Y_1, \\ldots, Y_n\\) is simply the sum of the individual entropies.\n\nWithout any source of constraint, what is the maximum entropy distribution of a binary variable?\nSuppose now that we have a sample of \\(n\\) pairs \\((Y_i, X_i)\\) where \\(X_i\\) is a random variable in \\(\\mathbb{R}^d\\). We denote \\(p_i(x_i) = \\mathbb{P}(Y_i = 1 \\mid X_i = x_i)\\), \\(i = 1, \\ldots, n\\).\nA priori, without using any information contained in the sample, what are the values of \\(p_i(x_i)\\) that maximize entropy?\nWe want to find the \\(p_i(x_i)\\) that maximize entropy while being consistent with the observations. This amounts to including constraints on the possible \\(p_i(x_i)\\). We choose the constraints:\n\n\\[\\sum_{i=1}^n y_i x_i = \\sum_{i=1}^n p_i(x_i) x_i.\\]\n(Since \\(x_i\\) is a vector of size \\(d\\), this is indeed a system of \\(d\\) constraints). These constraints are quite natural: we want the average of the \\(x_i\\) of individuals in the positive group (\\(y_i = 1\\)) to coincide with the average of the \\(x_i\\) weighted by the probability that \\(y_i\\) equals 1. In particular (for the constant variable 1), we want the proportion of \\(y_i = 1\\) to coincide with the sum of probabilities.\nFind the \\(p_i(x_i)\\) that maximize entropy while satisfying the previous constraints. One can give the solution up to an unknown (vector) constant.\n\nWhat is the connection with logistic regression?\n\n\n\n\nWe consider the break data, available on the Moodle course page. This dataset, of size \\(n = 33\\), contains three variables related to the state of an automobile:\n\nA variable fault that equals 1 if the car concerned had a breakdown, 0 otherwise;\nA variable age that gives the age of the car;\nA variable brand that gives the brand of the car.\n\nThe goal of the exercise is to model the variable fault.\n\nImport the data into R and recode the class of each variable if necessary.\nGraphically observe the potential relationship between fault and age on the one hand, and between fault and brand on the other hand.\nWe want to implement a logistic regression model explaining the probability of having a breakdown as a function of the car’s age and its brand. Launch this modeling in R and write the mathematical formula of the obtained model. In particular, specify the specific estimated model for cars of brand 0, then brand 1, then brand 2.\nAnalyze the overall quality of the model.\nRepeat the modeling including only the variable age in the model. Is it better?\nThe scatter plot between the variable age and the variable fault seems to suggest that breakdowns occur at the beginning of the vehicle’s life (“teething problems” or “break-in”) and at the end of life (wear breakdowns). This behavior of the breakdown probability in the form of a parabola encourages us to try to include a quadratic term in the variable age. Make this addition to the model also including the variable brand and analyze the results. Is the model significant?\nWhat is the odds ratio associated with brand “2” relative to brand “0” of the variable brand? Interpret this value and give a 95% confidence interval around this estimate. Is the inclusion of the variable brand in the model relevant?\n\n\n\n\nWe consider the mental data, contained in the file mental.txt available on the Moodle course page. This dataset, of size \\(n = 40\\), is extracted from a study on the mental health of adults living in Alachua County, Florida, USA. It contains three variables:\n\nA variable impair describing the mental state of the person concerned, from 1 (healthy) to 4 (in poor health),\nA variable ses that equals 1 if the person has a high socioeconomic status, 0 otherwise,\nA variable life measuring the number and intensity of upheavals the person has experienced over the past three years, from 0 (no change) to 9 (very significant changes).\n\nThe goal of the exercise is to model the variable impair.\n\nImport the data into R. Is the variable ses qualitative or quantitative? Same question for the variable life. Change their class in R if needed.\nPerform a small descriptive study to identify a potential relationship between the variable impair and the other variables in the dataset.\nMathematically write the proportional cumulative logistic regression model without interaction term linking impair to ses and life, and allowing estimation of the probabilities that impair = 1, ≤ 2 and ≤ 3. How many coefficients does this model have?\nEstimate the coefficients of this model using the vglm function from the VGAM package, associated with the option family = cumulative(parallel=TRUE). Check that the number of coefficients is indeed as expected.\nWrite the mathematical definition of the odds ratio associated with the variable ses and interpret this quantity.\nGive an asymptotic 95% confidence interval for the parameter related to the variable ses.\nDeduce an asymptotic 95% confidence interval for the associated odds ratio.\nIs there an influence of socioeconomic status on mental health status at the 5% level? And at the 10% level?\nWe want to see if a more complex model would fit the data better. Mathematically write then estimate a proportional cumulative logistic regression model with interaction term. Interpret the obtained result. Is this model significantly better, at the 5% asymptotic error level?\nSame question with a cumulative logistic regression model without proportional structure, but without interaction term.\nConversely, could we propose a simpler model?\nFinally, we decide not to exploit the fact that the variable impair is ordinal, and to model it by a multinomial logistic model. Compare this approach to the previous modeling.\n\n\n\n\nThe goal of the study is to study the diversity of ants at the Nouragues experimental site in French Guiana. 1 m² of litter was collected at several locations in 4 different forests (the plateau forest GPWT, the liana forest FLWT, the transition forest FTWT, and the Inselberg forest INWT). Each sample was weighed (variable Weight) and the number of different species present in the sample was recorded (variable Effectif). Finally, the collection conditions (humid or dry, variable Conditions) were noted to test their influence on the presence of ants.\n\nImport the data into R and transform categorical variables into factors.\nPerform a small descriptive study to identify a potential relationship between the number of observed species and the other available variables.\nModel the variable Effectifs as a function of all available variables by a Poisson log-linear model, including all their possible interactions. Analyze the model output.\nUsing the step function, perform a stepwise backward selection of the best sub-model of the previous model according to the AIC criterion, then according to the BIC criterion. Similarly perform a stepwise forward selection. Compare the obtained choices.\nThe inconsistency in the previous backward and forward selections suggests that there may be an alternative sub-model (not tested by these algorithms) that is even better. This encourages us to perform an exhaustive selection of the best sub-model, as proposed by the regsubsets function in linear regression. Unfortunately, the latter does not work with the Poisson model. If we wanted to implement this exhaustive selection ourselves, justify that there would be 30 sub-models (with constant) to test, counting the most general model.\nWe admit that at the end of such an exhaustive selection, the best sub-model in terms of AIC and BIC is the one whose Weight coefficients are broken down into as many crossed modalities as contained in the factors Site and Conditions (that is, 8), but which has an identical intercept for all crossed modalities of Site and Conditions. Estimate this model, calculate its AIC and BIC and compare with those of the previously selected models.\nAs an alternative, we want to try to fit a negative binomial generalized model. If we include all possible interactions, does this approach seem preferable to the Poisson model?\nAfter an exhaustive selection, we admit that the best negative binomial sub-model in terms of AIC involves the same variables as the best Poisson model. On the other hand, for the BIC criterion, it is the model involving only Weight and Site, in which the coefficient of Weight varies according to Site, but the intercept is constant. Estimate these two models and calculate their AIC and BIC.\nGiven the experts’ opinion, it seems important that the model takes into account humidity conditions. What final model should be retained?\nWrite the equation of the retained model according to the different sites and collection conditions.\nAccording to the selected model, what is the probability of observing more than 15 species on an INWT type soil in dry weather, based on a soil sample weighing 10kg? Same question if the weather is humid.\n\n\n\n\nThe crabs dataset contains observations of 173 female horseshoe crabs. These are marine animals that resemble crabs having a horseshoe shape. For each female horseshoe crab, we record its color color (coded from 1 to 4, from lightest to darkest), its width width, its weight weight, and satell: the number of satellite male horseshoe crabs (that is, attached to the female). Color is a sign of the age of the horseshoe crab, the latter tending to darken over time. We seek to model the number satell as a function of the available variables.\n\nImplement a Poisson log-linear model and a negative binomial model. Evaluate their quality. In particular, one can discuss the relevance of considering the variable color as a quantitative variable or a factor.\nImprove the modeling by taking into account zero inflation."
  },
  {
    "objectID": "teaching/linear_model/TD/TD6-8.html#exercise-1",
    "href": "teaching/linear_model/TD/TD6-8.html#exercise-1",
    "title": "TD2-5",
    "section": "",
    "text": "The dataset “chdage.txt”, available in Moodle, contains data from 100 patients aged 20 to 69 years (variable age), some of whom have coronary heart disease (variable chd taking values Yes or No).\n\nImport this data into R as a data.frame. Observe the content of each variable and transform their class if necessary.\nPropose one or more graphical visualization(s) to analyze the potential relationship between the variables chd and age.\nIs a relationship apparent? What simple statistical test would confirm it?\nLet \\(Y = \\mathbb{1}_{\\text{chd}=\\text{Yes}}\\) and \\(p(x) = \\mathbb{P}(Y = 1 \\mid \\text{age} = x)\\). Give the distribution of \\(Y\\) given that \\(\\text{age} = x\\) as a function of \\(p(x)\\).\nBased on this distribution, give the likelihood of the observations \\((y_1, \\ldots, y_n)\\) as a function of \\((p(x_1), \\ldots, p(x_n))\\) where \\(x_i\\) denotes the age of individual \\(i\\) and \\(y_i = 1\\) if the latter has coronary heart disease.\nAs a first estimate of \\(p(x)\\), we implement the following approach:\n\nUse the 8 age groups proposed via the variable agegrp in the dataset, which forms 8 groups of individuals associated with these classes. Calculate \\(\\bar{x}_1, \\ldots, \\bar{x}_8\\), the midpoint of each age class.\nCalculate the proportions of chd = Yes in each group, denoted \\(\\hat{p}_1, \\ldots, \\hat{p}_8\\) (one can use the functions table and prop.table).\n\nTo analyze the quality of the previous estimate, transform the variable chd into a numerical variable taking the two values 0 and 1, and represent on the same graph the scatter plot of age and chd (recoded) and the estimated proportions in each class \\((\\bar{x}_k, \\hat{p}_k)\\) for \\(k = 1, \\ldots, 8\\).\nWhat are the virtues and limitations of this estimation procedure?\nWe decide to model \\(p(x)\\) using a logistic regression model with parameter \\(\\beta = (\\beta_0, \\beta_1) \\in \\mathbb{R}^2\\). What hypothesis does this imply about the expression of \\(p(x)\\)? Is this compatible with the previous graph? What other modeling alternative(s) could we suggest?\nWrite the log-likelihood of the logistic model as a function of \\(\\beta\\) and deduce the system that the maximum likelihood estimator \\(\\hat{\\beta}\\) must solve. Can this system be solved analytically?\nCalculate the maximum likelihood estimator using the glm function.\nRecall the theoretical definition of the odds ratio of having coronary heart disease between an individual of age \\(x_1\\) and an individual of age \\(x_2\\). What is its value for the previous model when the 2 individuals are 10 years apart (that is, \\(x_1 = x_2 + 10\\))?\nWe are now interested in the probability ratio (not the odds ratio) of having coronary heart disease between an individual of age \\(x_1\\) and an individual of age \\(x_2\\). What is this ratio for the previous model when the 2 individuals are 10 years apart (that is, \\(x_1 = x_2 + 10\\))? We can represent this ratio as a function of \\(x_2\\), for \\(x_2\\) taking values from 20 to 70 years.\n\nWe recall that under “good conditions”, the maximum likelihood estimator \\(\\hat{\\beta}\\) in a logistic regression model with \\(p\\) explanatory variables and \\(n\\) individuals satisfies the following convergence in distribution, when \\(n \\to \\infty\\),\n\\[J_n(\\beta)^{1/2} (\\hat{\\beta} - \\beta) \\xrightarrow{\\mathcal{L}} \\mathcal{N}(0_p, I_p),\\]\nwhere \\(J_n(\\beta)\\) is the Fisher information matrix. The latter has the expression\n\\[J_n(\\beta) = X^T W_\\beta X\\]\nwhere \\(X\\) is the design matrix and \\(W_\\beta\\) is the diagonal matrix\n\\[W_\\beta = \\begin{pmatrix}\np_\\beta(x_1)(1 - p_\\beta(x_1)) & 0 & & \\\\\n0 & \\ddots & & \\\\\n& & \\ddots & 0 \\\\\n& & 0 & p_\\beta(x_n)(1 - p_\\beta(x_n))\n\\end{pmatrix}.\\]\n\nJustify that an asymptotic approximation of the distribution of \\((\\hat{\\beta} - \\beta)\\) is \\(\\mathcal{N}(0_p, J_n^{-1}(\\beta))\\).\nHow can we exploit this result to estimate the standard deviation of each coordinate of \\(\\hat{\\beta}\\)? Give the concrete procedure to apply, but it is not required to implement it numerically.\nThis estimation procedure is used by the glm function. Using its output, give an estimate of the standard deviation of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\nConstruct an asymptotic 95% confidence interval for the parameter \\(\\beta_1\\).\nAccording to the previous question, is the parameter \\(\\beta_1\\) different from 0 at the 5% asymptotic error level? What is the name of this test procedure? Give the p-value associated with this test and verify that it agrees with the output of glm.\nCalculate the deviance test statistic for the significance of the GLM model (relative to the null model). Deduce the p-value and conclude at the 10%, 5%, and 1% error levels. Compare with the results of the test performed in R using the anova function applied to the model, with the option test=\"Chisq\".\nRepeat the graph from question 7 and superimpose (in the form of a curve) the predicted values \\(\\hat{p}(x)\\) by the logistic model, calculated for a grid of values of \\(x\\) covering the range taken by the observations. One can use the predict function associated with the option type=\"response\".\nStarting from the convergence in distribution of \\(\\hat{\\beta}\\) recalled above, deduce a confidence interval at the 95% asymptotic level for \\(p(x)\\). Add this confidence interval for each \\(x\\) considered to the previous graph. One can profitably exploit the option se=TRUE of the predict function in the case type=\"link\"."
  },
  {
    "objectID": "teaching/linear_model/TD/TD6-8.html#exercise-2-the-logistic-model-is-natural",
    "href": "teaching/linear_model/TD/TD6-8.html#exercise-2-the-logistic-model-is-natural",
    "title": "TD2-5",
    "section": "",
    "text": "We have a pair of random variables \\((X, Y)\\) where \\(Y\\) is binary and \\(X\\) takes values in \\(\\mathbb{R}^d\\). We denote \\(p = \\mathbb{P}(Y = 1)\\), \\(f_0(\\cdot)\\) the conditional density of \\(X\\) given that \\(Y = 0\\) and \\(f_1(\\cdot)\\) the conditional density of \\(X\\) given that \\(Y = 1\\). We further denote\n\\[h(x) = \\log \\frac{f_1(x)}{f_0(x)} + \\log \\frac{p}{1 - p}, \\quad x \\in \\mathbb{R}^d.\\]\n\nShow that for all \\(x\\)\n\n\\[\\mathbb{P}(Y = 1 \\mid X = x) = \\frac{1}{1 + e^{-h(x)}}.\\]\n\nRecall that a distribution on \\(\\mathbb{R}^d\\) belongs to the exponential family if its density can be written as \\(a(x)b(\\theta)e^{\\theta^T T(x)}\\), for some parameter \\(\\theta \\in \\mathbb{R}^q\\), where \\(a\\) and \\(b\\) are positive functions and \\(T : \\mathbb{R}^d \\to \\mathbb{R}^q\\) is a function called the sufficient statistic.\nShow that if the conditional densities \\(f_0\\) and \\(f_1\\) belong to the same exponential family and differ only in the value of their associated parameter, then \\(\\mathbb{P}(Y = 1 \\mid X = x)\\) follows exactly a logistic regression model, specifying the parameter and variables."
  },
  {
    "objectID": "teaching/linear_model/TD/TD6-8.html#exercise-3-the-logistic-model-is-natural-bis",
    "href": "teaching/linear_model/TD/TD6-8.html#exercise-3-the-logistic-model-is-natural-bis",
    "title": "TD2-5",
    "section": "",
    "text": "Preamble: Entropy is a quantity found in thermodynamics to measure the state of disorder or randomness of a system. In the same spirit, it is also found in information theory and probability to quantify the disorder or amount of randomness that a probability distribution incorporates. A physical system tends to naturally evolve towards a state of maximum entropy. Following this principle, it is natural, to describe a given random experiment, to choose probability distributions that maximize entropy. This is the principle we will apply to seek to best choose \\(\\mathbb{P}(Y = 1)\\) when \\(Y\\) is binary.\nMathematically, given \\(Y\\) a binary variable and \\(p = \\mathbb{P}(Y = 1)\\), the entropy of the distribution of \\(Y\\) equals\n\\[-p \\log(p) - (1 - p) \\log(1 - p).\\]\nThe entropy of a vector of independent binary variables \\(Y_1, \\ldots, Y_n\\) is simply the sum of the individual entropies.\n\nWithout any source of constraint, what is the maximum entropy distribution of a binary variable?\nSuppose now that we have a sample of \\(n\\) pairs \\((Y_i, X_i)\\) where \\(X_i\\) is a random variable in \\(\\mathbb{R}^d\\). We denote \\(p_i(x_i) = \\mathbb{P}(Y_i = 1 \\mid X_i = x_i)\\), \\(i = 1, \\ldots, n\\).\nA priori, without using any information contained in the sample, what are the values of \\(p_i(x_i)\\) that maximize entropy?\nWe want to find the \\(p_i(x_i)\\) that maximize entropy while being consistent with the observations. This amounts to including constraints on the possible \\(p_i(x_i)\\). We choose the constraints:\n\n\\[\\sum_{i=1}^n y_i x_i = \\sum_{i=1}^n p_i(x_i) x_i.\\]\n(Since \\(x_i\\) is a vector of size \\(d\\), this is indeed a system of \\(d\\) constraints). These constraints are quite natural: we want the average of the \\(x_i\\) of individuals in the positive group (\\(y_i = 1\\)) to coincide with the average of the \\(x_i\\) weighted by the probability that \\(y_i\\) equals 1. In particular (for the constant variable 1), we want the proportion of \\(y_i = 1\\) to coincide with the sum of probabilities.\nFind the \\(p_i(x_i)\\) that maximize entropy while satisfying the previous constraints. One can give the solution up to an unknown (vector) constant.\n\nWhat is the connection with logistic regression?"
  },
  {
    "objectID": "teaching/linear_model/TD/TD6-8.html#exercise-4-break-data",
    "href": "teaching/linear_model/TD/TD6-8.html#exercise-4-break-data",
    "title": "TD2-5",
    "section": "",
    "text": "We consider the break data, available on the Moodle course page. This dataset, of size \\(n = 33\\), contains three variables related to the state of an automobile:\n\nA variable fault that equals 1 if the car concerned had a breakdown, 0 otherwise;\nA variable age that gives the age of the car;\nA variable brand that gives the brand of the car.\n\nThe goal of the exercise is to model the variable fault.\n\nImport the data into R and recode the class of each variable if necessary.\nGraphically observe the potential relationship between fault and age on the one hand, and between fault and brand on the other hand.\nWe want to implement a logistic regression model explaining the probability of having a breakdown as a function of the car’s age and its brand. Launch this modeling in R and write the mathematical formula of the obtained model. In particular, specify the specific estimated model for cars of brand 0, then brand 1, then brand 2.\nAnalyze the overall quality of the model.\nRepeat the modeling including only the variable age in the model. Is it better?\nThe scatter plot between the variable age and the variable fault seems to suggest that breakdowns occur at the beginning of the vehicle’s life (“teething problems” or “break-in”) and at the end of life (wear breakdowns). This behavior of the breakdown probability in the form of a parabola encourages us to try to include a quadratic term in the variable age. Make this addition to the model also including the variable brand and analyze the results. Is the model significant?\nWhat is the odds ratio associated with brand “2” relative to brand “0” of the variable brand? Interpret this value and give a 95% confidence interval around this estimate. Is the inclusion of the variable brand in the model relevant?"
  },
  {
    "objectID": "teaching/linear_model/TD/TD6-8.html#exercise-5-mental-data",
    "href": "teaching/linear_model/TD/TD6-8.html#exercise-5-mental-data",
    "title": "TD2-5",
    "section": "",
    "text": "We consider the mental data, contained in the file mental.txt available on the Moodle course page. This dataset, of size \\(n = 40\\), is extracted from a study on the mental health of adults living in Alachua County, Florida, USA. It contains three variables:\n\nA variable impair describing the mental state of the person concerned, from 1 (healthy) to 4 (in poor health),\nA variable ses that equals 1 if the person has a high socioeconomic status, 0 otherwise,\nA variable life measuring the number and intensity of upheavals the person has experienced over the past three years, from 0 (no change) to 9 (very significant changes).\n\nThe goal of the exercise is to model the variable impair.\n\nImport the data into R. Is the variable ses qualitative or quantitative? Same question for the variable life. Change their class in R if needed.\nPerform a small descriptive study to identify a potential relationship between the variable impair and the other variables in the dataset.\nMathematically write the proportional cumulative logistic regression model without interaction term linking impair to ses and life, and allowing estimation of the probabilities that impair = 1, ≤ 2 and ≤ 3. How many coefficients does this model have?\nEstimate the coefficients of this model using the vglm function from the VGAM package, associated with the option family = cumulative(parallel=TRUE). Check that the number of coefficients is indeed as expected.\nWrite the mathematical definition of the odds ratio associated with the variable ses and interpret this quantity.\nGive an asymptotic 95% confidence interval for the parameter related to the variable ses.\nDeduce an asymptotic 95% confidence interval for the associated odds ratio.\nIs there an influence of socioeconomic status on mental health status at the 5% level? And at the 10% level?\nWe want to see if a more complex model would fit the data better. Mathematically write then estimate a proportional cumulative logistic regression model with interaction term. Interpret the obtained result. Is this model significantly better, at the 5% asymptotic error level?\nSame question with a cumulative logistic regression model without proportional structure, but without interaction term.\nConversely, could we propose a simpler model?\nFinally, we decide not to exploit the fact that the variable impair is ordinal, and to model it by a multinomial logistic model. Compare this approach to the previous modeling."
  },
  {
    "objectID": "teaching/linear_model/TD/TD6-8.html#exercise-6-ants-data",
    "href": "teaching/linear_model/TD/TD6-8.html#exercise-6-ants-data",
    "title": "TD2-5",
    "section": "",
    "text": "The goal of the study is to study the diversity of ants at the Nouragues experimental site in French Guiana. 1 m² of litter was collected at several locations in 4 different forests (the plateau forest GPWT, the liana forest FLWT, the transition forest FTWT, and the Inselberg forest INWT). Each sample was weighed (variable Weight) and the number of different species present in the sample was recorded (variable Effectif). Finally, the collection conditions (humid or dry, variable Conditions) were noted to test their influence on the presence of ants.\n\nImport the data into R and transform categorical variables into factors.\nPerform a small descriptive study to identify a potential relationship between the number of observed species and the other available variables.\nModel the variable Effectifs as a function of all available variables by a Poisson log-linear model, including all their possible interactions. Analyze the model output.\nUsing the step function, perform a stepwise backward selection of the best sub-model of the previous model according to the AIC criterion, then according to the BIC criterion. Similarly perform a stepwise forward selection. Compare the obtained choices.\nThe inconsistency in the previous backward and forward selections suggests that there may be an alternative sub-model (not tested by these algorithms) that is even better. This encourages us to perform an exhaustive selection of the best sub-model, as proposed by the regsubsets function in linear regression. Unfortunately, the latter does not work with the Poisson model. If we wanted to implement this exhaustive selection ourselves, justify that there would be 30 sub-models (with constant) to test, counting the most general model.\nWe admit that at the end of such an exhaustive selection, the best sub-model in terms of AIC and BIC is the one whose Weight coefficients are broken down into as many crossed modalities as contained in the factors Site and Conditions (that is, 8), but which has an identical intercept for all crossed modalities of Site and Conditions. Estimate this model, calculate its AIC and BIC and compare with those of the previously selected models.\nAs an alternative, we want to try to fit a negative binomial generalized model. If we include all possible interactions, does this approach seem preferable to the Poisson model?\nAfter an exhaustive selection, we admit that the best negative binomial sub-model in terms of AIC involves the same variables as the best Poisson model. On the other hand, for the BIC criterion, it is the model involving only Weight and Site, in which the coefficient of Weight varies according to Site, but the intercept is constant. Estimate these two models and calculate their AIC and BIC.\nGiven the experts’ opinion, it seems important that the model takes into account humidity conditions. What final model should be retained?\nWrite the equation of the retained model according to the different sites and collection conditions.\nAccording to the selected model, what is the probability of observing more than 15 species on an INWT type soil in dry weather, based on a soil sample weighing 10kg? Same question if the weather is humid."
  },
  {
    "objectID": "teaching/linear_model/TD/TD6-8.html#exercise-7-horseshoe-crabs-data",
    "href": "teaching/linear_model/TD/TD6-8.html#exercise-7-horseshoe-crabs-data",
    "title": "TD2-5",
    "section": "",
    "text": "The crabs dataset contains observations of 173 female horseshoe crabs. These are marine animals that resemble crabs having a horseshoe shape. For each female horseshoe crab, we record its color color (coded from 1 to 4, from lightest to darkest), its width width, its weight weight, and satell: the number of satellite male horseshoe crabs (that is, attached to the female). Color is a sign of the age of the horseshoe crab, the latter tending to darken over time. We seek to model the number satell as a function of the available variables.\n\nImplement a Poisson log-linear model and a negative binomial model. Evaluate their quality. In particular, one can discuss the relevance of considering the variable color as a quantitative variable or a factor.\nImprove the modeling by taking into account zero inflation."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html",
    "href": "teaching/glm/slides/logistic_model.html",
    "title": "The Logistic Model",
    "section": "",
    "text": "\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\) \\(\\newcommand{\\E}{\\mathbb E}\\) \\(\\newcommand{\\P}{\\mathbb P}\\) \\(\\newcommand{\\Var}{\\mathbb V}\\) \\(\\newcommand{\\Cov}{\\mathrm{Cov}}\\) \\(\\newcommand{\\1}{\\mathbf 1}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#deviance-1",
    "href": "teaching/glm/slides/logistic_model.html#deviance-1",
    "title": "The Logistic Model",
    "section": "Deviance",
    "text": "Deviance\n. . .\nThe deviance of a model measures how much this model deviates from the saturated model (the ideal model in terms of likelihood).\n. . .\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\]\n\nwhere \\(L_{\\text{mod}}\\) denotes the log-likelihood for the model parameters.\n. . .\nWe always have \\(D \\geq 0\\).\n. . .\nIf all observations are distinct, \\(L_{\\text{sat}} = 0\\) therefore \\(D = -2L_{\\text{mod}}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-obesity-study",
    "href": "teaching/glm/slides/logistic_model.html#example-obesity-study",
    "title": "The Logistic Model",
    "section": "Example: Obesity Study",
    "text": "Example: Obesity Study\n. . .\nmodel=glm(Y~AGE+DBP+SEXE+ACTIV+WALK+MARITAL, family=binomial)\nsummary(model)\n. . .\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4459.5\\)\non \\(5290\\)\n\n\nAIC\n\\(4481.5\\)\n\n\n\n\n\n. . .\nThe model deviance is therefore \\(D = 4459.5\\).\nSignificance Test: We compare \\(D\\) to the null deviance \\(D_0 = 4610.8\\): \\(D_0 - D = 151.3\\). The p-value of the test equals \\(1 - \\chi^2_{10}(151.3) \\approx 0\\) where \\(\\chi^2_{10}\\) is the cdf of a \\(\\chi^2_{10}\\).\n. . .\nThe model is significant."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#classification",
    "href": "teaching/glm/slides/logistic_model.html#classification",
    "title": "The Logistic Model",
    "section": "Classification",
    "text": "Classification\n. . .\nWe have estimated \\(p_\\beta(x) = P(Y = 1|X = x)\\) by \\(p_{\\hat{\\beta}}(x)\\).\n. . .\nFor a threshold to choose \\(s \\in [0, 1]\\), we use the rule:\n\n\\[\\begin{cases}\n\\text{if } p_{\\hat{\\beta}}(x) &gt; s, & \\hat{Y} = 1 \\\\\n\\text{if } p_{\\hat{\\beta}}(x) &lt; s, & \\hat{Y} = 0\n\\end{cases}\\]\n\n. . .\nThe “natural” choice of threshold is \\(s = 0.5\\) but this choice can be optimized."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html",
    "href": "teaching/glm/slides/Introduction_glm.html",
    "title": "Introduction to the Generalized Linear Model",
    "section": "",
    "text": "\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\) \\(\\newcommand{\\E}{\\mathbb E}\\) \\(\\newcommand{\\P}{\\mathbb P}\\) \\(\\newcommand{\\Var}{\\mathbb V}\\) \\(\\newcommand{\\Cov}{\\mathrm{Cov}}\\) \\(\\newcommand{\\1}{\\mathbf 1}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.draft.html",
    "href": "teaching/glm/slides/logistic_model.draft.html",
    "title": "The Logistic Model",
    "section": "",
    "text": "\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\) \\(\\newcommand{\\E}{\\mathbb E}\\) \\(\\newcommand{\\P}{\\mathbb P}\\) \\(\\newcommand{\\Var}{\\mathbb V}\\) \\(\\newcommand{\\Cov}{\\mathrm{Cov}}\\) \\(\\newcommand{\\1}{\\mathbf 1}\\)"
  }
]