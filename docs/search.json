[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentation",
    "section": "",
    "text": "I am an assistant professor in statistics at ENSAI (Rennes) working on topics related to machine learning and high-dimensional statistics. My PhD centered on two areas in modern statistics: change-point detection and ranking problems.\nI am interested in GPU parallel computing. I’m currently developing a Julia library focused on optimizing fundamental functions like mapreduce (for operations such as sum) and accumulate (for operations like prefix sum). You can explore my experimental work at my GitHub repository Luma for more technical details.\ncontact: firstname.lastname@ensai.fr"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Presentation",
    "section": "Publications",
    "text": "Publications\n\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal rates for ranking a permuted isotonic matrix in polynomial time (2024) SODA\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal permutation estimation in crowd-sourcing problems (2023) Annals of Statistics. [arxiv], [presentation],[poster]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal multiple change-point detection for high-dimensional data (2023) EJS [arxiv]"
  },
  {
    "objectID": "index.html#vitae",
    "href": "index.html#vitae",
    "title": "Presentation",
    "section": "Vitae",
    "text": "Vitae\n\nResearch\n\nSep 2024 - Present: assistant professor in statistics at ENSAI\n2024: Postdoctoral researcher at ENS Lyon\nFeb 2020 - Apr 2022 and Oct 2022 - Dec 2023: PhD Student at Université de Montpellier and INRAE\n2019: Research Project at Ecole Normale Supérieure Paris Saclay with Vianney Perchet on Optimal Order Selection for an Online Reward Maximization problem\nApr 2019: Research Project at OVGU Magdeburg with Alexandra Carpentier on Signal Detection and Change-Point Detection\n2018: Research Internship at the University of Cambridge on Gaussian Free Field\n\nExperience\n\nApr 2022 - Oct 2022: Quantitative Intern at QRT\n\nEducation\n\n2016-2020: Student at Ecole Normale Supérieure de Lyon\n2013-2016: Preparatory School of Mathematics and Physics (CPGE MPSI/MP*) in Strasbourg\n\nTeaching\n\n2020-2022: Teaching assistant at Université de Montpellier."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#course-structure",
    "href": "teaching/linear_model/slides/introduction.html#course-structure",
    "title": "Introduction",
    "section": "Course Structure",
    "text": "Course Structure\n\n8 lecture sessions\nCourse materials and slides (both evolving) available on Moodle\n8 TD/TP sessions (tutorial/practical work)\nContinuous assessment: November 6 (date to be confirmed)\nFinal exam: December 18 (date to be confirmed)\nAttention: Some practical sessions may take place in the tutorial room with your personal computer (not the first session)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#objectives-of-a-regression-model",
    "href": "teaching/linear_model/slides/introduction.html#objectives-of-a-regression-model",
    "title": "Introduction",
    "section": "Objectives of a Regression Model",
    "text": "Objectives of a Regression Model\n\nExplain a quantity \\(Y\\) based on \\(p\\) quantities \\(X^{(1)}, ..., X^{(p)}\\) (explanatory variables, or regressors).\n\n\nFor this purpose, we have \\(n\\) observations of each quantity from \\(n\\) individuals."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#examples",
    "href": "teaching/linear_model/slides/introduction.html#examples",
    "title": "Introduction",
    "section": "Examples:",
    "text": "Examples:\n\n\\(Y\\): daily electricity consumption in France\n\n\\(X= X^{(1)}\\): average daily temperature\n\n\n\nThe data consists of a history of \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\) over \\(n\\) days\n\n\nQuestion: Do we have \\(Y \\approx f(X)\\) for a certain function f?\nSimplifying: Do we have \\(Y ≈ aX + b\\) for certain values \\(a\\) and \\(b\\)?\nIf yes, what is \\(a\\)? What is \\(b\\)? Is the relationship “reliable”?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#examples-1",
    "href": "teaching/linear_model/slides/introduction.html#examples-1",
    "title": "Introduction",
    "section": "Examples",
    "text": "Examples\n\n\\(Y \\in \\{0,1\\}\\): customer quality (\\(1\\): good; \\(0\\): not good)\n\n\\(X^{(1)}\\): customer income\n\n\\(X^{(2)}\\): socio-professional category (6-7 possibilities)\n\n\\(X^{(3)}\\): age\n\n\n\nData: n customers.\nIn this case, we model \\(p = P(Y = 1)\\).\nDo we have \\(p \\approx f(X^{(1)}, X^{(2)}, X^{(3)})\\) for a function f with values in \\([0, 1]\\)?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#predictivedescriptive-model",
    "href": "teaching/linear_model/slides/introduction.html#predictivedescriptive-model",
    "title": "Introduction",
    "section": "Predictive/Descriptive Model",
    "text": "Predictive/Descriptive Model\n\nThe “approximate” relationship we’re trying to establish between \\(Y\\) and \\(X^{(1)}\\), …, \\(X^{(p)}\\) is a model.\n\n\nWhy seek to establish such a model? Two main reasons:\n\n\nDescriptive objective: quantify the marginal effect of each variable. For example, if \\(X^{(1)}\\) increases by 10%, how does \\(Y\\) change?\n\n\nPredictive objective: given new values for \\(X^{(1)}\\), …, \\(X^{(p)}\\), we can deduce the (approximate) associated \\(Y\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#course-outline",
    "href": "teaching/linear_model/slides/introduction.html#course-outline",
    "title": "Introduction",
    "section": "Course Outline",
    "text": "Course Outline\n\nIntroduction → Bivariate analysis (review): relationship between 2 variables → General aspects of modeling\nLinear Regression → Quantitative \\(Y\\) as a function of quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)\nAnalysis of Variance and Covariance → Quantitative \\(Y\\) as a function of qualitative and/or quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)\nGeneralized Linear Regression → Qualitative or quantitative \\(Y\\) as a function of qualitative and/or quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#two-types-of-variables",
    "href": "teaching/linear_model/slides/introduction.html#two-types-of-variables",
    "title": "Introduction",
    "section": "Two Types of Variables",
    "text": "Two Types of Variables\nWe are interested in the relationship between \\(2\\) variables \\(X\\) and \\(Y\\). We distinguish two main categories, each divided into two types."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#quantitative-variables",
    "href": "teaching/linear_model/slides/introduction.html#quantitative-variables",
    "title": "Introduction",
    "section": "Quantitative Variables",
    "text": "Quantitative Variables\n\nA variable whose observation is a measured quantity. Examples: age, salary, number of infractions, etc.\nWe distinguish between:\n\nDiscrete quantitative variables whose possible values are finite or countable (Examples: number of children, number of infractions, etc.)\nContinuous quantitative variables which can take any value within an interval (Examples: height, salary, etc.)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#qualitative-variables-or-factors",
    "href": "teaching/linear_model/slides/introduction.html#qualitative-variables-or-factors",
    "title": "Introduction",
    "section": "Qualitative Variables (or Factors)",
    "text": "Qualitative Variables (or Factors)\n\nA variable whose observation results in a category or code. The possible observations are called the modalities of the qualitative variable. Examples: gender, socio-professional category, nationality, high school honors, etc.\nWe distinguish between:\n\nordinal qualitative variable: a natural order appears in the modalities (Examples: high school honors, etc.).\nnominal qualitative variable otherwise (Examples: gender, socio-professional category, etc.)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-of-the-pottery-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-of-the-pottery-dataset",
    "title": "Introduction",
    "section": "Example of the “Pottery” Dataset",
    "text": "Example of the “Pottery” Dataset\n\nData: chemical composition of pottery found at different archaeological sites in the United Kingdom\n\n\n\n\n\nSite\nAl\nFe\nMg\nCa\nNa\n\n\n\n\n1\nLlanedyrn\n14.4\n7.00\n4.30\n0.15\n0.51\n\n\n2\nLlanedyrn\n13.8\n7.08\n3.43\n0.12\n0.17\n\n\n3\nLlanedyrn\n14.6\n7.09\n3.88\n0.13\n0.20\n\n\n4\nLlanedyrn\n10.9\n6.26\n3.47\n0.17\n0.22\n\n\n5\nCaldicot\n11.8\n5.44\n3.94\n0.30\n0.04\n\n\n6\nCaldicot\n11.6\n5.39\n3.77\n0.29\n0.06\n\n\n7\nIsleThorns\n18.3\n1.28\n0.67\n0.03\n0.03\n\n\n8\nIsleThorns\n15.8\n2.39\n0.63\n0.01\n0.04\n\n\n9\nIsleThorns\n18.0\n1.88\n0.68\n0.01\n0.04\n\n\n10\nIsleThorns\n20.8\n1.51\n0.72\n0.07\n0.10\n\n\n11\nAshleyRails\n17.7\n1.12\n0.56\n0.06\n0.06\n\n\n12\nAshleyRails\n18.3\n1.14\n0.67\n0.06\n0.05\n\n\n13\nAshleyRails\n16.7\n0.92\n0.53\n0.01\n0.05\n\n\n\n\n\nIndividuals: pottery numbered from 1 to 13\n\nVariables: the archaeological site (factor with 4 modalities) and different chemical compounds (quantitative)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-of-the-no2traffic",
    "href": "teaching/linear_model/slides/introduction.html#example-of-the-no2traffic",
    "title": "Introduction",
    "section": "Example of the “NO2traffic”",
    "text": "Example of the “NO2traffic”\n\nData: NO2 concentration inside cars in Paris, type of road, (P, T, A, V or U) and traffic fluidity (A to D).\n\n\n\n\n\nNO2\nType\nFluidity\n\n\n\n\n1\n378.94\nP\nA\n\n\n2\n806.67\nT\nD\n\n\n3\n634.58\nA\nD\n\n\n4\n673.35\nT\nC\n\n\n5\n589.75\nP\nA\n\n\n…\n…\n…\n…\n\n\n283\n184.16\nP\nB\n\n\n284\n121.88\nV\nD\n\n\n285\n152.39\nU\nA\n\n\n286\n129.12\nU\nC\n\n\n\n\n\nIndividuals: vehicles numbered from 1 to 286\n\nVariables: NO2 (quantitative), type (factor with 5 modalities) and fluidity (ordinal factor with 4 modalities)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#pairwise-scatter-plots",
    "href": "teaching/linear_model/slides/introduction.html#pairwise-scatter-plots",
    "title": "Introduction",
    "section": "Pairwise Scatter Plots",
    "text": "Pairwise Scatter Plots\n\nWe observe\n\\(X=(X_1, \\ldots, X_n) \\in \\mathbb R^n\\) and \\(Y=(Y_1, \\ldots, Y_n) \\in \\mathbb R^n\\), (quantitative variables)\n\n\nRelationship between \\(X\\) and \\(Y\\): scatter plot of points \\((X_i, Y_i)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-pottery-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-pottery-dataset",
    "title": "Introduction",
    "section": "Example: Pottery Dataset",
    "text": "Example: Pottery Dataset"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-empirical-correlation",
    "href": "teaching/linear_model/slides/introduction.html#linear-empirical-correlation",
    "title": "Introduction",
    "section": "Linear Empirical Correlation",
    "text": "Linear Empirical Correlation\n\nThe linear relationship is quantified by Pearson’s linear correlation: \\(\\DeclareMathOperator{\\cov}{cov}\\) \\(\\DeclareMathOperator{\\var}{var}\\)\n\n\\[\\hat \\rho = \\frac{\\hat\\cov(X,Y)}{\\sqrt{\\hat \\var(X)\\hat \\var(Y)}}\\]\n\n\n\nwhere \\(\\hat \\var\\) and \\(\\hat \\cov\\) denote the empirical variance and covariance:\n\n\\(\\hat \\cov(X,Y)= \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\overline X)(Y_i - \\overline Y)\\)\n\\(\\hat \\var(X) = \\hat \\cov(X,X)\\), \\(\\hat \\var(Y)=\\hat\\cov(Y,Y)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#properties-of-empirical-correlation",
    "href": "teaching/linear_model/slides/introduction.html#properties-of-empirical-correlation",
    "title": "Introduction",
    "section": "Properties of Empirical Correlation",
    "text": "Properties of Empirical Correlation\n\nFrom the Cauchy-Schwarz inequality, we deduce that:\n\n\nThe correlation \\(\\hat \\rho\\) is always between \\(-1\\) and \\(1\\):\n\nIf \\(\\hat \\rho = 1\\): for all \\(i\\), \\(Y_i = aX_i + b\\) for some \\(a &gt; 0\\)\nIf \\(\\hat \\rho = -1\\): for all \\(i\\), \\(Y_i = aX_i + b\\) for some \\(a &lt; 0\\)\nIf \\(\\hat \\rho = 0\\): no linear relationship. notebook"
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X = (X_1, \\dots, X_n), iid with distribution \\mathcal N(\\mu, \\sigma). We assume that \\mu is unknown but that \\sigma is known.\n\n\n\n\n\n\nTest problems\n\n\n\n\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\nTest Statistic:  \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\n\\psi(X) \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\nTests\n\n\n\n\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFisher’s Quote\n\n\n\nThe value for which p=0.05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.\n\n\n\n\n\n\nMultiple VS Multiple Test Problem: \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} no longer test statistic.\nIdea: replace \\sigma by its estimator  \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\nThis gives \n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\nIs \\psi(X) pivotal under H_0 ? What is its distribution ?\n\n\n\n\n\n\n\n\n\n\nChi-Squared Distribution \\chi^2(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\sum_{i=1}^k Z_i^2\nwhere the Z_i’s are iid \\mathcal N(0,1).\n\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\n\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1) when k \\to +\\infty\n\n\n\n\n\n\n\n\n\nStudent distribution \\mathcal T(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\tfrac{Z}{\\sqrt{U/k}}\nZ, U are independent and follow resp. \\mathcal N(0,1) and a \\chi^2(k)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume X_i are iid \\mathcal N(\\mu_0, \\sigma).\n\nThe test statistic \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} pivotal (indep. of \\sigma).\nIt follows a Student distribution \\mathcal T(n-1).\n\n\n\nSketch of Proof.\nRemark that, the orthogonal projection of (X_1, \\dots, X_n) on (1, \\dots, 1) is equal to \\overline X \\cdot (1, \\dots, 1). This is precisely because the empirical mean minimizes the quantity \\tfrac{1}{n}\\sum (X_i - \\overline X)^2. Hence, the two vectors \\overline X \\cdot (1, \\dots, 1) and (X_1 - \\overline X, \\dots, X_n - \\overline X) are orthogonal. From the Cochran’s theorem, Orthogonality is equivalent to independence for Gaussian random variables, and we deduce that \\overline X is independent of \\hat \\sigma^2. \n\\tag*{$\\blacksquare$}\n\nThe student tests are the same as Gaussian tests with known variance, except that we replace the quantiles of the Gaussian the quantiles of the Student distribution.\n\nThe quantiles of the Student distribution are close to the quantile of the Standard Gaussian when the degree of freedom k is large:\nThe quantiles are slightly larger when k is small. The Student Distribution has a slightly heavier tail.\n\n\n\n\n\nMultiple VS multiple test problem X=(X_1, \\dots, X_n): \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n(Student) T-test statistic: \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe X=(X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu, \\sigma). \\mu, \\sigma are unknown. \\sigma_0 is fixed.\n\n\n\n\n\n\n\n\n\n\nRight-tailed test\n\n\n\n\nH_0: \\sigma \\leq \\sigma_0, H_1: \\sigma &gt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\nq_{1-\\alpha}: quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\n\n\n\n\n\nLeft-tailed test\n\n\n\n\nH_0: \\sigma \\geq \\sigma_0, H_1: \\sigma &lt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\nq_{\\alpha}: quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X = (X_1, \\dots, X_n), iid with distribution \\mathcal N(\\mu, \\sigma). We assume that \\mu is unknown but that \\sigma is known.\n\n\n\n\n\n\nTest problems\n\n\n\n\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\nTest Statistic:  \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\n\\psi(X) \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\nTests\n\n\n\n\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Fisher’s Quote\n\n\n\nThe value for which p=0.05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Multiple VS Multiple Test Problem: \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} no longer test statistic.\nIdea: replace \\sigma by its estimator  \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\nThis gives \n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\nIs \\psi(X) pivotal under H_0 ? What is its distribution ?",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Chi-Squared Distribution \\chi^2(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\sum_{i=1}^k Z_i^2\nwhere the Z_i’s are iid \\mathcal N(0,1).\n\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\n\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1) when k \\to +\\infty\n\n\n\n\n\n\n\n\n\nStudent distribution \\mathcal T(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\tfrac{Z}{\\sqrt{U/k}}\nZ, U are independent and follow resp. \\mathcal N(0,1) and a \\chi^2(k)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume X_i are iid \\mathcal N(\\mu_0, \\sigma).\n\nThe test statistic \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} pivotal (indep. of \\sigma).\nIt follows a Student distribution \\mathcal T(n-1).\n\n\n\nSketch of Proof.\nRemark that, the orthogonal projection of (X_1, \\dots, X_n) on (1, \\dots, 1) is equal to \\overline X \\cdot (1, \\dots, 1). This is precisely because the empirical mean minimizes the quantity \\tfrac{1}{n}\\sum (X_i - \\overline X)^2. Hence, the two vectors \\overline X \\cdot (1, \\dots, 1) and (X_1 - \\overline X, \\dots, X_n - \\overline X) are orthogonal. From the Cochran’s theorem, Orthogonality is equivalent to independence for Gaussian random variables, and we deduce that \\overline X is independent of \\hat \\sigma^2. \n\\tag*{$\\blacksquare$}\n\nThe student tests are the same as Gaussian tests with known variance, except that we replace the quantiles of the Gaussian the quantiles of the Student distribution.\n\nThe quantiles of the Student distribution are close to the quantile of the Standard Gaussian when the degree of freedom k is large:\nThe quantiles are slightly larger when k is small. The Student Distribution has a slightly heavier tail.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Multiple VS multiple test problem X=(X_1, \\dots, X_n): \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n(Student) T-test statistic: \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variance-unknown-mean",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variance-unknown-mean",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X=(X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu, \\sigma). \\mu, \\sigma are unknown. \\sigma_0 is fixed.\n\n\n\n\n\n\n\n\n\n\nRight-tailed test\n\n\n\n\nH_0: \\sigma \\leq \\sigma_0, H_1: \\sigma &gt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\nq_{1-\\alpha}: quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\n\n\n\n\n\nLeft-tailed test\n\n\n\n\nH_0: \\sigma \\geq \\sigma_0, H_1: \\sigma &lt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\nq_{\\alpha}: quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1^2) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_1, \\sigma_1^2).\n\\sigma_1, \\sigma_2 are known, \\mu_1, \\mu_2 are unknown\nTest Problem: H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\nIdea: Normalize \\overline X - \\overline Y: \n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\nTwo-Tailed Test for Testing Variance: \nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\nt_{1-\\alpha/2} is the (1-\\alpha/2)-quantile of a Gaussian distribution",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown\nVariance Testing Problem: \nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\nF-Test Statistic of the Variances (ANOVA) \n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher Distribution \\mathcal F(k_1,k_2)\n\n\n\n\n(k_1, k_2): degrees of freedom\nDistribution of \\frac{U_1/k_1}{U_2/k_2}\nWhere U_1, U_2 are indep. and follow \\chi^2(k_1), \\chi^2(k_2). wiki\n\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right) when k_1,k_2 \\to +\\infty\nExample: \\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1) if Z_i \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\n\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} is independent of \\mu_1, \\mu_2, \\sigma_1, \\sigma_2. It is pivotal\nIt follow distribution \\mathcal F(n_1-1, n_2-1)\n\n\n\n\n\nTwo-tailed test:  \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown, but we know that \\sigma_1=\\sigma_2\nEquality of Mean Testing Problem: \nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\nFormally, H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}.\n\n\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\n\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\nNormalize \\overline X - \\overline Y:\n\n\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\n\n\\psi(X,Y) is pivotal because \\sigma_1 = \\sigma_2.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown\nEquality of Mean Testing Problem: \nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\nFormally, H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}.\n\n\n\n\n\n\n\nStudent Welch Test Statistic\n\n\n\n\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\n\n\\psi(X,Y) is not pivotal\nGaussian approximation: \\psi(X,Y) \\approx \\mathcal N(0,1) when n_1, n_2 \\to \\infty\nBetter approximation: Student Welch",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\n\nCLT\n\n\n\n\nLet S_n = \\sum_{i=1}^n X_i with (X_1, \\dots, X_n) iid (L^2) then  \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \nEquality when X_i’s are \\mathcal N(\\mu, \\sigma)\nRule of thumb: n \\geq 30\n\n\n\n\n\n\n\n\n\nExample: Binomials\n\n\n\n\nIf p \\in (0,1)\n\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1) when n \\to \\infty\nn should be \\gg \\frac{1}{p}\n\n\n\nGood Approx for (n=100, p=0.2)\n\nBad Approx for (n=100, p=0.01)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe X \\sim Bin(n_1, p_1) and Y \\sim Bin(n_2, p_2).\nn_1, n_2 are known but p_1, p_2 are unknown in (0,1)\nH_0: p_1 = p_2 or H_1: p_1 \\neq p_2\n\n\n\n\n\n\n\nTest Statistic\n\n\n\n \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\n\n\\hat p_1 = X/n_1, \\hat p_2 = X/n_2\n\\hat p = \\frac{X+Y}{n_1+n_2}\nIf np_1, np_2 \\gg 1: \\psi(X) \\sim \\mathcal N(0,1)\nWe reject if |\\psi(X,Y)| \\geq t_{1-\\alpha/2} (gaussian quantile)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion: “should we raise taxes on cigarettes to pay for a healthcare reform ?”\np_1, p_2: proportion of non-smokers or smokers willing to raise taxes\nH_0: p_1=p_2 or H_1: p_1 &gt; p_2\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\hat p_1 \\approx 0.58, \\hat p_2 \\approx 0.21.\n\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\n\\mathbb P(\\psi(X,Y) &gt; 8.99) = 1-cdf(Normal(0,1), 8.99)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html",
    "title": "Goodness of Fit Tests",
    "section": "",
    "text": "Binomial distribution\n\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting.",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#multinomials",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#multinomials",
    "title": "Goodness of Fit Tests",
    "section": "",
    "text": "Binomial distribution\n\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting.",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit Test",
    "text": "\\chi^2 Goodness of Fit Test\n\nWe observe (X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q). This corresponds to n counts: X_1 + \\dots + X_m = n\nq = (q_1, \\dots, q_m) corresponds to probabilities of getting color 1, \\dots, m\nLet p = (p_1, \\dots, p_m) be a known vector s.t. p_1 + \\dots + p_m = 1.\nH_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\n\n\n\n\n\n\n\n\\chi^2 Goodness of Fit Test (Adéquation)\n\n\n\n\nChi-squared test statistic: \\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; , where n_i = np_i = \\mathbb E[X_i] is the expected number of counts for color i.\nWhen np_i = n_i are large, under H_0, we approximate the distribution of \\psi(X) as \\psi(X) \\sim \\chi^2(m-1)\nReject if \\psi(X) &gt; t_{1- \\alpha} (right-tail of \\chi^2(m-1))",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "title": "Goodness of Fit Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\n\n\n\n\n\n\nWe observe a bag of sweets containing n=100 sweets of m=3 different colors: red, green, and yellow.\nManufacturer: p_1= 40\\% red, p_2=35\\% green, and p_3=25\\% yellow.\nH_0: q=p (manufacturer’s claim is correct)\nH_1: q\\neq p (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\nColor\nObserved Counts\nExpected Counts\n\n\n\n\nRed\nX_1=50\nn_1=40\n\n\nGreen\nX_2=30\nn_2=35\n\n\nYellow\nX_3=20\nn_3=25\n\n\n\n\n\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\n\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878 (p_{value} \\approx 0.222)\nConclusion: do not reject H_0",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#histograms",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#histograms",
    "title": "Goodness of Fit Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nHistogram\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n\n\n\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\n\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\n\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\nwhere I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\n\n\n\n\n\n\n\n\n\nNormalization\n\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\nLaw of large numbers, Monte Carlo (informal)\n\n\n\n\nAssume that (X_1, \\dots, X_n) are iid of distrib P, and that a,b, k are fixed\nThe histogram \\mathrm{hist}(a,b,k) converges to the histogram of the density P",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit to a given distribution",
    "text": "\\chi^2 Goodness of Fit to a given distribution\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n, iid with unknown distrib P\nH_0: P = P_0, where P_0 is known\nH_1: P \\neq P_0\nIdea: under H_0, the counts in disjoint intervals I_1, I_2, \\dots, I_k follow a multinomial distribution \\mathrm{Mult}(n, (p_1, \\dots, p_{k})) where p_1 = P_0(I_1), \\dots, p_{k} = P_0(I_k)\n\n\n\n\n\n\n\n\n\n\nReduction to chi-squared test statistic\n\n\n\n\nCount the number of data (c_1, \\dots, c_k) in I_1, \\dots, I_k\nTheoretical counts nP_0(I_1) \\dots, nP_0(I_k)\nChi-squared statistic: \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_j)}\nDecide using an \\alpha-quantile of a \\chi^2(k-1) distribution\n\n\n\n\n\n\n\n\n\nCorrected \\chi^2 Test\n\n\n\n\nIf P_0 is unknown\nIn a class \\mathcal P parameterized by \\ell parameters\nThen estimate the \\ell parameters\nCompute theoretical counts with \\hat P_{\\hat \\theta}\nBut decide with \\chi^2(k - 1 - \\ell)",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Goodness of Fit Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\nH_0: X_i iid \\mathcal P(2)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n0\n1\n2 \n\\geq 3\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\n\n\n\n\n\n\nTo get 9.7, we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\gtrsim \\frac{(16-4)^2}{4} = 36\n(1-cdf(Chisq(3),36)) is very small: Reject\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nIf H_0 is X_i iid \\mathcal P(\\lambda) with unknown \\lambda\nNot \\chi^2(3) but \\chi^2(2)\n(1-cdf(Chisq(2),36)) is even smaller: (Still Reject)",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "title": "Goodness of Fit Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)}\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\n\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with y=x",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "title": "Goodness of Fit Tests",
    "section": "Kolmogorov-Smirnov Test",
    "text": "Kolmogorov-Smirnov Test\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)}\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\nEmpirical CDF: \\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf 1\\{X_i \\leq x\\}\nIdea: Max distance between empirical and true CDF\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\n\\psi(X) = \\sup_{x}|\\hat F(x) - F_0(x)|\nApprox: \\mathbb P_0(\\psi(X) &gt;c/\\sqrt{n}) \\to 2\\sum_{r=1}^{+\\infty}(-1)^{r-1}\\exp(-2c^2r^2) when n \\to +\\infty\nIn practice, use Julia, Python or R for KS Tests",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#organization",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#organization",
    "title": "Hypothesis Testing",
    "section": "Organization",
    "text": "Organization\n\n15h of lectures, 18h of TD\n\\(12^{th}\\) may: Exam (2h)\nLecture notes and slides on the website\nAbout english and programming languages\nWooclap sessions [Test]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#objective",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#objective",
    "title": "Hypothesis Testing",
    "section": "Objective",
    "text": "Objective\n\nGiven a general decision problem\n\nIntroduce precise notations to describe the pb\nformulate mathematically hypotheses \\(H_0\\) (a priori) and \\(H_1\\) (alternative)\n\nChoose a statistic adapted to the problem\nCompute this statistic and its pvalue (or an approx.)\nConclude and make a decision"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "General Principles",
    "text": "General Principles\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect Data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "Good and Bad Decisions",
    "text": "Good and Bad Decisions\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "title": "Hypothesis Testing",
    "section": "Dice Biased Toward \\(6\\)",
    "text": "Dice Biased Toward \\(6\\)\n\nObjective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-dice",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-dice",
    "title": "Hypothesis Testing",
    "section": "Fairness of Dice",
    "text": "Fairness of Dice\n\nWe observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\)\n\n\n\n\n\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_0\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_1\\) is “unlikely”)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "title": "Hypothesis Testing",
    "section": "Medical Test",
    "text": "Medical Test\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\n\n\n\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "title": "Hypothesis Testing",
    "section": "Recall of Proba",
    "text": "Recall of Proba\nConsider a probability measure \\(P\\) on \\(\\mathbb R\\).\n\nCDF (Cumulative Distribution Function): \\[x \\to P(~(-\\infty,x]~) = \\mathbb P(X \\leq x) ~~~~\\text{(if $X \\sim P$ under $\\mathbb P$)}\\]\n\n\n\nContinous Measures\n\ndensity wrp to Lebesgue: \\(\\mathbb P(X\\in[x,x+dx])=dP(x) = p(x)dx\\)\nPDF (Proba Density Function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha}) = \\alpha\\)\n\n\n\nDiscrete Measures\n\ndensity wrp to counting measure: \\(\\mathbb P(X=x) = P(\\{x\\})=p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nExponential \\(\\mathcal E(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-gammapoisson",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-gammapoisson",
    "title": "Hypothesis Testing",
    "section": "Examples Gamma/Poisson",
    "text": "Examples Gamma/Poisson\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\]\n\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\lambda\\)\n\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\n\nNumb. of tics before time \\(1\\) of clock \\(\\lambda\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_0\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]\nQuestion Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\nTest"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple\n\n\nTest Model\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_0\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\nMultiple VS Multiple"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nparametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nnon-parametric: Otherwise\n\n\nExample of Multiple VS Multiple Parametric Problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)\n\n\n\nSimple VS Multiple Non-Parametric Problems"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\n\nDecision Rule\n\n\n\nA Decision Rule or Test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\\(T(x) = 0\\) (or \\(1\\)) for all \\(x\\) is the trivial decision rule. Question: Decision Rule\n\n\n\n\n\n\n\n\n\n\n\n\nTest Statistic\n\n\n\na Test Statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter. Question: Test Statistic"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nExample of \\(\\mathcal R\\), for a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) \\not \\in [t_1, t_2]\\}:~~~~~~\\mathcal R = (-\\infty,t_1)\\cup (t_2, +\\infty)\\;\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "title": "Hypothesis Testing",
    "section": "Simple VS Simple Problem",
    "text": "Simple VS Simple Problem\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\nFor a given test \\(T\\) we define:\n\nlevel of \\(T\\): \\(\\alpha = P(T(X)=1)\\) (also: type-1 error)\npower of \\(T\\): \\(\\beta = Q(T(X)=1) = 1-Q(T(X)=0)\\) (1-\\(\\beta\\) is the type-2 error)\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta \\geq \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) ! But \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\n\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\) ?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\n\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta &gt; \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) !\nBut \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson’s Theorem",
    "text": "Neyman Pearson’s Theorem\n\n\n\n\n\n\n\n\n\n\nNeyman Pearson’s Theorem\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\n\n\n\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nWhere, if \\(X\\sim P\\), \\[ P(T^*(X)=1)=\\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\nEquivalent to Log-Likelihood Ratio Test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "title": "Hypothesis Testing",
    "section": "Example with Gaussians",
    "text": "Example with Gaussians\n\n\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\n\n\n\nLog-Likelihood Ratio Test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\nDistrib of \\(\\overline X\\) ?\n\\(t_{\\alpha}\\) ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "title": "Hypothesis Testing",
    "section": "Generalization: Exponential Families",
    "text": "Generalization: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood Ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\nLikelihood Ratio Test: (Q: Select Exp. Families) \\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "title": "Hypothesis Testing",
    "section": "Example: Radioactive Source",
    "text": "Example: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) &gt; 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) &gt; 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): Reject if \\(N \\geq 19\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#definitions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#definitions",
    "title": "Hypothesis Testing",
    "section": "Definitions",
    "text": "Definitions\n\n\n\n\\(H_0 = \\mathcal P_0=\\{P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\n\n\n\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(right-tailed: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(left-tailed: unilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(simple/multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( multiple/multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-tailed test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (right-tailed test)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\nHere, \\(\\Theta_0\\) is not necessarily a singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\n\nPivotal Test Statistic\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\n\n\n\n\n\n\nP-value: definition\n\n\nWe define \\(p_{value}(x_{\\mathrm{obs}}) =\\mathbb P(\\psi(X) \\geq x_{\\mathrm{obs}})\\) for a right-tailed test.\nFor a two-tailed test, \\(p_{value}(x_{\\mathrm{obs}}) =2\\min(\\mathbb P(\\psi(X) \\geq x_{\\mathrm{obs}}),\\mathbb P(\\psi(X) \\leq x_{\\mathrm{obs}}))\\)\n\n\n\n\n\n\n\n\n\n\n\nP-value under \\(H_0\\)\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(p_{value}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\n\n\n\n\n\nIn practice: reject if \\(p_{value}(x_{\\mathrm{obs}}) \\leq \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Correlation Test",
    "text": "Correlation Test\n\n\n\n\n\n\nPearson’s Correlation Test\n\n\n\n\\(r =  \\frac{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline X)^2\\sum_{i=1}^n (Y_i - \\overline Y)^2}}\\)\n\\(\\psi(X,Y) = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\)\nUnder \\(H_0\\), \\(\\psi(X,Y) \\approx \\mathcal T(n-2)\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Correlation Test",
    "text": "Correlation Test\nMonte Carlo Simulation with \\(n=4\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nFor each group \\(k\\) (eg a region), we observe \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) (eg salaries).\nWe assume that the \\(X_{ik}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\).\nEx: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\) [Wooclap]\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\n\nNote\n\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\) [Wooclap]\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\\)\n\\(SS_k \\sim \\sigma^2\\chi^2(N_k-1)\\) under \\(H_0\\)\nEmpirical var of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\n\n\n\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\)\n\\(SSB \\sim \\sigma^2\\chi^2(d-1)\\) under \\(H_0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#interpretation-of-variances-in-anova",
    "href": "teaching/hypothesis_testing/slides/dependency.html#interpretation-of-variances-in-anova",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Interpretation of variances in ANOVA",
    "text": "Interpretation of variances in ANOVA\n\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#chi2-homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#chi2-homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Homogeneity Test",
    "text": "\\(\\chi^2\\) Homogeneity Test\n\n\\(d\\) different bags (or groups), each containing balls of \\(m\\) potential colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix of counts:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Homogeneity Test",
    "text": "Homogeneity Test\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown [Wooclap]\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all color \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/slides/dependency.html#example-soft-drink-preferences",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\) (proportion of Coke lovers)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#computation-of-chi2-stat",
    "href": "teaching/hypothesis_testing/slides/dependency.html#computation-of-chi2-stat",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Computation of Chi2 stat",
    "text": "Computation of Chi2 stat\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#chi2-independence-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#chi2-independence-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Independence Test",
    "text": "\\(\\chi^2\\) Independence Test\n\n\n\n\nObservations: Paired Categorical Variables \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\ne.g. \\(X_i \\in \\{\\mathrm{male}, \\mathrm{female}\\}\\), \\(Y_i \\in \\{\\mathrm{coffee}, \\mathrm{tea}\\}\\) [Wooclap]\nIdea: regroup the data into bags, e.g. \\(\\{\\mathrm{male}, \\mathrm{female}\\}\\)\nBuild the contingency table\nPerform a chi-square homogeneity test\n\n\n\n\n\n\n\nExample of contingency table:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n30\n20\n50\n\n\nTea\n28\n22\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\nExpected counts:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n29\n21\n50\n\n\nTea\n29\n21\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\n\n\n\\(N_1 \\hat p_1 = 58 \\cdot 50/100  = 29\\), Degree of freedom \\(= 1\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\nSymetrization\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t) &=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy = \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]\n\\(X\\) indep of \\(Y\\) \\(\\implies\\) \\(X-Y\\) symmetric \\(\\implies\\) \\(\\mathrm{median}(X-Y) = 0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/slides/dependency.html#dependency-problem-for-paired-data",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The median of \\((X_i - Y_i)\\) is \\(0\\) for all \\(i\\)\n\\(H_1:\\) The median of \\((X_i - Y_i)\\) is not \\(0\\) for some \\(i\\) [Wooclap]\n\n\n\n\n\n\n\n\nGenerality of \\(H_0\\)\n\n\n\nIf \\(X_i-Y_i\\) is symmetric \\(i\\), then we are under \\(H_0\\).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#section",
    "href": "teaching/hypothesis_testing/slides/dependency.html#section",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "Warning\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\))."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where [Wooclap] \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#properties-on-the-signed-ranks",
    "href": "teaching/hypothesis_testing/slides/dependency.html#properties-on-the-signed-ranks",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Properties on the Signed Ranks",
    "text": "Properties on the Signed Ranks\nUnder \\(H_0\\),\n\nThe signs \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\nProperties on the Signed Ranks\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &gt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\nif \\(H_1\\): \\(\\mathrm{median}(D_i) &gt; 0\\): left-tailed test on \\(W_-\\).\n\n\n\n\n\n\nTo generate a \\(W_-\\) under \\(H_0\\):\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#section-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#section-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "This approximation fits well the exact distribution. Monte-Carlo simulation:"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#effect-of-drug-on-blood-pressure",
    "href": "teaching/hypothesis_testing/slides/dependency.html#effect-of-drug-on-blood-pressure",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Effect of Drug on Blood Pressure",
    "text": "Effect of Drug on Blood Pressure\n\n\\(H_0\\): the drug has no effect. \\(H_1\\): it lowers the blood pressure\n\n\n\n\n\n\nPatient\n\\(X_i\\) (Before)\n\\(Y_i\\)​ (After)\n\\(D_i = X_i-Y_i\\)​\n\\(R_i\\)\n\n\n\n\n1\n150\n140\n10\n6 (+)\n\n\n2\n135\n130\n5\n5 (+)\n\n\n3\n160\n162\n-2\n2 (-)\n\n\n4\n145\n146\n-1\n1 (-)\n\n\n5\n154\n150\n4\n4 (+)\n\n\n6\n171\n160\n11\n7 (+)\n\n\n7\n141\n138\n3\n3 (+)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#numerical-appli",
    "href": "teaching/hypothesis_testing/slides/dependency.html#numerical-appli",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Numerical Appli",
    "text": "Numerical Appli\n\n\\(W_- = 1+2 = 3\\).\nFrom a simulation, we approx \\(\\mathbb P(W_-=i)\\), for \\(i \\in \\{0, 1, 2, 3, 4, 5,6\\}\\) under \\(H_0\\) by\n[0.00784066, 0.00781442, 0.00781534, 0.01563892, 0.01562184, 0.02343478]\nFrom a simulation, \\(p_{value}=\\mathbb P(W_- \\leq 3) \\approx 0.039 &lt; 0.05\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html",
    "href": "teaching/hypothesis_testing/TDs/TP.html",
    "title": "TP: Hypothesis Testing",
    "section": "",
    "text": "We observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\). We assume that the vectors \\(X\\) and \\(Y\\) are independent. We want to test \\(H_0\\): \\(\\mu_1 = \\mu_2\\) VS \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\). We observe the data\nX=[-0.2657064426519085, -0.27538323622274347, 0.11419811877193782, 0.1158736466676504, 1.7071154417851981, 0.9306910454777643, 0.5834941669559498, -1.536447927372139, -1.4158768806157345, 1.0532694288697444, 1.2955133629200777, -0.4195557179577367]\nY=[-0.6452416530469819, 0.3662048411679129, -0.09943069837472361, 0.8738423322164134, 0.7163913715056272, -0.32450102319617485, 0.9159821874321818, -2.3583609849887224]\n\nCompute the student-Welch test statistic \\(\\tfrac{\\overline X - \\overline Y}{\\sqrt{\\hat\\sigma_1/n_1 + \\hat\\sigma_2/n_2}}\\)\nConclude using a Gaussian approximation (use the cdf of a N(0,1))\nConclude using an UnequalVarianceTTest (julia) or test.welch (R) or scipy.stats.ttest_ind(a, b, equal_var=False) (python)\nBonus. Conclude using a better chi-squared approximation. Compare these result to 3."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#student-test",
    "href": "teaching/hypothesis_testing/TDs/TP.html#student-test",
    "title": "TP: Hypothesis Testing",
    "section": "",
    "text": "We observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\). We assume that the vectors \\(X\\) and \\(Y\\) are independent. We want to test \\(H_0\\): \\(\\mu_1 = \\mu_2\\) VS \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\). We observe the data\nX=[-0.2657064426519085, -0.27538323622274347, 0.11419811877193782, 0.1158736466676504, 1.7071154417851981, 0.9306910454777643, 0.5834941669559498, -1.536447927372139, -1.4158768806157345, 1.0532694288697444, 1.2955133629200777, -0.4195557179577367]\nY=[-0.6452416530469819, 0.3662048411679129, -0.09943069837472361, 0.8738423322164134, 0.7163913715056272, -0.32450102319617485, 0.9159821874321818, -2.3583609849887224]\n\nCompute the student-Welch test statistic \\(\\tfrac{\\overline X - \\overline Y}{\\sqrt{\\hat\\sigma_1/n_1 + \\hat\\sigma_2/n_2}}\\)\nConclude using a Gaussian approximation (use the cdf of a N(0,1))\nConclude using an UnequalVarianceTTest (julia) or test.welch (R) or scipy.stats.ttest_ind(a, b, equal_var=False) (python)\nBonus. Conclude using a better chi-squared approximation. Compare these result to 3."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#monte-carlo-and-chi-squared-tests",
    "href": "teaching/hypothesis_testing/TDs/TP.html#monte-carlo-and-chi-squared-tests",
    "title": "TP: Hypothesis Testing",
    "section": "1. Monte Carlo and Chi-squared Tests",
    "text": "1. Monte Carlo and Chi-squared Tests\nA statistician observes \\(X = (X_1, \\dots, X_n)\\) where the \\(X_i's\\) are iid of distribution \\(P\\). If the problem is to test whether \\(P\\) is Gaussian with known \\(\\mu\\) and \\(\\sigma\\), the problem is:\n\\[H_0: P=\\mathcal N(\\mu, \\sigma) \\quad \\text{VS} \\quad H_1: P\\neq \\mathcal N(\\mu, \\sigma)\\]\nIf \\(\\mu\\) and \\(\\sigma\\) are unknown, the problem is \\[H_0: P\\in \\{\\mathcal N(\\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt;0\\}\\quad \\text{VS} \\quad H_1: P\\not \\in \\{\\mathcal N(\\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt;0\\}\\]\nWe first assume that \\(\\mu\\) and \\(\\sigma\\) are known, and that:\nmu = 0\nsigma = 1\nn = 100\nm = 5\nThis practical exercise aims to empirically demonstrate how a chi-squared test statistic converges to a chi-squared distribution in both known and unknown parameter scenarios. We will:\n\nDivide the observation space into 5 disjoint intervals\nCount how many observations fall into each interval for randomly generated data\nCalculate the chi-squared test statistic for randomly generated data\nRepeat this process 1,000 times to build an empirical distribution (an histogram)\nThe resulting empirical histogram should approach a theoretical chi-squared distribution as both the sample size \\(n\\) and the number of repetitions \\(N\\) approach infinity.\n\n\nQuestions\n\nGenerate a vector \\(X\\) made of \\(n\\) iid \\(\\mathcal N(\\mu, \\sigma)\\)\nCompute the vector \\(Y = \\frac{X-\\mu}{\\sigma}\\)\nCompute the list of counts \\(C\\) of \\(Y\\) in \\((-\\infty, -3)\\), \\([\\tfrac{3i}{m}, \\frac{3(i+1)}{m})\\) for \\(i\\) in \\(\\{-m, \\dots, m-1\\}\\) and \\([3,+\\infty)\\).\n\nHow many intervals do we have here?\nWhat is the expected number of entries of \\(Y\\) falling in \\([3, +\\infty)\\)? (compute this using the cdf function). Change the value of \\(n\\) so that we have at least \\(5\\) expected counts in \\([3, +\\infty)\\).\n\n\n\n\n#Julia: use the broadcasting .&lt;\nsum(x .&lt;= Y .&lt; y) # counts in [x, y)\n\n#R: use bitwise operator &\nsum(Y &gt;= x & Y &lt; y) # counts in [x, y)\n\n\n\nUsing the cdf of \\(\\mathcal N(0,1)\\), compute the list of expected counts in the same intervals\nCompute the Chi-squared test statistic using the two preceeding questions. We recall that \\(\\psi(Y) = \\sum_{i=1}^n \\tfrac{(c_i - e_i)^2}{e_i}\\) where \\(c_i\\) and \\(e_i\\) are the counts and expected counts.\nSummarize the preceeding questions into a function trial_chisq(X, mu, sigma, m) that normalizes \\(X\\), computes counts, expected counts and the chisq test statistic:\n\n# function trial_chisq(X, mu, sigma, m)\n# n = length(X)\n# Y = (X-mu)/sigma\n# Compute counts \n# Compute expcounts\n# Compute and Return chisq\n\nUsing the previous question, write a function monte_carlo_known that computes \\(N\\) chi-squared test statistics on iid random samples \\(X\\sim \\mathcal N(\\mu, \\sigma)^{\\otimes n}\\). It returns a list trials of length \\(N\\).\n\nN = 1000\n# function monte_carlo_known(N, mu, sigma, n, m)\n# empty list trials\n\n# for i = 1 ... N\n\n# Generate X made of n iid gaussian (mu, sigma)\n# append trial_chisq(X, mu, sigma, m) to trials\n\n# endfor\n# return trials\n\nPlot a histogram of a list of trials using a builtin function. Normalize it in density (area=1), and precise the bins (0:0.5:30).\nWhat is a good distribution to approximate the histogram? Plot the distribution’s density and check that it fits the histogram. Vary the parameters \\(m\\), \\(n\\), and \\(N\\).\n\nNow, we assume that \\(\\mu\\) and \\(\\sigma\\) are unknown.\n\nGiven \\(X\\), compute to estimators hatmu and hatsigma of mu and sigma\nSimilarly to Q.7, write a function monte_carlo_unknown(N, n, m) that computes a Monte-Carlo simulation. \\(\\hat \\mu\\) and \\(\\hat \\sigma\\) must be computed for all trial \\(i=1,\\dots,N\\).\nRevisit questions 8 and 9, considering the case where \\(\\mu\\) and \\(\\sigma\\) are unknown. How does this affect the distribution of the histogram?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#application-with-bitcoin",
    "href": "teaching/hypothesis_testing/TDs/TP.html#application-with-bitcoin",
    "title": "TP: Hypothesis Testing",
    "section": "2. Application with Bitcoin",
    "text": "2. Application with Bitcoin\n\nUse your favorite AI to write the code to import the last \\(500\\) hourly close prices of bitcoin in usdt from binance. Plot the prices and compute the returns defined as \\(R_t = \\tfrac{P_t}{P_{t-1}}-1\\), where \\(P_t\\) is the price at time \\(t\\) (in hours).\n\n\n\n\n\n\n\nR (Click to See a Solution)\n\n\n\n\n\nlibrary(httr)\nlibrary(jsonlite)\n# Define the API endpoint and parameters\napi_url &lt;- \"https://api.binance.com/api/v3/klines\"\nsymbol &lt;- \"BTCUSDT\" # Bitcoin to USDT trading pair\ninterval &lt;- \"1h\" # 1-hour interval\nlimit &lt;- 500 # Limit to 500 data points\n\n# Create the query URL with parameters\nquery_params &lt;- list(\n    symbol = symbol,\n    interval = interval,\n    limit = limit\n)\n\n# Fetch the data from Binance API\nresponse &lt;- GET(api_url, query = query_params)\nresponse.body\ndata &lt;- content(response, as = \"text\", encoding = \"UTF-8\")\ndata &lt;- fromJSON(data)\ndata &lt;- data.frame(data)[2]\ndata &lt;- data.frame(lapply(data, as.numeric))\nn &lt;- length(data$X2)\nR &lt;- (data[2:n,1] / data[1:(n - 1),1]) - 1\n\n\n\n\n\n\n\n\n\nJulia (Click to See a Solution)\n\n\n\n\n\nusing HTTP\nusing JSON\nusing DataFrames\n\nfunction BTC_returns()\n    # Define the API endpoint and parameters\n    api_url = \"https://api.binance.com/api/v3/klines\"\n    symbol = \"BTCUSDT\"  # Bitcoin to USDT trading pair\n    interval = \"1h\"     # 1-hour interval\n    limit = 1000         # Limit to 500 data points\n    \n    # Construct the full query URL\n    query_url = \"$api_url?symbol=$symbol&interval=$interval&limit=$limit\"\n    \n    # Fetch the data from Binance API\n    response = HTTP.get(query_url)\n    data = JSON.parse(String(response.body))\n    P = [parse(Float64, data[i][2]) for i in 1:length(data)]\n    R = [P[t] / P[t-1] - 1 for t in 2:length(P)]\n    return R\nend\n    R=BTC_returns()\n\n\n\n\nWe first test\n\\(H_0\\): the mean of the returns is zero VS \\(H_1\\): it is nonzero.\nCompute \\(\\hat \\sigma\\) as std(R) and the Student statistic \\(\\psi(R) = \\sqrt{n}\\tfrac{\\overline R}{\\hat \\sigma}\\). Compute the p-value using the cdf function of a Student(499) (or Gaussian). Obtain the same result with a library function like OneSampleTTest in Julia, t.test in R or ttest_1samp in Python\nPlot a histogram of the returns, normalized in density. Plot on the same graph the density of a Gaussian of mean mean(R) and of std std(R).\nUsing the previous exercise with \\(m=5\\), compute a chi-squared statistic and an approximated p-value.\nDo a scatter plot of \\((R_{t-1}, R_t)\\). Do you see any correlation between \\(R_{t-1}\\) and \\(R_t\\)?\nCompute the correlation \\(r\\) between \\((R_t)\\) and \\((R_{t-1})\\).\nCompute the p-value of a two-sided Pearson’s correlation test, using the test statistic \\(\\tfrac{r}{\\sqrt{1-r^2}} \\sqrt{n-2}\\) and the cdf of a Student distribution. Compare with the function CorrelationTest in Julia or cor.test in R or pearsonr in Python."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/MoreExercises.html",
    "href": "teaching/hypothesis_testing/TDs/MoreExercises.html",
    "title": "More Exercises",
    "section": "",
    "text": "A quality control analyst at a semiconductor manufacturing plant is tracking chip failures during production. For each production batch, the analyst records the number of chips that pass inspection before the batch is terminated.\nDataset: Number of working chips produced in \\(150\\) different production batches.\n\n\n\nWorking Chips\nNumber of Batches\nPercentage\n\n\n\n\n0\n8\n5.3%\n\n\n1\n10\n6.7%\n\n\n2\n16\n10.7%\n\n\n3\n17\n11.3%\n\n\n4\n21\n14.0%\n\n\n5\n20\n13.3%\n\n\n6\n17\n11.3%\n\n\n7\n14\n9.3%\n\n\n8\n10\n6.7%\n\n\n9\n7\n4.7%\n\n\n10\n3\n2.0%\n\n\n11\n5\n3.3%\n\n\n12\n2\n1.3%\n\n\nTotal\n150\n100%\n\n\n\nWe assume that each batch terminates after \\(r= 5\\) failures.\n\nWhat distribution the number of working chips in a batch should follow?\nTest the goodness of fit to this distribution.\nWhat does it change if the number of failures after which each batch ends is unknown?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/MoreExercises.html#exercise",
    "href": "teaching/hypothesis_testing/TDs/MoreExercises.html#exercise",
    "title": "More Exercises",
    "section": "",
    "text": "A quality control analyst at a semiconductor manufacturing plant is tracking chip failures during production. For each production batch, the analyst records the number of chips that pass inspection before the batch is terminated.\nDataset: Number of working chips produced in \\(150\\) different production batches.\n\n\n\nWorking Chips\nNumber of Batches\nPercentage\n\n\n\n\n0\n8\n5.3%\n\n\n1\n10\n6.7%\n\n\n2\n16\n10.7%\n\n\n3\n17\n11.3%\n\n\n4\n21\n14.0%\n\n\n5\n20\n13.3%\n\n\n6\n17\n11.3%\n\n\n7\n14\n9.3%\n\n\n8\n10\n6.7%\n\n\n9\n7\n4.7%\n\n\n10\n3\n2.0%\n\n\n11\n5\n3.3%\n\n\n12\n2\n1.3%\n\n\nTotal\n150\n100%\n\n\n\nWe assume that each batch terminates after \\(r= 5\\) failures.\n\nWhat distribution the number of working chips in a batch should follow?\nTest the goodness of fit to this distribution.\nWhat does it change if the number of failures after which each batch ends is unknown?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html",
    "href": "teaching/hypothesis_testing/TDs/TD4.html",
    "title": "TD4: Homogeneity/Dependency",
    "section": "",
    "text": "A sociologist wants to investigate whether the choice of transportation mode (Car, Bicycle, or Public Transit) varies among residents of three different cities: City A, City B, and City C.\nThe sociologist conducted a survey, and the responses are summarized in the contingency table below:\n\n\n\nTransportation Mode\nCity A\nCity B\nCity C\nTotal\n\n\n\n\nCar\n120\n150\n100\n370\n\n\nBicycle\n80\n60\n90\n230\n\n\nPublic Transit\n100\n90\n110\n300\n\n\nTotal\n300\n300\n300\n900\n\n\n\n\nFormulate the Hypothesis Testing Problem corresponding to the initial objective of the sociologist. Introduce the notation\nAnswer to the initial question using a chi-squared test at a \\(0.05\\) significance level"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-1",
    "title": "TD4: Homogeneity/Dependency",
    "section": "",
    "text": "A sociologist wants to investigate whether the choice of transportation mode (Car, Bicycle, or Public Transit) varies among residents of three different cities: City A, City B, and City C.\nThe sociologist conducted a survey, and the responses are summarized in the contingency table below:\n\n\n\nTransportation Mode\nCity A\nCity B\nCity C\nTotal\n\n\n\n\nCar\n120\n150\n100\n370\n\n\nBicycle\n80\n60\n90\n230\n\n\nPublic Transit\n100\n90\n110\n300\n\n\nTotal\n300\n300\n300\n900\n\n\n\n\nFormulate the Hypothesis Testing Problem corresponding to the initial objective of the sociologist. Introduce the notation\nAnswer to the initial question using a chi-squared test at a \\(0.05\\) significance level"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-2",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe statistician of an insurance company is tasked with studying the impact of an advertising campaign conducted in 7 regions where the company operates. To do this, he has extracted from the database the number of new clients acquired by a certain number of agents in each region.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nNumber of agents\n9\n7\n7\n6\n7\n6\n6\n\n\nAverage number of new clients\n26.88\n22.34\n19.54\n18.95\n27.17\n25.87\n25.72\n\n\nVariance of new clients\n13.54\n12.59\n12.87\n13.42\n13.17\n12.56\n12.64\n\n\n\nThe statistician decides to perform an analysis of variance to test whether the regional factor influences the number of new clients. Let \\(X_{ik}\\) denote the number of new clients of agent \\(i\\) in region \\(k\\), \\(N_k\\) the number of agents in region \\(k\\), \\(d = 7\\) the number of regions and \\(N_{\\mathrm{tot}} = 48\\) the total number of agents. Assume that the random variables \\(X_{ik}\\) are normal with mean \\(\\mu_k\\) and variance \\(\\sigma^2\\). Define:\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\nFormulate the hypothesis testing problem to test whether the number of new clients is homogeneous accross the regions.\nWhat do \\(\\overline X_k\\), \\(\\overline X\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\) represent?\nProve the analysis of variance formula: \\[V_T = V_W + V_B \\; .\\] substract and add \\(\\overline X_k\\) in the definition of \\(V_T\\)\nCompute \\(\\overline X\\), \\(V_W\\), \\(V_B\\), and \\(V_T\\).\nWrite the definition of the ANOVA test statistic in terms of \\(V_W\\) and \\(V_B\\).\nDid the advertising campaign have the same impact in all regions?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-3",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 3",
    "text": "Exercise 3\nSome data are collected from 7 students and we want to analyze the correlation between the number of hours students spend studying before an exam and their test scores.\n\n\n\nStudent\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nStudy Hours\n2.5\n3.0\n1.5\n4.0\n3.5\n5.0\n3.0\n\n\nTest Score\n56\n64\n45\n72\n68\n80\n59\n\n\n\n\nFormulate the hypothesis testing problem for a linear correlation test\nPerform the linear correlation test at level \\(0.05\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-4",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 4",
    "text": "Exercise 4\nBelow are stress scores for \\(10\\) patients before and after a sport session:\n\n\n\n\n\n\n\n\n\n\nParticipant\nStress Score (Before)\nStress Score (After)\nDifference\nRank/Sign\n\n\n\n\n1\n40\n32\n\n\n\n\n2\n38\n35\n\n\n\n\n3\n45\n40\n\n\n\n\n4\n50\n42.5\n\n\n\n\n5\n44\n41.5\n\n\n\n\n6\n48\n48\n\n\n\n\n7\n39\n30\n\n\n\n\n8\n42\n38\n\n\n\n\n9\n47\n46\n\n\n\n\n10\n46.5\n40\n\n\n\n\n\nWe want to test if sport has an effect on the stress of the patients\n\nFormulate the hypothesis testing problem\nComplete the above table\nPerform a Wilcoxon signed rank test"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-5",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-5",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 5",
    "text": "Exercise 5\nLet \\(X=(X_1, \\dots, X_N)\\) be a Gaussian vectors \\(\\mathcal N(0, I_N)\\) in \\(\\mathbb R^N\\) (i.e. \\(X_i\\) are iid \\(\\mathcal N(0,1)\\)).\n\n\nWhat is the distribution of \\(QX\\), if \\(Q\\) is an orthogonal matrix ? (\\(QQ^T = I_n\\))\nWhat is the distribution of \\(\\|PX\\|^2\\) if \\(P\\) is an orthogonal projector ?\nUse the rank of \\(P\\) defined as \\(rk(P) = dim(Im(P))\\)\nDefinition of orthogonal projector: (\\(P^2=P\\) and \\(P = P^T\\))\nShow that if \\(P\\) is an orthogonal projector, then \\(PX\\) is independent of \\((I-P)X\\).\nUse the fact that two centered gaussian vectors \\(X\\),\\(Y\\) are independent iif \\(\\mathbb E[X_iY_j] = 0\\) for all \\(i,j\\). Translate this fact in a matrix form.\nWhat is the distribution of \\(\\frac{n-rk(P)}{rk(P)}\\frac{\\|PX\\|^2}{\\|(I-P)X\\|^2}\\) ?\nShow that if \\(P\\), \\(P_0\\) are two orthogonal projectors such that \\(Im(P_0) \\subset Im(P)\\), then \\(P(I-P_0)X\\) is independent of \\((I-P)(I-P_0)X\\). What is the distribution of \\(\\|P(I-P_0)X\\|^2\\) ?\nShow first that \\(PP_0=P_0P= P_0\\), and that \\(P-P_0\\) is an orthogonal projector\nWhat is the orthogonal projector \\(P_0\\) on \\(\\mathrm{Span}(1, \\dots, 1)\\) ? Deduce that \\((X_i - \\overline X)\\) is independent of \\(\\overline X\\) for all \\(i\\).\n\nWe divide \\(N\\) into \\(d\\) blocks: \\(N = N_1 + \\dots + N_d\\). We write \\((X_1, \\dots, X_n) = ((X_{11}, \\dots, X_{N_11}), (X_{12}, \\dots X_{N_2 2}), \\dots, (X_{N_d 1}, \\dots, X_{N_d d}))\\).\n\nWhat is the orthogonal projection on \\(E_k =((0, \\dots, 0), \\dots, (0, \\dots, 0),(1, \\dots, 1), (0,\\dots, 0) ,\\dots, (0, \\dots, 0))\\) ? (\\(0\\) everywhere except on block \\(k\\) where we put ones everywhere)\nGive the orthogonal projection \\(P\\) on \\(\\mathrm{Span}(E_1, \\dots, E_d)\\). Explicit \\((I-P)(I-P_0)X\\) and \\(P(I-P_0)X\\).\nDeduce the distribution of \\(\\frac{d-1}{n-d}\\frac{\\|(I-P)(I-P_0)X\\|^2}{\\|P(I-P_0)X\\|^2}\\) and of the ANOVA Test Statistic under \\(H_0\\)."
  },
  {
    "objectID": "teaching/glossary.html",
    "href": "teaching/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "General\n\n\n\nEnglish\n\nincreasing function\nnondecreasing function\ninterval \\([a,b)\\)\n\n\n\n\nFrench\n\nfonction strictement croissante\nfonction croissante\nintervalle \\([a,b[\\)\n\n\n\n\n\n\nProbability/Statistics\n\n\n\nEnglish\n\nprobability distribution\nexpectation\nlikelihood\ndistribution tail\nCDF (Cumul. Distrib. Func.)\nPDF (Proba. Density Func.)\nCLT (Central Limit Theorem)\nLLN (Law of Large Numbers)\n\n\n\n\nFrench\n\nloi de probabilité\nespérance\nvraissemblance\nqueue de distribution\nFonction de Répartition\nDensité d’une distribution\nTLC (Th. de la limite centrale)\nLoi des grands nombres\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nEnglish\n\nA sample\none-sided test\ntwo-sided test\nchi-squared goodness of fit test\nchi-squared homogeneity test\n\n\n\n\nFrench\n\nUn échantillon\ntest unilatéral\ntest bilatéral\ntest d’adéquation du Khi-deux\ntest d’homogénéité du Khi-deux\n\n\n\n\n\n\nProgramming Languages (JupytR)\n\n\n\n\nJulia\nusing Distributions\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\ncdf(Binomial(n, p), x)\nquantile(Binomial(n,p), q)\ncdf(Normal(0,1), x)\nquantile(Normal(0,1), q)\n\ncdf(Chisq(n), x) # Chi-squared\ncdf(TDist(n), x) # Student\n\nn1,n2 = 5,10\ncdf(FDist(n1, n2), x) # Fisher\n\nlmbda = 2\ncdf(Gamma(n, lmbda), x) # Gamma\ncdf(Poisson(lmbda), x) # Poisson\n\n\n\nPython\nfrom scipy.stats import *\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\nbinom.cdf(x, n, p)\nbinom.ppf(q, n, p) # Quantile\nnorm.cdf(x)\nnorm.ppf(q)\n\nchi2.cdf(x, n)\nt.cdf(x, n) # Student\n\nn1,n2 = 5,10\nf.cdf(x, n1, n2) # Fisher\n\nlmbda = 2\ngamma.cdf(x,n,lmbda) # Gamma\npoisson.cdf(x, lmbda) # Poisson\n\n\n\nR\n\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\npbinom(x, n, p)\nqbinom(q, n, p)\npnorm(x)\nqnorm(q)\n\npchisq(x, n)\npt(x, n)\n\nn1,n2 = 5,10\npf(x, n1,n2)\n\nlmbda = 2\npgamma(x,n,lmbda) \nppois(x, lmbda)"
  },
  {
    "objectID": "teaching/probability/density_likelihood.html",
    "href": "teaching/probability/density_likelihood.html",
    "title": "Density, Likelihood and Radon Nikodym",
    "section": "",
    "text": "Notation\nIn this note, \\((\\Omega,\\mathbb P)\\) denote a common probability measured space for all the random variables introduced in this note. We also write \\(\\mathbb E\\) for the expectation. Let \\(X\\) be a random variable with values in a measurable space \\((\\mathcal X, \\mathcal A)\\). Let say for simplicity that \\(\\mathcal X = \\mathbb R^n\\).\nWe say that \\(X\\) has distribution \\(P\\) if \\(\\mathbb P(X \\in A)=P(A)\\) for any measurable set \\(A\\). For clarity, we sometimes write \\(\\mathbb P_{X \\sim P}(X \\in A)\\), which means “the probability of \\(X\\) being in \\(A\\) if \\(X\\) follows distribution \\(P\\)”. Sometimes, we do the slight abuse of notation by writing that \\(P(A) = P(X \\in A)\\).\n\\(P\\) can be seen as the “pushforward” measure of the common probability measure \\(\\mathbb P\\) by random variable \\(X\\), since by definition, \\(\\mathbb P(X \\in A) = \\mathbb P(\\{\\omega \\in \\Omega, X(\\omega) \\in A\\})= \\mathbb P(X^{-1}(A))\\).\n\n\nContinuous Densities\nA measure \\(P\\) has density \\(p\\) with respect to the Lebesgue measure if for any event \\(A\\) (which is simply a measurable set of \\(\\mathbb R^n\\)), \\[P(A)= \\int_{x \\in \\mathbb R^n}\\mathbf 1\\{x \\in A\\}p(x)dx \\; .\\] \\(p(x)\\) is sometimes called the likelihood of a random variables that has distribution \\(P\\) at point \\(x\\). An equivalent condition is that for any “kind” real valued function \\(f\\) (e.g. continuous with bounded support), \\[\\mathbb E_{X \\sim P}[f(X)] \\stackrel{\\mathrm{def}}{=} \\int_{x \\in \\mathbb R^n}f(x)dP =\\int_{x \\in \\mathbb R^n}f(x)p(x)dx \\; .\\] We write that \\[dP(x) = p(x)dx ~~~ \\text{or}~~~~ \\tfrac{dP}{dx}(x) = p(x) \\; ,\\] and \\(\\tfrac{dP}{dx}\\) is called the Radon-Nikodym derivative of \\(P\\) with respect to the Lebesgue measure. The intuition of this abstract notation is the following. If \\(x \\in \\mathbb R^n\\) and \\(h\\) is a small quantity that goes to \\(0\\), \\(dP\\) represents the measure of the interval \\([x, x+h]\\), with respect to the measure \\(P\\). Then, \\(d P(x) = P([x, x+h]) = \\int_x^{x+h}p(x)dx \\sim p(x)h\\).\n\n\nThe Counting Measure for Discrete Random Variables\nRandom variables in \\(\\mathbb{R}\\) that take on a finite number of values are referred to as discrete random variables, and they do not have a density with respect to the Lebesgue measure. However, this case is much simpler and is handled within measure theory using the counting measure. As its name indicates, the counting measure \\(\\mu\\) on \\(\\mathcal X=\\mathbb R^n\\) counts the elements of a given set \\(A\\): \\[ \\mu(A) = |A| \\enspace .\\] In particular, \\(\\mu(A)\\) is infinite if \\(A\\) is an infinite set.\nLet \\(X\\) be a discrete random variable that takes values in \\(\\{x_1, \\dots, x_N\\}\\), e.g. a Bernoulli, Binomial or Poisson random variable, and let \\(P\\) be its probability distribution. Let \\(p(x_i)\\) be the probability that \\(X = x_i\\), that is \\(p(x_i) = P(\\{x_i\\}) = P(X = x_i) \\in [0,1]\\). In this discrete case, the probability \\(p(x_i)\\) represents the likelihood of the value \\(x_i\\) for the random variable \\(X\\). While \\(X\\) has not a density with respect to the Lebesgue measure, it has density \\(p\\) with respect to the counting measure \\(\\mu\\), that is \\[\\mathbb E_{X \\sim P}[f(X)] =\\int_{x \\in \\mathbb R^n}f(x)p(x)d\\mu, ~~~~~~~ \\frac{dP}{d\\mu}(x) = p(x) \\; .\\] \\[ \\] ### The General Radon Nikodym Theorem\nThe Radon Nikodym Theorem tells us that any probability \\(P\\) admits a density with respect to a given measure \\(\\nu\\) if it is absolutely continuous with respect to \\(\\nu\\), that is \\[ \\nu(A) = 0 \\implies P(A) = 0 \\; .\\] In this case, the density is the Radon Nikodym derivative \\(\\frac{dP}{d\\nu}\\) of \\(P\\) with respect to \\(\\nu\\) and satisfies\n\\[ \\mathbb E_{X \\sim P}[f(X)] =\\int_{x \\in \\mathbb R^n}f(x)\\frac{dP}{d\\nu}(x)d\\nu \\; .\\] Informally, the \\(d\\nu\\) simplify so that \\(\\frac{dP}{d\\nu}d\\nu\\) = \\(dP\\).\n\n\nGeneralized Likelihood Ratio\nIf \\(P\\) and \\(Q\\) are two probability measures such that \\(P\\) is absolutely continuous with respect to \\(Q\\), then the Radon Nikodym derivative \\(\\frac{dP}{d\\nu}\\) of \\(P\\) with respect to \\(Q\\) is a generalized likelihood ratio.\nIf \\(P\\) and \\(Q\\) are both absolutely continuous with respect to another measure \\(\\nu\\) (for example the Lebesgue measure), then the generalized likelihood ratio can be written \\[\n\\frac{dP}{dQ} = \\frac{\\frac{dP}{d\\nu}}{\\frac{dQ}{d\\nu}} \\; .\n\\]\nIn particular, if \\(P\\) and \\(Q\\) have positive densities \\(p\\) and \\(q\\) with respect to the Lebesgue measure, that is \\(\\frac{dP}{dx} = p(x)\\) and \\(\\frac{dQ}{dx} = q(x)\\), then\n\\[\n\\frac{dP}{dQ}(x) = \\frac{p(x)}{q(x)} \\; .\n\\]\nIn particular, the likelihood ratio does not depend on the reference measure (here Lebesgue).\n\n\nChange of Measure\nIf \\(\\mathbb E_{P}\\) (resp. \\(\\mathbb E_{Q}\\)) denotes the expectation when the random variable \\(X\\) follows distribution \\(P\\) (resp. \\(Q\\)), then for any measurable and bounded function \\(f\\),\n\\[\\mathbb E_{P}[f(X)] = \\mathbb E_{Q}\\left[f(X) \\frac{dP}{dQ}(X)\\right] \\; .\\] In other words, we simply replace the real random variable \\(f(X)\\) by the random variable \\(f(X) \\frac{dP}{dQ}(X)\\) when we observe \\(X\\) under \\(Q\\) instead of \\(P\\). This results directly follows from Radon-Nikodym: \\[\n\\int_{x \\in R^n} f(x) dP(x) = \\int_{x \\in R^n} f(x) \\frac{dP}{dQ}(x) dQ(x) \\; .\n\\]",
    "crumbs": [
      "Probability",
      "Density and Likelihood"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html",
    "href": "teaching/hypothesis_testing/TDs/TD1.html",
    "title": "TD1: Introduction to statistical models",
    "section": "",
    "text": "We aim to determine whether ENSAI students have any preference for cats or dogs. We assume that, a priori, they have no preference on average. We ask \\(n\\) students what their preferences are, and we let \\(X\\) be the number of “cat” answers.\n\nDefine \\(H_0\\) and \\(H_1\\). Is it a one-sided (unilatéral) or two-sided (bilatéral) test ?\nWe observe \\(10\\) students and \\(X=8\\) “cat” answers. Compute the pvalue in this specific case. Interprete the result.\nWrite the expression of the pvalue in terms of \\(n\\) and \\(X\\) and \\(F\\), the cdf of \\(\\mathrm{Bin}(n,0.5)\\).\nWrite a line of code to compute the pvalue in Julia, Python or R.\nWhat is the pvalue if \\(H_1\\) is instead\n\n“Students prefer cats” or\n“Students prefer dogs”"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-1",
    "title": "TD1: Introduction to statistical models",
    "section": "",
    "text": "We aim to determine whether ENSAI students have any preference for cats or dogs. We assume that, a priori, they have no preference on average. We ask \\(n\\) students what their preferences are, and we let \\(X\\) be the number of “cat” answers.\n\nDefine \\(H_0\\) and \\(H_1\\). Is it a one-sided (unilatéral) or two-sided (bilatéral) test ?\nWe observe \\(10\\) students and \\(X=8\\) “cat” answers. Compute the pvalue in this specific case. Interprete the result.\nWrite the expression of the pvalue in terms of \\(n\\) and \\(X\\) and \\(F\\), the cdf of \\(\\mathrm{Bin}(n,0.5)\\).\nWrite a line of code to compute the pvalue in Julia, Python or R.\nWhat is the pvalue if \\(H_1\\) is instead\n\n“Students prefer cats” or\n“Students prefer dogs”"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-2",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet \\((X_1, X_2, \\ldots, X_n)\\) be iid random variables that follow distribution \\(\\mathcal E(\\lambda)\\). We want to test: \\[\nH_0: \\lambda = \\frac{1}{2} \\quad \\text{vs.} \\quad H_1: \\lambda = 1.\n\\]\n\nShow that if \\(X \\sim E(\\lambda)\\) and \\(Y \\sim \\Gamma(k, \\lambda)\\) are independent, with \\(k \\in \\mathbb{N}^*\\), then \\(X + Y \\sim \\Gamma(k+1, \\lambda)\\). We recall that the density of \\(\\Gamma(\\lambda, k)\\) is given by \\(p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\)\nDeduce that \\(S_n=\\sum_{i=1}^n X_i\\) follows a Gamma distribution \\(\\Gamma(n, \\lambda)\\).\nFor a sample of size \\(n = 10\\), what is the rejection region of \\(S_n\\) for the simple likelihood ratio test with \\(0.05\\) significance level?\nWe admit that a Gamma distribution \\(\\Gamma(n, \\frac{1}{2})\\) is a chi-squared distribution with \\(2n\\) degrees of freedom, \\(\\chi^2(2n)\\). Bonus: Show this fact for \\(n=1\\), using a polar change of variable.\nThe empirical mean is \\(\\bar{x}_{10} = 2.5\\). What can we conclude?\nRecall what a cdf is, and read the p-value on the cdf of the \\(\\chi^2(20)\\) distribution \nCompare the p-value if we use a Gaussian approximation of \\(\\sum X_i\\) with the TCL. We recall that \\(\\mathbb V(X_1) = \\frac{1}{\\lambda^2}\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-3",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet \\(X_1, X_2, \\ldots, X_n\\) be random variables drawn from a normal distribution \\(N(\\theta, 1)\\). To test \\(H_0: \\theta = 5\\) against \\(H_1: \\theta &gt; 5\\), we propose the following test:\n\\[\nT = \\mathbf 1\\{\\bar{x} &gt; 5 + u \\},\n\\]\nwhere \\(\\bar{x}\\) is the empirical mean and \\(u\\) is to be fixed.\n\n\nDerive the function \\(g:~t \\to \\mathbb P(Z \\geq t) - e^{-t^2/2}\\), where \\(Z \\sim \\mathcal N(0,1)\\)\nDeduce that \\(\\mathbb P(Z \\geq t) \\leq e^{-t^2/2}\\) for all \\(t \\geq 0\\).\n\nDeduce a value of \\(u\\) such that the type I error of this test is smaller than a given \\(\\alpha\\). Rewrite the test \\(T\\) in function of \\(\\alpha\\).\nFix \\(\\alpha = 1/e\\) (and \\(u= \\sqrt{2/n}\\)). Compute the power function."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-4",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 4",
    "text": "Exercise 4\nLet the family of Pareto distributions with known parameter \\(a\\) and unknown parameter \\(\\theta\\):\n\\[\nf(x) =\n\\begin{cases}\n\\frac{\\theta}{a} \\left( \\frac{a}{x} \\right)^{\\theta+1}, & \\text{if } x \\geq a, \\\\\n0, & \\text{if } x &lt; a.\n\\end{cases}\n\\]\n\nCompute the mean and variance of \\(X\\), if \\(X\\) follows a Pareto distribution of parameter \\(a\\) and \\(\\theta\\).\nRewrite the density in the form \\(f(x) = a(x)b(\\theta)e^{c(\\theta)d(x)}\\) and identify \\(a\\),\\(b\\),\\(c\\) and \\(d\\).\nDeduce the general form of the uniformly most powerful test \\(UMP_\\alpha\\) for \\(H_0: \\theta \\geq \\theta_0\\) vs. \\(H_1: \\theta &lt; \\theta_0\\).\nFor \\(a = 1\\), construct the test for the null hypothesis: the mean of the distribution is smaller than or equal to 2.\nWhat is the density of \\(d(X_1)\\) ? \\(d\\) is defined in Q.2\nWrite a line of code in Julia, Python or R to compute the rejection region at level \\(\\alpha=0.05\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html",
    "href": "teaching/hypothesis_testing/TDs/TD3.html",
    "title": "TD3: Goodness of Fit",
    "section": "",
    "text": "We want to test if a die is biased. it is rolled \\(1000\\) times, and the number of occurrences for each face is recorded. The data is as follows:\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nCounts\n159\n168\n167\n160\n175\n171\n\n\n\n\nFormulate the hypothesis testing problem\nCompute the expected counts under \\(H_0\\), give the degree of freedom \\(d\\) of the chi-squared test statistic and give the approximated p-value, using the cdf of \\(\\chi^2(d)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-1",
    "title": "TD3: Goodness of Fit",
    "section": "",
    "text": "We want to test if a die is biased. it is rolled \\(1000\\) times, and the number of occurrences for each face is recorded. The data is as follows:\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nCounts\n159\n168\n167\n160\n175\n171\n\n\n\n\nFormulate the hypothesis testing problem\nCompute the expected counts under \\(H_0\\), give the degree of freedom \\(d\\) of the chi-squared test statistic and give the approximated p-value, using the cdf of \\(\\chi^2(d)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-2",
    "title": "TD3: Goodness of Fit",
    "section": "Exercise 2",
    "text": "Exercise 2\nIn a survey of \\(825\\) families with \\(3\\) children, the number of boys was recorded:\n\\[\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{Number of Boys} & 0 & 1 & 2 & 3 & \\text{Total} \\\\\n\\hline\n\\text{Number of Families} & 71 & 297 & 336 & 121 & 825 \\\\\n\\hline\n\\end{array}\n\\]\nWe assume under \\(H_0\\) that the genders of children in successive births within a family are independent categorical variables and that the probability \\(p\\) of having a boy remains constant.\n\nDetermine the distribution of the number of boys in a family with 3 children as a function of \\(p\\).\nEstimate \\(p\\) using a maximum likelihood estimator.\nTest the goodness of fit to the distribution obtained in question 1."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-3",
    "title": "TD3: Goodness of Fit",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe observe X = [0, 1, 0, 0, 0, 0, 0, 0.5, 1, 1, 1, 0.7, 0.9, 1, 1, 1, 1, 0, 0.1, 0, 1] We assume that the entries of \\(X\\) are iid of distribution \\(P\\). We consider the following hypothesis testing problem:\n\\(H_0\\): \\(P= \\mathcal B(0.5)\\) (Bernoulli)\\(\\quad\\) VS \\(\\quad\\) \\(H_1\\): \\(P \\neq \\mathcal B(0.5)\\).\n\nWhat can you say about the assumptions, \\(H_0\\) and \\(H_1\\)?\nDraw on the same graph the CDF of a Bernoulli \\(0.5\\) and the empirical CDF of the observed data \\(X\\).\nApply the Kolmogorov-Smirnov Test at level \\(0.1\\). To do so, use this table.\nComment on the result."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html",
    "href": "teaching/hypothesis_testing/TDs/TD2.html",
    "title": "TD2: Gaussian Populations",
    "section": "",
    "text": "A bread manufacturing factory wants to establish control procedures with the primary goal of reducing overproduction issues, which result in losses for the factory. Here, we focus on the weights of baguettes produced by the factory, with a target weight of \\(250\\) grams. For a sample of \\(n = 30\\) baguettes, the empirical mean is \\(\\bar{X}_n = 256.3\\) and the empirical variance is \\(S^2_n = 82.1\\). A priori, the factory reaches the target weight of \\(250\\) grams. We aim to test at the level of significance \\(\\alpha = 0.01\\) whether there is an overproduction issue.\n\nIs this a one-sided (unilatéral) or two-sided (bilatéral) testing problem?\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWhat is the test statistic to use, and what is its distribution under \\(H_0\\)?\nDetermine the rejection region.\nDoes the factory have an overproduction issue?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-1",
    "title": "TD2: Gaussian Populations",
    "section": "",
    "text": "A bread manufacturing factory wants to establish control procedures with the primary goal of reducing overproduction issues, which result in losses for the factory. Here, we focus on the weights of baguettes produced by the factory, with a target weight of \\(250\\) grams. For a sample of \\(n = 30\\) baguettes, the empirical mean is \\(\\bar{X}_n = 256.3\\) and the empirical variance is \\(S^2_n = 82.1\\). A priori, the factory reaches the target weight of \\(250\\) grams. We aim to test at the level of significance \\(\\alpha = 0.01\\) whether there is an overproduction issue.\n\nIs this a one-sided (unilatéral) or two-sided (bilatéral) testing problem?\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWhat is the test statistic to use, and what is its distribution under \\(H_0\\)?\nDetermine the rejection region.\nDoes the factory have an overproduction issue?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-2",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe want to test the precision of a method for measuring blood alcohol concentration on a blood sample. Precision is defined as twice the standard deviation of the method (assumed to follow a Gaussian distribution). The reference sample is divided into \\(6\\) test tubes, which are subjected to laboratory analysis. The following blood alcohol concentrations were obtained in g/L: \\[\n1.35, \\; 1.26, \\; 1.48, \\; 1.32, \\; 1.50, \\; 1.44.\n\\]\nWe aim to test the hypothesis that the precision is less than or equal to \\(0.1 \\, \\text{g/L}\\).\n\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWrite the test statistic and give its distribution under \\(H_0\\).\nPerform the test at a significance level of \\(\\alpha = 0.05\\).\nShow that the p-value of this test lies between \\(0.001\\) and \\(0.01\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-3",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 3",
    "text": "Exercise 3\nA candidate for the European elections wants to know if their popularity differs between men and women. A survey was conducted with \\(250\\) men, of whom \\(42\\%\\) expressed support for the candidate, and \\(250\\) women, of whom \\(51\\%\\) expressed support.\n\nFormulate the hypothesis testing problem.\nAt a significance level of \\(\\alpha = 0.05\\), can we say that these values indicate a statistically significant difference in popularity?\nGive an approximation of the p-value in terms of \\(F\\) ,the CDF of \\(\\mathcal N(0,1)\\) and read it on the graph below."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-4",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 4",
    "text": "Exercise 4\nWe aim to compare the average daily durations (in hours) of home-to-work commutes in two departments, labeled \\(A\\) and \\(B\\). We randomly surveyed 26 people in \\(A\\) and 22 in \\(B\\). Let \\(X_i\\) of pepople \\(i\\) be the random variable representing the commute duration in department \\(A\\), and \\(Y_j\\) that in department \\(B\\) of people \\(j\\). We assume the samples obtained are i.i.d following a Gaussian distribution: \\[\nX_i \\sim \\mathcal{N}(\\mu_A, \\sigma_A) \\quad \\text{and} \\quad Y_j \\sim \\mathcal{N}(\\mu_B, \\sigma_B).\n\\]\nHere is a summary of the data:\n\n\n\nDepartment A\nDepartment B\n\n\n\n\n\\(n_A= 26\\)\n\\(n_B=22\\)\n\n\n\\(\\sum x_i = 535\\)\n\\(\\sum y_j = 395\\)\n\n\n\\(\\sum x_i^2 = 11400\\)\n\\(\\sum y_j^2 = 7900\\)\n\n\n\n\nFormulate the hypothesis testing problem\nTest the equality of variances at a significance level of \\(\\alpha = 0.1\\)\nTest the equality of mean commute times between the two departments at a significance level of \\(\\alpha = 0.05\\), and conclude\nGive a Gaussian approximation of the test statistic using the CLT and the LLN, and approximate the p-value using the graph of the cdf of \\(\\mathcal N(0,1)\\) given in the previous exercise"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#multinomials",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#multinomials",
    "title": "Goodness of Fit Tests",
    "section": "Multinomials",
    "text": "Multinomials\n\n\n\n\n\n\nBinomial distribution\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red [Wooclap]\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1), Y=n-X \\sim \\mathrm{Bin}(n,1-p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color [Wooclap]\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m)) [Wooclap]\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit Test",
    "text": "\\chi^2 Goodness of Fit Test\n\n\n\n\nWe observe (X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q). This corresponds to n counts: X_1 + \\dots + X_m = n\nq = (q_1, \\dots, q_m) corresponds to probabilities of getting color 1, \\dots, m\nLet p = (p_1, \\dots, p_m) be a known vector s.t. p_1 + \\dots + p_m = 1.\nH_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\n\n\n\n\n\n\n\n\n\n\n\\chi^2 Goodness of Fit Test (Adéquation)\n\n\n\nChi-squared test statistic:: \\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; , where n_i = np_i = \\mathbb E[X_i] is the expected number of counts for color i.\nWhen np_i = n_i are large, under H_0, we approximate the distribution of \\psi(X) as \\psi(X) \\sim \\chi^2(m-1)\nReject if \\psi(X) &gt; t_{1- \\alpha} (right-tail of \\chi^2(m-1))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "title": "Goodness of Fit Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\n\n\n\nWe observe a bag of sweets containing n=100 sweets of m=3 different colors: red, green, and yellow.\nManufacturer: p_1= 40\\% red, p_2=35\\% green, and p_3=25\\% yellow.\nH_0: q=p (manufacturer’s claim is correct)\nH_1: q\\neq p (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\n\n\n\nColor\nObserved Counts\n\n\n\n\nRed\nX_1=50\n\n\nGreen\nX_2=30\n\n\nYellow\nX_3=20\n\n\n\n\n\n\n\n\n\nExpected Counts\n\n\n\n\nn_1=40\n\n\nn_2=35\n\n\nn_3=25\n\n\n\n\n\n\n\n\n\n\n\n\n\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\n\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878 (p_{value} \\approx 0.222)\nConclusion: do not reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#histograms",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#histograms",
    "title": "Goodness of Fit Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nhistogram\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n\n\n\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\n\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\n\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\nwhere I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\n\n\n\n\n\n\n\n\n\n\n\n\nNormalization\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of large numbers, Monte Carlo (informal)\n\n\n\nAssume that (X_1, \\dots, X_n) are iid of distrib P, and that a,b, k are fixed\nThe histogram \\mathrm{hist}(a,b,k) converges to the histogram of the density P"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit to a given distribution",
    "text": "\\chi^2 Goodness of Fit to a given distribution\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n, iid with unknown distrib P\nH_0: P = P_0, where P_0 is known\nH_1: P \\neq P_0 [Wooclap]\nIdea: under H_0, the counts in disjoint intervals I_1, I_2, \\dots, I_k follow a multinomial distribution \\mathrm{Mult}(n, (p_1, \\dots, p_{k})) where p_1 = P_0(I_1), \\dots, p_{k} = P_0(I_k)\n\n\n\n\n\n\n\n\n\n\nReduction to chi-squared test statistic\n\n\n\nCount the number of data (c_1, \\dots, c_k) in I_1, \\dots, I_k\nTheoretical counts nP_0(I_1) \\dots, nP_0(I_k)\nChi-squared statistic: \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_j)}\nDecide using an 1-\\alpha-quantile of a \\chi^2(k-1) distribution\n\n\n\n\n\n\n\n\n\n\nCorrected \\chi^2 Test\n\n\n\nIf P_0 is unknown\nIn a class \\mathcal P parameterized by \\ell parameters\nThen estimate the \\ell parameters\nCompute theoretical counts with \\hat P_{\\hat \\theta}\nBut decide with \\chi^2(k - 1 - \\ell)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Goodness of Fit Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\nH_0: X_i iid \\mathcal P(2)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n0\n1\n2 \n\\geq 3\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\n\n\n\n\n\n\nTo get 9.7, we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\gtrsim \\frac{(16-4)^2}{4} = 36\n(1-cdf(Chisq(3),36)) is very small: Reject\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf H_0 is X_i iid \\mathcal P(\\lambda) with unknown \\lambda\n\\hat \\lambda = 0.8, \\sum_{i=1}^4 \\frac{(X_i - n_i)^2}{n_i} \\approx 9.4\nNot \\chi^2(3) but \\chi^2(2)\n(1-cdf(Chisq(2),9.4)) \\approx 0.009. Reject at level 1%\n\n\n\n\n\n\n\n[Wooclap]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "title": "Goodness of Fit Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)} [Wooclap]\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\n\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with y=x"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "title": "Goodness of Fit Tests",
    "section": "Kolmogorov-Smirnov Test",
    "text": "Kolmogorov-Smirnov Test\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile:\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n} [Wooclap]\nEmpirical CDF: \\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf 1\\{X_i \\leq x\\}\nIdea: Max distance between empirical and true CDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\\psi(X) = \\sup_{x}|\\hat F(x) - F_0(x)|\nApprox: \\mathbb P_0(\\psi(X) &gt;c/\\sqrt{n}) \\to 2\\sum_{r=1}^{+\\infty}(-1)^{r-1}\\exp(-2c^2r^2) when n \\to +\\infty\nIn practice, use Julia, Python or R for KS Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#definition-and-link-with-clt",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#definition-and-link-with-clt",
    "title": "Gaussian Populations",
    "section": "Definition and link with CLT",
    "text": "Definition and link with CLT\nWe observe \\((X_1, \\dots, X_n)\\) iid real valued random variables.\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n X_i\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when } n \\to \\infty \\]\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\nEquality when \\(X_i\\)’s are Gaussian \\(\\mathcal N(\\mu, \\sigma^2)\\), that is \\[\n\\mathbb P(X_1 \\in [x,x+dx]) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Known Variance",
    "text": "Testing Mean with Known Variance\n\n\\(X = (X_1, \\dots, X_n)\\), iid with distribution \\(\\mathcal N(\\mu, \\sigma^2)\\).\n\n\n\n\n\n\n\nTest problems\n\n\n\\[\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTest statistic: \\[ \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\\]\n\\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nQ1, Q2\n\n\n\n\n\n\nTests\n\n\n\\[\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{(left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "Why 0.05 and 1.96 ?",
    "text": "Why 0.05 and 1.96 ?\n\n\n\n\n\nFisher’s Quote\n\n\nThe value for which \\(p=0.05\\), or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Unknown Variance",
    "text": "Testing Mean with Unknown Variance\n\nMultiple VS multiple test problem: \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n\\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\) no longer test statistic.\nIdea: replace \\(\\sigma\\) by its estimator \\[ \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\mu_0)^2} \\; .\\]\nThis gives \\[\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\\]\nIs \\(\\psi(X)\\) pivotal under \\(H_0\\) ? What is its distribution ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "Chi-Square and Student Distributions",
    "text": "Chi-Square and Student Distributions\n\n\n\n\n\n\nChi-squared distribution \\(\\chi^2(k)\\)\n\n\n\nDistrib of \\(\\sum_{i=1}^k Z_i^2\\) where the \\(Z_i\\)’s are iid \\(\\mathcal N(0,1)\\).\n\\(\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\\)\n\\(k\\): degree of freedom\n\\(\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\n\nStudent distribution \\(\\mathcal T(k)\\)\n\n\n\nDistrib of \\(\\tfrac{Z}{\\sqrt{U/k}}\\) where \\(Z\\), \\(U\\) are independent and follow resp. \\(\\mathcal N(0,1)\\) and a \\(\\chi^2(k)\\)\n\\(k\\): degree of freedom\n\\(\\mathcal T(k) \\sim \\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\nAssume \\(X_i\\) are iid \\(\\mathcal N(\\mu_0, \\sigma^2)\\).\n\nThe test statistic \\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma}\\) pivotal (indep. of \\(\\sigma\\)).\nIt follows a Student Distribution \\(\\mathcal T(n-1)\\).\n\n\n\n\n\nProof idea: \\(\\overline X \\cdot (1, \\dots, 1)\\) and \\((X_1 - \\overline X, \\dots, X_n - \\overline X)\\) are orthogonal in \\(\\mathbb R^n\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "Student T-Test",
    "text": "Student T-Test\n\nMultiple VS multiple test problem \\(X=(X_1, \\dots, X_n)\\): \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n(Student) T-test statistic: \\[\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variance-unknown-mean",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variance-unknown-mean",
    "title": "Gaussian Populations",
    "section": "Testing Variance, Unknown Mean",
    "text": "Testing Variance, Unknown Mean\n\n\n\n\nWe observe \\(X=(X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu, \\sigma^2)\\). \\(\\mu\\), \\(\\sigma\\) are unknown. \\(\\sigma_0\\) is fixed and known.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(H_0\\): \\(\\sigma \\leq \\sigma_0\\), \\(H_1\\): \\(\\sigma &gt; \\sigma_0\\)\n\\(\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum_{i=1}^n (X_i - \\overline X)^2\\) Wooclap\n\\(T(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\\)\n\\(q_{1-\\alpha}\\): quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\\(H_0\\): \\(\\sigma \\geq \\sigma_0\\), \\(H_1\\): \\(\\sigma &lt; \\sigma_0\\)\n\\(\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum_{i=1}^n (X_i - \\overline X)^2\\)\n\\(T(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\\)\n\\(q_{\\alpha}\\): quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\) are known, \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nTest problem: \\(H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\\)\nIdea: normalize \\(\\overline X - \\overline Y\\): \\[\n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\\]\nTwo-tailed test for testing means: \\[\nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\\]\n\\(t_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of a Gaussian distribution"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example",
    "title": "Gaussian Populations",
    "section": "Example",
    "text": "Example\n\nObjective. Test if a new medication is efficient to lower cholesterol level\nExperiment.\n\nGroup A: \\(n_A = 45\\) patients receiving the new medication\nGroup B: \\(n_B = 50\\) patients receiving a placebo\n\nTest Problem.\n\nWe observe \\((X_1, \\dots, X_{n_A})\\) iid \\(\\mathcal N(\\mu_A,\\sigma^2)\\) and \\((Y_1, \\dots, Y_{n_B})\\) iid \\(N(\\mu_B,\\sigma^2)\\) the chol levels. \\(\\sigma = 8\\) mg/dL is known from calibration.\n\\(H_0: \\mu_A = \\mu_B\\) VS \\(H_1: \\mu_A &lt; \\mu_B\\)\n\nTest Statistic. \\(\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2}}}\\)\nData. \\(\\overline X = 24.5\\) mg/dL and \\(\\overline Y = 21.3\\) mg/dL. Hence \\(\\psi(X,Y)= 5.5\\).\nConclusion. Do not reject, and do not use this medication!"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe \\((X_1, \\dots, X_{n})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nVariance Testing Problem: \\[\nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\\]\nF-Test Statistic of the Variances (ANOVA) \\[\n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher distribution \\(\\mathcal F(k_1,k_2)\\)\n\n\n\nDistribution of \\(\\frac{U_1/k_1}{U_2/k_2}\\), where \\(U_1\\), \\(U_2\\) are indep. and follow \\(\\chi^2(k_1)\\), \\(\\chi^2(k_2)\\). wiki Wooclap\n\\(\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right)\\) when \\(k_1,k_2 \\to +\\infty\\)\n\\((k_1, k_2)\\): degrees of freedom\nExample: \\(\\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1)\\) if \\(Z_i \\sim \\mathcal N(0,1)\\)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2}\\) is independent of \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1\\), \\(\\sigma_2\\). It is pivotal\nIt follow distribution \\(\\mathcal F(n_1-1, n_2-1)\\)\n\n\n\n\n\n\n\n\nTwo-tailed test: \\[ \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown, but we know that \\(\\sigma_1=\\sigma_2\\)\nEquality of mean testing problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}\\).\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\\(\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\\)\nNormalize \\(\\overline X - \\overline Y\\): \\[\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\\]\n\\(\\psi(X,Y)\\) is pivotal because \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nEquality of Mean Testing Problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}\\).\n\n\n\n\n\nStudent Welch test statistic\n\n\n\\[\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\\]\n\nWooclap \\(\\psi(X,Y)\\) is not pivotal\nGaussian approximation: \\(\\psi(X,Y) \\approx \\mathcal N(0,1)\\) when \\(n_1, n_2 \\to \\infty\\)\nBetter approximation: Student Welch"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \\]\nEquality when \\(X_i\\)’s are \\(\\mathcal N(\\mu, \\sigma^2)\\)\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\n\n\n\nExample: binomials\n\n\n\nIf \\(p \\in (0,1)\\)\n\\(\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1)\\) when \\(n \\to \\infty\\)\n\\(n\\) should be \\(\\gg \\frac{1}{p}\\)\n\n\n\n\n\n\n\n\n\nGood Approx for (\\(n=100\\), \\(p=0.2\\))\n\n\n\n\nBad Approx for (\\(n=100\\), \\(p=0.01\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe \\(X \\sim Bin(n_1, p_1)\\) and \\(Y \\sim Bin(n_2, p_2)\\).\n\\(n_1\\), \\(n_2\\) are known but \\(p_1\\), \\(p_2\\) are unknown in \\((0,1)\\)\n\\(H_0\\): \\(p_1 = p_2\\) or \\(H_1\\): \\(p_1 \\neq p_2\\)\n\n\n\n\n\nTest Statistic\n\n\n\\[ \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\\]\n\n\\(\\hat p_1 = X/n_1\\), \\(\\hat p_2 = Y/n_2\\)\n\\(\\hat p = \\frac{X+Y}{n_1+n_2}\\) [Wooclap]\nIf \\(np_1, np_2 \\gg 1\\): \\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nWe reject if \\(|\\psi(X,Y)| \\geq t_{1-\\alpha/2}\\) (gaussian quantile)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion: “should we raise taxes on cigarettes to pay for a healthcare reform ?”\n\\(p_1\\), \\(p_2\\): proportion of non-smokers or smokers willing to raise taxes\n\\(H_0\\): \\(p_1=p_2\\) or \\(H_1\\): \\(p_1 &gt; p_2\\)\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\(\\hat p_1 = \\overline X= \\approx 0.58\\), \\(\\hat p_2=\\overline Y= \\approx 0.21\\).\n\\(\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\\)\n\\(\\mathbb P(\\psi(X,Y) &gt; 8.99)\\) = 1-cdf(Normal(0,1), 8.99)"
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html",
    "href": "teaching/hypothesis_testing/lectures/dependency.html",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "We observe iid paired data \\((X_1, Y_1), \\dots, (X_n,Y_n)\\) of unknown mean \\(\\mu_X, \\mu_Y\\) and cov matrix \\(\\Sigma\\).\nCov matrix: \\(\\Sigma =\n\\left(\\begin{matrix}\n\\sigma_X^2 & \\mathrm{Cov(X,Y)} \\\\\n\\mathrm{Cov(X,Y)} & \\sigma_Y^2 \\\\\n\\end{matrix}\\right)\\)\n\\(H_0: \\mathrm{Cov}(X,Y)=0\\) or \\(H_1: \\mathrm{Cov}(X,Y)\\neq 0\\)\n\\(\\mathrm{Cov(X,Y)} = \\mathbb E[(X- \\mathbb E[X])(Y- \\mathbb E[Y])]\\)\n\\(\\sigma_X^2 = \\mathrm{Cov(X,X)}\\)\n\\(\\sigma_Y^2 = \\mathrm{Cov(Y,Y)}\\)\n\\(\\mathrm{Cor(X,Y)} = \\frac{\\mathrm{Cov(X,Y)}}{\\sigma_X \\sigma_Y}\\)\n\n\n\n\n\n\n\n\n\n\nPearson’s Correlation Test\n\n\n\n\n\\(r =  \\frac{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline X)^2\\sum_{i=1}^n (Y_i - \\overline Y)^2}}\\)\n\\(\\psi(X,Y) = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\)\nUnder \\(H_0\\), \\(\\psi(X,Y) \\approx \\mathcal T(n-2)\\)\n\n\n\n\n\n\nMonte Carlo Simulation with \\(n=4\\):\n\n\n\n\n\n\n\n\n\n\n\n\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nGroup \\(k\\): \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\). Ex: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\)\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2 \\sim \\sigma^2\\chi^2(N_k-1)\\) (under \\(H_0\\))\nVar of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\sim\\sigma^2\\chi^2(d-1)\\) (under \\(H_0\\))\n\n\n\n\n\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}{\\tfrac{1}{d-1}SSB}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\).\n\n\n\n\n\n\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#correlation-test-1",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#correlation-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "Monte Carlo Simulation with \\(n=4\\):",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#anova-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#anova-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nGroup \\(k\\): \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\). Ex: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\)\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2 \\sim \\sigma^2\\chi^2(N_k-1)\\) (under \\(H_0\\))\nVar of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\sim\\sigma^2\\chi^2(d-1)\\) (under \\(H_0\\))\n\n\n\n\n\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}{\\tfrac{1}{d-1}SSB}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#interpretation-of-variances-in-anova",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#interpretation-of-variances-in-anova",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#chi2-homogeneity-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#chi2-homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Homogeneity Test",
    "text": "\\(\\chi^2\\) Homogeneity Test\n\n\\(d\\) different bags (or groups), each containing balls of \\(m\\) potential colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix of counts:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\n\n\n\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all color \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#example-soft-drink-preferences",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\)\n\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#chi2-independence-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#chi2-independence-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Independence Test",
    "text": "\\(\\chi^2\\) Independence Test\n\n\n\n\n\n\n\nObservations: Paired Categorical Variables \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\ne.g. \\(X_i \\in \\{\\mathrm{male}, \\mathrm{female}\\}\\), \\(Y_i \\in \\{\\mathrm{coffee}, \\mathrm{tea}\\}\\)\nIdea: regroup the data into bags, e.g. \\(\\{\\mathrm{male}, \\mathrm{female}\\}\\)\nBuild the contingency table\nPerform a chi-square homogeneity test\n\n\n\n\nExample of contingency table:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n30\n20\n50\n\n\nTea\n28\n22\n50\n\n\nTotal\n58\n42\n100\n\n\n\nExpected counts:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n29\n21\n50\n\n\nTea\n29\n21\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\nDegree of freedom: \\((2-1)(2-1) = 1\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#symetric-random-variable",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)\n\n\n\n\n\n\n\n\n\n\n\nSymetrization\n\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t)\n&=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy \\\\\n&= \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]\n\\(X\\) indep of \\(Y\\) \\(\\implies\\) \\(X-Y\\) symmetric \\(\\implies\\) \\(\\mathrm{median}(X-Y) = 0\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#dependency-problem-for-paired-data",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The median of \\((X_i - Y_i)\\) is \\(0\\) for all \\(i\\)\n\\(H_1:\\) The median of \\((X_i - Y_i)\\) is not \\(0\\) for some \\(i\\)\n\n\n\n\n\n\n\n\n\n\nGenerality of \\(H_0\\)\n\n\n\n\nIf \\(X_i-Y_i\\) is symmetric \\(i\\), then we are under \\(H_0\\).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\).\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\)).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\n\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]\n\n\n\n\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\n\nUnder \\(H_0\\),\n\nSigns of the \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &gt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\nif \\(H_1\\): \\(\\mathrm{median}(D_i) &gt; 0\\): left-one sided test on \\(W_-\\).\n\n\n\nThis approximation fits well the exact distribution. Monte-Carlo simulation:\n\nTo generate a \\(W_-\\) under \\(H_0\\) in Julia:\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#effect-of-drug-on-blood-pressure",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#effect-of-drug-on-blood-pressure",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Effect of Drug on Blood Pressure",
    "text": "Effect of Drug on Blood Pressure\n\n\\(H_0\\): the drug has no effect. \\(H_1\\): it lowers the blood pressure\n\n\n\n\nPatient\n\\(X_i\\) (Before)\n\\(Y_i\\)​ (After)\n\\(D_i = X_i-Y_i\\)​\n\\(R_i\\)\n\n\n\n\n1\n150\n140\n10\n6 (+)\n\n\n2\n135\n130\n5\n5 (+)\n\n\n3\n160\n162\n-2\n2 (-)\n\n\n4\n145\n146\n-1\n1 (-)\n\n\n5\n154\n150\n4\n4 (+)\n\n\n6\n171\n160\n11\n7 (+)\n\n\n7\n141\n138\n3\n3 (+)\n\n\n\n\n\\(W_- = 1+2 = 3\\).\nFrom a simulation, we approx \\(\\mathbb P(W_-=i)\\), for \\(i \\in \\{0, 1, 2, 3, 4, 5,6\\}\\) under \\(H_0\\) by\n\n[0.00784066, 0.00781442, 0.00781534, 0.01563892, 0.01562184, 0.02343478]\n\nFrom a simulation, \\(p_{value}=\\mathbb P(W_- \\leq 3) \\approx 0.039 &lt; 0.05\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Tests provide the theoretical basis for decision-making based on data. For example, doctors diagnose diseases using specific biological markers, industrial quality engineers evaluate the quality of a production batch, and climate scientists determine whether there are significant changes in measurements compared to the pre-industrial era.\n\n\nHypothesis testing is a key statistical method that enables professionals to make informed decisions. The process involves setting up two competing hypotheses:\n\nthe null hypothesis \\(H_0\\), which assumes no effect or no difference. This is usually an a priori on the data.\nthe alternative hypothesis \\(H_1\\), which suggests that there is a significant effect or difference.\n\nLet’s illustrate this with the example of a doctor diagnosing a disease, such as diabetes, using blood glucose levels:\n\nThe doctor begins by assuming the null hypothesis \\(H_0\\): the patient’s blood glucose levels are normal, indicating no diabetes.\nThe alternative hypothesis (\\(H_1\\)) would suggest that the patient’s glucose levels are abnormally high, indicating diabetes.\n\nAfter collecting the patient’s blood glucose data, the doctor compares it to a standard threshold for diagnosing diabetes. A hypothesis test is then performed to evaluate whether the observed glucose level is significantly higher than the threshold, or whether the difference could simply be due to random variation.\nThe testing process can be summarized as follows:\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests?\n\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)\n\n\n\n\nIn general, observing \\(T=1\\) is just one piece of evidence supporting \\(H_1\\). It’s possible that we were simply unlucky and the data led the test to reject \\(H_0\\). Additionally, the sample used might not be representative, or the elevated glucose level could be caused by another condition, not diabetes. Therefore, we cannot blindly accept \\(H_1\\) based on this result alone. Further investigation is essential when \\(T=1\\) to minimize the risk of making a Type I error.\nThe same applies when \\(T=0\\). A result of \\(T=0\\) simply indicates that the test provides no evidence in favor of \\(H_1\\), but it doesn’t mean that \\(H_1\\) is false. It is possible that the patient exhibits other symptoms that could still suggest a diagnosis of diabetes, despite the lack of evidence from the test.\n\n\n\n\n\n\nObjective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?\n\nIn the above example, we formally observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\) Imagine that instead of ten \\(6\\), we observed \\(600\\) tosses leading to the following counts:\n\nThe number of \\(6\\) is really close to \\(100 = 600/6\\), which implies that we would not reject \\(H_0\\) in the previous example. We would reject however if we wanted to test whether the dice is fair, since it is highly biased toward \\(2\\).\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_1\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_0\\) is “unlikely”)\n\n\n\n\n\n\n\n\nSee also the (Pluto notebook: illustration of pvalue)\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\) (“\\(95^{th}\\) percentile”)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Hypothesis testing is a key statistical method that enables professionals to make informed decisions. The process involves setting up two competing hypotheses:\n\nthe null hypothesis \\(H_0\\), which assumes no effect or no difference. This is usually an a priori on the data.\nthe alternative hypothesis \\(H_1\\), which suggests that there is a significant effect or difference.\n\nLet’s illustrate this with the example of a doctor diagnosing a disease, such as diabetes, using blood glucose levels:\n\nThe doctor begins by assuming the null hypothesis \\(H_0\\): the patient’s blood glucose levels are normal, indicating no diabetes.\nThe alternative hypothesis (\\(H_1\\)) would suggest that the patient’s glucose levels are abnormally high, indicating diabetes.\n\nAfter collecting the patient’s blood glucose data, the doctor compares it to a standard threshold for diagnosing diabetes. A hypothesis test is then performed to evaluate whether the observed glucose level is significantly higher than the threshold, or whether the difference could simply be due to random variation.\nThe testing process can be summarized as follows:\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Decision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)\n\n\n\n\nIn general, observing \\(T=1\\) is just one piece of evidence supporting \\(H_1\\). It’s possible that we were simply unlucky and the data led the test to reject \\(H_0\\). Additionally, the sample used might not be representative, or the elevated glucose level could be caused by another condition, not diabetes. Therefore, we cannot blindly accept \\(H_1\\) based on this result alone. Further investigation is essential when \\(T=1\\) to minimize the risk of making a Type I error.\nThe same applies when \\(T=0\\). A result of \\(T=0\\) simply indicates that the test provides no evidence in favor of \\(H_1\\), but it doesn’t mean that \\(H_1\\) is false. It is possible that the patient exhibits other symptoms that could still suggest a diagnosis of diabetes, despite the lack of evidence from the test.",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Objective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?\n\nIn the above example, we formally observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\) Imagine that instead of ten \\(6\\), we observed \\(600\\) tosses leading to the following counts:\n\nThe number of \\(6\\) is really close to \\(100 = 600/6\\), which implies that we would not reject \\(H_0\\) in the previous example. We would reject however if we wanted to test whether the dice is fair, since it is highly biased toward \\(2\\).\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_1\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_0\\) is “unlikely”)\n\n\n\n\n\n\n\n\nSee also the (Pluto notebook: illustration of pvalue)\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\) (“\\(95^{th}\\) percentile”)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#section",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\n\n\nTest",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#type-of-problems",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#type-of-problems",
    "title": "Hypothesis Testing",
    "section": "Type of Problems",
    "text": "Type of Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\n\n\nMultiple VS Multiple",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nParametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nNon-parametric: otherwise\n\n\nExample of multiple VS multiple parametric problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\nDecision rule\n\n\n\n\nA decision rule or test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\n\n\n\n\n\n\n\n\nTest statistic\n\n\n\n\na test statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nExample of \\(\\mathcal R\\), for a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) \\not \\in [t_1, t_2]\\}:~~~~~~\\mathcal R = (-\\infty,t_1)\\cup (t_2, +\\infty)\\;\n\\end{aligned}\n\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#continous-measures",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#continous-measures",
    "title": "Hypothesis Testing",
    "section": "Continous Measures",
    "text": "Continous Measures\n\ndensity wrp to Lebesgue: \\(dP(x) = p(x)dx\\)\nPDF (proba density function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha} = \\alpha)\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#discrete-measures",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#discrete-measures",
    "title": "Hypothesis Testing",
    "section": "Discrete Measures",
    "text": "Discrete Measures\n\ndensity wrp to counting measure: \\(P(X=x) = p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\nGaussian/Bernoulli\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)\n\n\n\n\n\nExponential/Geometric\n\n\n\nExponential \\(\\mathcal E(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)\n\n\n\n\n\nGamma/Poisson\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\]\n\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\lambda\\)\n\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\n\nNumber of tics before time \\(1\\) of an atomic clock of rate \\(\\lambda\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\)?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nEquivalently, we can consider the log-likelihood ratio test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#neyman-pearson",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#neyman-pearson",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson",
    "text": "Neyman Pearson\n\n\n\n\n\n\nNeyman Pearson’s theorem\n\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\nProof of Neyman Pearson’s theorem\nWe prove the theorem in the case where \\(P\\) and \\(Q\\) each have a density \\(p\\) and \\(q\\) on \\(\\mathbb R^n\\). For any \\(t &gt; 0\\), define \\[I_t(P, Q) = \\int_{x \\in \\mathbb R^n} |q(x) - tp(x)|dx \\; .\\]\nWhen \\(t=1\\), this quantity is equal to the total-variation distance between \\(P\\) and \\(Q\\). For any event A in \\(\\mathbb R^n\\) , it holds that\n\\[\n\\begin{split}\n  I_t(P, Q) &= \\int_{x \\in \\mathbb R^n} |q(x) - tp(x)|dx \\\\\n  &= \\int_{q(x)&gt; tp(x)} q(x) - tp(x)dx + \\int_{q(x)&lt; tp(x)} tp(x) - q(x)dx\\\\\n  &=  2\\int_{x \\in \\mathbb R^n} \\mathbf1_{q(x)&gt; tp(x)}(q(x) - tp(x))dx + t-1 \\\\\n  &\\geq 2\\int_{x \\in \\mathbb R^n} \\mathbf 1_A \\mathbf1_{q(x)&gt; tp(x)}(q(x) - tp(x))dx + t-1\\\\\n  &\\geq t-1 + 2(Q(A) - tP(A)) \\; .\n\\end{split}\n\\]\nIf \\(A  = \\{x \\in \\mathbb R^n : q(x) &gt; tp(x)\\}\\), then the two last inequalities are equalities. In particular, \\[I_t(P, Q) = t-1 + 2\\sup_{A \\subset \\mathbb R^n}(Q(A) - tP(A))) \\; .\\]\nAssume that the type-1 error of \\(T\\) is smaller than \\(\\alpha\\): \\(P(T=1) \\leq \\alpha\\).\nThe power of \\(T\\), \\(Q(T = 1)\\), is upper-bounded as follows: \\[\n\\begin{split}\n  Q(T=1) &\\leq Q(T=1) + t(\\alpha - P(T=1))\\\\\n  &= \\alpha t + Q(T=1) - tP(T=1) \\\\\n  &\\leq \\alpha t + \\frac{t-1}{2} + \\frac{1}{2}I_t \\; .\n\\end{split}\n\\]\n\nThere is equality in the second inequality if \\(T=1\\) is the event \\(\\{ \\frac{q(X)}{p(X)} &gt; t \\}\\).\nThere is equality in the first inequality if \\(P(T=1) = \\alpha\\).\n\nLet \\(t_{\\alpha}\\) be such that \\(P(\\frac{q(X)}{p(X)}&gt; t_{\\alpha}) = \\alpha\\). The test \\(T^*(X) = \\mathbf1 \\{\\frac{q(X)}{p(X)}&gt; t_{\\alpha}\\}\\) satisfies the two above points, since its rejection event is exactly \\(\\{T^*(X) = 1\\} = \\{\\frac{q(X)}{p(X)}&gt; t_{\\alpha}\\}\\) and since \\(P(T^* = 1) = \\alpha.\\) Hence, for any test \\(T\\) of type 1 error smaller than \\(\\alpha\\), it holds that \\(Q(T = 1) \\leq Q(T^* = 1)\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples-2",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples-2",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\nExample (Gaussians)\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\nLog-likelihood ratio test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\n\n\n\nExample: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\n\n\\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]\n\n\nExample: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): reject if \\(N \\geq 19\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#generalities",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#generalities",
    "title": "Hypothesis Testing",
    "section": "Generalities",
    "text": "Generalities\n\n\\(H_0 = \\{ \\mathcal P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(right-tailed: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(left-tailed: unilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(simple/multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( multiple/multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-tailed test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (right-tailed test).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#pivotal-test-statistic",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#pivotal-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic",
    "text": "Pivotal Test Statistic\n\nConsider \\(\\Theta_0\\) not singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\nPivotal test statistic\n\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#p-value",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#p-value",
    "title": "Hypothesis Testing",
    "section": "P-value",
    "text": "P-value\nSee the (Pluto notebook: Illustration of pvalue)\n\n\n\n\n\n\nP-value: definition\n\n\n\nWe define \\(p_{value}(x_{\\mathrm{obs}}) =\\mathbb P(\\psi(X) &gt; x_{\\mathrm{obs}})\\) for a right-tailed test.\nFor a two-tailed test, \\(p_{value}(x_{\\mathrm{obs}}) =2\\min(\\mathbb P(\\psi(X) &gt; x_{\\mathrm{obs}}),\\mathbb P(\\psi(X) &lt; x_{\\mathrm{obs}}))\\)\n\n\n\n\n\n\n\n\nP-value under \\(H_0\\)\n\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(p_{value}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\nProof.\nThe p-value is a probability, so it belongs to \\([0,1]\\). Let \\(F_{\\psi}\\) be the cumulative distribution function of a random variable \\(\\psi(X)\\) when \\(X\\) follows distribution \\(P\\), that is \\(G_{\\psi}(t) = \\mathbb P(\\psi(X) &gt; t)\\). It holds that \\[\n\\mathbb P(\\psi(X') &gt; \\psi(X) ~|~ \\psi(X)) = F_{\\psi}(\\psi(X))\n\\] If \\(u \\in [0,1]\\), then \\[\n\\begin{aligned}\n\\mathbb P(\\mathrm{pvalue}(X) &gt; u) &= \\mathbb P(F_{\\psi}(\\psi(X)) &gt; u) \\\\\n&= \\mathbb P(\\psi(X) &gt; F_{\\psi}^{-1}(u)) \\\\\n&= 1- F_{\\psi}(F_{\\psi}^{-1}(u)) = 1-u\n\\end{aligned}\n\\] Hence, \\(\\mathrm{pvalue}(X)\\) is uniform when \\(X\\) follows distribution \\(\\mathbb P\\). \\[\\tag*{$\\blacksquare$}\\]\n\n\n\nIn practice: reject if \\(p_{value}(X) &lt; \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\) under \\(H_0\\):",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I currently am a teaching three topics in statistics at ENSAI Rennes: Hypothesis Testing, Linear and Generalized Linear Model and Times Series (TD 2024).\nSee the Glossary for English/French translations. See also the note recalling some notion on Density and Likelihood."
  },
  {
    "objectID": "teaching.html#slides",
    "href": "teaching.html#slides",
    "title": "Teaching",
    "section": "Slides",
    "text": "Slides\n\nTesting Models\nClassical Tests on Gaussian Populations\nGoodness of Fit and Homogeneity Tests\nTests for Dependency"
  },
  {
    "objectID": "teaching.html#lectures",
    "href": "teaching.html#lectures",
    "title": "Teaching",
    "section": "Lectures",
    "text": "Lectures\n\nTesting Models\n\nClassical Tests on Gaussian Populations\nGoodness of Fit and Homogeneity Tests\nTests for Dependency"
  },
  {
    "objectID": "teaching.html#pluto-notebooks",
    "href": "teaching.html#pluto-notebooks",
    "title": "Teaching",
    "section": "Pluto Notebooks",
    "text": "Pluto Notebooks\n\nProba Basics\nApproximation of Distributions\nIllustration of pvalue"
  },
  {
    "objectID": "teaching.html#td-tp",
    "href": "teaching.html#td-tp",
    "title": "Teaching",
    "section": "TD-TP",
    "text": "TD-TP\n\nTD1\nTD2\nTD3\nTD4\nTP"
  },
  {
    "objectID": "teaching.html#slides-1",
    "href": "teaching.html#slides-1",
    "title": "Teaching",
    "section": "Slides",
    "text": "Slides\n\nLinear Model\n\nIntroduction\n\n\n\nGeneralized Linear Model"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-test",
    "href": "teaching/linear_model/slides/introduction.html#correlation-test",
    "title": "Introduction",
    "section": "Correlation test",
    "text": "Correlation test\n\n\\(\\hat \\rho(X, Y)\\) is an estimator of the unknown theoretical correlation \\(\\rho\\) between \\(X\\) and \\(Y\\) defined by \\[\\rho = \\frac{\\mathbb E[(X - \\mathbb E(X))(Y - \\mathbb E(Y))]}{\\sqrt{\\mathbb V(X)\\mathbb V(Y)}}\\]\n\n\nCorrelation test problem:\n\\[H_0: \\rho = 0 \\quad \\text{VS}\\quad  H_1: \\rho \\neq 0\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-test-1",
    "href": "teaching/linear_model/slides/introduction.html#correlation-test-1",
    "title": "Introduction",
    "section": "Correlation test",
    "text": "Correlation test\n\nTest statistic (here we use \\(\\psi\\) for test statistics and \\(T\\) for tests) \\[\\psi(X,Y) = \\frac{\\hat \\rho\\sqrt{n-2}}{\\sqrt{1-\\hat \\rho^2}}\\]\n\n\nTest\nUnder \\(H_0\\), if \\((X,Y)\\) is Gaussian, \\(\\psi(X,Y) \\sim \\mathcal T(n-2)\\) (Student distribution of degree of freedom \\(n-2\\))\n\n\n\\[T(X,Y) = \\mathbf{1}\\{|\\psi(X,Y)| &gt; t_{1-\\alpha/2}\\}\\]\nIn R: cor.test"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#least-square-p1",
    "href": "teaching/linear_model/slides/introduction.html#least-square-p1",
    "title": "Introduction",
    "section": "Least Square \\((p=1)\\)",
    "text": "Least Square \\((p=1)\\)\n\nGiven observations \\((X_i, Y_i)\\), we consider \\(\\hat \\alpha\\), \\(\\hat \\mu\\) that minimize, over all \\((\\alpha, \\mu) \\in \\mathbb R^2\\):\n\n\\[\nL(\\alpha, \\mu) = \\sum_{i=1}^n (Y_i - \\alpha X_i - \\mu)^2\n\\]\n\n\n\nSolution: (check homogeneity!)\n\n\\[\\hat \\alpha = \\hat \\cov(X,Y)  \\quad \\text{and} \\quad \\hat \\mu = \\overline Y - \\hat a \\overline X\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#contingency-table-and-notation",
    "href": "teaching/linear_model/slides/introduction.html#contingency-table-and-notation",
    "title": "Introduction",
    "section": "Contingency Table and Notation",
    "text": "Contingency Table and Notation\n\nWe observe \\(X=(X_1, \\dots, X_n)\\) and \\(Y=(Y_1, \\dots, Y_n)\\), where\n\n\\(X_k \\in \\{1, \\dots, I\\}\\) (factor with \\(I\\) categories, “colors”)\n\\(Y_k \\in \\{1, \\dots, J\\}\\) (factor with \\(J\\) categories, “bags”)\n\n\n\n\n\n\nCategory X/Y\nBag 1\nBag 2\nBag 3\nTotals\n\n\n\n\nCol 1\n\\(n_{11}\\)\n\\(n_{12}\\)\n\\(n_{13}\\)\n\\(R_1\\)\n\n\nCol 2\n\\(n_{21}\\)\n\\(n_{22}\\)\n\\(n_{23}\\)\n\\(R_2\\)\n\n\nTotals\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\\(n_{ij}\\): number of individuals having category \\(i\\) for \\(X\\) and \\(j\\) for \\(Y\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example",
    "href": "teaching/linear_model/slides/introduction.html#example",
    "title": "Introduction",
    "section": "Example",
    "text": "Example\nExample 1. If we assume that the distribution of \\((Y, X)\\) is Gaussian. Then:\n\\(E(Y|X) = E(Y) + (X - E(X))'\\beta\\)\nwhere $= ^{-1}(Cov(Y, X^{(1)}), , Cov(Y, X^{(p)}))’$ and where \\(\\Sigma = V(X)\\).\nIn other words, in the Gaussian case,\n\\(f(X^{(1)}, \\ldots, X^{(p)}) = \\beta_0 + \\beta_1X^{(1)} + \\cdots + \\beta_p X^{(p)}\\)\ndenoting \\(\\beta = (\\beta_1, \\ldots, \\beta_p)\\) and \\(\\beta_0 = E(Y) - E(X)'\\beta\\).\nThe function \\(f\\) we are looking for is simply an affine function in \\(X^{(1)}, \\ldots, X^{(p)}\\).\nThe parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are unknown a priori.\nBut the estimation problem becomes much simpler: estimating the function \\(f\\) in this context simply amounts to estimating these parameters."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#section",
    "href": "teaching/linear_model/slides/introduction.html#section",
    "title": "Introduction",
    "section": "",
    "text": "fluidity_types = [\"A\", \"B\", \"C\", \"D\"]\ntype_p = [21, 20, 17, 20]\ntype_u = [21, 17, 17, 20]\ntype_a = [19, 16, 16, 18]\ntype_t = [9, 8, 8, 8]\ntype_v = [9, 7, 7, 8]\n\n# Create a matrix for the grouped bar plot\n# Each row represents a fluidity type, each column represents a measurement type\ndata_matrix = hcat(type_p, type_u, type_a, type_t, type_v)\n\n# Create a grouped bar plot\np1 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Dodge (Beside)\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:dodge,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\np2 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Stack\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:stack,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\nplot(p1,p2)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-no2trafic-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-no2trafic-dataset",
    "title": "Introduction",
    "section": "Example: NO2trafic dataset",
    "text": "Example: NO2trafic dataset\ncontingency table of variable “Type” and “Fluidity”\n\n\n\nFluidity/Type\nP\nU\nA\nT\nV\n\n\n\n\nA\n21\n21\n19\n9\n9\n\n\nB\n20\n17\n16\n8\n7\n\n\nC\n17\n17\n16\n8\n7\n\n\nD\n20\n20\n18\n8\n8\n\n\n\nIn R: table(X,Y)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#in-julia",
    "href": "teaching/linear_model/slides/introduction.html#in-julia",
    "title": "Introduction",
    "section": "In Julia",
    "text": "In Julia\n\nfluidity_types = [\"A\", \"B\", \"C\", \"D\"]\ntype_p = [21, 20, 17, 20]\ntype_u = [21, 17, 17, 20]\ntype_a = [19, 16, 16, 18]\ntype_t = [9, 8, 8, 8]\ntype_v = [9, 7, 7, 8]\n\n# Create a matrix for the grouped bar plot\n# Each row represents a fluidity type, each column represents a measurement type\ndata_matrix = hcat(type_p, type_u, type_a, type_t, type_v)\n\n# Create a grouped bar plot\np1 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Dodge (Beside)\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:dodge,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\np2 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Stack\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:stack,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\nplot(p1,p2)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test",
    "text": "\\(\\chi^2\\) Dependency Test\n\nChi-squared statistic, or chi-squared distance:\n\n\\[\\psi(X,Y) = \\sum_{i=1}^I\\sum_{j=1}^J \\frac{(n_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\]\n\n\nApproximation: \\(\\psi(X,Y) \\sim \\chi^2((I-1)(J-1))\\) when \\(n \\to \\infty\\)\nTest: \\(T=\\mathbf 1\\{\\psi(X,Y) \\geq t_{1-\\alpha/2}\\}\\), where\n\\(t_{0.975}\\) = quantile(Chisq(I-1,J-1), 0.975)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-1",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-1",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test",
    "text": "\\(\\chi^2\\) Dependency Test\n\nTotal proportion of individuals \\(k\\) such that \\(Y_k =i\\):\n\\(\\hat p_{i}=\\frac{R_i}{N}\\) \\(= \\tfrac{1}{N}\\sum_{j=1}^{J}n_{ij}\\)\n\n\nChi-squared statistic, or chi-squared distance:\n\n\\[\\psi(X,Y) = \\sum_{i=1}^I\\sum_{j=1}^J \\frac{(n_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\]\n\n\nApproximation: \\(\\psi(X,Y) \\sim \\chi^2((I-1)(J-1))\\) when \\(n \\to \\infty\\)\nTest: \\(T=\\mathbf 1\\{\\psi(X,Y) \\geq t_{1-\\alpha/2}\\}\\), where\n\\(t_{0.975}\\) = quantile(Chisq(I-1,J-1), 0.975)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#definitions",
    "href": "teaching/linear_model/slides/introduction.html#definitions",
    "title": "Introduction",
    "section": "Definitions",
    "text": "Definitions\n\nEntries of the table:\n\n\\[n_{ij} = \\sum_{k=1}^n \\mathbf 1\\{X_{k} = i\\}\\mathbf 1\\{Y_k=j\\}\\]\n\n\n\nTotal proportion of individuals \\(k\\) of color \\(X_k =i\\):\n\n\n\\(\\hat p_{i}=\\frac{R_i}{N}\\) \\(= \\tfrac{1}{N}\\sum_{j=1}^{J}n_{ij}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-problem",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-problem",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test Problem",
    "text": "\\(\\chi^2\\) Dependency Test Problem\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)\n\nWe observe\n\\(X=(X_1, \\dots, X_n) \\in \\{1, \\dots, I\\}^n\\) and \\(Y=(Y_1, \\dots, Y_n) \\in \\{1, \\dots, J\\}^n\\)\n\n\nAssumptions: \\((X_k,Y_k)\\) are independent, each pair has unknown distribution \\(P_{XY}\\)\n\n\ndependency test problem:\n\n\\[H_0: P_{XY}=P_{X}P_Y \\VS H_1: P_{XY} \\neq P_{X}P_{Y}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#box-plots",
    "href": "teaching/linear_model/slides/introduction.html#box-plots",
    "title": "Introduction",
    "section": "Box Plots",
    "text": "Box Plots\n\\(Y=(Y_1, \\dots, Y_n) \\in \\mathbb R^n\\) (Quanti), $X=(X_1, , X_n) {}"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#setting",
    "href": "teaching/linear_model/slides/introduction.html#setting",
    "title": "Introduction",
    "section": "Setting",
    "text": "Setting\n\nWe observe \\(X=(X_1, \\dots, X_n)\\) and \\(Y=(Y_1, \\dots, Y_n)\\), where\n\n\\(X_k \\in \\{1, \\dots, I\\}\\) (Quali)\n\\(Y_k \\in \\mathbb R\\) (Quanti)\n\n\n\nBoxplot: represents \\(0, 25, 75\\) and \\(100\\) percentiles."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-plot",
    "href": "teaching/linear_model/slides/introduction.html#correlation-plot",
    "title": "Introduction",
    "section": "Correlation Plot",
    "text": "Correlation Plot\n\nplot_cor=@df pottery_num corrplot(cols(1:4),grid=false, compact=true) #Julia"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-with-singer-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-with-singer-dataset",
    "title": "Introduction",
    "section": "Example with singer dataset",
    "text": "Example with singer dataset\n\\(X\\): Height, \\(Y\\): Type of singer"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-singer-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-singer-dataset",
    "title": "Introduction",
    "section": "Example: Singer dataset",
    "text": "Example: Singer dataset\n\n\\(X\\): Height (in inches), \\(Y\\): Type of singer"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-singer-dataset-julia-statsplots",
    "href": "teaching/linear_model/slides/introduction.html#example-singer-dataset-julia-statsplots",
    "title": "Introduction",
    "section": "Example: Singer Dataset (Julia: StatsPlots)",
    "text": "Example: Singer Dataset (Julia: StatsPlots)\n\n\\(X\\): Height (in inches), \\(Y\\): Type of singer"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#singer-dataset-julia-statsplots",
    "href": "teaching/linear_model/slides/introduction.html#singer-dataset-julia-statsplots",
    "title": "Introduction",
    "section": "Singer Dataset (Julia StatsPlots)",
    "text": "Singer Dataset (Julia StatsPlots)\n\n\\(X\\): Height (in inches), \\(Y\\): Type of singer"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#definitions-1",
    "href": "teaching/linear_model/slides/introduction.html#definitions-1",
    "title": "Introduction",
    "section": "Definitions",
    "text": "Definitions\n\n\\(X=(X_1, \\dots, X_n) \\in \\{1, \\dots, I\\}^n\\), \\(Y=(Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\nif \\(i \\in \\{1, \\dots, I\\}\\), we define partial means as\n\n\n\\[N_i = \\sum_{k=1}^n \\mathbf 1\\{X_k=i\\} \\and \\overline Y_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\mathbf 1\\{X_k=i\\}\\]\n\n\n\n\nTotal mean:\n\\(Y_i = \\frac{1}{N}\\sum_{k=1}^n Y_k = \\frac{1}{N}\\sum_{i=1}^I\\sum N_i \\overline Y_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#variance-decomposition",
    "href": "teaching/linear_model/slides/introduction.html#variance-decomposition",
    "title": "Introduction",
    "section": "Variance Decomposition",
    "text": "Variance Decomposition\n\n\n\n\\[\\frac{1}{n}\\underbrace{\\sum_{k=1}^n(Y_k - \\overline Y)^2}_{SST} =\n\\frac{1}{n}\\underbrace{\\sum_{i=1}^IN_i(\\overline Y_i - \\overline Y)^2}_{SSB}\n+ \\frac{1}{n}\\underbrace{\\sum_{k=1}^n\\mathbf 1\\{X_k=i\\}(Y_k - \\overline Y_i)^2}_{SSW}\\]\n\n\n\n\ncorrelation ratio:\n\n\\[ \\hat \\eta^2 = \\frac{SSB}{SST}  \\in [0,1]\\]\n\n\n\nThis is an estimator of unknown \\(\\eta = \\frac{\\mathbb V(\\mathbb E[Y|X])}{\\mathbb V(Y)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#anova-test",
    "href": "teaching/linear_model/slides/introduction.html#anova-test",
    "title": "Introduction",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\\((X_1, \\dots, X_n) \\in \\{1, \\dots I\\}^n\\)\n\\((Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\nAssumption: \\(Y_i\\) are Gaussian of same variance. \\(\\mu_i = \\mathbb E[Y|X=i]=\\frac{\\mathbb E[Y\\mathbf 1\\{X=i\\}]}{\\mathbb P(X=i)}\\) (unknown)\n\n\nProblem:\n\n\\[H_0: \\mu_1=\\dots \\mu_I \\VS H_1: \\mu_i \\neq \\mu_j \\text{ for some $i,j$}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#anova-test-1",
    "href": "teaching/linear_model/slides/introduction.html#anova-test-1",
    "title": "Introduction",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\nTest Statistic\n\n\\[\\psi(X,Y) = \\frac{SSB/(I-1)}{SSW/(N-I)}\\]\n\n\n\n\\(\\psi(X,Y) \\sim \\mathcal F(I-1, N-I)\\) under \\(H_0\\)\n\n\npvalue:\n1-cdf(FDist(I-1, N-1), psiobs)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#context",
    "href": "teaching/linear_model/slides/introduction.html#context",
    "title": "Introduction",
    "section": "Context",
    "text": "Context\n\n\\(n\\) individuals, \\(p\\) explanatory variables \\(X=(X^{(1)}, \\dots, X^{(p)})\\).\nGoal: Explain/Predict \\(Y\\) in function of \\(X\\)\n\n\nWe observe\n\\[Y=\\begin{pmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X=\\begin{pmatrix}\nX^{(1)}_1 & \\cdots & X^{(p)}_1 \\\\\nX^{(1)}_2 & \\cdots & X^{(p)}_1 \\\\\n\\vdots & & \\vdots \\\\\nX^{(1)}_n & \\cdots & X^{(p)}_1\n\\end{pmatrix}\\]\nEach individual \\(i\\) correspond to \\(Y_i\\) a row \\((X^{(1)}_i, \\dots, X^{(p)}_i)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#where-is-randomness",
    "href": "teaching/linear_model/slides/introduction.html#where-is-randomness",
    "title": "Introduction",
    "section": "Where is Randomness?",
    "text": "Where is Randomness?\nGenerally, we don’t know any values a priori. Example:\n\nindividual characteristics of a customer\n\n\n\\(Y\\) and \\(X^{(1)}, \\ldots, X^{(p)}\\) are random variables.\n\n\nWe observe realizations the \\(Y\\)’s and \\(X\\)’s\n\n\nSometimes \\(X = (X^{(1)}, \\ldots, X^{(p)})\\) is chosen a priori. Example:\n\n\\(X\\): medication dosages (and \\(Y\\): a physiological response)\n\n\n\nIn this context, \\(Y\\) is random, but \\(X^{(1)}, \\ldots, X^{(p)}\\) are not."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#what-relation",
    "href": "teaching/linear_model/slides/introduction.html#what-relation",
    "title": "Introduction",
    "section": "What Relation?",
    "text": "What Relation?\nGoal: to best approximate \\(Y\\) as a function of \\(X = (X^{(1)}, \\ldots, X^{(p)})\\).\nMathematically: we seek the “best” function \\(f\\) of \\(X\\) that approximates \\(Y\\).\nBest in the sense of quadratic cost: we seek \\(f\\) that minimizes \\(E[(Y - f(X^{(1)}, \\ldots, X^{(p)}))^2]\\).\nThe solution is known, it is: \\(f(X^{(1)}, \\ldots, X^{(p)}) = E(Y|X^{(1)}, \\ldots, X^{(p)})\\).\nStatistically: we therefore seek to estimate \\(f(x) = E(Y|X = x)\\) (for all relevant \\(x\\)) from the \\(n\\) realizations of the pair \\((Y, X)\\).\nThis is the objective of most machine learning models.\nBut… estimating a function of \\(p\\) variables is somewhat ambitious. \\(\\Rightarrow\\) we generally make assumptions about the form of \\(E(Y|X = x)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-1",
    "href": "teaching/linear_model/slides/introduction.html#example-1",
    "title": "Introduction",
    "section": "Example 1",
    "text": "Example 1\nIf we assume that the distribution of \\((Y, X)\\) is Gaussian. Then:\n\\(E(Y|X) = E(Y) + (X - E(X))'\\beta\\)\nwhere $= ^{-1}(Cov(Y, X^{(1)}), , Cov(Y, X^{(p)}))’$ and where \\(\\Sigma = V(X)\\).\nIn other words, in the Gaussian case,\n\\(f(X^{(1)}, \\ldots, X^{(p)}) = \\beta_0 + \\beta_1X^{(1)} + \\cdots + \\beta_p X^{(p)}\\)\ndenoting \\(\\beta = (\\beta_1, \\ldots, \\beta_p)\\) and \\(\\beta_0 = E(Y) - E(X)'\\beta\\).\nThe function \\(f\\) we are looking for is simply an affine function in \\(X^{(1)}, \\ldots, X^{(p)}\\).\nThe parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are unknown a priori.\nBut the estimation problem becomes much simpler: estimating the function \\(f\\) in this context simply amounts to estimating these parameters."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-2",
    "href": "teaching/linear_model/slides/introduction.html#example-2",
    "title": "Introduction",
    "section": "Example 2",
    "text": "Example 2\nRather than making an assumption about the distribution of \\((Y, X)\\), we can make an assumption about the form of \\(f(X^{(1)}, \\ldots, X^{(p)}) = E(Y|X^{(1)}, \\ldots, X^{(p)})\\).\nIf we reduce the space of possibilities for \\(f\\), we facilitate its estimation.\nSimplest hypothesis: \\(f\\) is affine\n\\(f(X^{(1)}, \\ldots, X^{(p)}) = \\beta_0 + \\beta_1X^{(1)} + \\cdots + \\beta_p X^{(p)}\\)\nfor certain parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\), which will need to be estimated.\nThis is the exact form of \\(f\\) when \\((Y, X)\\) is Gaussian (Example 1).\nIt is an approximation, more or less good, in other cases.\nThis is the framework of linear regression."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-3",
    "href": "teaching/linear_model/slides/introduction.html#example-3",
    "title": "Introduction",
    "section": "Example 3",
    "text": "Example 3\nExample 3. In some cases, \\(f\\) is constrained.\nIf \\(Y\\) takes only 2 values (say 0 and 1), then \\(E(Y|X^{(1)}, \\ldots, X^{(p)}) = P(Y = 1|X^{(1)}, \\ldots, X^{(p)})\\)\nTherefore \\(f\\) has values in \\([0, 1]\\)\nIn this case, an affine form is not suitable (because it has values in \\(\\mathbb{R}\\))\nHowever, it is possible to transform \\(f\\) to assume it is affine.\nLet \\(g\\) be a function (of our choice) that goes from \\([0, 1]\\) to \\(\\mathbb{R}\\), we can assume that \\(g(f(X^{(1)}, \\ldots, X^{(p)}))\\) is affine.\nThis modeling makes sense and requires few parameters to estimate.\nThis approach falls within the framework of generalized linear regression."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#complements",
    "href": "teaching/linear_model/slides/introduction.html#complements",
    "title": "Introduction",
    "section": "Complements",
    "text": "Complements\nWe have seen that the general objective is to estimate \\(f(X^{(1)}, \\ldots, X^{(p)}) = E(Y|X^{(1)}, \\ldots, X^{(p)})\\).\nThis viewpoint implicitly assumes: - that \\(X^{(1)}, \\ldots, X^{(p)}\\) are random variables, - that all variables are quantitative.\nWhat happens otherwise?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#complements-case-of-deterministic-x1-ldots-xp",
    "href": "teaching/linear_model/slides/introduction.html#complements-case-of-deterministic-x1-ldots-xp",
    "title": "Introduction",
    "section": "Complements: case of deterministic \\(X^{(1)}, \\ldots, X^{(p)}\\)",
    "text": "Complements: case of deterministic \\(X^{(1)}, \\ldots, X^{(p)}\\)\nLet’s assume that \\(Y\\) is random but not \\(X^{(1)}, \\ldots, X^{(p)}\\).\nIn this case, the goal remains to best explain \\(Y\\) as a function of \\(X^{(1)}, \\ldots, X^{(p)}\\) (deterministic).\nWe assume that the distribution of \\(Y\\) depends on \\(X^{(1)}, \\ldots, X^{(p)}\\). Otherwise, the initial objective would not make sense.\nIn particular, the expectation of \\(Y\\) depends on \\(X^{(1)}, \\ldots, X^{(p)}\\), i.e., \\(E(Y) = f(X^{(1)}, \\ldots, X^{(p)})\\).\nAnd we seek to estimate \\(f\\): the objective is therefore similar to the previous case.\nIn both cases, we seek to estimate the expectation of \\(Y\\) given the variables \\(X^{(1)}, \\ldots, X^{(p)}\\).\nThe estimation methods are identical.\nExample 3. In some cases, \\(f\\) is constrained.\nIf \\(Y\\) takes only 2 values (say 0 and 1), then \\(E(Y|X^{(1)}, \\ldots, X^{(p)}) = P(Y = 1|X^{(1)}, \\ldots, X^{(p)})\\)\nTherefore \\(f\\) has values in \\([0, 1]\\)\nIn this case, an affine form is not suitable (because it has values in \\(\\mathbb{R}\\))\nHowever, it is possible to transform \\(f\\) to assume it is affine.\nLet \\(g\\) be a function (of our choice) that goes from \\([0, 1]\\) to \\(\\mathbb{R}\\), we can assume that \\(g(f(X^{(1)}, \\ldots, X^{(p)}))\\) is affine.\nThis modeling makes sense and requires few parameters to estimate.\nThis approach falls within the framework of generalized linear regression."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#complements-case-of-categorical-variables",
    "href": "teaching/linear_model/slides/introduction.html#complements-case-of-categorical-variables",
    "title": "Introduction",
    "section": "Complements: case of categorical variables",
    "text": "Complements: case of categorical variables\nWe encode each categorical variable using indicator variables. This is called “one hot encoding” in machine learning.\n\nSuppose that \\(Y\\) is a categorical variable with 2 modalities (“A” or “not A”). To model \\(Y\\), we model \\(\\tilde{Y} = 1_{Y=\"A\"}\\), that is \\(\\tilde{Y} = \\begin{cases}\n1 & \\text{if } Y = \\text{\"A\"}\\\\\n0 & \\text{otherwise}.\n\\end{cases}\\)\n\nThis binary variable can then be modeled as in example 3.\n\nSimilarly, if \\(Y\\) takes \\(K\\) modalities \\(A_1, \\ldots, A_K\\), we introduce the \\(K\\) variables \\(\\tilde{Y}_k = 1_{Y=\"A_k\"}\\). Modeling \\(Y\\) in this case therefore amounts to modeling \\(K\\) binary variables. In reality only \\(K - 1\\) because the last modality can be deduced from the others (\\(\\sum_{k=1}^K \\tilde{Y}_k = 1\\)).\nWe apply the same transformation to the variables \\(X^{(1)}, \\ldots, X^{(p)}\\) if needed. This transformation, however, brings some specificities in the writing of the model and its interpretation: see the chapter on ANOVA and ANCOVA."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#where-is-randomness-1",
    "href": "teaching/linear_model/slides/introduction.html#where-is-randomness-1",
    "title": "Introduction",
    "section": "Where is Randomness?",
    "text": "Where is Randomness?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#summary",
    "href": "teaching/linear_model/slides/introduction.html#summary",
    "title": "Introduction",
    "section": "Summary:",
    "text": "Summary:\n\n\\(Y\\) is always viewed as a random variable\n\n\n\\(X^{(1)}, \\ldots, X^{(p)}\\) are viewed as random variables or deterministic variables, depending on the context"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#general-model",
    "href": "teaching/linear_model/slides/introduction.html#general-model",
    "title": "Introduction",
    "section": "General Model",
    "text": "General Model\n\n\\(Y=(Y_1, \\dots, Y_n)\\)\n\\(X^{(k)} = (X^{(k)}_1, \\dots, X^{(k)}_n)\\), \\(k= 1, \\dots, p\\) (row notation)\n\n\nGeneral model:\n\n\\[Y_i = F(X_i^{(1)}, \\dots, X_i^{(p)}, \\varepsilon_i)\\]\n\n\n\\(F\\) is an unknown and deterministic function of \\(p\\) variables.\n\\(\\varepsilon_i\\) are iid random representing external independent noise\n\n\n\nNonparametric problem: space of all \\(f\\) is of infinite dimension!"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-model",
    "href": "teaching/linear_model/slides/introduction.html#linear-model",
    "title": "Introduction",
    "section": "Linear Model",
    "text": "Linear Model\n\nIdea: reduce to a smaller class of function \\(F \\in \\mathcal F\\).\n\n\nLinear Model:\n\n\\[\nY_i = \\mu + \\beta_1 X^{(1)}_i + \\beta_2 X^{(2)}_i + \\dots + \\beta_p X^{(p)}_i + \\sigma \\varepsilon_i\n\\]\n\n\n\nSpace of affine function:\n\\[\n\\mathcal F = \\{F:~ F(x, \\varepsilon) = \\mu + \\beta^T x + \\sigma \\varepsilon, (\\mu, \\beta, \\sigma) \\in \\mathbb R^{p+2}\\}\n\\]\n\n\n\\(\\dim(\\mathcal F) = p+2\\) (number of unknown parameters)\n\n\nmuch easier to estimate \\(f\\) (and perhaps less overfitting)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#wait-what-is",
    "href": "teaching/linear_model/slides/introduction.html#wait-what-is",
    "title": "Introduction",
    "section": "Wait, what is + ?",
    "text": "Wait, what is + ?\n\nIf the \\(X^{(k)}\\) are qualitative factors,\n\n\nWhat is the meaning of\n\\[\nY_i = \\mu + \\beta_1 X^{(1)}_i + \\beta_2 X^{(2)}_i + \\dots + \\beta_p X^{(p)}_i + \\sigma \\varepsilon_i\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#wait-what-is-for-qualitative",
    "href": "teaching/linear_model/slides/introduction.html#wait-what-is-for-qualitative",
    "title": "Introduction",
    "section": "Wait… what is + for Qualitative ?",
    "text": "Wait… what is + for Qualitative ?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#case-of-categorical-variables",
    "href": "teaching/linear_model/slides/introduction.html#case-of-categorical-variables",
    "title": "Introduction",
    "section": "Case of categorical variables",
    "text": "Case of categorical variables\n\nEncode each category\n\n\nIf \\(Y \\in \\{A, B\\}\\) has \\(2\\) categories, we encode \\[\\widetilde Y = \\mathbf 1\\{Y = A\\}\\]\n\n\nIf \\(Y\\in \\{A_1, \\dots, A_k\\}\\), we use one hot encoding:\n\\[\\widetilde{Y}_k = \\mathbf 1\\{Y=A_k\\}\\]\n\nAlso encode \\(X^{(1)}, \\ldots, X^{(p)}\\) if needed.\nsee also the chapter on ANOVA and ANCOVA."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x1-ldots-xp",
    "href": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x1-ldots-xp",
    "title": "Introduction",
    "section": "From random to deterministic \\(X^{(1)}, \\ldots, X^{(p)}\\)",
    "text": "From random to deterministic \\(X^{(1)}, \\ldots, X^{(p)}\\)\n\nIf \\(X^{(1)}, \\ldots, X^{(p)}\\) are random,\n\n\nThen for all deterministic \\(x^{(1)}, \\dots, x^{(p)}\\)\n\n\nConditionnally to \\((X^{(1)}=x^{(1)}, \\dots, X^{(p)} =x^{(p)})\\), we have the general model\n\\[Y = F(x^{(1)},\\dots, x^{(p)}, \\varepsilon)\\]\n\nBecause \\(\\varepsilon\\) is independent of \\(X\\)\nReplace \\(X^{(k)}\\) by their observations \\(x^{(k)}\\). The only randomness is now in \\(\\varepsilon\\)!\n\nExample 3. In some cases, \\(f\\) is constrained.\nIf \\(Y\\) takes only 2 values (say 0 and 1), then \\(E(Y|X^{(1)}, \\ldots, X^{(p)}) = P(Y = 1|X^{(1)}, \\ldots, X^{(p)})\\)\nTherefore \\(f\\) has values in \\([0, 1]\\)\nIn this case, an affine form is not suitable (because it has values in \\(\\mathbb{R}\\))\nHowever, it is possible to transform \\(f\\) to assume it is affine.\nLet \\(g\\) be a function (of our choice) that goes from \\([0, 1]\\) to \\(\\mathbb{R}\\), we can assume that \\(g(f(X^{(1)}, \\ldots, X^{(p)}))\\) is affine.\nThis modeling makes sense and requires few parameters to estimate.\nThis approach falls within the framework of generalized linear regression."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x",
    "href": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x",
    "title": "Introduction",
    "section": "From random to deterministic \\(X\\)",
    "text": "From random to deterministic \\(X\\)\n\nIf \\(X^{(1)}, \\ldots, X^{(p)}\\) are random,\n\n\nThen for all deterministic \\(x^{(1)}, \\dots, x^{(p)}\\)\n\n\nConditionnally to \\((X^{(1)}=x^{(1)}, \\dots, X^{(p)} =x^{(p)})\\), we have the general model\n\\[Y = F(x^{(1)},\\dots, x^{(p)}, \\varepsilon)\\]\n\nBecause \\(\\varepsilon\\) is independent of \\(X\\)\nReplace \\(X^{(k)}\\) by their observations \\(x^{(k)}\\).\nThe only randomness is now in \\(\\varepsilon\\)!"
  }
]