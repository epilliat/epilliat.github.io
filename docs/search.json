[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentation",
    "section": "",
    "text": "I am an assistant professor in statistics at ENSAI (Rennes) working on topics related to machine learning and high-dimensional statistics. My PhD centered on two areas in modern statistics: change-point detection and ranking problems.\nI am interested in GPU parallel computing. I’m currently developing a Julia library focused on optimizing fundamental functions like mapreduce (for operations such as sum) and accumulate (for operations like prefix sum). You can explore my experimental work at my GitHub repository Luma for more technical details.\ncontact: firstname.lastname@ensai.fr"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Presentation",
    "section": "Publications",
    "text": "Publications\n\nE. Pilliat, Recovering Labels from Crowdsourced Data: An Optimal and Polynomial-Time Method (2025) COLT [presentation]\nA. Garivier, E. Pilliat, On Sparsity and Sub-Gaussianity in the Johnson-Lindenstrauss Lemma (2024) [arxiv]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal rates for ranking a permuted isotonic matrix in polynomial time (2024) SODA [presentation]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal permutation estimation in crowd-sourcing problems (2023) Annals of Statistics. [arxiv], [presentation],[poster]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal multiple change-point detection for high-dimensional data (2023) EJS [arxiv]"
  },
  {
    "objectID": "index.html#vitae",
    "href": "index.html#vitae",
    "title": "Presentation",
    "section": "Vitae",
    "text": "Vitae\n\nResearch\n\nSep 2024 - Present: assistant professor in statistics at ENSAI\n2024: Postdoctoral researcher at ENS Lyon\nFeb 2020 - Apr 2022 and Oct 2022 - Dec 2023: PhD Student at Université de Montpellier and INRAE\n2019: Research Project at Ecole Normale Supérieure Paris Saclay with Vianney Perchet on Optimal Order Selection for an Online Reward Maximization problem\nApr 2019: Research Project at OVGU Magdeburg with Alexandra Carpentier on Signal Detection and Change-Point Detection\n2018: Research Internship at the University of Cambridge on Gaussian Free Field\n\nExperience\n\nApr 2022 - Oct 2022: Quantitative Intern at QRT\n\nEducation\n\n2016-2020: Student at Ecole Normale Supérieure de Lyon\n2013-2016: Preparatory School of Mathematics and Physics (CPGE MPSI/MP*) in Strasbourg\n\nTeaching\n\n2020-2022: Teaching assistant at Université de Montpellier."
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#introduction",
    "href": "research/presentations/crowdsourcing/presentation.html#introduction",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Introduction",
    "text": "Introduction\n\nThank you for having me here today.\nFor context, much of this work was conducted during my postdoctoral position at ENS Lyon, where was able to build upon my earlier PhD research in crowdsourcing.\nSo, let’s start with a scenario. Picture a group of workers, and each one is given binary classification tasks.\nFor each task, they need to make yes-or-no decisions.\nThere are many situations where this happens. Think of image classification, text moderation, or sentiment analysis\n\n\nWorkers are given binary tasks to which they have to give a response: YES or NO\nExamples\n\nImage Classification: “Does this image contain a dog?”\nText Moderation: “Is this comment toxic or offensive?”\nSentiment Analysis: “Does this review express positive sentiment?”"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#questions",
    "href": "research/presentations/crowdsourcing/presentation.html#questions",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Questions",
    "text": "Questions\n\nThree natural questions emerge\n\nwhat are the actual true labels?\nCan we compare workers and identify who performs better or worse?\nHow well do workers perform on a given task?\n\nI will touch on all these issues during my talk, but my primary focus will be on the central question of recovering the true labels.\n\n\n3 questions\n\nrecover the true label?\nrank the workers?\nestimate their abilities?\n\n\n\nMain Quetion: How can we accurately recover the labels?"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#this-talk",
    "href": "research/presentations/crowdsourcing/presentation.html#this-talk",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "This Talk",
    "text": "This Talk\n\nI will break down my presentation in 3 parts.\nI’ll start by introducing isotonic model.\nNext, I’ll review two existing methods.\nIn the final part, I’ll go over an optimal and computationnally feasible method for this problem.\n\n\nIntroducing the non-parametric isotonic model and main result\nPresenting two already existing algorithms\nIterative Spectral Voting (ISV) algo and key insights"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#illustration",
    "href": "research/presentations/crowdsourcing/presentation.html#illustration",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration",
    "text": "Illustration\n\nLet me start with an illustration.\nConsider binary tasks where labels are either \\(-1\\) or \\(+1\\). There is a vector \\(x^*\\) of true labels that we do not know.\nWe observe a matrix \\(Y\\) where each entry \\((i,k)\\) represents the response of worker \\(i\\) to task \\(k\\).\nWe can have missing values. When a worker doesn’t respond to a particular task, we simply put \\(0\\) in that matrix position.\nWe write \\(\\lambda\\) for the rate of partial observations .\n\n\nTwo binary tasks with labels in \\(\\{\\color{green}{-1},\\color{blue}{+1}\\}\\)\nUnknown vector \\(x^*\\) of labels with \\(d=9\\) tasks \\(x^*= (\\color{blue}{+1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1})\\)\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1}\n\\end{array}\\right)\n\\]\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{red}{0} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0}\n\\end{array}\\right)\n\\]\n\n\n\n\nRate of observations: \\(\\color{red}{\\lambda= 0.72}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#observation-model",
    "href": "research/presentations/crowdsourcing/presentation.html#observation-model",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nWe assume that the matrix of responses follows this model.\n\n\\(x^*\\) is the vector of unknown labels\n\\(M \\in [0,1]^{n \\times d}\\) is the ability matrix. If \\(M_{ik}=1\\), it means that worker \\(i\\) is very good at question \\(k\\), and \\(M_{ik}=0\\) means that they answer randomly.\n\\(E\\) is a matrix of independent sub-gaussian noise\nThis odot is a Hadamard Product so you can think of \\(B\\) as a Bernoulli mask matrix.\n\n\n\nWe observe\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\nwhere \\(\\odot\\) is the Hadamard product.\n\n\\(x^*\\in \\{-1, 1\\}^d\\) is the vector of unknown labels\n\\(M \\in [0,1]^{n \\times d}\\) is the ability matrix\n\\(E\\) is a matrix of independent sub-gaussian noise\n\\(B\\) is a Bernoulli \\(\\mathcal B(\\lambda)\\) “mask” matrix, modelling partial observations"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#shape-constraints",
    "href": "research/presentations/crowdsourcing/presentation.html#shape-constraints",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nUntil now, I have’nt assumed anything on the ability matrix \\(M\\) except that it has coef. in \\([0,1]\\).\nFrom now on, we also assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) of its rows, that is,\nit has increasing columns, up to an unknown permutation \\(\\pi^*\\)\nIt means that, on average, a worker \\(i\\) is either uniformly better or uniformly worse than another worker \\(j\\)\n\n\n\\(M \\in [0,1]^{n \\times d}\\) represents the ability matrix of worker/task pairs\nWe assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) , that is\n\n\n\n\\[\nM_{\\pi^*}=\\left(\\begin{array}{ccccccccc}\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8} \\\\\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6}\n\\end{array}\\right)\n\\]\n\n\n\\[\nM_{\\phantom{{\\pi^*}}}=\\left(\\begin{array}{ccccccccc}\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6} \\\\\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "href": "research/presentations/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Square Norm Loss VS Hamming Loss",
    "text": "Square Norm Loss VS Hamming Loss\n\nA usual loss for an estimator \\(\\hat x\\) is the Hamming loss. It simply consists in summing up all the mistakes we’ve made.\nI do not consider Hamming Loss Here. Instead, we restrict \\(M\\) to incorrectly labeled task, and take the square frobenius norm.\n\nIf workers are bad (\\(M\\) close to \\(0\\)), square norm loss is small but Hamming Loss can be of order \\(d\\)\nHence, a very good feature of the square norm loss is that it evaluates the quality of the estimator \\(\\hat x\\) instead of the performance of the workers!\n\n\n\n\n\n\nHamming Loss: \\[ \\sum_{k=1}^d \\mathbf 1\\{\\hat x_k \\neq x_k^*\\}\\]\n\n\n\n\n\nSquare Norm Loss: \\[ \\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\]\n\n\n\n\nIf workers are bad (\\(M \\sim 0\\)), \\(M\\) is small but Hamming Loss is large (\\(\\sim d\\))\nSquare norm loss evaluates the quality of \\(\\hat x\\) rather than performances of workers"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#crowdsourcing-problems",
    "href": "research/presentations/crowdsourcing/presentation.html#crowdsourcing-problems",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\nIn addition to recovering labels, we can define the two other problems I mentioned earlier:\nranking the workers and estimating their abilities. Each objective corresponds to a similar square norm loss.\n\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\n\nObjectives\n\n\nRecovering the labels (\\(x^*\\))\n\n\nRanking the workers (\\(\\pi^*\\))\n\n\nEstimating their abilities (\\(M\\))\n\n\n\nLosses\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\)\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2\\)\n\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M - \\hat M\\|_F^2\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "href": "research/presentations/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\nObjectives\nRecovering the labels (\\(x^*\\))\n\nRanking the workers (\\(\\pi^*\\))\n\nEstimating their abilities (\\(M\\))\n\nRisks\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#minimax-risk-for-recovering-labels",
    "href": "research/presentations/crowdsourcing/presentation.html#minimax-risk-for-recovering-labels",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "MiniMax Risk for Recovering Labels",
    "text": "MiniMax Risk for Recovering Labels\n\nThis brings us to the minimax risk for recovering labels.\nIt is defined as the risk of the best estimator \\(\\hat x\\) in the worst case.\n\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\n\nminimize on all estimator \\(\\hat x\\)\nmaximize square norm loss on all \\(M\\), \\(\\pi^*\\), \\(x^*\\) such that \\(M_{\\pi^*}\\) is isotonic"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#main-results",
    "href": "research/presentations/crowdsourcing/presentation.html#main-results",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Results",
    "text": "Main Results\n\nThe main result I want to present, is that there is no computational-statistical gap here.\nThere is a polynomial-time methods which nearly achieves the minimax risk for recovering the labels.\nMoreover, the minimax risk is of order \\(d/\\lambda\\)\n\n\n\n\n\nTheorem\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), there exists a poly. time method \\(\\hat x\\) achieving \\(\\mathcal R^*_{\\mathrm{reco}}\\) up to polylog factors, i.e. for all \\(M\\in[0,1]^{n\\times d}\\), \\(\\pi^*\\) and \\(x^* \\in \\{-1,1\\}^d\\), \\[\n\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2] \\lesssim \\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\enspace .\n\\]\n\nMoreover, up to polylogs,\n\n\\[\n\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\asymp \\frac{d}{\\lambda} \\enspace .\n\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#majority-voting",
    "href": "research/presentations/crowdsourcing/presentation.html#majority-voting",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Majority Voting",
    "text": "Majority Voting\n\nMajority voting is the simplest method you can think of. Sum each column of \\(Y\\) and take the sign to estimate each label.\nUnfortunately, it does not achieve the optimal \\(d/\\lambda\\) rate. So we have this additional \\(\\sqrt{n}\\) compared to the minimax risk.\n\n\n\n\\(\\hat x^{(maj)}_k = \\mathrm{sign} \\left( \\sum_{i=1}^n Y_{ik} \\right)\\)\n\n\n\nMaximum risk of \\(\\hat x^{(maj)}\\):\n\n\\[\\max_{M,\\pi^*, x^*}\\mathbb E\\|M\\mathrm{diag}(\\hat x^{maj} \\neq x^*)\\|_F^2 \\asymp \\tfrac{d \\sqrt{n}}{\\lambda}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "href": "research/presentations/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "OBI-WAN (Shah et al., 2020)",
    "text": "OBI-WAN (Shah et al., 2020)\n\nIn the Obi-Wan Method from Shah and co. The main idea is to use that \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T\\) does not depend on \\(x^*\\).\nand to exploit this fact by computing the leading left eigen vector of \\(Y\\)\n\n\nIdea: Population term \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T= MM^T\\) is independent of \\(x^*\\)\n\n\nPCA Step:\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "href": "research/presentations/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Result on Obi-Wan Method",
    "text": "Result on Obi-Wan Method\n\nIt turns out that the Obi-Wan method is minimax optimal in a rank \\(1\\) model, it achieves the rate \\(d/\\lambda\\).\nHowever, it is not minimax optimal in the isotonic model. It only achieves a rate that is comparable to majority voting in the worst case.\n\n\n\n\n\n\nTheorem 1 (Shah et al., 2020)\n\n\nIn submodel where \\(\\mathrm{rk}(M)= 1\\), \\(\\hat x^{(\\mathrm{Obi-Wan})}\\) achieves minimax risk up to polylogs: \\[\\mathbb E\\|M\\mathrm{diag}(\\hat x^{\\text{Obi-Wan}} \\neq x^*)\\|_F^2 \\lesssim \\frac{d}{\\lambda} \\quad \\textbf{(minimax)}\\]\n\n\n\n\n\n\n\n\n\n\nTheorem 2 (Shah et al., 2020)\n\n\nIn the model where \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic up to polylogs: \\[\\mathbb E\\|M\\mathrm{diag}(\\hat x^{\\text{Obi-Wan}} \\neq x^*)\\|_F^2 \\lesssim \\frac{d\\sqrt{n}}{\\lambda} \\quad \\textbf{(not minimax)}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#pca-step",
    "href": "research/presentations/crowdsourcing/presentation.html#pca-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "PCA Step",
    "text": "PCA Step\n\nFirst, we compute the leading eigenvector as Shah et al. did.\nThe insight is that since \\(M\\) is isotonic, then we can prove that \\(M\\) is close to low rank, so it makes sense to look for leading eigen vectors\n\n\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]\n\n\n\nMain idea: Since \\(M\\) is isotonic, \\(M\\) is close to low rank\n\n\\[\\|MM^T\\|_{\\mathrm{op}} \\gtrsim \\|M\\|_F^2\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#voting-step",
    "href": "research/presentations/crowdsourcing/presentation.html#voting-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Voting Step",
    "text": "Voting Step\n\nWithout going into too much details, the idea is then to do define weights and perform weighted votes.\n\n\nDefine the weighted vote vector (on second sample)\n\n\\[\\hat w = \\hat v^T Y\\]\n\n\n\nDefine the estimated label as\n\n\\[\\hat x_k^{(1)} = \\mathrm{sign}(\\hat w_k)\\mathbf{1}\\bigg\\{|\\hat w_k| \\gg \\sqrt{\\sum_{i=1}^n \\hat v_i B_{ik}}\\bigg\\}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#iterate-these-two-steps",
    "href": "research/presentations/crowdsourcing/presentation.html#iterate-these-two-steps",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Iterate these two Steps",
    "text": "Iterate these two Steps\n\nWe then iterate these PCA and voting steps on labels that are left uncertain,\nuntil we get a final estimator.\n\n\nKeep certain labels: if \\(\\hat x^{(t)}_k\\neq 0\\), set \\(\\hat x^{(t+1)}_k= \\hat x^{(t)}_k\\).\n\n\nRestrict columns \\(k\\) of \\(Y\\) to uncertain labels \\(\\hat x^{(t)}_k=0\\)\n\n\nRepeat PCA Step + Voting Step on \\(Y \\mathrm{diag}(\\hat x^{(t)} \\neq 0)\\) a polylogarithmic number of times.\n\n\nOutput last estimator \\(\\hat x^{(T)}\\)\n\\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#ranking-and-estimating-abilities",
    "href": "research/presentations/crowdsourcing/presentation.html#ranking-and-estimating-abilities",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Ranking and Estimating Abilities",
    "text": "Ranking and Estimating Abilities\n\nThis approach allows us to recover some of the true labels.\nThen, we can do ranking and ability estimation!\nTo do that, we first restrict matrix of responses \\(Y\\) to the labels for which we have a good guess\nThen, we use previous work in the litterature to estimate \\(\\pi^*\\) and \\(M\\)\n\n\nUse ISV to recover some labels \\(\\hat x\\)\nRestrict to columns \\(k\\) such that \\(\\hat x_k \\in \\{-1, 1\\}\\)\nUse methods from Pilliat et al. (2024) to estimate \\(\\pi^*\\) and \\(M\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation.html#conclusion",
    "href": "research/presentations/crowdsourcing/presentation.html#conclusion",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nLet me wrap up my talk in three key points.\n\nFirst, there are three interconnected problems in crowdsourcing: recovering labels, ranking workers and estimating their abilities.\nSecond, we use the isotonic model because it’s very flexible for tackling these problems.\nMost importantly, there is no computational statistical gap.\n\n\n\nThree connected problems: recovering labels, ranking workers and estimating their abilities\nNon parametric isotonic model very flexible in crowdsourcing problems\nNo computational-statistical gap recovering labels, ranking or estimating ability"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I currently am a teaching three topics in statistics at ENSAI Rennes: Hypothesis Testing, Linear and Generalized Linear Model and Times Series (TD 2024).\n\nIntroduction to Hypothesis Testing\nSee the Glossary for English/French translations. See also the note recalling some notion on Density and Likelihood.\n\nSlidesLecturesExercisesAnnalsPluto Notebooks\n\n\n\nTesting Models\nClassical Tests on Gaussian Populations\nGoodness of Fit and Homogeneity Tests\nTests for Dependency\n\n\n\n\nTesting Models\n\nClassical Tests on Gaussian Populations\nGoodness of Fit and Homogeneity Tests\nTests for Dependency\n\n\n\n\nTD1\nTD2\nTD3\nTD4\nTP\n\n\n\n\nExam 2025\nCorrection 2025\n\n\n\n\nProba Basics\nApproximation of Distributions\nIllustration of pvalue\n\n\n\n\n\n\nLinear and Generalized Linear Model\n\nSlides\n\n\n\nLinear Model\n\nIntroduction\nDefinition of the Linear Model\nInference\nValidation\nModel Selection\nANOVA/ANCOVA\n\n\n\nGeneralized Linear Model\n\nIntroduction to GLM\nLogistic Model\nModels for Categorical Data\nCounting Models"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html",
    "href": "teaching/linear_model/lectures/linear_model.html",
    "title": "Linear Regression Model",
    "section": "",
    "text": "AI was used to assist with the formatting and writing of the proofs on this page."
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#proof-of-the-gauss-markov-theorem",
    "href": "teaching/linear_model/lectures/linear_model.html#proof-of-the-gauss-markov-theorem",
    "title": "Linear Regression Model",
    "section": "Proof of the Gauss-Markov Theorem",
    "text": "Proof of the Gauss-Markov Theorem\n\nSetup\nLet \\(\\tilde{\\beta} = CY\\) be any linear unbiased estimator of \\(\\beta\\), where \\(C\\) is an \\(n \\times p\\) matrix of constants.\n\n\nStep 1: Unbiasedness Constraint\nSince \\(\\tilde{\\beta}\\) is unbiased: \\(\\E[\\tilde{\\beta}] = \\beta\\) for all \\(\\beta\\)\n\\[\\E[\\tilde{\\beta}] = \\E[CY] = \\E[C(X\\beta + \\varepsilon)]\\]\n\\[= CX\\beta + C\\E[\\varepsilon] = CX\\beta\\]\nFor unbiasedness: \\(CX\\beta = \\beta\\) for all \\(\\beta\\)\nTherefore: \\(CX = I\\) (the \\(p \\times p\\) identity matrix)\n\n\nStep 2: Express Any Linear Unbiased Estimator\nSince \\(CX = I\\), we can write: \\[C = (X^TX)^{-1}X^T + D\\]\nwhere \\(D\\) is any matrix satisfying \\(DX = 0\\).\nVerification: \\((X^TX)^{-1}X^TX + DX = I + 0 = I\\) ✓\n\n\nStep 3: Express the Estimator\n\\[\\tilde{\\beta} = CY = [(X^TX)^{-1}X^T + D]Y\\]\n\\[= (X^TX)^{-1}X^TY + DY\\]\n\\[= \\hat{\\beta} + DY\\]\n\n\nStep 4: Calculate Variance\n\\[\\text{Var}(\\tilde{\\beta}) = \\text{Var}(\\hat{\\beta} + DY)\\]\n\\[= \\text{Var}(\\hat{\\beta}) + \\text{Var}(DY) + 2\\text{Cov}(\\hat{\\beta}, DY)\\]\n\n\nStep 5: Show Covariance Term is Zero\n\\[\\text{Cov}(\\hat{\\beta}, DY) = \\text{Cov}((X^TX)^{-1}X^TY, DY)\\]\n\\[= (X^TX)^{-1}X^T \\text{Cov}(Y, Y) D^T\\]\n\\[= (X^TX)^{-1}X^T (\\sigma^2I) D^T\\]\n\\[= \\sigma^2(X^TX)^{-1}X^TD^T\\]\nSince \\(DX = 0\\), we have \\(X^TD^T = 0\\), therefore: \\[\\text{Cov}(\\hat{\\beta}, DY) = 0\\]\n\n\nStep 6: Final Comparison\n\\[\\text{Var}(\\tilde{\\beta}) = \\text{Var}(\\hat{\\beta}) + \\text{Var}(DY)\\]\n\\[= \\sigma^2(X^TX)^{-1} + \\sigma^2DD^T\\]\nSince \\(DD^T \\succeq 0\\) (positive semidefinite), we have:\n\\[\\text{Var}(\\tilde{\\beta}) - \\text{Var}(\\hat{\\beta}) = \\sigma^2DD^T \\succeq 0\\]\n\n\nConclusion\nThis proves that \\(\\text{Var}(\\hat{\\beta}) \\preceq \\text{Var}(\\tilde{\\beta})\\) in the matrix sense, establishing that the OLS estimator \\(\\hat{\\beta}\\) has minimum variance among all linear unbiased estimators. □"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#maximum-likelihood-estimator",
    "href": "teaching/linear_model/lectures/linear_model.html#maximum-likelihood-estimator",
    "title": "Linear Regression Model",
    "section": "Maximum Likelihood Estimator",
    "text": "Maximum Likelihood Estimator\n\n\n\n\n\n\nMLE\n\n\n\nLet \\(\\hat \\beta_{MLE}\\) and \\(\\hat \\sigma_{MLE}^2\\) be the MLE of \\(\\beta\\) and \\(\\sigma^2\\), respectively.\n\n\\(\\hat{\\beta}_{MLE} = \\hat{\\beta}\\) et \\(\\hat{\\sigma}^2_{MLE} = \\frac{SCR}{n} = \\frac{n-p}{n} \\hat{\\sigma}^2\\).\n\\(\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\\).\n\\(\\frac{n-p}{\\sigma^2} \\hat{\\sigma}^2 = \\frac{n}{\\sigma^2} \\hat{\\sigma}^2_{MLE} \\sim \\chi^2(n - p)\\).\n\\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#proof-mle",
    "href": "teaching/linear_model/lectures/linear_model.html#proof-mle",
    "title": "Linear Regression Model",
    "section": "Proof",
    "text": "Proof\n\nSetup\nModel: \\(Y = X\\beta + \\varepsilon\\) where \\(\\varepsilon \\sim N(0, \\sigma^2 I)\\)\nThis means: \\(Y \\sim N(X\\beta, \\sigma^2 I)\\)\n\n\nLikelihood Function\nFor \\(n\\) observations, the likelihood function is:\n\\[L(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} (Y - X\\beta)^T(Y - X\\beta)\\right)\\]\n\n\nLog-Likelihood Function\n\\[\\ell(\\beta, \\sigma^2) = \\log L(\\beta, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} (Y - X\\beta)^T(Y - X\\beta)\\]\n\\[\\ell(\\beta, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} (Y - X\\beta)^T(Y - X\\beta)\\]\n\n\nFinding MLE for \\(\\beta\\)\nTaking the partial derivative with respect to \\(\\beta\\):\n\\[\\frac{\\partial \\ell}{\\partial \\beta} = \\frac{1}{\\sigma^2} X^T(Y - X\\beta)\\]\nSetting equal to zero: \\[X^T(Y - X\\hat{\\beta}_{MLE}) = 0\\]\n\\[X^TY - X^TX\\hat{\\beta}_{MLE} = 0\\]\n\\[X^TX\\hat{\\beta}_{MLE} = X^Ty\\]\nTherefore: \\[\\hat{\\beta}_{MLE} = (X^TX)^{-1}X^Ty\\]\n\n\nFinding MLE for \\(\\sigma^2\\)\nTaking the partial derivative with respect to \\(\\sigma^2\\):\n\\[\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} (Y - X\\beta)^T(Y - X\\beta)\\]\nSetting equal to zero: \\[-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} (Y - X\\hat{\\beta}_{MLE})^T(Y - X\\hat{\\beta}_{MLE}) = 0\\]\nMultiplying by \\(2\\sigma^4\\): \\[-n\\sigma^2 + (y - X\\hat{\\beta}_{MLE})^T(y - X\\hat{\\beta}_{MLE}) = 0\\]\nTherefore: \\[\\hat{\\sigma}^2 = \\frac{1}{n} (y - X\\hat{\\beta}_{MLE})^T(y - X\\hat{\\beta}_{MLE}) = \\frac{SSR}{n}\\]\n\n\nVerification (Second-Order Conditions)\nThe Hessian matrix has:\n\\[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta'} = -\\frac{1}{\\sigma^2} X^TX\\]\nThis is negative definite (assuming \\(X^TX\\) is invertible), confirming \\(\\hat{\\beta}\\) is a maximum.\n\\[\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\frac{n}{2\\sigma^4} - \\frac{1}{\\sigma^6} (y - X\\beta)^T(y - X\\beta)\\]\nAt the MLE: \\(\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2}\\bigg|_{\\hat{\\sigma}^2} = \\frac{n}{2\\sigma^4} - \\frac{n}{\\sigma^4} = -\\frac{n}{2\\sigma^4} &lt; 0\\)\nThis confirms \\(\\hat{\\sigma}^2\\) is a maximum.\n\n\nKey Properties\n\nConsistency: Both estimators are consistent\nBias: \\(\\hat{\\beta}\\) is unbiased, but \\(\\hat{\\sigma}^2\\) is biased (divides by \\(n\\) instead of \\(n-k\\))\nEfficiency: Under normality, these MLEs achieve the Cramér-Rao lower bound\nRelationship to OLS: \\(\\hat{\\beta}_{MLE} = \\hat{\\beta}_{OLS}\\) under normality assumption"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#setup-2",
    "href": "teaching/linear_model/lectures/linear_model.html#setup-2",
    "title": "Linear Regression Model",
    "section": "Setup",
    "text": "Setup\nConsider the linear regression model: \\[Y = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2 I)\\]\nWe want to prove that \\(\\hat \\beta = (X^TX)^{-1}X^TY\\) is efficient."
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#definition-of-efficiency",
    "href": "teaching/linear_model/lectures/linear_model.html#definition-of-efficiency",
    "title": "Linear Regression Model",
    "section": "Definition of Efficiency",
    "text": "Definition of Efficiency\nAn unbiased estimator is efficient if it achieves the Cramér-Rao lower bound: \\[\\text{Var}(\\hat \\beta) = [I(\\beta)]^{-1}\\] where \\(I(\\beta)\\) is the Fisher Information Matrix. This comes from the Cramér-Rao lower bound"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#step-1-fisher-information-matrix",
    "href": "teaching/linear_model/lectures/linear_model.html#step-1-fisher-information-matrix",
    "title": "Linear Regression Model",
    "section": "Step 1: Fisher Information Matrix",
    "text": "Step 1: Fisher Information Matrix\nThe log-likelihood function is: \\[\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}(Y - X\\beta)^T(Y - X\\beta)\\]\nFirst derivative with respect to \\(\\beta\\): \\[\\frac{\\partial \\ell}{\\partial \\beta} = \\frac{1}{\\sigma^2}X^T(Y - X\\beta)\\]\nSecond derivative: \\[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^T} = -\\frac{1}{\\sigma^2}X^TX\\]\nFisher Information Matrix for \\(\\beta\\): \\[I(\\beta) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^T}\\right] = \\frac{1}{\\sigma^2}X^TX\\]\nCramér-Rao lower bound: \\[[I(\\beta)]^{-1} = \\sigma^2(X^TX)^{-1}\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#step-2-variance-of-hat-beta",
    "href": "teaching/linear_model/lectures/linear_model.html#step-2-variance-of-hat-beta",
    "title": "Linear Regression Model",
    "section": "Step 2: Variance of \\(\\hat \\beta\\)",
    "text": "Step 2: Variance of \\(\\hat \\beta\\)\n\\[\\hat \\beta = (X^TX)^{-1}X^TY = (X^TX)^{-1}X^T(X\\beta + \\varepsilon) = \\beta + (X^TX)^{-1}X^T\\varepsilon\\]\nSince \\(\\varepsilon \\sim N(0, \\sigma^2 I)\\): \\[\\text{Var}(\\hat \\beta) = \\text{Var}((X^TX)^{-1}X^T\\varepsilon)\\]\n\\[= (X^TX)^{-1}X^T \\cdot \\text{Var}(\\varepsilon) \\cdot X(X^TX)^{-1}\\]\n\\[= (X^TX)^{-1}X^T \\cdot \\sigma^2 I \\cdot X(X^TX)^{-1}\\]\n\\[= \\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}\\]\n\\[= \\sigma^2(X^TX)^{-1}\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#step-3-verification-of-efficiency",
    "href": "teaching/linear_model/lectures/linear_model.html#step-3-verification-of-efficiency",
    "title": "Linear Regression Model",
    "section": "Step 3: Verification of Efficiency",
    "text": "Step 3: Verification of Efficiency\nWe have shown: - Cramér-Rao bound: \\([I(\\beta)]^{-1} = \\sigma^2(X^TX)^{-1}\\) - Variance of \\(\\hat \\beta\\): \\(\\text{Var}(\\hat \\beta) = \\sigma^2(X^TX)^{-1}\\)\nSince: \\[\\text{Var}(\\hat \\beta) = [I(\\beta)]^{-1}\\]\nThe estimator \\(\\hat \\beta\\) achieves the Cramér-Rao lower bound."
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#conclusion-1",
    "href": "teaching/linear_model/lectures/linear_model.html#conclusion-1",
    "title": "Linear Regression Model",
    "section": "Conclusion",
    "text": "Conclusion\nTherefore, \\(\\hat \\beta = (X^TX)^{-1}X^TY\\) is an efficient estimator of \\(\\beta\\) in the Gaussian linear regression model."
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#additional-notes",
    "href": "teaching/linear_model/lectures/linear_model.html#additional-notes",
    "title": "Linear Regression Model",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nThis efficiency holds specifically under the normality assumption\n\\(\\hat \\beta\\) is also the Best Linear Unbiased Estimator (BLUE) by the Gauss-Markov theorem\nUnder normality, \\(\\hat \\beta\\) is the Best Unbiased Estimator (BUE) among all estimators, not just linear ones"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#ordinary-least-square-estimator-ols",
    "href": "teaching/linear_model/slides/inference.html#ordinary-least-square-estimator-ols",
    "title": "Inference",
    "section": "Ordinary Least Square Estimator (OLS)",
    "text": "Ordinary Least Square Estimator (OLS)\n\nWe observe \\(Y \\in \\mathbb R^{n\\times 1}\\) and \\(X \\in \\mathbb R^{n \\times p}\\).\n\n\nWhat is the best estimator \\(\\hat \\beta\\) of \\(\\beta\\) such that \\(Y = X\\beta + \\varepsilon\\)?\n\n\nOrdinary Least Square (OLS) Estimator:\n\\(\\newcommand{\\argmin}{\\mathrm{argmin}}\\)\n\n\\[\\hat \\beta = \\argmin_{\\beta'}\\|Y-X\\beta'\\|^2\\]\n\nHere, \\(\\|Y-X\\beta'\\|^2 = \\sum_{i=1}^n(Y_i-\\beta_1X_{i}^{(1)}- \\dots - \\beta_pX_i^{(p)})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\n\nIf \\(\\mathrm{rk}(X) = p\\), it holds that\n\n\\[\\hat \\beta = (X^TX)^{-1}X^TY\\]\n\n\n\n\\(X\\hat \\beta = X(X^TX)^{-1}X^TY\\) is the projection of \\(Y\\) on the space generated by columns of \\(X\\): \\[[X]=\\mathrm{Im}(X)=\\mathrm{Span}(X^{(1)},\\dots, X^{(p)})=\\{X\\alpha, \\alpha \\in \\mathbb R^p\\}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-1",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-1",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\n\n\n\\[P_{[X]} = X(X^TX)^{-1}X^T \\and \\widehat{Y} = P_{[X]}Y = X\\hat \\beta\\]\n\n\n\nCheck that \\(P_{[X]}\\) is the orthogonal projector on \\([X]\\)\n\n\nThat is \\(P_{[X]}^2 = P_{[X]}\\), \\(P_{[X]}=P_{[X]}^T\\) and \\(\\mathrm{Im} (P_{[X]}) = [X]\\)\n\n\nOr, without computation:\n\\(P_{[X]}Y\\) minimizes \\(\\|Y-Y'\\|^2\\) over all \\(Y' \\in [X]\\).\n\n\nSo \\(P_{[X]}Y\\) must be the orthogonal projection by Pythagorean theorem"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-2",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-2",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\nWe can decompose\n\n\\[Y = \\underbrace{P_{[X]}Y}_{\\widehat Y} + \\underbrace{(I-P_{[X]})Y}_{\"residuals\"}\\]\n\n\nNotice that \\(I-P_{[X]}= P_{[X]^{\\perp}}\\) is the orthogonal projection on \\([X]^{\\perp} = \\{\\alpha\\in \\mathbb R^p:~ X\\alpha =0\\}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-3",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-3",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\n\n(image: elements of statistical learning. In yellow: \\([X]\\) with \\(p=2\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-on-hat-beta",
    "href": "teaching/linear_model/slides/inference.html#properties-on-hat-beta",
    "title": "Inference",
    "section": "Properties on \\(\\hat \\beta\\)",
    "text": "Properties on \\(\\hat \\beta\\)\n\n\n\n\nExpectation and variance\n\n\nAssume that \\(rk(X)=p\\), \\(\\mathbb E[\\varepsilon] = 0\\) and \\(\\mathbb V(\\varepsilon) = \\sigma^2I_n\\). Then,\n\n\\(\\hat \\beta = (X^TX)^{-1}X Y\\) is a linear estimator\n\\(\\mathbb E[\\hat \\beta] = \\beta\\) unbiased estimator\n\\(\\mathbb V(\\hat \\beta) = \\sigma^2(X^TX)^{-1}\\)\n\n\n\n\n\n\n\n\n\nGauss-Markov Theorem\n\n\nUnder the same assumptions, if \\(\\tilde \\beta\\) is another linear and unbiased estimator then \\[\\mathbb V(\\hat \\beta) \\preceq \\mathbb V(\\tilde \\beta),\\]\nwhere \\(A\\preceq B\\) means that \\(B-A\\) is a symmetric positive semidefinite matrix Elements of proof"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#residuals",
    "href": "teaching/linear_model/slides/inference.html#residuals",
    "title": "Inference",
    "section": "Residuals",
    "text": "Residuals\n\nWe define the residuals \\(\\hat \\varepsilon\\) as\n\n\\[\n\\hat \\varepsilon = Y- \\hat Y = Y-X\\hat \\beta\n\\]\n\n\n\nIt can be computed from the data.\n\n\nIt is the orthogonal projection of \\(Y\\) on \\([X]^{\\perp}\\):\n\n\n\n\\[Y = \\underbrace{P_{[X]}Y}_{\\widehat Y} + \\underbrace{(I-P_{[X]})Y}_{\\hat \\varepsilon=\"residuals\"}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#residuals-1",
    "href": "teaching/linear_model/slides/inference.html#residuals-1",
    "title": "Inference",
    "section": "Residuals",
    "text": "Residuals\n\n\\[\n\\begin{aligned}\nY &= X\\beta + \\varepsilon & \\quad \\text{(model)} \\\\\nY &= \\widehat Y + \\hat \\varepsilon & \\quad \\text{(estimation)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-on-residuals",
    "href": "teaching/linear_model/slides/inference.html#properties-on-residuals",
    "title": "Inference",
    "section": "Properties on residuals",
    "text": "Properties on residuals\n\n\n\nExpectation and Variance of \\(\\hat\\varepsilon\\)\n\n\nIf \\(rk(X)=p\\), \\(\\mathbb E[\\varepsilon] = 0\\) and \\(\\mathbb V(\\varepsilon)=\\sigma^2I_n\\), then\n\n\\(\\mathbb E[\\hat\\varepsilon]=0\\)\n\\(\\mathbb V(\\hat \\varepsilon) = \\sigma^2P_{[X]^\\perp} = \\sigma^2(I_n - X(X^TX)^{-1}X^T)\\)\n\n\n\n\n\nRemark: if a constant vector is in \\([X]\\), e.g. \\(\\forall i,X_i^{(1)}=1\\), then \\(\\hat \\varepsilon \\perp \\mathbf 1\\) and\n\\[\n\\overline{\\hat\\varepsilon} = \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i = 0 \\and \\overline{\\widehat Y} = \\overline Y\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#estimation-of-sigma2-1",
    "href": "teaching/linear_model/slides/inference.html#estimation-of-sigma2-1",
    "title": "Inference",
    "section": "Estimation of \\(\\sigma^2\\)",
    "text": "Estimation of \\(\\sigma^2\\)\n\nRecall that in the model \\(Y=X\\beta + \\varepsilon\\), both \\(\\beta\\) and \\(\\sigma^2=\\mathbb E[\\varepsilon_i^2]\\) are unknown (\\(p+1\\) parameters)\n\n\n\n\\[\n\\hat\\varepsilon = Y- \\hat Y = P_{[X]^\\perp}\\varepsilon \\and dim([X]^{\\perp}) = =n-p\n\\]\n\n\n\nWe estimate \\(\\sigma^2\\) with\n\n\\[\\hat \\sigma^2 = \\frac{1}{n-p}\\sum_{i=1}^n \\hat \\varepsilon_i^2\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-of-hat-sigma2",
    "href": "teaching/linear_model/slides/inference.html#properties-of-hat-sigma2",
    "title": "Inference",
    "section": "Properties of \\(\\hat \\sigma^2\\)",
    "text": "Properties of \\(\\hat \\sigma^2\\)\n\n\n\nProposition\n\n\nIf \\(rk(X)=p\\), \\(\\E[\\varepsilon]=0\\) and \\(\\Var(\\varepsilon)= \\sigma^2I_n\\), then\n\\[\\hat \\sigma^2 = \\frac{1}{n-p}\\sum_{i=1}^n \\hat \\varepsilon_i^2=\\frac{\\mathrm{SSR}}{n-p}\\]\nis an unbiased estimator of \\(\\sigma^2\\). If moreover the \\(\\varepsilon_i\\)’s are iid, then \\(\\hat \\sigma^2\\) is a consistent estimator.\n\n\n\n\nunbiased: \\(\\E[\\hat \\sigma^2]= \\sigma^2\\)\nconsistent: \\(\\hat \\sigma^2 \\to \\sigma^2\\) a.s. as \\(n\\to +\\infty\\)\nSSR: Sum of squared residuals"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#gaussian-model",
    "href": "teaching/linear_model/slides/inference.html#gaussian-model",
    "title": "Inference",
    "section": "Gaussian Model",
    "text": "Gaussian Model\n\nUntil then, we assumed that \\(Y=X\\beta+\\varepsilon\\), where \\(\\E[\\varepsilon]=0\\) and \\(\\Var(\\varepsilon)= \\sigma^2I_n\\).\n\n\nThe \\(\\varepsilon_i\\) are uncorrelated but there can be dependency\n\n\nNo assumption was made on the distribution of \\(\\varepsilon\\)\n\n\nNow (Gaussian Model):\n\n\\[\\varepsilon \\sim \\mathcal N(0, \\sigma^2I_n) \\quad \\text{i.e.} \\quad Y \\sim \\mathcal N(X\\beta, \\sigma^2I_n)\\]\n\n\n\nEquivalently we assume that the \\(\\varepsilon_i\\)’s are iid \\(\\mathcal N(0, \\sigma^2)\\).\n\n\nIn this simpler model, we can do maximum likelihood estimation (MLE)!"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#maximum-likelihood-estimation",
    "href": "teaching/linear_model/slides/inference.html#maximum-likelihood-estimation",
    "title": "Inference",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\n\n\n\nMLE\n\n\nLet \\(\\hat \\beta_{MLE}\\) and \\(\\hat \\sigma_{MLE}^2\\) be the MLE of \\(\\beta\\) and \\(\\sigma^2\\), respectively.\n\n\\(\\hat{\\beta}_{MLE} = \\hat{\\beta}\\) et \\(\\hat{\\sigma}^2_{MLE} = \\frac{SCR}{n} = \\frac{n-p}{n} \\hat{\\sigma}^2\\).\n\\(\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\\).\n\\(\\frac{n-p}{\\sigma^2} \\hat{\\sigma}^2 = \\frac{n}{\\sigma^2} \\hat{\\sigma}^2_{MLE} \\sim \\chi^2(n - p)\\).\n\\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent\n\nElements of proof"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#efficient-estimator",
    "href": "teaching/linear_model/slides/inference.html#efficient-estimator",
    "title": "Inference",
    "section": "Efficient Estimator",
    "text": "Efficient Estimator\n\n\n\n\nTheorem\n\n\nIn the Gaussian Model, \\(\\hat \\beta\\) is an efficient estimator of \\(\\hat \\beta\\). This means that \\[\n\\Var(\\hat \\beta) \\preceq \\Var(\\tilde \\beta)\\; ,\n\\] for any estimator \\(\\tilde \\beta\\). See Elements of proof\n\n\n\n\nThis is stronger than Gauss-Markov\nBetter than any \\(\\tilde \\beta\\), not only linear \\(\\tilde  \\beta\\). \\(\\hat \\beta\\) is “BUE” (Best Unbiased Estimator)\nIf \\(n\\) is large, most of the result in Gaussian case remains valid."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#pivotal-distribution-in-gaussian-case",
    "href": "teaching/linear_model/slides/inference.html#pivotal-distribution-in-gaussian-case",
    "title": "Inference",
    "section": "Pivotal Distribution in Gaussian Case",
    "text": "Pivotal Distribution in Gaussian Case\n\nRecall that \\(\\hat \\sigma^2 = \\frac{1}{n-p}\\|\\hat \\varepsilon\\|^2\\)\n\n\n\n\n\nProperty\n\n\nIn the Gaussian model,\n\\[ \\frac{\\hat \\beta_j - \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) \\]\n(Student Distribution of degree \\(n-p\\), \\((X^TX)^{-1}_{jj}\\) is the \\(j^{th}\\) element of the matrix \\((X^TX)^{-1}\\))\n\n\n\n\n\n\\(\\mathbb V(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\\) implies that \\(\\mathbb V(\\hat{\\beta}_j) = \\sigma^2 (X^T X)^{-1}_{jj}\\). \\(\\hat{\\sigma}^2_{\\hat{\\beta}_j}:=\\hat\\sigma^2 (X^T X)^{-1}\\) is an estimator of \\(\\sqrt{\\mathbb V(\\hat{\\beta}_j)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#student-signifactivity-test",
    "href": "teaching/linear_model/slides/inference.html#student-signifactivity-test",
    "title": "Inference",
    "section": "Student Signifactivity Test",
    "text": "Student Signifactivity Test\n\nWe observe \\(Y = X\\beta+\\varepsilon\\), where \\(\\beta \\in \\mathbb R^p\\) is unknown.\n\n\nWe want to test whether the \\(j^{th}\\) feature \\(X^{(j)}\\) is significant in the LM, that is:\n\n\\(H_0: \\beta_j =0 \\VS H_1: \\beta_j \\neq 0\\).\n\nWe use the test statistic\n\n\n\\[\\psi_j(X,Y)= \\frac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}} = \\frac{\\hat \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) ~~\\text{(under $H_0$)}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#student-signifactivity-test-1",
    "href": "teaching/linear_model/slides/inference.html#student-signifactivity-test-1",
    "title": "Inference",
    "section": "Student Signifactivity Test",
    "text": "Student Signifactivity Test\n\n\n\n\\[\\psi_j(X,Y)= \\frac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}} = \\frac{\\hat \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) ~~\\text{(under $H_0$)}\\]\n\n\n\n\nWe reject if \\(|\\psi(X,Y)| \\geq t_{1-\\alpha/2}\\), where \\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of \\(\\mathcal T(n-p)\\).\n\n\n\n\\[p_{value}=2\\min(F(\\psi_j(X,Y)), 1-F(\\psi_j(X,Y)))\\]\n\nwhere \\(F\\) is the cdf of \\(\\mathcal T(n-p)\\).\n\n\nIf we get \\(\\beta_j=0\\), we can remove \\(X^{(j)}\\) from the model."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#confidence-interval",
    "href": "teaching/linear_model/slides/inference.html#confidence-interval",
    "title": "Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nConfidence interval with proba \\(1-\\alpha\\) around \\(\\beta_j\\):\n\n\\[CI_{1-\\alpha} = [\\hat \\beta_j \\pm t\\hat \\sigma_{\\hat \\beta_j}]\\]\n\n\nHere, \\(t\\) is the \\(1-\\alpha/2\\) quantile of \\(\\mathcal T(n-p)\\)\nRecall that \\(\\hat \\sigma_{\\hat \\beta_j} = \\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}\\)\n\n\nWe check that\n\\[\n\\P(\\beta \\in CI_{1-\\alpha}) = 1- \\alpha\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#remarks",
    "href": "teaching/linear_model/slides/inference.html#remarks",
    "title": "Inference",
    "section": "Remarks",
    "text": "Remarks\n\nThis is what is computed on \\(R\\)\nThis is valid in Gaussian case, but is also true when \\(n\\) is large for other distributions"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-setting",
    "href": "teaching/linear_model/slides/inference.html#prediction-setting",
    "title": "Inference",
    "section": "Prediction Setting",
    "text": "Prediction Setting\n\nConsider the LM \\(Y = X\\beta + \\varepsilon\\).\n\n\nFrom previous slides, we estimate \\((\\beta, \\sigma)\\) with \\((\\hat \\beta, \\hat \\sigma)\\) from observations \\(Y\\) and matrix \\(X=(X^{(1)}, \\dots, X^{(p)})\\).\n\n\nWe observe a new individual \\(o\\), with unknown \\(Y_o\\) and vector \\(X_o=X_{o,\\cdot}=(X^{(1)}_o, \\dots, X^{(p)}_o)\\), and independent noise \\(\\varepsilon_o\\).\n\n\nwith this definition, \\(X_o\\) is a row vector and\n\n\\[Y_o = X_{o}\\beta + \\varepsilon_o = \\beta_1X^{(1)}_o + \\dots + \\beta_p X^{(p)}_o+\\varepsilon_o\\]\n\nHere, \\(\\mathbb E[\\varepsilon_o] = 0\\) and \\(\\mathbb V(\\varepsilon_o)=\\sigma^2\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-1",
    "href": "teaching/linear_model/slides/inference.html#prediction-1",
    "title": "Inference",
    "section": "Prediction",
    "text": "Prediction\n\nWe want predict \\(Y_o\\) (unknown) from \\(X_o\\) (known). Natural predictor:\n\n\\(\\hat Y_o = X_o \\hat \\beta + \\varepsilon_o\\)\n\n\n\nPrediction error \\(Y_o - \\hat Y_o\\) decomposes in two terms:\n\n\\[Y_o - \\hat Y_o = \\underbrace{X_o(\\beta - \\hat \\beta)}_{\\text{Estimation error}} + \\underbrace{\\varepsilon_o}_{\\text{Random error}}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#error-decomposition",
    "href": "teaching/linear_model/slides/inference.html#error-decomposition",
    "title": "Inference",
    "section": "Error Decomposition",
    "text": "Error Decomposition\n\n\n\n\\[Y_o - \\hat Y_o = \\underbrace{X_o(\\beta - \\hat \\beta)}_{\\text{Estimation error}} + \\underbrace{\\varepsilon_o}_{\\text{Random error}}\\]\n\n\n\n\n\\(\\E(Y_o - \\hat Y_o)\\): prediction error is \\(0\\) on average\n\n\n\\[\\begin{aligned}\n\\Var(Y_o - \\hat Y_o) &= \\color{blue}{ \\Var(X_o(\\beta - \\hat \\beta))} + \\color{red}{\\Var(\\varepsilon_o)} \\\\\n&=\\color{blue}{\\sigma^2X_o(X^TX)^{-1}X_o} + \\color{red}{\\sigma^2} \\\\\n\\end{aligned}\\]\n\n\n\\(\\color{blue}{\\Var(X_o(\\beta - \\hat \\beta))\\to 0}\\) when \\(n \\to +\\infty\\) but \\(\\color{red}{\\Var(\\varepsilon_o) =\\sigma^2}\\)\n\n\nEstimation error is negligible when \\(n \\to +\\infty\\) but random error is incompressible."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-interval",
    "href": "teaching/linear_model/slides/inference.html#prediction-interval",
    "title": "Inference",
    "section": "Prediction Interval",
    "text": "Prediction Interval\n\nIn the Gaussian model, \\(Y_o - \\hat Y_o \\sim \\mathcal N(0,  \\sigma^2X_o(X^TX)^{-1}X_o^T+\\sigma^2)\\).\n\n\nSince \\(\\hat \\sigma\\) is indep of \\(\\hat Y_o\\) (check this with projections!),\n\n\n\\[\\frac{Y_o - \\hat Y_o}{\\hat \\sigma\\sqrt{X_o(X^TX)^{-1}X_o^T+1}} \\sim \\mathcal T(n-p)\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-interval-1",
    "href": "teaching/linear_model/slides/inference.html#prediction-interval-1",
    "title": "Inference",
    "section": "Prediction Interval",
    "text": "Prediction Interval\n\nWe deduce the prediction interval\n\n\n\\[PI_{1-\\alpha}(Y_o)=\\left[\\hat Y_o \\pm \\sqrt{\\color{blue}{\\hat \\sigma^2X_o(X^TX)^{-1}X_o^T} + \\color{red}{\\hat \\sigma^2}}\\right]\\]\n\n\nsuch that \\(\\P(Y_o \\in PI_{1-\\alpha})= 1-\\alpha\\)\n\n\nIf we only want to estimate \\(\\mathbb E[Y_o]=X_o \\beta\\), (point on the hyperplane), we get the confidence interval\n\n\n\\[CI_{1-\\alpha}(X_o\\beta)=\\left[\\hat Y_o \\pm \\sqrt{\\color{blue}{\\hat \\sigma^2X_o(X^TX)^{-1}X_o^T}}\\right]\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#example",
    "href": "teaching/linear_model/slides/inference.html#example",
    "title": "Inference",
    "section": "Example",
    "text": "Example\nFor 100 trees, we record their volume and their girth."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#model",
    "href": "teaching/linear_model/slides/inference.html#model",
    "title": "Inference",
    "section": "Model",
    "text": "Model\n\nVolume = \\(\\beta_1\\) + \\(\\beta_2\\) Girth + \\(\\varepsilon\\)\n# In R\nreg=lm(Volume ~ Girth, data=trees)\nsummary(reg)\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#results",
    "href": "teaching/linear_model/slides/inference.html#results",
    "title": "Inference",
    "section": "Results",
    "text": "Results\n\nEstimation:\n\n\n\\[\\hat \\beta_1=-36.9 \\and \\hat \\beta_2=5.07\\] \\[\\hat \\sigma_{\\hat \\beta_1} =3.4 \\and \\hat \\sigma_{\\hat \\beta_2} =0.25\\]\n\n\n\n\nStatistics:\n\n\n\\[\\frac{\\hat \\beta_1}{\\hat \\sigma_{\\hat \\beta_1}}=-10.98 \\and \\frac{\\hat \\beta_2}{\\hat \\sigma_{\\hat \\beta_2}}=20.48\\]\n\n\n\n\n\\(Pr(&gt;|t|)\\): pvalues of the student tests. Last row: \\(\\hat \\sigma=4.25\\) and df."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#illustration",
    "href": "teaching/linear_model/slides/inference.html#illustration",
    "title": "Inference",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#r-code",
    "href": "teaching/linear_model/slides/inference.html#r-code",
    "title": "Inference",
    "section": "R code",
    "text": "R code\nrequire(stats); require(graphics)\npairs(trees, main = \"trees data\")\ntrees[,c(\"Girth\", \"Volume\")]\nreg &lt;- lm(Volume ~ Girth, data = trees)\n# Create sequence of x values for smooth curves\nx_seq &lt;- seq(min(trees$Girth), max(trees$Girth), length.out = 100)\n\n# Calculate confidence and prediction intervals\nconf_int &lt;- predict(reg, newdata = data.frame(Girth = x_seq), \n                   interval = \"confidence\", level = 0.95)\npred_int &lt;- predict(reg, newdata = data.frame(Girth = x_seq), \n                   interval = \"prediction\", level = 0.95)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#pythagorean-decomposition",
    "href": "teaching/linear_model/slides/validation.html#pythagorean-decomposition",
    "title": "Validation",
    "section": "Pythagorean Decomposition",
    "text": "Pythagorean Decomposition\n\nLet \\(\\mathbf{1}\\) be the constant column vector in \\(\\mathbb R^{n\\times 1}\\).\n\n\nIf \\(\\mathbf{1} \\in [X]\\) (eg if we consider an intercept) \\[ \\underbrace{\\|Y-\\overline Y \\1\\|^2}_{SST} = \\underbrace{\\|Y-\\widehat Y\\|^2}_{SSR}+\\underbrace{\\|\\widehat Y-\\overline Y \\1\\|^2}_{SSE}\\]\nIn the general case,\n\n\n\\[\\|Y\\|^2 = \\|Y-\\widehat Y\\|^2 + \\|\\widehat Y\\|^2\\]\nGood model if sum of squares of residuals \\(SSR \\ll 1\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#r2",
    "href": "teaching/linear_model/slides/validation.html#r2",
    "title": "Validation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\in [X]\\)\n\n\n\\[R^2 = \\frac{SSE}{SST} = 1-\\frac{SSR}{SST}\\]\n\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\not \\in [X]\\)\n\n\n\\[R^2 = \\frac{\\|\\widehat Y\\|^2}{\\|Y\\|^2} = 1 - \\frac{SCR}{\\|Y\\|^2}\\]\n\n\n\n\n\n\\(0 \\leq R^2 \\leq 1\\). Better model if \\(R^2\\) close to \\(1\\)\nTwo definitions of \\(R^2\\) when \\(\\1 \\in [X]\\) or not\nIn simple linear regression \\((Y_i = \\beta_1+\\beta_2X_i+\\varepsilon_i)\\): \\(R^2 = \\hat \\rho^2\\) is the square empirical correlation between \\(Y\\) and \\(X\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#adjusted-r2",
    "href": "teaching/linear_model/slides/validation.html#adjusted-r2",
    "title": "Validation",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nMain flaw of \\(R^2\\): adding a new variables decreases \\(R^2\\) (because \\([X]\\) is a bigger projection space)\n\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\in [X]\\)\n\n\n\\[R^2_a = 1-\\frac{n-1}{n-p}\\frac{SSR}{SST}\\]\n\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\not \\in [X]\\)\n\n\n\\[R^2_a = 1 - \\frac{n}{n-p}\\frac{SCR}{\\|Y\\|^2}\\]\n\n\n\n\n\n\nWith a new variable, \\(SCR\\) decreases but \\(p \\to p+1\\)\n\n\n\\(R_a^2\\)​ only decreases when adding a new variable if that variable significantly reduces the residual sum of squares. ()"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#r-output-interpretation",
    "href": "teaching/linear_model/slides/validation.html#r-output-interpretation",
    "title": "Validation",
    "section": "R output interpretation",
    "text": "R output interpretation\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nHere, \\(R^2=0.9353\\) and \\(R^2_a=0.9331\\).\n\n\\(\\approx 93\\%\\) of the variability is explained by the model."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linear-constraints",
    "href": "teaching/linear_model/slides/validation.html#linear-constraints",
    "title": "Validation",
    "section": "Linear Constraints",
    "text": "Linear Constraints\n\nWe want to test q linear constraints on the coefficient vector \\(\\beta \\in \\mathbb R^p\\).\n\n\nThis is formulated as: \\[H_0: R\\beta = 0 \\quad \\text{vs} \\quad H_1: R\\beta \\neq 0\\]\nwhere R is a (q × p) constraint matrix encoding the restrictions, with \\(q \\leq p\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#common-test-types",
    "href": "teaching/linear_model/slides/validation.html#common-test-types",
    "title": "Validation",
    "section": "Common Test Types",
    "text": "Common Test Types\n\\[H_0: R\\beta = 0 \\quad \\text{vs} \\quad H_1: R\\beta \\neq 0\\]\n\nStudent’s t-test: is variable \\(j\\) significant?\n\\(H_0: \\beta_j = 0\\) vs \\(H_1: \\beta_j \\neq 0\\)\nGlobal F-test: is any variable significant? Identity matrix excluding intercept\n\\(H_0: \\beta_2 = \\cdots = \\beta_p = 0\\) vs \\(H_1: \\exists j \\in \\{2,\\ldots,p\\}\\) s.t. \\(\\beta_j \\neq 0\\)\nNested model test: are q variables jointly significant?\n\\(H_0: \\beta_{p-q+1} = \\cdots = \\beta_p = 0\\) vs \\(H_1\\): the contrary"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#key-applications",
    "href": "teaching/linear_model/slides/validation.html#key-applications",
    "title": "Validation",
    "section": "Key Applications",
    "text": "Key Applications\n\nIndividual significance: Testing if a single predictor matters\nOverall model significance: Testing if the model explains anything beyond the intercept\n\nVariable subset significance: Testing if a group of variables contributes to the model"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#fisher-test",
    "href": "teaching/linear_model/slides/validation.html#fisher-test",
    "title": "Validation",
    "section": "Fisher Test",
    "text": "Fisher Test\n\n\n\nTheorem\n\n\n\n\\(SSR\\): sum of squares of residuals in the unconstrained regression model\n\\(SSR_c\\): sum of squares of residuals in the constrained regression model, i.e., in the sub-model satisfying \\(R\\beta = 0\\)\n\n\nIf \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R)=q\\) and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{{SSR}_c - {SSR}}{{SSR}} \\sim F(q, n-p)\\]\nwhere \\(F(q, n-p)\\) denotes the Fisher distribution with \\((q, n-p)\\) degrees of freedom. Elements of proof"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#rejection-region",
    "href": "teaching/linear_model/slides/validation.html#rejection-region",
    "title": "Validation",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nKey argument: \\(SSR_c - SSR\\) is equal to \\(\\|P_{V}Y\\|^2\\), where \\(V=X(Ker(R))^{\\perp} \\cap [X]\\) and \\(\\text{dim}(V)=q\\).\n\n\nTherefore, the critical region at significance level \\(\\alpha\\) for testing \\(H_0: R\\beta = 0\\) against \\(H_1: R\\beta \\neq 0\\) is:\n\n\\[RC_\\alpha = \\{F &gt; f_{q,n-p}(1-\\alpha)\\}\\]\n\nwhere \\(f_{q,n-p}(1-\\alpha)\\) denotes the \\((1-\\alpha)\\)-quantile of an \\(F(q, n-p)\\) distribution."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-1-student-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-1-student-test",
    "title": "Validation",
    "section": "Particular Case 1: Student Test",
    "text": "Particular Case 1: Student Test\n\nFix some variable \\(j\\) and consider\n\n\\(H_0: \\beta_j=0\\) VS \\(H_1:\\beta_j\\neq 0\\)\n\n\n\nOnly one constraint: \\(q=1\\), so that\n\n\\[ F = (n-p) \\frac{SCR_c-SCR}{SCR} \\sim \\mathcal F(1,n-p) \\sim \\mathcal T^2(n-p)\\]\n\n\n\nIn fact, Here \\(F = \\big(\\tfrac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}}\\big)^2\\) so \\(F\\) is the student test presented before"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-2-global-fisher-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-2-global-fisher-test",
    "title": "Validation",
    "section": "Particular Case 2: Global Fisher Test",
    "text": "Particular Case 2: Global Fisher Test\n\n\n\\(H_0: \\beta_2= \\dots = \\beta_p=0\\) VS \\(H_1\\): contrary\n\n\\(q = p-1\\) in this case . . .\n\n\\[F = \\frac{n-p}{p-1}\\frac{SSE}{SSR} = \\frac{n-p}{p-1}\\frac{R^2}{1-R^2} \\sim \\mathcal F(p-1, n-p)\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-3-nested-fisher-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-3-nested-fisher-test",
    "title": "Validation",
    "section": "Particular Case 3: Nested Fisher Test",
    "text": "Particular Case 3: Nested Fisher Test\n\n\n\\(H_0: \\beta_{p-q+1}= \\dots = \\beta_p=0\\) VS \\(H_1\\): contrary\n\n\n\n\n\\[F = \\frac{n-p}{q}\\frac{SCR_c - SCR}{SCR} \\sim \\mathcal F(q, n-p)\\]\n\n\n\nInterpretation:\nIf \\(F \\geq f_{q,n-p}(1-\\alpha)\\) (\\(1-\\alpha\\)-quantile of Fisher dist.) then constraints are not satisfied. We do not accept the submodel with respect to larger model."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#example-of-r-output",
    "href": "teaching/linear_model/slides/validation.html#example-of-r-output",
    "title": "Validation",
    "section": "Example of R Output",
    "text": "Example of R Output\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\nHere, Fisher global significance test statistic is \\(F=419.4\\), \\(q=1\\) and \\(n-p=29\\) (\\(n=31\\)). pvalue is negligible\n\n\nHere, \\(q=1\\) and \\(Global Fisher test\\) is a Student Test."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#model-assumptions",
    "href": "teaching/linear_model/slides/validation.html#model-assumptions",
    "title": "Validation",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nThe linear regression model relies on the following key assumptions:\n\nModel specification: \\(Y = X\\beta + \\varepsilon\\) (linear relationship)\nFull rank design: \\(\\text{rank}(X) = p\\) (no perfect multicollinearity)\nZero mean errors: \\(\\mathbb{E}(\\varepsilon) = 0\\)\nHomoscedastic errors: \\(\\text{Var}(\\varepsilon) = \\sigma^2 I_n\\) (constant variance and uncorrelated errors)\n\n\nNow: diagnostic tools to verify each assumption and remedial strategies when they fail."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linearity-assumption",
    "href": "teaching/linear_model/slides/validation.html#linearity-assumption",
    "title": "Validation",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\nThe linearity assumption \\(\\mathbb{E}(Y) = X\\beta\\) is the fundamental hypothesis of linear regression.\nDiagnostic Tools:\n\nPre-modeling: Scatter plots \\((X^{(j)}, Y)\\) and empirical correlations for each predictor\nPost-modeling: Residual analysis - non-linearity manifests as patterns in \\(\\hat{\\varepsilon}\\)\nAdvanced: Partial residual plots (not covered here)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linearity-assumption-1",
    "href": "teaching/linear_model/slides/validation.html#linearity-assumption-1",
    "title": "Validation",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\nThe linearity assumption \\(\\mathbb{E}(Y) = X\\beta\\) is the fundamental hypothesis of linear regression.\nRemedial Strategies:\n\nTransformations: Apply transformations to \\(Y\\) and/or predictors \\(X^{(j)}\\) to achieve linearity\nAlternative models: If transformations fail, consider nonlinear regression models"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#full-rank-assumption",
    "href": "teaching/linear_model/slides/validation.html#full-rank-assumption",
    "title": "Validation",
    "section": "Full Rank Assumption",
    "text": "Full Rank Assumption\nThe condition \\(\\text{rank}(X) = p\\) ensures no predictor \\(X^{(j)}\\) is a linear combination of others.\n\nWhy it matters:\n\nIdentifiability: Without full rank, \\(\\beta\\) is not uniquely defined\nEstimation: \\((X^TX)\\) becomes non-invertible, making \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\) undefined\nInfinitely many solutions satisfy \\(X^TX\\hat{\\beta} = X^TY\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#full-rank-assumption-1",
    "href": "teaching/linear_model/slides/validation.html#full-rank-assumption-1",
    "title": "Validation",
    "section": "Full Rank Assumption",
    "text": "Full Rank Assumption\nThe condition \\(\\text{rank}(X) = p\\) ensures no predictor \\(X^{(j)}\\) is a linear combination of others.\nIn practice:\n\nPerfect collinearity is rare, so \\(\\text{rank}(X) = p\\) usually holds\nNear-collinearity is the real concern - when predictors are “almost” linearly dependent"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#near-collinearity-issues",
    "href": "teaching/linear_model/slides/validation.html#near-collinearity-issues",
    "title": "Validation",
    "section": "Near-Collinearity Issues",
    "text": "Near-Collinearity Issues\nWhen a variable is highly correlated with others (correlation close to but not exactly \\(\\pm 1\\)):\n\nMathematical consequences:\n\n\\(X'X\\) remains invertible, but its smallest eigenvalue approaches zero\n\\((X'X)^{-1}\\) becomes numerically unstable"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#near-collinearity-issues-1",
    "href": "teaching/linear_model/slides/validation.html#near-collinearity-issues-1",
    "title": "Validation",
    "section": "Near-Collinearity Issues",
    "text": "Near-Collinearity Issues\n\nStatistical implications:\n\nInstability: Adding/removing a single observation can drastically change \\((X'X)^{-1}\\)\nUnreliable estimates: \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\) becomes highly unstable\nInflated variance: \\(\\text{Var}(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\) becomes very large\n\n\n\nThis is undesirable from a statistical point of view"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#detecting-collinearity-vif",
    "href": "teaching/linear_model/slides/validation.html#detecting-collinearity-vif",
    "title": "Validation",
    "section": "Detecting Collinearity: VIF",
    "text": "Detecting Collinearity: VIF\nCompute the VIF (Variance Inflation Factor) for each \\(X^{(j)}\\):\n\nRegress \\(X^{(j)}\\) on all other \\(X^{(k)}\\) (where \\(k \\neq j\\))\nCompute \\(R_j^2\\) from this regression\nCalculate: \\(\\text{VIF}_j = \\frac{1}{1 - R_j^2}\\)\n\n\nProperties:\n\n\\(\\text{VIF}_j \\geq 1\\) always\nHigh VIF indicates collinearity. Common threshold: \\(\\text{VIF}_j \\geq 5\\). In R: vif() from car package"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#remedies-for-multicollinearity",
    "href": "teaching/linear_model/slides/validation.html#remedies-for-multicollinearity",
    "title": "Validation",
    "section": "Remedies for Multicollinearity",
    "text": "Remedies for Multicollinearity\n\nVariable removal: Drop variables with high VIF\nPreferably remove those least correlated with \\(Y\\)\nPenalized regression: Ridge, LASSO, or elastic net methods (not covered here)\n\n\nImportant Distinction on Multicollinearity\n\nParameter estimation: Multicollinearity severely affects \\(\\hat{\\beta}\\) reliability\nPrediction: Not problematic: \\(\\hat{Y}\\) remains well-defined and stable since projection on \\([X]\\) is still unique"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#analysis-of-the-residuals",
    "href": "teaching/linear_model/slides/validation.html#analysis-of-the-residuals",
    "title": "Validation",
    "section": "Analysis of the Residuals",
    "text": "Analysis of the Residuals\n\nRecall that \\(\\hat \\varepsilon = Y - \\widehat Y = P_{[X]^{\\perp}} \\varepsilon\\)\n\n\nResidual Properties\n\n\\(\\E(\\hat{\\varepsilon}) = 0\\)\n\\(\\Var(\\hat{\\varepsilon}) = \\sigma^2P_{[X]}^{\\perp}\\)\n\\(\\text{Cov}(\\hat{\\varepsilon}, \\hat{Y}) = 0\\)\nIf \\(\\1 \\in [X]\\): \\(\\bar{\\hat{\\varepsilon}} = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#diagnostic-tools",
    "href": "teaching/linear_model/slides/validation.html#diagnostic-tools",
    "title": "Validation",
    "section": "Diagnostic Tools",
    "text": "Diagnostic Tools\n\nGraphical Assessment: Visual evaluation of model quality\nHomoscedasticity Test: Test: \\(\\Var(\\varepsilon_i) = \\sigma^2\\) for all \\(i\\) (constant variance)\nNon-correlation Test:\nTest: \\(\\Var(\\varepsilon)\\) is diagonal (uncorrelated errors)\nNormality Test: Examine normality of residuals"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#residual-vs.-fitted-plot",
    "href": "teaching/linear_model/slides/validation.html#residual-vs.-fitted-plot",
    "title": "Validation",
    "section": "Residual vs. Fitted Plot",
    "text": "Residual vs. Fitted Plot\n\nThe scatter plot between \\(\\hat{Y}\\) and \\(\\hat{\\varepsilon}\\) is informative.\nSince \\(\\text{Cov}(\\hat{\\varepsilon}, \\hat{Y}) = 0\\), no structure should appear.\nIf patterns emerge, this may indicate violations of:\n\nLinearity assumption\nHomoscedasticity assumption\n\nNon-correlation assumption\nOr a combination of these…"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#homosced.-test-breusch-pagan",
    "href": "teaching/linear_model/slides/validation.html#homosced.-test-breusch-pagan",
    "title": "Validation",
    "section": "Homosced. Test (Breusch-Pagan)",
    "text": "Homosced. Test (Breusch-Pagan)\n\nWe want to test whether \\(\\Var(\\varepsilon_i)=\\sigma^2, ~~\\forall i\\)\n\n\nPrinciple: Assume \\(\\varepsilon_i\\) has variance \\(\\sigma_i^2 = \\sigma^2 + z_i^T\\gamma\\) where:\n\n\\(z_i\\) is a \\(k\\)-vector of variables that might explain heteroscedasticity (known)\nDefault in R: \\(z_i = (X_i^{(1)}, \\ldots, X_i^{(p)})\\), so \\(k = p\\)\n\\(\\gamma\\) is an unknown \\(k\\)-dimensional parameter\n\n\n\n\\(H_0: \\gamma = 0\\) (homosced.) VS \\(H_1: \\gamma \\neq 0\\) (heterosced.)\n\n\nR function: bptest from lmtest library"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#consequences-of-heteroscedasticity",
    "href": "teaching/linear_model/slides/validation.html#consequences-of-heteroscedasticity",
    "title": "Validation",
    "section": "Consequences of Heteroscedasticity",
    "text": "Consequences of Heteroscedasticity\n\nWhat happens:\n\nOLS estimation of \\(\\beta\\) is no longer optimal but remains consistent\nInference tools (tests, confidence intervals) become invalid because they rely on \\(\\hat{\\sigma}^2\\) estimation"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#solutions-for-heteroscedasticity",
    "href": "teaching/linear_model/slides/validation.html#solutions-for-heteroscedasticity",
    "title": "Validation",
    "section": "Solutions for Heteroscedasticity",
    "text": "Solutions for Heteroscedasticity\n\nTransformation: Transform \\(Y\\) (e.g., log transformation) to stabilize variance\nModeling: Model heteroscedasticity explicitly and account for it in estimation\nGLS: Use Generalized Least Squares (not detailed here)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#non-correlation-test",
    "href": "teaching/linear_model/slides/validation.html#non-correlation-test",
    "title": "Validation",
    "section": "3. Non-correlation Test",
    "text": "3. Non-correlation Test\n\nPurpose: Test if \\(\\Var(\\varepsilon)\\) is diagonal (uncorrelated errors)\n\n\nCorrelation between \\(\\varepsilon_i\\) often occurs with temporal data (index \\(i\\) represents time)\n\n\nAuto-correlation Model of order \\(r\\):\n\n\\[\\varepsilon_i = \\rho_1 \\varepsilon_{i-1} + \\dots + \\rho_r \\varepsilon_{i-r} + \\eta_i\\] where \\(\\eta_i \\sim \\text{iid } N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#tests-for-correlation",
    "href": "teaching/linear_model/slides/validation.html#tests-for-correlation",
    "title": "Validation",
    "section": "Tests for Correlation",
    "text": "Tests for Correlation\n\nIn the auto-correlation model \\(\\varepsilon_i = \\rho_1 \\varepsilon_{i-1} + \\dots + \\rho_r \\varepsilon_{i-r} + \\eta_i\\):\n\n\nDurbin-Watson Test (for \\(r = 1\\) only):\n\n\\(H_0: \\rho_1 = 0\\) VS \\(H_1: \\rho_1 \\neq 0\\)\nR function: dwtest from lmtest\n\n\n\nBreusch-Godfrey Test (for any \\(r\\)):\n\n\\(H_0: \\rho_1 = \\cdots = \\rho_r = 0\\) VS \\(H_1:\\) at least one \\(\\rho_j \\neq 0\\)\nUser chooses order \\(r\\) (default \\(r = 1\\))\nR function: bgtest from lmtest"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#consequences-of-auto-correlation",
    "href": "teaching/linear_model/slides/validation.html#consequences-of-auto-correlation",
    "title": "Validation",
    "section": "Consequences of Auto-correlation",
    "text": "Consequences of Auto-correlation\n\nOLS estimation of \\(\\beta\\) is no longer optimal but remains consistent\nInference tools (tests, confidence intervals) become invalid\n\n\nSolutions:\n\nGLS modeling: Model the dependence structure (complex, risky if wrong)\nModel improvement: Exploit the dependence to enhance the model\nEx: Explain \\(Y_i\\) using \\(Y_{i-1}\\) in addition to \\((X_i^{(1)}, \\dots, X_i^{(p)})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#normality-test",
    "href": "teaching/linear_model/slides/validation.html#normality-test",
    "title": "Validation",
    "section": "4. Normality Test",
    "text": "4. Normality Test\n\nPurpose: Examine normality of residuals \\(\\hat \\varepsilon\\)\nReminder on Normality Assumption\n\nNot essential when \\(n\\) is large\nAll tests remain asymptotically valid\nOnly prediction intervals truly require normality\n\n\n\nWhy examine it anyway?\nIf \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\) then \\(\\hat{\\varepsilon} \\sim N(0, \\sigma^2 P_{[X]}^{\\perp})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#diagnostic-tools-for-normality-of-hat-varepsilon",
    "href": "teaching/linear_model/slides/validation.html#diagnostic-tools-for-normality-of-hat-varepsilon",
    "title": "Validation",
    "section": "Diagnostic Tools for Normality of \\(\\hat \\varepsilon\\)",
    "text": "Diagnostic Tools for Normality of \\(\\hat \\varepsilon\\)\n\nQ-Q Plot (Henry’s line):\n\nPlot theoretical vs. sample quantiles of \\(\\hat{\\varepsilon}\\)\nR function: qqnorm\n\n\n\nShapiro-Wilk, \\(\\chi^2\\) or KS Tests:\n\nFormal test of normality for \\(\\hat{\\varepsilon}\\)\n\\(H_0\\): residuals are normally distributed\n\\(H_1\\): residuals are not normally distributed\nR function: shapiro.test"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#outlier-analysis",
    "href": "teaching/linear_model/slides/validation.html#outlier-analysis",
    "title": "Validation",
    "section": "Outlier Analysis",
    "text": "Outlier Analysis\n\nAn individual is atypical when:\n\nPoorly explained by the model, and/or\nHeavily influences coefficient estimation\n\n\n\nIdentify these individuals to:\n\nUnderstand the reason for this particularity\nPotentially modify the model accordingly\n\nPotentially exclude the individual from the study"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#poorly-explained-individuals",
    "href": "teaching/linear_model/slides/validation.html#poorly-explained-individuals",
    "title": "Validation",
    "section": "Poorly Explained Individuals",
    "text": "Poorly Explained Individuals\n\nIndividual \\(i\\) is poorly explained if its residual \\(\\hat{\\varepsilon}_i\\) is “abnormally” large.\n\n\nHow to quantify “abnormally”?\nLet \\(h_{ij}\\) be elements of matrix \\(P_{[X]}\\) (hat matrix).\nFor a Gaussian model: \\(\\hat{\\varepsilon}_i \\sim N(0, (1-h_{ii})\\sigma^2)\\)\n\n\nStandardized Residuals\n\n\\[t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#poorly-explained-individuals-1",
    "href": "teaching/linear_model/slides/validation.html#poorly-explained-individuals-1",
    "title": "Validation",
    "section": "Poorly Explained Individuals",
    "text": "Poorly Explained Individuals\n\n\n\\[h_{ij} = (P_{[X]})_{ij} \\and t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\]\n\n\n\nWe expect \\(t_i \\sim St(n-p)\\) (not strictly true since \\(\\hat{\\varepsilon}_i \\not\\perp \\hat{\\sigma}^2\\))\n\n\n\nDefinition\n\n\nIndividual \\(i\\) is considered poorly explained by the model if: \\[|t_i| &gt; t_{n-p}(1-\\alpha/2)\\] for predetermined \\(\\alpha\\), typically \\(\\alpha = 0.05\\), giving \\(t_{n-p}(1-\\alpha/2) \\approx 2\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#leverage-points",
    "href": "teaching/linear_model/slides/validation.html#leverage-points",
    "title": "Validation",
    "section": "Leverage Points",
    "text": "Leverage Points\n\nA point is influential if it contributes significantly to \\(\\hat{\\beta}\\) estimation.\n\n\nLeverage value: \\(h_{ii}\\) corresponds to the weight of \\(Y_i\\) on its own estimation \\(\\hat{Y}_i\\)\n\n\nWe know that: \\(\\sum_{i=1}^n h_{ii} = \\text{tr}(P_{[X]}) = p\\)\n\n\nTherefore, on average: \\(h_{ii} \\approx p/n\\)\n\n\n\n\n\nDefinition\n\n\nIndividual \\(i\\) is called a leverage point if \\(h_{ii} \\gg p/n\\)\nTypically: \\(h_{ii} &gt; 2p/n\\) or \\(h_{ii} &gt; 3p/n\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#outlier-analysis-cooks-distance",
    "href": "teaching/linear_model/slides/validation.html#outlier-analysis-cooks-distance",
    "title": "Validation",
    "section": "Outlier Analysis: Cook’s Distance",
    "text": "Outlier Analysis: Cook’s Distance\n\nCook’s Distance\nQuantifies the influence of individual \\(i\\) on \\(\\hat{Y}\\):\n\n\\[C_i = \\frac{\\|\\hat{Y} - \\hat{Y}_{(-i)}\\|^2}{p\\hat{\\sigma}^2}\\]\n\nwhere \\(\\hat{Y}_{(-i)} = X\\hat{\\beta}_{(-i)}\\) with \\(\\hat{\\beta}_{(-i)}\\): estimation of \\(\\beta\\) without individual \\(i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#cooks-distance-alternative-formula",
    "href": "teaching/linear_model/slides/validation.html#cooks-distance-alternative-formula",
    "title": "Validation",
    "section": "Cook’s Distance, Alternative Formula",
    "text": "Cook’s Distance, Alternative Formula\n\n\n\\[C_i = \\frac{1}{p} \\cdot \\frac{h_{ii}}{1-h_{ii}} \\cdot t_i^2,\\]\n\nwhere \\(t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\).\nThis formula shows that Cook’s distance \\(C_i\\) combines:\n\nAberrant effect of individual (through \\(t_i\\))\nLeverage effect (through \\(h_{ii}\\))\n\n\n\nR functions: cooks.distance and last plot of plot.lm"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-lectures",
    "href": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-lectures",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Link with Previous Lectures",
    "text": "Link with Previous Lectures\n\nPreviously, we considered that:\n\nResponse variable \\(Y\\) is quantitative\nExplanatory variables \\(X^{(j)}\\) are quantitative\n\n\n\nWe still assume \\(Y\\) is quantitative, but explanatory variables can be qualitative and/or quantitative."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#terminology",
    "href": "teaching/linear_model/slides/anova_ancova.html#terminology",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Terminology",
    "text": "Terminology\n\nANOVA (Analysis of Variance): All explanatory variables \\(X^{(j)}\\) are qualitative\nANCOVA (Analysis of Covariance): Explanatory variables mix both quantitative and qualitative variables\n\n\nWe’ll see that these situations reduce to the case of the previous chapter."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#context-notations",
    "href": "teaching/linear_model/slides/anova_ancova.html#context-notations",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Context, Notations",
    "text": "Context, Notations\n\nWe seek to explain \\(Y\\) using a single qualitative variable \\(A\\).\nWe observe \\((Y, X)\\), and define\n\n\n\n\n\\[N_i = \\sum_{k=1}^n \\mathbf 1\\{X_k=i\\} \\and \\overline Y_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\mathbf 1\\{X_k=i\\}\\]\n\n\n\n\nTotal mean:\n\\(Y_i = \\frac{1}{N}\\sum_{k=1}^n Y_k = \\frac{1}{N}\\sum_{i=1}^I\\sum N_i \\overline Y_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#setting",
    "href": "teaching/linear_model/slides/anova_ancova.html#setting",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Setting",
    "text": "Setting\n\nFor \\(k = 1, \\ldots, n\\)\n\n\\[Y_k = \\sum_{i=1}^I \\mu_i \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\]\n\nwhere \\(\\E[\\varepsilon_k]=0\\), \\(\\Cov(\\varepsilon_k,\\varepsilon_l)=\\sigma^2\\1\\{k=l\\}\\)\n\n\\(Y_k\\) is random and has expectation \\(\\mu_i\\) if \\(X_k=A_i\\)\n\\(\\Var(Y_k)=\\sigma^2\\) is the same regardless of modality \\(A_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#problem",
    "href": "teaching/linear_model/slides/anova_ancova.html#problem",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Problem",
    "text": "Problem\n\nDo we have \\(\\mu_1 = \\mu_2 \\ldots = \\mu_I\\)? (Does factor \\(A\\) influences \\(Y\\)?)\nDoes a \\(\\mu_i\\) has more influence on \\(Y\\)?\nHow do we estimate the \\(\\mu_i\\)’s?"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#one-hot-encoding",
    "href": "teaching/linear_model/slides/anova_ancova.html#one-hot-encoding",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "One Hot Encoding",
    "text": "One Hot Encoding\n\nWe use one-hot encoding: \\(X_{ki}=\\1\\{X_k=A_i\\}\\)\n\n\nThat is, for individual \\(k\\), \\(X_{k\\cdot} = (\\1\\{X_k=A_1\\}, \\dots, \\1\\{X_k=A_I\\}) \\in \\{0,1\\}^I\\)\n\n\nOr, using previous notations, \\(X = (X^{(1)}, \\dots, X^{(I)})\\) where\n\\[X^{(i)}= \\begin{pmatrix}\nX^{(i)}_1 \\\\\n\\vdots  \\\\\nX^{(i)}_n\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#in-other-words",
    "href": "teaching/linear_model/slides/anova_ancova.html#in-other-words",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "In Other Words",
    "text": "In Other Words\n\nIf there are \\(3\\) categories e.g. blue, orange, green, we replace the column \\(X\\) by \\(3\\) columns\nExample with \\(I=3\\) categories and \\(n=5\\) individuals \\[\n\\begin{pmatrix}\n\\color{blue}{\\mathrm{blue}} \\\\\n\\mathrm{green} \\\\\n\\color{blue}{\\mathrm{blue}} \\\\\n\\mathrm{orange} \\\\\n\\mathrm{orange} \\\\\n\\end{pmatrix} \\quad  \\text{becomes} \\quad\nX=\\begin{pmatrix}\n\\color{blue}{\\mathrm{1}} & \\mathrm{0} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{0} & \\mathrm{1}\\\\\n\\color{blue}{\\mathrm{1}} & \\mathrm{0} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{1} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{1} & \\mathrm{0}\\\\\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-matrix-form",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-matrix-form",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model, Matrix Form",
    "text": "Model, Matrix Form\n\nWe rewrite the model \\(Y_k = \\sum_{i=1}^I \\mu_i \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\) as\n\n\\[Y = X\\mu + \\varepsilon\\]\n\n\nHere, \\(X_{k\\cdot} = (\\1\\{X_k=A_1\\}, \\dots, \\1\\{X_k=A_I\\}) \\in \\{0,1\\}^I\\)\nThere is no constant in this model"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-with-constant",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-with-constant",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model with Constant",
    "text": "Model with Constant\n\nIf we want to model the constant (intercept) and \\(I-1\\) modalities, we assume\n\n\\[Y_k = \\mu_1+\\sum_{i=2}^I (\\mu_i-\\mu_1) \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\]\n\n\n\nWhy \\(\\mu_i - \\mu_1\\) and not just \\(\\mu_i\\)?\n\n\\(\\E[Y_k | X_k=A_i] = \\mu_1 + \\mu_i - \\mu_1 = \\mu_i\\)\n\\(\\sum_{i=2}^{I}\\{X_k = A_i\\} = 1\\) (collinearity problem)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#in-r",
    "href": "teaching/linear_model/slides/anova_ancova.html#in-r",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "In R",
    "text": "In R\n\n\nwith intercept (default)\n\nlm(Y~A) #with intercept\n\n\nInterpretation: gives expectation \\(\\E[Y_k] = \\mu_1\\) and coefficients \\(\\alpha_i= \\mu_i - \\mu_1\\)\n \n\n\n\n\nwithout intercept\n\nlm(Y~A-1) #without intercept\n\n\nInterpretation: gives coefficients \\(\\alpha_i= \\mu_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#estimation-of-mu",
    "href": "teaching/linear_model/slides/anova_ancova.html#estimation-of-mu",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Estimation of \\(\\mu\\)",
    "text": "Estimation of \\(\\mu\\)\n\nWhichever the model we choose (with constant or not), estimation of \\(\\mu_i=\\E[Y_k|X_k=A_i]\\) is the same. Same for \\(\\Var(\\varepsilon_k)=\\sigma^2\\).\n\n\n\n\n\nProposition\n\n\nIn category \\(i\\), OLS estimation of \\(\\mu_i\\) leads to:\n\n\\(\\hat{\\mu}_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\1\\{X_k=A_i\\} = \\overline{Y}_i\\)\n\nAn unbiased estimator of \\(\\sigma^2\\) is\n\n\\(\\hat{\\sigma}^2 = \\frac{1}{n-I} \\sum_{k=1}^{n}\\sum_{i=1}^I (Y_{k} - \\overline{Y}_i)^2\\1\\{X_k=A_i\\}\\)\n\n\n\n\n\n\nProof (OLS): derive \\(\\sum_{k=1}^n (Y_k - \\mu'_i)^2\\) with respect to \\(\\mu'_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-for-factor-effect-anova-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-for-factor-effect-anova-test",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing for Factor Effect (ANOVA Test)",
    "text": "Testing for Factor Effect (ANOVA Test)\n\nWe want to test \\(H_0: \\mu_1 = \\cdots = \\mu_I\\).\nThis is a linear constraints test! (See previous chapters)\n\n\n\n\n\nProposition\n\n\nIf \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: \\mu_1 = \\cdots = \\mu_I\\):\n\\[F = \\frac{SSB/(I-1)}{SSW/(n-I)} \\sim F(I-1, n-I)\\]\n\n\\(SSB = \\sum_{i=1}^I N_i (\\overline{Y}_i - \\overline{Y})^2\\)\n\\(SSW = \\sum_{i=1}^I \\sum_{k=1}^{n} (Y_{k} - \\overline{Y}_i)^2\\1\\{X_k=A_i\\}\\)\n\nCritical region at level \\(\\alpha\\): \\(RC_\\alpha = \\{F &gt; f_{I-1,n-I}(1-\\alpha)\\}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-chapter",
    "href": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-chapter",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Link with previous chapter",
    "text": "Link with previous chapter\nIn \\(\\mu_1= \\ldots = \\mu_I\\), there are \\(I-1\\) constraints to test.\n\n\\[F = \\frac{n-I}{I-1} \\cdot \\frac{SSR_c - SSR}{SSR}\\]\n\n\nWe show that\n\n\\(SSR = SSW\\)\n\\(SSR_c = SST = \\sum_{k=1}^n(Y_k - \\overline Y)^2\\)\n\\(SST = SSB + SSW\\).\n\n\n\nIn R: anova(lm(Y~A))"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#factor-significance-analysis-of-variance-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#factor-significance-analysis-of-variance-test",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Factor Significance: Analysis of Variance Test",
    "text": "Factor Significance: Analysis of Variance Test\n\n\n\n\nWarning\n\n\nThe previous analysis of variance test tests equality of means between modalities not equality of variances\n\n\n\n\n\nIt is valid under the assumption \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\).\n\nGaussian assumption: Not critical if \\(n\\) is large\nHomoscedasticity: Important assumption"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#homoscedasticity-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#homoscedasticity-tests",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Homoscedasticity Tests",
    "text": "Homoscedasticity Tests\n\nHow to test equality of variances in each modality:\n\nLevene test\nBartlett test\n\n\n\nIn R: leveneTest or bartlett.test from car library"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis-multiple-testing-problem",
    "href": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis-multiple-testing-problem",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Post-hoc Analysis: Multiple Testing Problem",
    "text": "Post-hoc Analysis: Multiple Testing Problem\n\nIf factor \\(A\\) is significant, we want to know more:\nwhich modality(ies) differs from others?\n\n\nWe want to perform all tests:\n\n\\[H_{0}^{i,j}: \\mu_i = \\mu_j \\quad \\text{vs} \\quad H_{1}^{i,j}: \\mu_i \\neq \\mu_j\\]\n\n\n\nfor all \\(i \\neq j\\) in \\(\\{1, \\ldots, I\\}\\), corresponding to \\(I(I-1)/2\\) tests."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-naive-approach",
    "href": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-naive-approach",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Multiple Testing: Naive Approach",
    "text": "Multiple Testing: Naive Approach\n\nPerform all Student’s t-tests for mean comparison (1 constraint), each at level \\(\\alpha\\).\nProblem: Given the number of tests, this would lead to many false positives.\n\n\nFalse positives are a well-known problem in multiple testing.\n\n\nSolution: Apply a correction to the decision rule, e.g.\n\nBonferroni correction\nBenjamini-Hochberg correction\n\n\n\nFor one-way ANOVA: Tukey’s test addresses the problem."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-tukeys-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-tukeys-test",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Multiple Testing: Tukey’s Test",
    "text": "Multiple Testing: Tukey’s Test\n\n\n\\[Q = \\max_{(i,j)} \\frac{|\\overline{Y}_i - \\overline{Y}_j|}{\\hat{\\sigma}\\sqrt{\\frac{1}{N_i} + \\frac{1}{N_j}}}\\]\n\n\n\n\n\n\nDistribution of Tukey’s Test Statistic\n\n\nUnder \\(H_0: \\mu_1 = \\cdots = \\mu_I\\) and assuming \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\):\n\\[Q \\sim Q_{I,n-I}\\]\nwhere \\(Q_{I,n-I}\\) denotes the Tukey distribution with \\((I, n-I)\\) degrees of freedom.\nNote: This is exact if all \\(n_i\\) are equal, otherwise the distribution is approximately Tukey."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#individual-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#individual-tests",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Individual Tests",
    "text": "Individual Tests\nTo test each \\(H_0^{i,j}: \\mu_i = \\mu_j\\), we use the critical regions:\n\n\n\n\\[RC_\\alpha^{i,j} = \\left\\{|\\overline{Y}_i - \\overline{Y}_j| &gt; \\frac{\\hat{\\sigma}}{\\sqrt{2}} \\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}} \\cdot Q_{I,n-I}(1-\\alpha)\\right\\}\\]\n\n\nwhere \\(Q_{I,n-I}(1-\\alpha)\\) denotes the \\((1-\\alpha)\\) quantile of a tukey distribution \\(Q(I,n-I)\\) of degrees \\((I, n-I)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#key-properties-of-tukeys-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#key-properties-of-tukeys-test",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Key Properties of Tukey’s Test",
    "text": "Key Properties of Tukey’s Test\nThe form of the previous \\(RC_\\alpha^{i,j}\\) ensures that: \\[\\mathbb{P}_{\\mu_1 = \\cdots = \\mu_I}\\left(\\bigcup_{(i,j)} H_1^{i,j}\\right) = \\alpha\\]\nInterpretation: If all null hypotheses \\(H_0^{i,j}\\) are true (\\(\\mu_1 = \\cdots = \\mu_I\\)), then the probability of concluding at least one \\(H_1^{i,j}\\) equals \\(\\alpha\\).\n\nThis is the simultaneous Type I error rate equals \\(\\alpha\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#comparison-with-students-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#comparison-with-students-tests",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Comparison with Student’s tests",
    "text": "Comparison with Student’s tests\n\nIndividual Student’s t-tests at level \\(\\alpha\\): only guarantee \\(\\mathbb{P}_{\\mu_i = \\mu_j}(H_1^{i,j}) = \\alpha\\)\nWhen cumulated: \\(\\mathbb{P}_{\\mu_1 = \\cdots = \\mu_I}\\left[\\cup_{(i,j)} H_1^{i,j}\\right] \\approx 1\\) → false positives\n\n\nAdvantage\nWith Tukey’s test, two significantly different means are truly different, not just due to false positives.\nIn R: TukeyHSD"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-of-r-output",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-of-r-output",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Example of R output",
    "text": "Example of R output\n\nHomoscedasticity Test:\nleveneTest(Loss∼Exercise)\nWe get\nLevene’s Test for Homogeneity of Variance (center = median)\nDf F value Pr(&gt;F)\ngroup 3 0.6527 0.584\n68\n\n\nHomoscedasticity is ok. ANOVA?\nreg=lm(Loss∼Exercise) # Exercise has 4 categories\nanova(reg)\n\n\nWe get\nResponse: Loss\n         Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nExercise   3 712.56  237.519  20.657 1.269e-09 ***\nResiduals 68 781.89   11.498"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#results-interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#results-interpretation",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Results Interpretation",
    "text": "Results Interpretation\n\nThe means are significantly different. We read in particular:\n\n\\(I - 1 = 3\\), \\(n - I = 68\\)\n\\(SSB = 712.56\\), \\(SSW = 781.89\\)\n\\(F = \\frac{SSB/(I-1)}{SSW/(n-I)} = 20.657\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#r-example-post-hoc-analysis",
    "href": "teaching/linear_model/slides/anova_ancova.html#r-example-post-hoc-analysis",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "R Example: Post-hoc Analysis",
    "text": "R Example: Post-hoc Analysis\n\nWe finish by analyzing the mean differences more precisely:\n\n\nTukeyHSD(aov(Loss~Exercise))\n\n\nExercise\n     diff        lwr        upr     p adj\n2-1  7.1666667   4.1897551 10.1435782 0.0000001\n3-1  3.8888889   0.9119773  6.8658005 0.0053823\n4-1 -0.6111111  -3.5880227  2.3658005 0.9487355\n3-2 -3.2777778  -6.2546894 -0.3008662 0.0252761\n4-2 -7.7777778 -10.7546894 -4.8008662 0.0000000\n4-3 -4.5000000  -7.4769116 -1.5230884 0.0009537\nConclusion: All differences are significant, except between exercise 4 and exercise 1."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#setting-1",
    "href": "teaching/linear_model/slides/anova_ancova.html#setting-1",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Setting",
    "text": "Setting\n\nWe observe \\(Y\\) and explanatory variables \\(X^{(1)}, X^{(2)}\\)\n\n\\(X^{(1)}_k \\in \\{A_1, \\dots, A_I\\}\\) (\\(I\\) modalities)\n\\(X^{(2)}_k \\in \\{B_1, \\dots, B_J\\}\\) (\\(J\\) modalities)\n\\(N_{ij} = \\sum_{k=1}^n \\1\\{X^{(1)}_k \\in A_i\\}\\1\\{X_k^{(2)} \\in B_j\\}\\) individuals in modality \\(A_i\\) and \\(B_j\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model",
    "href": "teaching/linear_model/slides/anova_ancova.html#model",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model",
    "text": "Model\n\n\n\n\\[\\begin{aligned}\nY_k &= m + \\alpha_i\\sum_{i=1}^I\\1\\{X^{(1)}_k \\in A_i\\} +\\beta_i\\sum_{j=1}^J\\1\\{X^{(2)}_k \\in B_j\\} \\\\\n&+ \\gamma_{ij}\\sum_{i=1}^I\\sum_{j=1}^J\\1\\{X^{(1)}_k \\in A_i\\}\\1\\{X^{(2)}_k \\in B_i\\} + \\varepsilon_k\n\\end{aligned}\\]\n\n\n\n\nIn other words, in modality \\(A_i\\) and \\(B_j\\),\n\n\n\n\\(Y_k =m+\\alpha_i + \\beta_j + \\gamma_{ij}+ \\varepsilon_k\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#interpretation",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Interpretation",
    "text": "Interpretation\n\nThis is a model of the type \\(Y_k = \\mu_{ij} + \\varepsilon_k\\).\n\n\nIn modalities \\((i,j)\\), \\(\\E[Y_k] = \\mu_{ij} = m + \\alpha_i + \\beta_j + \\gamma_{ij}\\)\n\n\\(m\\): the average effect of \\(Y\\) (without considering \\(A\\) and \\(B\\))\n\\(\\alpha_i = \\mu_{i.} - m\\): the marginal effect due to \\(A\\)\n\\(\\beta_j = \\mu_{.j} - m\\): the marginal effect due to \\(B\\)\n\n\\(\\gamma_{ij} = \\mu_{ij} - m - \\alpha_i - \\beta_j\\): the remaining effect, due to interaction between \\(A\\) and \\(B\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-1",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-1",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Example 1",
    "text": "Example 1\n\n\\(Y\\): employee satisfaction\n\\(A\\): schedule type (flexible or fixed)\n\\(B\\): training level (basic or advanced)\n\n\nWe can imagine:\n\nEffect due to \\(A\\): satisfaction is greater with flexible schedules\nEffect due to \\(B\\): satisfaction is higher with advanced training\nNo particular interaction between A and B"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-2",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-2",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Example 2",
    "text": "Example 2\n\n\\(Y\\): plant yield\n\\(A\\): fertilizer type (1 or 2)\n\\(B\\): water quantity (low, medium, high)\nWe can imagine:\n\nEffect due to \\(A\\): yield differs according to fertilizer used\nEffect due to \\(B\\): yield is better when there is lots of water\nInteraction: fertilizer 1 is better with water, and vice versa for fertilizer 2\n\n\n\nMaybe interaction is so strong that the effect due to A seems absent!"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#constraints-on-the-parameters",
    "href": "teaching/linear_model/slides/anova_ancova.html#constraints-on-the-parameters",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Constraints on the Parameters",
    "text": "Constraints on the Parameters\n\nIn modalities \\((i,j)\\), \\(\\E[Y_k] = \\mu_{ij} = m + \\alpha_i + \\beta_j + \\gamma_{ij}\\)\n\n\nInitial two-factor ANOVA problem: \\(I \\times J\\) parameters \\(\\mu_{ij}\\).\n\n\nNow: \\(1 + I + J + IJ\\) parameters (\\(m\\), \\(\\alpha_i\\), \\(\\beta_j\\), \\(\\gamma_{ij}\\)).\n\n\nTherefore, we need \\(1 + I + J\\) constraints for identifiability:\n\n\n\n\\(\\sum_{i=1}^{I} \\alpha_i = 0 \\and \\sum_{j=1}^{J} \\beta_j = 0\\)\n\n\n\n\n\\(\\sum_{i=1}^{I} \\gamma_{ij} = 0 \\and \\sum_{j=1}^{J} \\gamma_{ij} = 0\\)\n\n\n\nThese constraints ensure model identifiability by removing the redundant parameters that cause multicollinearity issues."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#implementation-in-r",
    "href": "teaching/linear_model/slides/anova_ancova.html#implementation-in-r",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Implementation in R",
    "text": "Implementation in R\n\nThe complete model (with interaction) is launched with the command:\nlm(Y ~ A + B + A:B) ## or equivalently, lm(Y ~ A*B)\n\n\nWith these constraints, the parameter interpretation is as follows:\nIntercept: \\(m = \\mu_{11}\\)\nMain effect A: \\(\\alpha_i = \\mu_{i1} - \\mu_{11}\\)\nMain effect B: \\(\\beta_j = \\mu_{1j} - \\mu_{11}\\)\nInteraction: \\(\\gamma_{ij} = \\mu_{ij} - \\mu_{i1} - \\mu_{1j} + \\mu_{11}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#important-note-on-interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#important-note-on-interpretation",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Important Note on Interpretation",
    "text": "Important Note on Interpretation\n\nFrom\nlm(Y ~ A + B + A:B) ## or equivalently, lm(Y ~ A*B)\n\n\nIntercept: \\(m = \\mu_{11}\\)\nMain effect A: \\(\\alpha_i = \\mu_{i1} - \\mu_{11}\\)\nMain effect B: \\(\\beta_j = \\mu_{1j} - \\mu_{11}\\)\nInteraction: \\(\\gamma_{ij} = \\mu_{ij} - \\mu_{i1} - \\mu_{1j} + \\mu_{11}\\)\n\n\n\n\n\n\nBe Cautious on the Interpretation of the Coefficients\n\n\nTo impose the constraints from the previous approach (sum-to-zero constraints):\nlm(Y ~ A*B, contrasts = list(A = contr.sum, B = contr.sum))"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#estimation",
    "href": "teaching/linear_model/slides/anova_ancova.html#estimation",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Estimation",
    "text": "Estimation\n\nThe choice of constraints does not affect the estimation of the expectation of \\(Y\\) in each crossed modality \\(A_i \\cap B_j\\).\n\n\n\n\n\nProposition\n\n\nWhatever the linear constraints chosen, the OLS leads to, for all \\(i = 1, \\ldots, I\\), \\(j = 1, \\ldots, J\\) and \\(k = 1, \\ldots, N_{ij}\\), if \\(X^{(1)}_{k}=A_i\\) and \\(X_k^{(2)}=B_j\\):\n\\[\\widehat{Y}_{k} = \\overline{Y}_{ij}:= \\frac{1}{N_{ij}} \\sum_{k=1}^{n} Y_{k}\\1\\{X_k^{(1)}=A_i \\and X_k^{(2)}=B_j\\}\\]\nand to the estimation of the residual variance:\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - IJ} \\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^n (Y_{k} - \\overline{Y}_{ij})^2\\1\\{X_k^{(1)}=A_i \\and X_k^{(2)}=B_j\\}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#significance-tests-for-effects",
    "href": "teaching/linear_model/slides/anova_ancova.html#significance-tests-for-effects",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Significance Tests for Effects",
    "text": "Significance Tests for Effects\n\n\nIs the effect due to the interaction between A and B significant?\nIs the marginal effect due to A significant?\nIs the marginal effect due to B significant?\n\n\n\nFirst, in the additive model with interaction, \\(Y_{ijk} = m + \\alpha_i + \\beta_j + \\varepsilon_{ijk}\\)\n\n\nDo we have \\(\\gamma_{ij} = 0\\) for all \\(i, j\\)?"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#interpretation-on-plots",
    "href": "teaching/linear_model/slides/anova_ancova.html#interpretation-on-plots",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Interpretation on Plots:",
    "text": "Interpretation on Plots:\n\nPlot \\(\\overline Y_{ij}\\) in function of modalities \\((i,j)\\)\n\n\n\nwithout interactions, lines should be almost parallel\n\n\n\ninteraction.plot(A,B,Y)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#plot-presence-of-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#plot-presence-of-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Plot: Presence of Interaction",
    "text": "Plot: Presence of Interaction\nLines cross in presence of interaction"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#significance-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#significance-tests",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Significance Tests",
    "text": "Significance Tests\n\nWe want to test for the presence of an interaction:\n\n\\[H_0^{(AB)}: \\gamma_{ij} = 0 \\text{ for all } i, j\\]\n\n\n\nIf we conclude \\(H_0^{(AB)}\\) (accept the null hypothesis), we then want to test the marginal effects:\n\n\\[H_0^{(A)}: \\alpha_i = 0 \\text{ for all } i \\and H_0^{(B)}: \\beta_j = 0 \\text{ for all } j\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#presence-of-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#presence-of-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Presence of Interaction",
    "text": "Presence of Interaction\n\n\n\\[H_0^{(AB)}: \\gamma_{ij} = 0 \\text{ for all } i, j\\]\n\n\n\n\n\n\nWarning\n\n\nIf we reject \\(H_0^{(AB)}\\), it makes no sense to test whether A or B have an effect: they have one through their interaction.\n\n\n\n\n\nThese tests reduce to constraint tests in the regression model."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#balanced-design-analysis-of-variance",
    "href": "teaching/linear_model/slides/anova_ancova.html#balanced-design-analysis-of-variance",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Balanced Design Analysis of Variance",
    "text": "Balanced Design Analysis of Variance\n\nWe assume that “the design is balanced”: this means that \\(N_{ij}:=N\\) does not depend on \\(i\\) or \\(j\\). (Otherwise, everything becomes complicated).\n\n\nIn this case, we have the analysis of variance formula:"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#anova-formula",
    "href": "teaching/linear_model/slides/anova_ancova.html#anova-formula",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "ANOVA Formula",
    "text": "ANOVA Formula\n\n\n\n\\[S_T^2 = S_A^2 + S_B^2 + S_{AB}^2 + S_R^2\\]\n\n\n\\(S_T^2 = \\sum_{k=1}^{n} (Y_{k} - \\overline{Y})^2\\): total sum of squares\n\\(S_A^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{i.} - \\overline{Y})^2\\): \\(S^2_{between}\\) in the case of one-factor ANOVA where the factor is \\(A\\)\n\\(S_B^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{.j} - \\overline{Y})^2\\): \\(S^2_{between}\\) in the case of one-factor ANOVA where the factor is \\(B\\)\n\\(S_{AB}^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{ij} - \\overline{Y}_{i.} - \\overline{Y}_{.j} + \\overline{Y})^2\\) quantifies the interaction\n\\(S_R^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{N_{ij}} (Y_{k} - \\overline{Y}_{ij})^2\\1\\{X^{(1)}_k=A_i \\and X^{(2)}_k=B_j\\}\\): \\(S_{within}\\) in one-factor ANOVA"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing the Interaction",
    "text": "Testing the Interaction\n\nFor testing the interaction, \\(H_0^{(AB)}: \\gamma_{ij} = 0\\) for all \\(i, j\\)\n\n\nWe the linear constraint test statistic \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\gamma_{ij}=0\\) for all \\((i,j)\\).\n\n\n\n\\[F^{(AB)} = \\frac{S_{AB}^2/(I-1)(J-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2 I_n)\\), \\(F^{(AB)} \\sim \\mathcal F((I-1)(J-1), n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-a",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-a",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing the Effect of \\(A\\)",
    "text": "Testing the Effect of \\(A\\)\n\nFor testing the main effect of \\(A\\), \\(H_0^{(A)}: \\alpha_i = 0\\) for all \\(i\\):\nWe use \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\alpha_i=0\\) for all \\(i\\).\n\n\n\n\\[F^{(A)} = \\frac{S_A^2/(I-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\), \\(F^{(A)} \\sim \\mathcal{F}(I-1, n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-b",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-b",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing the Effect of \\(B\\)",
    "text": "Testing the Effect of \\(B\\)\n\nFor testing the main effect of \\(B\\), \\(H_0^{(B)}: \\beta_j = 0\\) for all \\(j\\):\nWe use \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\beta_j=0\\) for all \\(j\\).\n\n\n\n\\[F^{(B)} = \\frac{S_B^2/(J-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\), \\(F^{(B)} \\sim \\mathcal{F}(J-1, n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#r-outputs",
    "href": "teaching/linear_model/slides/anova_ancova.html#r-outputs",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "R outputs",
    "text": "R outputs\n\nIn software, these tests are summarized in a table as shown below.\nIn R: anova(lm(Y ~ A*B))\n\n\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\n\\(A\\)\n\\(I-1\\)\n\\(S_A^2\\)\n\\(S_A^2/(I-1)\\)\n\\(F^{(A)}\\)\n…\n\n\n\\(B\\)\n\\(J-1\\)\n\\(S_B^2\\)\n\\(S_B^2/(J-1)\\)\n\\(F^{(B)}\\)\n…\n\n\n\\(A:B\\)\n\\((I-1)(J-1)\\)\n\\(S_{AB}^2\\)\n\\(S_{AB}^2/((I-1)(J-1))\\)\n\\(F^{(AB)}\\)\n…\n\n\nResiduals\n\\(n-IJ\\)\n\\(S_R^2\\)\n\\(S_R^2/(n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#assumptions",
    "href": "teaching/linear_model/slides/anova_ancova.html#assumptions",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nFisher tests are based on the assumption \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\)\n\nNormality is not critical, but homoscedasticity is\nWe can test equality of variances in each modality of A (or B), or in each crossed modality if the \\(n_{ij}\\) are sufficiently large\nThis can be done with Levene’s test or Bartlett’s test"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#practical-procedure",
    "href": "teaching/linear_model/slides/anova_ancova.html#practical-procedure",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Practical Procedure",
    "text": "Practical Procedure\n\nTest equality of variances\nForm the ANOVA table (independent of chosen constraints)\nIf the \\(AB\\) interaction is significant: don’t change anything\nIf the interaction is not significant: analyze the marginal effects of A and B\n\n\nIf they are significant: the model is additive: lm(Y ~ A + B)\nOtherwise: we can remove \\(A\\) (or \\(B\\)) from the model"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis",
    "href": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Post-Hoc Analysis",
    "text": "Post-Hoc Analysis\nOnce effects are identified: perform post-hoc analysis by examining differences between (crossed) modalities more closely, using Tukey’s test as in one-factor ANOVA"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#principle-and-limits",
    "href": "teaching/linear_model/slides/anova_ancova.html#principle-and-limits",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Principle and Limits",
    "text": "Principle and Limits\n\nWe seek to explain \\(Y\\) using \\(k\\) qualitative variables \\(A=X^{(1)}\\), \\(B=X^{(2)}\\), \\(C=X^{(3)}\\), …\n\n\nWe can assume that \\(\\E[Y_k]\\) depends on each factor and their interactions:\n\nTwo-way interactions: \\(AB\\), \\(BC\\), \\(AC\\), etc.\nThree-way interactions: \\(ABC\\), etc.\nHigher-order interactions: and potentially more\n\n\n\n\n\n\nWarning\n\n\nThis results in \\(2^k - 1\\) possible effects for \\(k\\) factors."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-approach",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-approach",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Testing Approach",
    "text": "Testing Approach\n\nEach effect can be tested using linear constraint tests. However, this approach presents several challenges:\n\nMultiple testing burden: Too many tests to perform\nSample size limitations: Risk of insufficient sample sizes in each crossed modality\n\n\n\nIn practice, choices are made to include only a limited number of effects and interactions in the analysis."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-components",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-components",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model Components",
    "text": "Model Components\n\nMost general situation: we seek to explain Y using both quantitative and qualitative variables.\n\n\nQuantitative variable effects: Through each regression coefficient \\(\\beta_j\\) associated with each variable\n\n\nFactor effects and interactions: As in ANOVA analysis\n\nMain effects of factors (quali. var.)\nInteractions between factors\n\n\n\nMixed interactions: Effects of interactions between factors and quantitative variables"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-of-mixed-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-of-mixed-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Example of Mixed Interaction",
    "text": "Example of Mixed Interaction\n\n\\(A \\in \\{\\text{Labrador}, \\text{Chihuaha}\\}\\)\n\\(X^{(1)}\\): size of the dog\nThere is clearly a mixed interraction between factor \\(A\\) and \\(X^{(1)}\\)!"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#statistical-testing",
    "href": "teaching/linear_model/slides/anova_ancova.html#statistical-testing",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Statistical Testing",
    "text": "Statistical Testing\n\nEach effect can be tested using a Fisher test.\nObviously, choices must be made regarding which effects and interactions to include in the model."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-without-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-without-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model Without Interaction",
    "text": "Model Without Interaction\n\n\\(Y\\): quantitative response variable\n\n\n\\(X\\): quantitative variable,\n\\(Z\\): factor with \\(I\\) modalities \\(\\{A_1, \\dots, A_I\\}\\)\n\n\nlm(Y~X+Z) estimates the model without interaction, where, for each individual \\(k = 1, \\ldots, n\\):\n\n\\[Y_k = m + \\beta X_k + \\sum_{i=2}^{I} \\alpha_i \\mathbf{1}\\{Z_k=A_i\\} + \\varepsilon_k\\]\n\nConstraint \\(\\alpha_1 = 0\\) is adopted to make the model identifiable."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-with-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-with-interaction",
    "title": "Analysis of Variance - ANOVA, and of Covariance - ANCOVA",
    "section": "Model With Interaction",
    "text": "Model With Interaction\n\nlm(Y~X+A+X:A) or lm(Y~X*A) estimates the model with interaction:\n\n\n\n\\[\n\\begin{aligned}\nY_k &= m + \\beta X_k + \\sum_{i=2}^{I} \\beta_i X_k \\mathbf{1}\\{Z_k=A_i\\} \\\\\n&+ \\sum_{i=2}^{I} \\alpha_i \\mathbf{1}\\{Z_k=A_i\\} + \\varepsilon_k\n\\end{aligned}\n\\]\n\n\n\nKey Insight: In the interaction model, the coefficient \\(\\beta\\) associated with \\(X\\) varies according to the modality \\(A_i\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Tests provide the theoretical basis for decision-making based on data. For example, doctors diagnose diseases using specific biological markers, industrial quality engineers evaluate the quality of a production batch, and climate scientists determine whether there are significant changes in measurements compared to the pre-industrial era.\n\n\nHypothesis testing is a key statistical method that enables professionals to make informed decisions. The process involves setting up two competing hypotheses:\n\nthe null hypothesis \\(H_0\\), which assumes no effect or no difference. This is usually an a priori on the data.\nthe alternative hypothesis \\(H_1\\), which suggests that there is a significant effect or difference.\n\nLet’s illustrate this with the example of a doctor diagnosing a disease, such as diabetes, using blood glucose levels:\n\nThe doctor begins by assuming the null hypothesis \\(H_0\\): the patient’s blood glucose levels are normal, indicating no diabetes.\nThe alternative hypothesis (\\(H_1\\)) would suggest that the patient’s glucose levels are abnormally high, indicating diabetes.\n\nAfter collecting the patient’s blood glucose data, the doctor compares it to a standard threshold for diagnosing diabetes. A hypothesis test is then performed to evaluate whether the observed glucose level is significantly higher than the threshold, or whether the difference could simply be due to random variation.\nThe testing process can be summarized as follows:\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests?\n\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)\n\n\n\n\nIn general, observing \\(T=1\\) is just one piece of evidence supporting \\(H_1\\). It’s possible that we were simply unlucky and the data led the test to reject \\(H_0\\). Additionally, the sample used might not be representative, or the elevated glucose level could be caused by another condition, not diabetes. Therefore, we cannot blindly accept \\(H_1\\) based on this result alone. Further investigation is essential when \\(T=1\\) to minimize the risk of making a Type I error.\nThe same applies when \\(T=0\\). A result of \\(T=0\\) simply indicates that the test provides no evidence in favor of \\(H_1\\), but it doesn’t mean that \\(H_1\\) is false. It is possible that the patient exhibits other symptoms that could still suggest a diagnosis of diabetes, despite the lack of evidence from the test.\n\n\n\n\n\n\nObjective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?\n\nIn the above example, we formally observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\) Imagine that instead of ten \\(6\\), we observed \\(600\\) tosses leading to the following counts:\n\nThe number of \\(6\\) is really close to \\(100 = 600/6\\), which implies that we would not reject \\(H_0\\) in the previous example. We would reject however if we wanted to test whether the dice is fair, since it is highly biased toward \\(2\\).\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_1\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_0\\) is “unlikely”)\n\n\n\n\n\n\n\n\nSee also the (Pluto notebook: illustration of pvalue)\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\) (“\\(95^{th}\\) percentile”)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Hypothesis testing is a key statistical method that enables professionals to make informed decisions. The process involves setting up two competing hypotheses:\n\nthe null hypothesis \\(H_0\\), which assumes no effect or no difference. This is usually an a priori on the data.\nthe alternative hypothesis \\(H_1\\), which suggests that there is a significant effect or difference.\n\nLet’s illustrate this with the example of a doctor diagnosing a disease, such as diabetes, using blood glucose levels:\n\nThe doctor begins by assuming the null hypothesis \\(H_0\\): the patient’s blood glucose levels are normal, indicating no diabetes.\nThe alternative hypothesis (\\(H_1\\)) would suggest that the patient’s glucose levels are abnormally high, indicating diabetes.\n\nAfter collecting the patient’s blood glucose data, the doctor compares it to a standard threshold for diagnosing diabetes. A hypothesis test is then performed to evaluate whether the observed glucose level is significantly higher than the threshold, or whether the difference could simply be due to random variation.\nThe testing process can be summarized as follows:\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Decision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)\n\n\n\n\nIn general, observing \\(T=1\\) is just one piece of evidence supporting \\(H_1\\). It’s possible that we were simply unlucky and the data led the test to reject \\(H_0\\). Additionally, the sample used might not be representative, or the elevated glucose level could be caused by another condition, not diabetes. Therefore, we cannot blindly accept \\(H_1\\) based on this result alone. Further investigation is essential when \\(T=1\\) to minimize the risk of making a Type I error.\nThe same applies when \\(T=0\\). A result of \\(T=0\\) simply indicates that the test provides no evidence in favor of \\(H_1\\), but it doesn’t mean that \\(H_1\\) is false. It is possible that the patient exhibits other symptoms that could still suggest a diagnosis of diabetes, despite the lack of evidence from the test.",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Objective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?\n\nIn the above example, we formally observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\) Imagine that instead of ten \\(6\\), we observed \\(600\\) tosses leading to the following counts:\n\nThe number of \\(6\\) is really close to \\(100 = 600/6\\), which implies that we would not reject \\(H_0\\) in the previous example. We would reject however if we wanted to test whether the dice is fair, since it is highly biased toward \\(2\\).\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_1\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_0\\) is “unlikely”)\n\n\n\n\n\n\n\n\nSee also the (Pluto notebook: illustration of pvalue)\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\) (“\\(95^{th}\\) percentile”)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#section",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\n\n\nTest",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#type-of-problems",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#type-of-problems",
    "title": "Hypothesis Testing",
    "section": "Type of Problems",
    "text": "Type of Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\n\n\nMultiple VS Multiple",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nParametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nNon-parametric: otherwise\n\n\nExample of multiple VS multiple parametric problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\nDecision rule\n\n\n\n\nA decision rule or test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\n\n\n\n\n\n\n\n\nTest statistic\n\n\n\n\na test statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nExample of \\(\\mathcal R\\), for a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) \\not \\in [t_1, t_2]\\}:~~~~~~\\mathcal R = (-\\infty,t_1)\\cup (t_2, +\\infty)\\;\n\\end{aligned}\n\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#continous-measures",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#continous-measures",
    "title": "Hypothesis Testing",
    "section": "Continous Measures",
    "text": "Continous Measures\n\ndensity wrp to Lebesgue: \\(dP(x) = p(x)dx\\)\nPDF (proba density function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha} = \\alpha)\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#discrete-measures",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#discrete-measures",
    "title": "Hypothesis Testing",
    "section": "Discrete Measures",
    "text": "Discrete Measures\n\ndensity wrp to counting measure: \\(P(X=x) = p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\nGaussian/Bernoulli\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)\n\n\n\n\n\nExponential/Geometric\n\n\n\nExponential \\(\\mathcal E(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)\n\n\n\n\n\nGamma/Poisson\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\]\n\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\lambda\\)\n\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\n\nNumber of tics before time \\(1\\) of an atomic clock of rate \\(\\lambda\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\)?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nEquivalently, we can consider the log-likelihood ratio test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#neyman-pearson",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#neyman-pearson",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson",
    "text": "Neyman Pearson\n\n\n\n\n\n\nNeyman Pearson’s theorem\n\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\nProof of Neyman Pearson’s theorem\nWe prove the theorem in the case where \\(P\\) and \\(Q\\) each have a density \\(p\\) and \\(q\\) on \\(\\mathbb R^n\\). For any \\(t &gt; 0\\), define \\[I_t(P, Q) = \\int_{x \\in \\mathbb R^n} |q(x) - tp(x)|dx \\; .\\]\nWhen \\(t=1\\), this quantity is equal to the total-variation distance between \\(P\\) and \\(Q\\). For any event A in \\(\\mathbb R^n\\) , it holds that\n\\[\n\\begin{split}\n  I_t(P, Q) &= \\int_{x \\in \\mathbb R^n} |q(x) - tp(x)|dx \\\\\n  &= \\int_{q(x)&gt; tp(x)} q(x) - tp(x)dx + \\int_{q(x)&lt; tp(x)} tp(x) - q(x)dx\\\\\n  &=  2\\int_{x \\in \\mathbb R^n} \\mathbf1_{q(x)&gt; tp(x)}(q(x) - tp(x))dx + t-1 \\\\\n  &\\geq 2\\int_{x \\in \\mathbb R^n} \\mathbf 1_A \\mathbf1_{q(x)&gt; tp(x)}(q(x) - tp(x))dx + t-1\\\\\n  &\\geq t-1 + 2(Q(A) - tP(A)) \\; .\n\\end{split}\n\\]\nIf \\(A  = \\{x \\in \\mathbb R^n : q(x) &gt; tp(x)\\}\\), then the two last inequalities are equalities. In particular, \\[I_t(P, Q) = t-1 + 2\\sup_{A \\subset \\mathbb R^n}(Q(A) - tP(A))) \\; .\\]\nAssume that the type-1 error of \\(T\\) is smaller than \\(\\alpha\\): \\(P(T=1) \\leq \\alpha\\).\nThe power of \\(T\\), \\(Q(T = 1)\\), is upper-bounded as follows: \\[\n\\begin{split}\n  Q(T=1) &\\leq Q(T=1) + t(\\alpha - P(T=1))\\\\\n  &= \\alpha t + Q(T=1) - tP(T=1) \\\\\n  &\\leq \\alpha t + \\frac{t-1}{2} + \\frac{1}{2}I_t \\; .\n\\end{split}\n\\]\n\nThere is equality in the second inequality if \\(T=1\\) is the event \\(\\{ \\frac{q(X)}{p(X)} &gt; t \\}\\).\nThere is equality in the first inequality if \\(P(T=1) = \\alpha\\).\n\nLet \\(t_{\\alpha}\\) be such that \\(P(\\frac{q(X)}{p(X)}&gt; t_{\\alpha}) = \\alpha\\). The test \\(T^*(X) = \\mathbf1 \\{\\frac{q(X)}{p(X)}&gt; t_{\\alpha}\\}\\) satisfies the two above points, since its rejection event is exactly \\(\\{T^*(X) = 1\\} = \\{\\frac{q(X)}{p(X)}&gt; t_{\\alpha}\\}\\) and since \\(P(T^* = 1) = \\alpha.\\) Hence, for any test \\(T\\) of type 1 error smaller than \\(\\alpha\\), it holds that \\(Q(T = 1) \\leq Q(T^* = 1)\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples-2",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples-2",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\nExample (Gaussians)\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\nLog-likelihood ratio test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\n\n\n\nExample: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\n\n\\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]\n\n\nExample: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): reject if \\(N \\geq 19\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#generalities",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#generalities",
    "title": "Hypothesis Testing",
    "section": "Generalities",
    "text": "Generalities\n\n\\(H_0 = \\{ \\mathcal P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(right-tailed: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(left-tailed: unilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(simple/multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( multiple/multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-tailed test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (right-tailed test).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#pivotal-test-statistic",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#pivotal-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic",
    "text": "Pivotal Test Statistic\n\nConsider \\(\\Theta_0\\) not singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\nPivotal test statistic\n\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#p-value",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#p-value",
    "title": "Hypothesis Testing",
    "section": "P-value",
    "text": "P-value\nSee the (Pluto notebook: Illustration of pvalue)\n\n\n\n\n\n\nP-value: definition\n\n\n\nWe define \\(p_{value}(x_{\\mathrm{obs}}) =\\mathbb P(\\psi(X) &gt; x_{\\mathrm{obs}})\\) for a right-tailed test.\nFor a two-tailed test, \\(p_{value}(x_{\\mathrm{obs}}) =2\\min(\\mathbb P(\\psi(X) &gt; x_{\\mathrm{obs}}),\\mathbb P(\\psi(X) &lt; x_{\\mathrm{obs}}))\\)\n\n\n\n\n\n\n\n\nP-value under \\(H_0\\)\n\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(p_{value}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\nProof.\nThe p-value is a probability, so it belongs to \\([0,1]\\). Let \\(F_{\\psi}\\) be the cumulative distribution function of a random variable \\(\\psi(X)\\) when \\(X\\) follows distribution \\(P\\), that is \\(G_{\\psi}(t) = \\mathbb P(\\psi(X) &gt; t)\\). It holds that \\[\n\\mathbb P(\\psi(X') &gt; \\psi(X) ~|~ \\psi(X)) = F_{\\psi}(\\psi(X))\n\\] If \\(u \\in [0,1]\\), then \\[\n\\begin{aligned}\n\\mathbb P(\\mathrm{pvalue}(X) &gt; u) &= \\mathbb P(F_{\\psi}(\\psi(X)) &gt; u) \\\\\n&= \\mathbb P(\\psi(X) &gt; F_{\\psi}^{-1}(u)) \\\\\n&= 1- F_{\\psi}(F_{\\psi}^{-1}(u)) = 1-u\n\\end{aligned}\n\\] Hence, \\(\\mathrm{pvalue}(X)\\) is uniform when \\(X\\) follows distribution \\(\\mathbb P\\). \\[\\tag*{$\\blacksquare$}\\]\n\n\n\nIn practice: reject if \\(p_{value}(X) &lt; \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\) under \\(H_0\\):",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html",
    "href": "teaching/hypothesis_testing/lectures/dependency.html",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "We observe iid paired data \\((X_1, Y_1), \\dots, (X_n,Y_n)\\) of unknown mean \\(\\mu_X, \\mu_Y\\) and cov matrix \\(\\Sigma\\).\nCov matrix: \\(\\Sigma =\n\\left(\\begin{matrix}\n\\sigma_X^2 & \\mathrm{Cov(X,Y)} \\\\\n\\mathrm{Cov(X,Y)} & \\sigma_Y^2 \\\\\n\\end{matrix}\\right)\\)\n\\(H_0: \\mathrm{Cov}(X,Y)=0\\) or \\(H_1: \\mathrm{Cov}(X,Y)\\neq 0\\)\n\\(\\mathrm{Cov(X,Y)} = \\mathbb E[(X- \\mathbb E[X])(Y- \\mathbb E[Y])]\\)\n\\(\\sigma_X^2 = \\mathrm{Cov(X,X)}\\)\n\\(\\sigma_Y^2 = \\mathrm{Cov(Y,Y)}\\)\n\\(\\mathrm{Cor(X,Y)} = \\frac{\\mathrm{Cov(X,Y)}}{\\sigma_X \\sigma_Y}\\)\n\n\n\n\n\n\n\n\n\n\nPearson’s Correlation Test\n\n\n\n\n\\(r =  \\frac{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline X)^2\\sum_{i=1}^n (Y_i - \\overline Y)^2}}\\)\n\\(\\psi(X,Y) = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\)\nUnder \\(H_0\\), \\(\\psi(X,Y) \\approx \\mathcal T(n-2)\\)\n\n\n\n\n\n\nMonte Carlo Simulation with \\(n=4\\):\n\n\n\n\n\n\n\n\n\n\n\n\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nGroup \\(k\\): \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\). Ex: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\)\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2 \\sim \\sigma^2\\chi^2(N_k-1)\\) (under \\(H_0\\))\nVar of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\sim\\sigma^2\\chi^2(d-1)\\) (under \\(H_0\\))\n\n\n\n\n\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}{\\tfrac{1}{d-1}SSB}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\).\n\n\n\n\n\n\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#correlation-test-1",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#correlation-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "Monte Carlo Simulation with \\(n=4\\):",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#anova-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#anova-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nGroup \\(k\\): \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\). Ex: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\)\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2 \\sim \\sigma^2\\chi^2(N_k-1)\\) (under \\(H_0\\))\nVar of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\sim\\sigma^2\\chi^2(d-1)\\) (under \\(H_0\\))\n\n\n\n\n\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}{\\tfrac{1}{d-1}SSB}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#interpretation-of-variances-in-anova",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#interpretation-of-variances-in-anova",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#chi2-homogeneity-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#chi2-homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Homogeneity Test",
    "text": "\\(\\chi^2\\) Homogeneity Test\n\n\\(d\\) different bags (or groups), each containing balls of \\(m\\) potential colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix of counts:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\n\n\n\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all color \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#example-soft-drink-preferences",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\)\n\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#chi2-independence-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#chi2-independence-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Independence Test",
    "text": "\\(\\chi^2\\) Independence Test\n\n\n\n\n\n\n\nObservations: Paired Categorical Variables \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\ne.g. \\(X_i \\in \\{\\mathrm{male}, \\mathrm{female}\\}\\), \\(Y_i \\in \\{\\mathrm{coffee}, \\mathrm{tea}\\}\\)\nIdea: regroup the data into bags, e.g. \\(\\{\\mathrm{male}, \\mathrm{female}\\}\\)\nBuild the contingency table\nPerform a chi-square homogeneity test\n\n\n\n\nExample of contingency table:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n30\n20\n50\n\n\nTea\n28\n22\n50\n\n\nTotal\n58\n42\n100\n\n\n\nExpected counts:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n29\n21\n50\n\n\nTea\n29\n21\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\nDegree of freedom: \\((2-1)(2-1) = 1\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#symetric-random-variable",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)\n\n\n\n\n\n\n\n\n\n\n\nSymetrization\n\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t)\n&=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy \\\\\n&= \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]\n\\(X\\) indep of \\(Y\\) \\(\\implies\\) \\(X-Y\\) symmetric \\(\\implies\\) \\(\\mathrm{median}(X-Y) = 0\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#dependency-problem-for-paired-data",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The median of \\((X_i - Y_i)\\) is \\(0\\) for all \\(i\\)\n\\(H_1:\\) The median of \\((X_i - Y_i)\\) is not \\(0\\) for some \\(i\\)\n\n\n\n\n\n\n\n\n\n\nGenerality of \\(H_0\\)\n\n\n\n\nIf \\(X_i-Y_i\\) is symmetric \\(i\\), then we are under \\(H_0\\).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\).\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\)).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\n\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]\n\n\n\n\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\n\nUnder \\(H_0\\),\n\nSigns of the \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &gt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\nif \\(H_1\\): \\(\\mathrm{median}(D_i) &gt; 0\\): left-one sided test on \\(W_-\\).\n\n\n\nThis approximation fits well the exact distribution. Monte-Carlo simulation:\n\nTo generate a \\(W_-\\) under \\(H_0\\) in Julia:\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#effect-of-drug-on-blood-pressure",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#effect-of-drug-on-blood-pressure",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Effect of Drug on Blood Pressure",
    "text": "Effect of Drug on Blood Pressure\n\n\\(H_0\\): the drug has no effect. \\(H_1\\): it lowers the blood pressure\n\n\n\n\nPatient\n\\(X_i\\) (Before)\n\\(Y_i\\)​ (After)\n\\(D_i = X_i-Y_i\\)​\n\\(R_i\\)\n\n\n\n\n1\n150\n140\n10\n6 (+)\n\n\n2\n135\n130\n5\n5 (+)\n\n\n3\n160\n162\n-2\n2 (-)\n\n\n4\n145\n146\n-1\n1 (-)\n\n\n5\n154\n150\n4\n4 (+)\n\n\n6\n171\n160\n11\n7 (+)\n\n\n7\n141\n138\n3\n3 (+)\n\n\n\n\n\\(W_- = 1+2 = 3\\).\nFrom a simulation, we approx \\(\\mathbb P(W_-=i)\\), for \\(i \\in \\{0, 1, 2, 3, 4, 5,6\\}\\) under \\(H_0\\) by\n\n[0.00784066, 0.00781442, 0.00781534, 0.01563892, 0.01562184, 0.02343478]\n\nFrom a simulation, \\(p_{value}=\\mathbb P(W_- \\leq 3) \\approx 0.039 &lt; 0.05\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "",
    "text": "Note sur la correction\n\n\n\nComme répété à plusieurs reprises en cours, la qualité de la rédaction ainsi que les détails d’explications sont grandement pris en compte dans l’évaluation, surtout pour les introductions des modèles (premières questions des exercices)."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables",
    "text": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables\nNous cherchons à déterminer si les citoyens d’une région ont une quelconque préférence pour les sources d’énergie renouvelables (par exemple, solaire, éolienne) ou les sources d’énergie non renouvelables (par exemple, charbon, gaz naturel). Nous supposons que, a priori, il n’y a aucune préférence en moyenne. Nous interrogeons \\(n\\) individus, et nous notons \\(X\\) le nombre de répondants qui préfèrent les énergies renouvelables."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#questions",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#questions",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions :",
    "text": "Questions :\n\nFormalisez le problème de test d’hypothèse, et définissez \\(H_0\\) et \\(H_1\\)​. Indiquez si ce test est unilatéral ou bilatéral.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\(X\\) le nombre d’individus qui préfèrent les ENR.\nOn suppose que \\(X\\) suit une loi \\(Bin(n,p)\\), où \\(p\\) est un paramètre inconnu.\nOn considère le problème de test: \\(H_0: p=1/2\\) VS \\(H_1: p\\neq 1/2\\)\n\nC’est un problème de test bilatéral\n\n\n\nNous interrogeons \\(n=100\\) individus, et \\(X=58\\) préfèrent les sources d’énergie renouvelables. Écrivez la p-valeur en fonction de \\(F\\), la fonction de répartition d’une distribution Binomiale Bin\\((100,0.5)\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\nSous \\(H_0\\), \\(X\\) suit une loi \\(Bin(100, 0.5)\\) de cdf \\(F\\). Ainsi, \\(p_{valeur} = 2\\min(\\mathbb P(X \\leq 58), \\mathbb P(X \\geq 58)) =  2\\mathbb P(X \\geq 58) = 2(1- F(57))\\).\nd’où \\(p_{valeur} = 2(1- F(57))\\).\n(58 accepté aussi car \\(\\mathbb P(X=58)\\) est petit)\n\n\n\nÉcrivez une ligne de code qui calculerait la p-valeur exacte en Julia, Python, ou R.\n\n2*(1-cdf(Binomial(100, 0.5), 57)) # Julia\n2*(1-pbinom(57,100,0.5)) # R\n# Résultat: 0.133\n\nDonnez une approximation de la p-valeur en utilisant une approximation gaussienne et le graphique de la fonction de répartition de \\(\\mathcal N(0,1)\\) fourni dans le sujet. Quelle est votre conclusion ?\n\n\n\n\n\n\n\nNote\n\n\n\nSous \\(H_0\\), \\(\\mathbb E_0[X]=n/2\\) et \\(\\mathbb V_0(X)=n\\frac{1}{2}(1-\\frac{1}{2})=n/4\\)\n\nOn renormalise \\(X\\) pour obtenir la statistique de test suivante: \\[\\psi(X) = \\frac{X-\\mathbb E_0[X]}{\\sqrt{\\mathbb V_0(X)}}=\\frac{X-n/2}{\\sqrt{n/4}}\\]\nLorsque \\(n \\to \\infty\\), \\(\\psi(X)\\) converge en loi vers \\(\\mathcal N(0,1)\\). Ainsi, on obtient suite à cette approximation Gaussienne (\\(n/2\\) est assez grand):\n\\[\np_{valeur} = 2\\mathbb P(\\psi(X) \\geq \\psi(X_{obs})) \\asymp 2\\mathbb P(Z \\geq \\psi(58)),\n\\] où \\(Z\\) est une VA qui suit une loi \\(\\mathcal N(0,1)\\) sous \\(\\mathbb P\\). On calcule \\(\\psi(58) \\asymp 1.6\\), et par lecture graphique, \\(2*P(Z \\geq \\psi(58)) \\asymp 0.11\\)\nLa \\(p_{valeur}\\) est grande, on ne donc rejette pas à un niveau de \\(5\\%\\).\n\n\n\nRedéfinissez l’hypothèse alternative \\(H_1\\)​ et calculez une p-valeur approximative si nous cherchons à déterminer si les citoyens ont une préférence pour :\n\nles sources d’énergie renouvelables.\nles sources d’énergie non renouvelables. Quelles sont vos conclusions pour ces deux autres problèmes ?\n\n\n\n\n\n\n\n\nRéponse\n\n\n\nLes problèmes de test deviennent unilatéraux.\n\n\\(H_1: p &gt; 0.5\\), \\(p_{valeur} \\asymp 0.11/2 \\asymp 0.055\\) on pourrait rejeter à un niveau \\(10\\%\\)\n\\(H_1: p &lt; 0.5\\) , \\(p_{valeur} \\asymp (1-0.11)/2 \\asymp 0.445\\), on ne rejette pas."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 2 : Surveillance environnementale de la pollution fluviale",
    "text": "Exercice 2 : Surveillance environnementale de la pollution fluviale\nUne agence environnementale surveille les niveaux de pollution d’une rivière pour déterminer si une usine à proximité provoque une augmentation de la concentration de produits chimiques nocifs. La concentration cible pour un produit chimique spécifique est de \\(15 \\, \\text{ppm}\\) (parties par million), ce qui est considéré comme sûr pour la vie aquatique. Pour un échantillon de \\(n = 20\\) prélèvements d’eau effectués en aval de l’usine, la concentration moyenne empirique est \\(\\bar{X}_n = 16,3 \\, \\text{ppm}\\), et la variance empirique est \\(S^2_n = 2,4 \\, \\text{ppm}^2\\).\nA priori, on suppose que la rivière respecte le seuil de pollution sans danger de \\(15 \\, \\text{ppm}\\).\nNous visons à tester, avec un niveau de signification \\(\\alpha = 0,05\\), si la concentration chimique en aval dépasse le seuil de sécurité, indiquant une pollution provenant de l’usine.\n\nQuestions :\n\nEn utilisant une hypothèse Gaussienne, formalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\). S’agit-il d’un test unilatéral ou bilatéral ?\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\((X_1, \\dots, X_n)\\) les concentrations des prélèvement en ppm (toujours bien de mettre les unités)\nOn suppose que les \\(X_i\\) sont iid de loi \\(\\mathcal N(\\mu, \\sigma^2)\\) où \\(\\mu\\) et \\(\\sigma^2\\) sont des paramètres inconnus\nOn souhaite tester s’il y a un problème de pollution, cad\n\\(H_0: \\mu = 15\\) ppm VS \\(H_1: \\mu &gt; 15\\) ppm\n\nC’est un problème de test unilatéral droit (\\(\\mu \\leq 15\\) ppm aussi accepté pour \\(H_0\\))\n\n\n\nDéfinissez la statistique de test. Quelle est sa distribution sous \\(H_0\\) ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn utilise la statistique de test de Student \\[\\psi(X) = \\sqrt{n}\\frac{\\overline X - 15}{\\hat \\sigma},\\]\noù \\(\\overline X=\\frac{1}{n}\\sum_{i=1}^n X_i\\) et \\(\\hat \\sigma^2 = \\tfrac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline X)^2\\) est un estimateur non biaisé de \\(\\sigma^2\\) sous \\(H_0\\). Comme les \\(X_i\\) sont iid de loi normale, \\(\\psi(X)\\) suit une loi de student \\(\\mathcal T(n-1)\\) sous \\(H_0\\). Ce n’est pas une approximation ici, on peut même dire que \\(\\psi\\) est une statistique de test pivot\n\n\n\nDéterminez la zone de rejet. Vous pouvez utiliser une approximation Gaussienne et le graphe de l’exercice 1.\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn souhaite tester (à droite) au niveau \\(0.05\\). Ainsi, on rejette si \\(\\psi(X) \\geq t_{0.95}\\), où \\(t_{0.95}\\) est le quantile \\(0.95\\) de \\(\\mathcal T(n-1)\\)\nLorsque \\(n\\) tend vers \\(+\\infty\\), \\(\\mathcal T(n-1)\\) converge en loi vers \\(\\mathcal N(0,1)\\). On approxime donc \\(t_{0.95}\\) par le quantile de la loi Gaussienne (inverse de la cdf sur le graphe), d’où \\(t_{0.95} \\asymp 1.6\\)\nOn rejette si \\(\\psi(X) \\geq 1.6\\)\n\n\n\nEcrivez une ligne de code permettant de calculer le seuil de rejet exact.\n\nquantile(TDist(19), 0.95) # julia\nqt(0.95,n) # R, (1.73 légèrement plus grand que 1.6)\n\nLa rivière présente-t-elle une concentration chimique accrue qui pourrait indiquer une pollution provenant de l’usine ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn calcule \\(\\psi(X_{\\mathrm{obs}}) = 3.75\\), on rejette donc \\(H_0\\), la rivière est peut être polluée"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux",
    "text": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux\nUn chercheur en faune sauvage étudie le comportement d’une certaine espèce d’oiseaux qui migrent vers une réserve naturelle. Le chercheur a une hypothèse sur la façon dont les oiseaux se répartissent entre différents types d’habitats dans la réserve. La distribution attendue, basée sur des données historiques, est la suivante :\n\nPrairie : 40%\nZones humides : 30%\nForêts : 20%\nZones rocheuses : 10%\n\nPour tester cette hypothèse, le chercheur observe 200 oiseaux et enregistre leurs préférences d’habitat. Les comptages observés sont les suivants :\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé\n90\n60\n30\n20\n\n\n\n\nQuestions :\n\nFormalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\((X_1, X_2,X_3,X_4)\\) les effectifs d’oiseaux présent respectivement dans les Prairie,…, zones rocheuses.\nOn suppose que le vecteur \\((X_1, X_2,X_3,X_4)\\) suit une loi multinomiale de paramètre \\(n=200\\) et \\(q=(q_1,q_2,q_3,q_4)\\) inconnu\nOn veut tester si les observations correspondent à la distribution attendue, c’est à dire \\(H_0: q=(0.4,0.3,0.2,0.1)\\) VS \\(H_1: q\\neq (0.4,0.3,0.2,0.1)\\) Ce problème correspond au test d’adéquation du Chi2 (Goodness of fit).\n\n\n\n\nCalculez les effectifs attendus.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé (\\(X_i\\))\n90\n60\n30\n20\n\n\nAttendus (\\(E_i\\))\n80\n60\n40\n20\n\n\n\n\n\n\nCalculez la statistique du chi-deux.\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\psi(X) = \\sum_{i=1}^4 \\frac{(X_i - E_i)^2}{E_i} = 100/80 + 100/40 = 3.75\\)\n\n\n\nDéterminez le degré de liberté \\(df\\) de la statistique du chi-deux, et lisez la p-value sur le graphique suivant de la fonction de répartition.\n\n\n\n\n\n\n\nNote\n\n\n\nLorsque \\(n \\to +\\infty\\), \\(\\psi(X)\\) converge en loi vers une loi de \\(\\chi^2\\) de degré \\(df=3\\). On lit pvaleur = 1-cdf(Chisq(df), 3.75)=0.3\n\n\n\nQuelle est votre conclusion ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nLa \\(p_{valeur}\\) est assez grande, la distribution colle avec celle attendue et on ne rejette donc pas \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-4",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 4",
    "text": "Exercice 4\n\nProductivité des employés entre départements\nUne entreprise souhaite évaluer si un nouveau style de management a eu un effet uniforme sur la productivité des employés dans cinq départements. Chaque département a adopté une variation spécifique du style de management pendant trois mois, et l’entreprise a enregistré le nombre moyen de tâches accomplies par employé durant cette période.\nDonnées :\n\n\n\nDépartement\n1\n2\n3\n4\n5\n\n\n\n\nNombre d’employés\n12\n10\n8\n9\n11\n\n\nMoyenne des tâches accomplies\n72,4\n68,9\n75,6\n74,3\n69,7\n\n\nVariance des tâches\n8,5\n9,2\n10,1\n7,8\n9,6\n\n\n\nL’entreprise cherche à comprendre si les niveaux de productivité varient significativement entre les départements, indiquant que les styles de management pourraient avoir des impacts différents.\nSoit \\(d=5\\) le nombre de départements et \\(N_{\\text{tot}} = 50\\) le nombre total d’employés. Pour tout département \\(j\\), nous notons \\(N_k\\) le nombre d’employés dans le département \\(k\\), et \\(P_{ik}\\) le nombre de tâches accomplies par l’employé \\(i\\) dans le département \\(k\\). Nous supposons que les \\(P_{ik}\\) sont indépendants et suivent une distribution normale de moyenne \\(\\mu_k\\) et de variance \\(\\sigma^2\\).\nNous écrivons \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDéfinissez les hypothèses du problème pour tester si les styles de management ont eu un impact uniforme sur la productivité.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nDans le département \\(k\\), on observe \\(P_{ik}\\) le nombre de tâche accompli par l’employé \\(i\\)\nOn suppose que les \\(P_{ik}\\) sont indépendants, et que \\(P_{ik}\\) suit une \\(\\mathcal N(\\mu_k, \\sigma^2)\\) où \\((\\mu_1, \\dots, \\mu_5)\\) et \\(\\sigma^2\\) sont des paramètres inconnus.\nProblème de test: \\(H_0\\): \\(\\mu_1 = \\dots = \\mu_5\\) VS \\(H_1\\): \\(\\exists k,l\\) tels que \\(\\mu_k \\neq \\mu_l\\)\n\n\n\n\nDonnez une brève interprétation de chacune des quantités \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\n\n\n\n\n\n\n\nNote\n\n\n\nCf cours\n\n\n\nDémontrez la formule d’analyse de la variance : \\(V_T = V_W + V_B\\)\n\n\n\n\n\n\n\nNote\n\n\n\nExercice à refaire\n\n\n\nCalculez \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), et \\(V_T\\).\n\n\n\n\n\n\n\nNote\n\n\n\nOn applique les formules…\n\\(\\overline P=71.96\\)\n\\(V_W = 9.01\\)\n\\(V_B=6.15\\)\n\\(V_T=15.16\\)\n\n\n\n\nExprimez la statistique de test ANOVA en termes de \\(V_W\\) et \\(V_B\\).\nQuelles sont les distributions de \\(N_k V_k\\) et de \\(N_{\\mathrm{tot}}V_W\\) sous \\(H_0\\) ? Changent-elles sous \\(H_1\\) ?\n\n\n\n\n\n\n\nNote\n\n\n\n\\(N_kV_k\\) suit une loi \\(\\sigma^2\\chi^2(N_k-1)\\).\nAinsi, \\(N_{\\mathrm{tot}}V_W/\\sigma^2\\) suit une loi \\(\\chi^2\\) de degré \\(\\sum_{k=1}^5(N_k-1) = N_{tot}-5\\) Ces lois ne changes pas sous \\(H_1\\)! C’est en fait la loi de la variance interclasses \\(V_B\\) qui va changer sous \\(H_1\\).\n\n\n\nRappelez la définition de la statistique de test ANOVA, donnez sa distribution \\(\\mathcal D\\) sous \\(H_0\\) et effectuez le test ANOVA au niveau \\(\\alpha =0,05\\). On donne les quantiles \\(0.05\\) et \\(0.95\\) de \\(\\mathcal D\\): \\(0.18\\) and \\(2.58\\). Concluez si la productivité diffère significativement entre les départements.\n\n\n\n\n\n\n\nRéponse\n\n\n\nLa statistique ANOVA se définit comme \\(\\psi((P_{ik})) = \\frac{N_k-1}{d-1}\\frac{V_B}{V_W}\\). Ici, on calcule \\(\\psi((P_{ik}))=7.7\\). Comme \\(7.7 &gt; 2.5\\) (On regarde le quantile à droite), on rejette \\(H_0\\) au niveau \\(0.05\\). La productivité n’est pas homogène entre les départements."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#questions-de-cours",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#questions-de-cours",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions de cours",
    "text": "Questions de cours\n\nRappelez la définition d’une statistique de test \\(\\psi\\) et d’un test (ou règle de décision) \\(T\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nUne statistique de test \\(\\psi\\) est une fonction mesurable des donnée à valeurs réelles et qui ne dépend pas des paramètres inconnus du modèle.\nUne règle de décision \\(T\\) a la même définition, sauf qu’elle prend ses valeurs dans \\(\\{0,1\\}\\) (\\(0\\) correspond à conserver \\(H_0\\) et \\(1\\) correspond au rejet de \\(H_0\\))\n\n\n\n\nQuels sont les deux types d’erreur que nous pouvons commettre ?\n\n\n\n\n\n\n\nNote\n\n\n\n\nL’erreur de type 1: on rejette \\(H_0\\) alors qu’elle est vraie\nL’erreur de type 2: on “accepte” \\(H_0\\) alors qu’elle est fausse Ne pas écrire de probabilité, à moins de préciser le cadre! (exemple: \\(H_0\\) est simple)\n\n\\(\\mathbb P_{H_0}(X \\in A)\\) n’a aucun sens si \\(H_0\\) est multiple. C’est quoi \\(\\mathbb P_{H_0}\\)??\n\n\n\nPour une statistique de test \\(\\psi\\) donnée et un problème de test bilatéral, rappelez la définition de la p-valeur.\n\n\n\n\n\n\n\nRéponse\n\n\n\nSoit un problème de test où \\(H_0\\) est simple. Soit \\(X\\) la variable aléatoire, et \\(X_{obs}\\) une observation (c’est formellement une réalisation de \\(X\\))\nPour un problème de test bilatéral et une statistique de test \\(\\psi\\) donnée, la pvaleur s’écrit \\(2*\\min(\\mathbb P(\\psi(X) \\geq \\psi(X_{\\mathrm{obs}})), \\mathbb P(\\psi(X) \\leq \\psi(X_{\\mathrm{obs}})))\\)\n\n\n\nÉnoncez le théorème de Neyman-Pearson.\n\n\n\n\n\n\n\nRéponse\n\n\n\nDans le cas d’un problème simple VS simple, le rapport de vraissemblance est optimal pour un niveau de test \\(\\alpha\\) fixé, au sens où aucun test de niveau \\(\\alpha\\) ne peut avoir une plus grande puissance. Cf cours"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#definition-and-link-with-clt",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#definition-and-link-with-clt",
    "title": "Gaussian Populations",
    "section": "Definition and link with CLT",
    "text": "Definition and link with CLT\nWe observe \\((X_1, \\dots, X_n)\\) iid real valued random variables.\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n X_i\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when } n \\to \\infty \\]\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\nEquality when \\(X_i\\)’s are Gaussian \\(\\mathcal N(\\mu, \\sigma^2)\\), that is \\[\n\\mathbb P(X_1 \\in [x,x+dx]) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Known Variance",
    "text": "Testing Mean with Known Variance\n\n\\(X = (X_1, \\dots, X_n)\\), iid with distribution \\(\\mathcal N(\\mu, \\sigma^2)\\).\n\n\n\n\n\n\n\nTest problems\n\n\n\\[\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTest statistic: \\[ \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\\]\n\\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nQ1, Q2\n\n\n\n\n\n\nTests\n\n\n\\[\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{(left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "Why 0.05 and 1.96 ?",
    "text": "Why 0.05 and 1.96 ?\n\n\n\n\n\nFisher’s Quote\n\n\nThe value for which \\(p=0.05\\), or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Unknown Variance",
    "text": "Testing Mean with Unknown Variance\n\nMultiple VS multiple test problem: \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n\\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\) no longer test statistic.\nIdea: replace \\(\\sigma\\) by its estimator \\[ \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\mu_0)^2} \\; .\\]\nThis gives \\[\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\\]\nIs \\(\\psi(X)\\) pivotal under \\(H_0\\) ? What is its distribution ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "Chi-Square and Student Distributions",
    "text": "Chi-Square and Student Distributions\n\n\n\n\n\n\nChi-squared distribution \\(\\chi^2(k)\\)\n\n\n\nDistrib of \\(\\sum_{i=1}^k Z_i^2\\) where the \\(Z_i\\)’s are iid \\(\\mathcal N(0,1)\\).\n\\(\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\\)\n\\(k\\): degree of freedom\n\\(\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\n\nStudent distribution \\(\\mathcal T(k)\\)\n\n\n\nDistrib of \\(\\tfrac{Z}{\\sqrt{U/k}}\\) where \\(Z\\), \\(U\\) are independent and follow resp. \\(\\mathcal N(0,1)\\) and a \\(\\chi^2(k)\\)\n\\(k\\): degree of freedom\n\\(\\mathcal T(k) \\sim \\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\nAssume \\(X_i\\) are iid \\(\\mathcal N(\\mu_0, \\sigma^2)\\).\n\nThe test statistic \\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma}\\) pivotal (indep. of \\(\\sigma\\)).\nIt follows a Student Distribution \\(\\mathcal T(n-1)\\).\n\n\n\n\n\nProof idea: \\(\\overline X \\cdot (1, \\dots, 1)\\) and \\((X_1 - \\overline X, \\dots, X_n - \\overline X)\\) are orthogonal in \\(\\mathbb R^n\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "Student T-Test",
    "text": "Student T-Test\n\nMultiple VS multiple test problem \\(X=(X_1, \\dots, X_n)\\): \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n(Student) T-test statistic: \\[\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variance-unknown-mean",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variance-unknown-mean",
    "title": "Gaussian Populations",
    "section": "Testing Variance, Unknown Mean",
    "text": "Testing Variance, Unknown Mean\n\n\n\n\nWe observe \\(X=(X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu, \\sigma^2)\\). \\(\\mu\\), \\(\\sigma\\) are unknown. \\(\\sigma_0\\) is fixed and known.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(H_0\\): \\(\\sigma \\leq \\sigma_0\\), \\(H_1\\): \\(\\sigma &gt; \\sigma_0\\)\n\\(\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum_{i=1}^n (X_i - \\overline X)^2\\) Wooclap\n\\(T(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\\)\n\\(q_{1-\\alpha}\\): quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\\(H_0\\): \\(\\sigma \\geq \\sigma_0\\), \\(H_1\\): \\(\\sigma &lt; \\sigma_0\\)\n\\(\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum_{i=1}^n (X_i - \\overline X)^2\\)\n\\(T(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\\)\n\\(q_{\\alpha}\\): quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\) are known, \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nTest problem: \\(H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\\)\nIdea: normalize \\(\\overline X - \\overline Y\\): \\[\n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\\]\nTwo-tailed test for testing means: \\[\nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\\]\n\\(t_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of a Gaussian distribution"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example",
    "title": "Gaussian Populations",
    "section": "Example",
    "text": "Example\n\nObjective. Test if a new medication is efficient to lower cholesterol level\nExperiment.\n\nGroup A: \\(n_A = 45\\) patients receiving the new medication\nGroup B: \\(n_B = 50\\) patients receiving a placebo\n\nTest Problem.\n\nWe observe \\((X_1, \\dots, X_{n_A})\\) iid \\(\\mathcal N(\\mu_A,\\sigma^2)\\) and \\((Y_1, \\dots, Y_{n_B})\\) iid \\(N(\\mu_B,\\sigma^2)\\) the chol levels. \\(\\sigma = 8\\) mg/dL is known from calibration.\n\\(H_0: \\mu_A = \\mu_B\\) VS \\(H_1: \\mu_A &lt; \\mu_B\\)\n\nTest Statistic. \\(\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2}}}\\)\nData. \\(\\overline X = 24.5\\) mg/dL and \\(\\overline Y = 21.3\\) mg/dL. Hence \\(\\psi(X,Y)= 5.5\\).\nConclusion. Do not reject, and do not use this medication!"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe \\((X_1, \\dots, X_{n})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nVariance Testing Problem: \\[\nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\\]\nF-Test Statistic of the Variances (ANOVA) \\[\n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher distribution \\(\\mathcal F(k_1,k_2)\\)\n\n\n\nDistribution of \\(\\frac{U_1/k_1}{U_2/k_2}\\), where \\(U_1\\), \\(U_2\\) are indep. and follow \\(\\chi^2(k_1)\\), \\(\\chi^2(k_2)\\). wiki Wooclap\n\\(\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right)\\) when \\(k_1,k_2 \\to +\\infty\\)\n\\((k_1, k_2)\\): degrees of freedom\nExample: \\(\\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1)\\) if \\(Z_i \\sim \\mathcal N(0,1)\\)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2}\\) is independent of \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1\\), \\(\\sigma_2\\). It is pivotal\nIt follow distribution \\(\\mathcal F(n_1-1, n_2-1)\\)\n\n\n\n\n\n\n\n\nTwo-tailed test: \\[ \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown, but we know that \\(\\sigma_1=\\sigma_2\\)\nEquality of mean testing problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}\\).\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\\(\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\\)\nNormalize \\(\\overline X - \\overline Y\\): \\[\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\\]\n\\(\\psi(X,Y)\\) is pivotal because \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nEquality of Mean Testing Problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}\\).\n\n\n\n\n\nStudent Welch test statistic\n\n\n\\[\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\\]\n\nWooclap \\(\\psi(X,Y)\\) is not pivotal\nGaussian approximation: \\(\\psi(X,Y) \\approx \\mathcal N(0,1)\\) when \\(n_1, n_2 \\to \\infty\\)\nBetter approximation: Student Welch"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \\]\nEquality when \\(X_i\\)’s are \\(\\mathcal N(\\mu, \\sigma^2)\\)\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\n\n\n\nExample: binomials\n\n\n\nIf \\(p \\in (0,1)\\)\n\\(\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1)\\) when \\(n \\to \\infty\\)\n\\(n\\) should be \\(\\gg \\frac{1}{p}\\)\n\n\n\n\n\n\n\n\n\nGood Approx for (\\(n=100\\), \\(p=0.2\\))\n\n\n\n\nBad Approx for (\\(n=100\\), \\(p=0.01\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe \\(X \\sim Bin(n_1, p_1)\\) and \\(Y \\sim Bin(n_2, p_2)\\).\n\\(n_1\\), \\(n_2\\) are known but \\(p_1\\), \\(p_2\\) are unknown in \\((0,1)\\)\n\\(H_0\\): \\(p_1 = p_2\\) or \\(H_1\\): \\(p_1 \\neq p_2\\)\n\n\n\n\n\nTest Statistic\n\n\n\\[ \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\\]\n\n\\(\\hat p_1 = X/n_1\\), \\(\\hat p_2 = Y/n_2\\)\n\\(\\hat p = \\frac{X+Y}{n_1+n_2}\\) [Wooclap]\nIf \\(np_1, np_2 \\gg 1\\): \\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nWe reject if \\(|\\psi(X,Y)| \\geq t_{1-\\alpha/2}\\) (gaussian quantile)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion: “should we raise taxes on cigarettes to pay for a healthcare reform ?”\n\\(p_1\\), \\(p_2\\): proportion of non-smokers or smokers willing to raise taxes\n\\(H_0\\): \\(p_1=p_2\\) or \\(H_1\\): \\(p_1 &gt; p_2\\)\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\(\\hat p_1 = \\overline X= \\approx 0.58\\), \\(\\hat p_2=\\overline Y= \\approx 0.21\\).\n\\(\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\\)\n\\(\\mathbb P(\\psi(X,Y) &gt; 8.99)\\) = 1-cdf(Normal(0,1), 8.99)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#multinomials",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#multinomials",
    "title": "Goodness of Fit Tests",
    "section": "Multinomials",
    "text": "Multinomials\n\n\n\n\n\n\nBinomial distribution\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red [Wooclap]\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1), Y=n-X \\sim \\mathrm{Bin}(n,1-p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color [Wooclap]\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m)) [Wooclap]\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit Test",
    "text": "\\chi^2 Goodness of Fit Test\n\n\n\n\nWe observe (X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q). This corresponds to n counts: X_1 + \\dots + X_m = n\nq = (q_1, \\dots, q_m) corresponds to probabilities of getting color 1, \\dots, m\nLet p = (p_1, \\dots, p_m) be a known vector s.t. p_1 + \\dots + p_m = 1.\nH_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\n\n\n\n\n\n\n\n\n\n\n\\chi^2 Goodness of Fit Test (Adéquation)\n\n\n\nChi-squared test statistic:: \\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; , where n_i = np_i = \\mathbb E[X_i] is the expected number of counts for color i.\nWhen np_i = n_i are large, under H_0, we approximate the distribution of \\psi(X) as \\psi(X) \\sim \\chi^2(m-1)\nReject if \\psi(X) &gt; t_{1- \\alpha} (right-tail of \\chi^2(m-1))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "title": "Goodness of Fit Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\n\n\n\nWe observe a bag of sweets containing n=100 sweets of m=3 different colors: red, green, and yellow.\nManufacturer: p_1= 40\\% red, p_2=35\\% green, and p_3=25\\% yellow.\nH_0: q=p (manufacturer’s claim is correct)\nH_1: q\\neq p (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\n\n\n\nColor\nObserved Counts\n\n\n\n\nRed\nX_1=50\n\n\nGreen\nX_2=30\n\n\nYellow\nX_3=20\n\n\n\n\n\n\n\n\n\nExpected Counts\n\n\n\n\nn_1=40\n\n\nn_2=35\n\n\nn_3=25\n\n\n\n\n\n\n\n\n\n\n\n\n\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\n\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878 (p_{value} \\approx 0.222)\nConclusion: do not reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#histograms",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#histograms",
    "title": "Goodness of Fit Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nhistogram\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n\n\n\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\n\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\n\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\nwhere I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\n\n\n\n\n\n\n\n\n\n\n\n\nNormalization\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of large numbers, Monte Carlo (informal)\n\n\n\nAssume that (X_1, \\dots, X_n) are iid of distrib P, and that a,b, k are fixed\nThe histogram \\mathrm{hist}(a,b,k) converges to the histogram of the density P"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit to a given distribution",
    "text": "\\chi^2 Goodness of Fit to a given distribution\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n, iid with unknown distrib P\nH_0: P = P_0, where P_0 is known\nH_1: P \\neq P_0 [Wooclap]\nIdea: under H_0, the counts in disjoint intervals I_1, I_2, \\dots, I_k follow a multinomial distribution \\mathrm{Mult}(n, (p_1, \\dots, p_{k})) where p_1 = P_0(I_1), \\dots, p_{k} = P_0(I_k)\n\n\n\n\n\n\n\n\n\n\nReduction to chi-squared test statistic\n\n\n\nCount the number of data (c_1, \\dots, c_k) in I_1, \\dots, I_k\nTheoretical counts nP_0(I_1) \\dots, nP_0(I_k)\nChi-squared statistic: \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_j)}\nDecide using an 1-\\alpha-quantile of a \\chi^2(k-1) distribution\n\n\n\n\n\n\n\n\n\n\nCorrected \\chi^2 Test\n\n\n\nIf P_0 is unknown\nIn a class \\mathcal P parameterized by \\ell parameters\nThen estimate the \\ell parameters\nCompute theoretical counts with \\hat P_{\\hat \\theta}\nBut decide with \\chi^2(k - 1 - \\ell)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Goodness of Fit Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\nH_0: X_i iid \\mathcal P(2)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n0\n1\n2 \n\\geq 3\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\n\n\n\n\n\n\nTo get 9.7, we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\gtrsim \\frac{(16-4)^2}{4} = 36\n(1-cdf(Chisq(3),36)) is very small: Reject\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf H_0 is X_i iid \\mathcal P(\\lambda) with unknown \\lambda\n\\hat \\lambda = 0.8, \\sum_{i=1}^4 \\frac{(X_i - n_i)^2}{n_i} \\approx 9.4\nNot \\chi^2(3) but \\chi^2(2)\n(1-cdf(Chisq(2),9.4)) \\approx 0.009. Reject at level 1%\n\n\n\n\n\n\n\n[Wooclap]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "title": "Goodness of Fit Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)} [Wooclap]\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\n\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with y=x"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "title": "Goodness of Fit Tests",
    "section": "Kolmogorov-Smirnov Test",
    "text": "Kolmogorov-Smirnov Test\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile:\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n} [Wooclap]\nEmpirical CDF: \\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf 1\\{X_i \\leq x\\}\nIdea: Max distance between empirical and true CDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\\psi(X) = \\sup_{x}|\\hat F(x) - F_0(x)|\nApprox: \\mathbb P_0(\\psi(X) &gt;c/\\sqrt{n}) \\to 2\\sum_{r=1}^{+\\infty}(-1)^{r-1}\\exp(-2c^2r^2) when n \\to +\\infty\nIn practice, use Julia, Python or R for KS Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html",
    "href": "teaching/hypothesis_testing/TDs/TP.html",
    "title": "TP: Hypothesis Testing",
    "section": "",
    "text": "We observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\). We assume that the vectors \\(X\\) and \\(Y\\) are independent. We want to test \\(H_0\\): \\(\\mu_1 = \\mu_2\\) VS \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\). We observe the data\nX=[-0.2657064426519085, -0.27538323622274347, 0.11419811877193782, 0.1158736466676504, 1.7071154417851981, 0.9306910454777643, 0.5834941669559498, -1.536447927372139, -1.4158768806157345, 1.0532694288697444, 1.2955133629200777, -0.4195557179577367]\nY=[-0.6452416530469819, 0.3662048411679129, -0.09943069837472361, 0.8738423322164134, 0.7163913715056272, -0.32450102319617485, 0.9159821874321818, -2.3583609849887224]\n\nCompute the student-Welch test statistic \\(\\tfrac{\\overline X - \\overline Y}{\\sqrt{\\hat\\sigma_1/n_1 + \\hat\\sigma_2/n_2}}\\)\nConclude using a Gaussian approximation (use the cdf of a N(0,1))\nConclude using an UnequalVarianceTTest (julia) or test.welch (R) or scipy.stats.ttest_ind(a, b, equal_var=False) (python)\nBonus. Conclude using a better chi-squared approximation. Compare these result to 3."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#student-test",
    "href": "teaching/hypothesis_testing/TDs/TP.html#student-test",
    "title": "TP: Hypothesis Testing",
    "section": "",
    "text": "We observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\). We assume that the vectors \\(X\\) and \\(Y\\) are independent. We want to test \\(H_0\\): \\(\\mu_1 = \\mu_2\\) VS \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\). We observe the data\nX=[-0.2657064426519085, -0.27538323622274347, 0.11419811877193782, 0.1158736466676504, 1.7071154417851981, 0.9306910454777643, 0.5834941669559498, -1.536447927372139, -1.4158768806157345, 1.0532694288697444, 1.2955133629200777, -0.4195557179577367]\nY=[-0.6452416530469819, 0.3662048411679129, -0.09943069837472361, 0.8738423322164134, 0.7163913715056272, -0.32450102319617485, 0.9159821874321818, -2.3583609849887224]\n\nCompute the student-Welch test statistic \\(\\tfrac{\\overline X - \\overline Y}{\\sqrt{\\hat\\sigma_1/n_1 + \\hat\\sigma_2/n_2}}\\)\nConclude using a Gaussian approximation (use the cdf of a N(0,1))\nConclude using an UnequalVarianceTTest (julia) or test.welch (R) or scipy.stats.ttest_ind(a, b, equal_var=False) (python)\nBonus. Conclude using a better chi-squared approximation. Compare these result to 3."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#monte-carlo-and-chi-squared-tests",
    "href": "teaching/hypothesis_testing/TDs/TP.html#monte-carlo-and-chi-squared-tests",
    "title": "TP: Hypothesis Testing",
    "section": "1. Monte Carlo and Chi-squared Tests",
    "text": "1. Monte Carlo and Chi-squared Tests\nA statistician observes \\(X = (X_1, \\dots, X_n)\\) where the \\(X_i's\\) are iid of distribution \\(P\\). If the problem is to test whether \\(P\\) is Gaussian with known \\(\\mu\\) and \\(\\sigma\\), the problem is:\n\\[H_0: P=\\mathcal N(\\mu, \\sigma) \\quad \\text{VS} \\quad H_1: P\\neq \\mathcal N(\\mu, \\sigma)\\]\nIf \\(\\mu\\) and \\(\\sigma\\) are unknown, the problem is \\[H_0: P\\in \\{\\mathcal N(\\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt;0\\}\\quad \\text{VS} \\quad H_1: P\\not \\in \\{\\mathcal N(\\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt;0\\}\\]\nWe first assume that \\(\\mu\\) and \\(\\sigma\\) are known, and that:\nmu = 0\nsigma = 1\nn = 100\nm = 5\nThis practical exercise aims to empirically demonstrate how a chi-squared test statistic converges to a chi-squared distribution in both known and unknown parameter scenarios. We will:\n\nDivide the observation space into 5 disjoint intervals\nCount how many observations fall into each interval for randomly generated data\nCalculate the chi-squared test statistic for randomly generated data\nRepeat this process 1,000 times to build an empirical distribution (an histogram)\nThe resulting empirical histogram should approach a theoretical chi-squared distribution as both the sample size \\(n\\) and the number of repetitions \\(N\\) approach infinity.\n\n\nQuestions\n\nGenerate a vector \\(X\\) made of \\(n\\) iid \\(\\mathcal N(\\mu, \\sigma)\\)\nCompute the vector \\(Y = \\frac{X-\\mu}{\\sigma}\\)\nCompute the list of counts \\(C\\) of \\(Y\\) in \\((-\\infty, -3)\\), \\([\\tfrac{3i}{m}, \\frac{3(i+1)}{m})\\) for \\(i\\) in \\(\\{-m, \\dots, m-1\\}\\) and \\([3,+\\infty)\\).\n\nHow many intervals do we have here?\nWhat is the expected number of entries of \\(Y\\) falling in \\([3, +\\infty)\\)? (compute this using the cdf function). Change the value of \\(n\\) so that we have at least \\(5\\) expected counts in \\([3, +\\infty)\\).\n\n\n\n\n#Julia: use the broadcasting .&lt;\nsum(x .&lt;= Y .&lt; y) # counts in [x, y)\n\n#R: use bitwise operator &\nsum(Y &gt;= x & Y &lt; y) # counts in [x, y)\n\n\n\nUsing the cdf of \\(\\mathcal N(0,1)\\), compute the list of expected counts in the same intervals\nCompute the Chi-squared test statistic using the two preceeding questions. We recall that \\(\\psi(Y) = \\sum_{i=1}^n \\tfrac{(c_i - e_i)^2}{e_i}\\) where \\(c_i\\) and \\(e_i\\) are the counts and expected counts.\nSummarize the preceeding questions into a function trial_chisq(X, mu, sigma, m) that normalizes \\(X\\), computes counts, expected counts and the chisq test statistic:\n\n# function trial_chisq(X, mu, sigma, m)\n# n = length(X)\n# Y = (X-mu)/sigma\n# Compute counts \n# Compute expcounts\n# Compute and Return chisq\n\nUsing the previous question, write a function monte_carlo_known that computes \\(N\\) chi-squared test statistics on iid random samples \\(X\\sim \\mathcal N(\\mu, \\sigma)^{\\otimes n}\\). It returns a list trials of length \\(N\\).\n\nN = 1000\n# function monte_carlo_known(N, mu, sigma, n, m)\n# empty list trials\n\n# for i = 1 ... N\n\n# Generate X made of n iid gaussian (mu, sigma)\n# append trial_chisq(X, mu, sigma, m) to trials\n\n# endfor\n# return trials\n\nPlot a histogram of a list of trials using a builtin function. Normalize it in density (area=1), and precise the bins (0:0.5:30).\nWhat is a good distribution to approximate the histogram? Plot the distribution’s density and check that it fits the histogram. Vary the parameters \\(m\\), \\(n\\), and \\(N\\).\n\nNow, we assume that \\(\\mu\\) and \\(\\sigma\\) are unknown.\n\nGiven \\(X\\), compute to estimators hatmu and hatsigma of mu and sigma\nSimilarly to Q.7, write a function monte_carlo_unknown(N, n, m) that computes a Monte-Carlo simulation. \\(\\hat \\mu\\) and \\(\\hat \\sigma\\) must be computed for all trial \\(i=1,\\dots,N\\).\nRevisit questions 8 and 9, considering the case where \\(\\mu\\) and \\(\\sigma\\) are unknown. How does this affect the distribution of the histogram?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#application-with-bitcoin",
    "href": "teaching/hypothesis_testing/TDs/TP.html#application-with-bitcoin",
    "title": "TP: Hypothesis Testing",
    "section": "2. Application with Bitcoin",
    "text": "2. Application with Bitcoin\n\nUse your favorite AI to write the code to import the last \\(500\\) hourly close prices of bitcoin in usdt from binance. Plot the prices and compute the returns defined as \\(R_t = \\tfrac{P_t}{P_{t-1}}-1\\), where \\(P_t\\) is the price at time \\(t\\) (in hours).\n\n\n\n\n\n\n\nR (Click to See a Solution)\n\n\n\n\n\nlibrary(httr)\nlibrary(jsonlite)\n# Define the API endpoint and parameters\napi_url &lt;- \"https://api.binance.com/api/v3/klines\"\nsymbol &lt;- \"BTCUSDT\" # Bitcoin to USDT trading pair\ninterval &lt;- \"1h\" # 1-hour interval\nlimit &lt;- 500 # Limit to 500 data points\n\n# Create the query URL with parameters\nquery_params &lt;- list(\n    symbol = symbol,\n    interval = interval,\n    limit = limit\n)\n\n# Fetch the data from Binance API\nresponse &lt;- GET(api_url, query = query_params)\nresponse.body\ndata &lt;- content(response, as = \"text\", encoding = \"UTF-8\")\ndata &lt;- fromJSON(data)\ndata &lt;- data.frame(data)[2]\ndata &lt;- data.frame(lapply(data, as.numeric))\nn &lt;- length(data$X2)\nR &lt;- (data[2:n,1] / data[1:(n - 1),1]) - 1\n\n\n\n\n\n\n\n\n\nJulia (Click to See a Solution)\n\n\n\n\n\nusing HTTP\nusing JSON\nusing DataFrames\n\nfunction BTC_returns()\n    # Define the API endpoint and parameters\n    api_url = \"https://api.binance.com/api/v3/klines\"\n    symbol = \"BTCUSDT\"  # Bitcoin to USDT trading pair\n    interval = \"1h\"     # 1-hour interval\n    limit = 1000         # Limit to 500 data points\n    \n    # Construct the full query URL\n    query_url = \"$api_url?symbol=$symbol&interval=$interval&limit=$limit\"\n    \n    # Fetch the data from Binance API\n    response = HTTP.get(query_url)\n    data = JSON.parse(String(response.body))\n    P = [parse(Float64, data[i][2]) for i in 1:length(data)]\n    R = [P[t] / P[t-1] - 1 for t in 2:length(P)]\n    return R\nend\n    R=BTC_returns()\n\n\n\n\nWe first test\n\\(H_0\\): the mean of the returns is zero VS \\(H_1\\): it is nonzero.\nCompute \\(\\hat \\sigma\\) as std(R) and the Student statistic \\(\\psi(R) = \\sqrt{n}\\tfrac{\\overline R}{\\hat \\sigma}\\). Compute the p-value using the cdf function of a Student(499) (or Gaussian). Obtain the same result with a library function like OneSampleTTest in Julia, t.test in R or ttest_1samp in Python\nPlot a histogram of the returns, normalized in density. Plot on the same graph the density of a Gaussian of mean mean(R) and of std std(R).\nUsing the previous exercise with \\(m=5\\), compute a chi-squared statistic and an approximated p-value.\nDo a scatter plot of \\((R_{t-1}, R_t)\\). Do you see any correlation between \\(R_{t-1}\\) and \\(R_t\\)?\nCompute the correlation \\(r\\) between \\((R_t)\\) and \\((R_{t-1})\\).\nCompute the p-value of a two-sided Pearson’s correlation test, using the test statistic \\(\\tfrac{r}{\\sqrt{1-r^2}} \\sqrt{n-2}\\) and the cdf of a Student distribution. Compare with the function CorrelationTest in Julia or cor.test in R or pearsonr in Python."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/MoreExercises.html",
    "href": "teaching/hypothesis_testing/TDs/MoreExercises.html",
    "title": "More Exercises",
    "section": "",
    "text": "A quality control analyst at a semiconductor manufacturing plant is tracking chip failures during production. For each production batch, the analyst records the number of chips that pass inspection before the batch is terminated.\nDataset: Number of working chips produced in \\(150\\) different production batches.\n\n\n\nWorking Chips\nNumber of Batches\nPercentage\n\n\n\n\n0\n8\n5.3%\n\n\n1\n10\n6.7%\n\n\n2\n16\n10.7%\n\n\n3\n17\n11.3%\n\n\n4\n21\n14.0%\n\n\n5\n20\n13.3%\n\n\n6\n17\n11.3%\n\n\n7\n14\n9.3%\n\n\n8\n10\n6.7%\n\n\n9\n7\n4.7%\n\n\n10\n3\n2.0%\n\n\n11\n5\n3.3%\n\n\n12\n2\n1.3%\n\n\nTotal\n150\n100%\n\n\n\nWe assume that each batch terminates after \\(r= 5\\) failures.\n\nWhat distribution the number of working chips in a batch should follow?\nTest the goodness of fit to this distribution.\nWhat does it change if the number of failures after which each batch ends is unknown?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/MoreExercises.html#exercise",
    "href": "teaching/hypothesis_testing/TDs/MoreExercises.html#exercise",
    "title": "More Exercises",
    "section": "",
    "text": "A quality control analyst at a semiconductor manufacturing plant is tracking chip failures during production. For each production batch, the analyst records the number of chips that pass inspection before the batch is terminated.\nDataset: Number of working chips produced in \\(150\\) different production batches.\n\n\n\nWorking Chips\nNumber of Batches\nPercentage\n\n\n\n\n0\n8\n5.3%\n\n\n1\n10\n6.7%\n\n\n2\n16\n10.7%\n\n\n3\n17\n11.3%\n\n\n4\n21\n14.0%\n\n\n5\n20\n13.3%\n\n\n6\n17\n11.3%\n\n\n7\n14\n9.3%\n\n\n8\n10\n6.7%\n\n\n9\n7\n4.7%\n\n\n10\n3\n2.0%\n\n\n11\n5\n3.3%\n\n\n12\n2\n1.3%\n\n\nTotal\n150\n100%\n\n\n\nWe assume that each batch terminates after \\(r= 5\\) failures.\n\nWhat distribution the number of working chips in a batch should follow?\nTest the goodness of fit to this distribution.\nWhat does it change if the number of failures after which each batch ends is unknown?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html",
    "href": "teaching/hypothesis_testing/TDs/TD4.html",
    "title": "TD4: Homogeneity/Dependency",
    "section": "",
    "text": "A sociologist wants to investigate whether the choice of transportation mode (Car, Bicycle, or Public Transit) varies among residents of three different cities: City A, City B, and City C.\nThe sociologist conducted a survey, and the responses are summarized in the contingency table below:\n\n\n\nTransportation Mode\nCity A\nCity B\nCity C\nTotal\n\n\n\n\nCar\n120\n150\n100\n370\n\n\nBicycle\n80\n60\n90\n230\n\n\nPublic Transit\n100\n90\n110\n300\n\n\nTotal\n300\n300\n300\n900\n\n\n\n\nFormulate the Hypothesis Testing Problem corresponding to the initial objective of the sociologist. Introduce the notation\nAnswer to the initial question using a chi-squared test at a \\(0.05\\) significance level"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-1",
    "title": "TD4: Homogeneity/Dependency",
    "section": "",
    "text": "A sociologist wants to investigate whether the choice of transportation mode (Car, Bicycle, or Public Transit) varies among residents of three different cities: City A, City B, and City C.\nThe sociologist conducted a survey, and the responses are summarized in the contingency table below:\n\n\n\nTransportation Mode\nCity A\nCity B\nCity C\nTotal\n\n\n\n\nCar\n120\n150\n100\n370\n\n\nBicycle\n80\n60\n90\n230\n\n\nPublic Transit\n100\n90\n110\n300\n\n\nTotal\n300\n300\n300\n900\n\n\n\n\nFormulate the Hypothesis Testing Problem corresponding to the initial objective of the sociologist. Introduce the notation\nAnswer to the initial question using a chi-squared test at a \\(0.05\\) significance level"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-2",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe statistician of an insurance company is tasked with studying the impact of an advertising campaign conducted in 7 regions where the company operates. To do this, he has extracted from the database the number of new clients acquired by a certain number of agents in each region.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nNumber of agents\n9\n7\n7\n6\n7\n6\n6\n\n\nAverage number of new clients\n26.88\n22.34\n19.54\n18.95\n27.17\n25.87\n25.72\n\n\nVariance of new clients\n13.54\n12.59\n12.87\n13.42\n13.17\n12.56\n12.64\n\n\n\nThe statistician decides to perform an analysis of variance to test whether the regional factor influences the number of new clients. Let \\(X_{ik}\\) denote the number of new clients of agent \\(i\\) in region \\(k\\), \\(N_k\\) the number of agents in region \\(k\\), \\(d = 7\\) the number of regions and \\(N_{\\mathrm{tot}} = 48\\) the total number of agents. Assume that the random variables \\(X_{ik}\\) are normal with mean \\(\\mu_k\\) and variance \\(\\sigma^2\\). Define:\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\nFormulate the hypothesis testing problem to test whether the number of new clients is homogeneous accross the regions.\nWhat do \\(\\overline X_k\\), \\(\\overline X\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\) represent?\nProve the analysis of variance formula: \\[V_T = V_W + V_B \\; .\\] substract and add \\(\\overline X_k\\) in the definition of \\(V_T\\)\nCompute \\(\\overline X\\), \\(V_W\\), \\(V_B\\), and \\(V_T\\).\nWrite the definition of the ANOVA test statistic in terms of \\(V_W\\) and \\(V_B\\).\nDid the advertising campaign have the same impact in all regions?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-3",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 3",
    "text": "Exercise 3\nSome data are collected from 7 students and we want to analyze the correlation between the number of hours students spend studying before an exam and their test scores.\n\n\n\nStudent\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nStudy Hours\n2.5\n3.0\n1.5\n4.0\n3.5\n5.0\n3.0\n\n\nTest Score\n56\n64\n45\n72\n68\n80\n59\n\n\n\n\nFormulate the hypothesis testing problem for a linear correlation test\nPerform the linear correlation test at level \\(0.05\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-4",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 4",
    "text": "Exercise 4\nBelow are stress scores for \\(10\\) patients before and after a sport session:\n\n\n\n\n\n\n\n\n\n\nParticipant\nStress Score (Before)\nStress Score (After)\nDifference\nRank/Sign\n\n\n\n\n1\n40\n32\n\n\n\n\n2\n38\n35\n\n\n\n\n3\n45\n40\n\n\n\n\n4\n50\n42.5\n\n\n\n\n5\n44\n41.5\n\n\n\n\n6\n48\n48\n\n\n\n\n7\n39\n30\n\n\n\n\n8\n42\n38\n\n\n\n\n9\n47\n46\n\n\n\n\n10\n46.5\n40\n\n\n\n\n\nWe want to test if sport has an effect on the stress of the patients\n\nFormulate the hypothesis testing problem\nComplete the above table\nPerform a Wilcoxon signed rank test"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-5",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-5",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 5",
    "text": "Exercise 5\nLet \\(X=(X_1, \\dots, X_N)\\) be a Gaussian vectors \\(\\mathcal N(0, I_N)\\) in \\(\\mathbb R^N\\) (i.e. \\(X_i\\) are iid \\(\\mathcal N(0,1)\\)).\n\n\nWhat is the distribution of \\(QX\\), if \\(Q\\) is an orthogonal matrix ? (\\(QQ^T = I_n\\))\nWhat is the distribution of \\(\\|PX\\|^2\\) if \\(P\\) is an orthogonal projector ?\nUse the rank of \\(P\\) defined as \\(rk(P) = dim(Im(P))\\)\nDefinition of orthogonal projector: (\\(P^2=P\\) and \\(P = P^T\\))\nShow that if \\(P\\) is an orthogonal projector, then \\(PX\\) is independent of \\((I-P)X\\).\nUse the fact that two centered gaussian vectors \\(X\\),\\(Y\\) are independent iif \\(\\mathbb E[X_iY_j] = 0\\) for all \\(i,j\\). Translate this fact in a matrix form.\nWhat is the distribution of \\(\\frac{n-rk(P)}{rk(P)}\\frac{\\|PX\\|^2}{\\|(I-P)X\\|^2}\\) ?\nShow that if \\(P\\), \\(P_0\\) are two orthogonal projectors such that \\(Im(P_0) \\subset Im(P)\\), then \\(P(I-P_0)X\\) is independent of \\((I-P)(I-P_0)X\\). What is the distribution of \\(\\|P(I-P_0)X\\|^2\\) ?\nShow first that \\(PP_0=P_0P= P_0\\), and that \\(P-P_0\\) is an orthogonal projector\nWhat is the orthogonal projector \\(P_0\\) on \\(\\mathrm{Span}(1, \\dots, 1)\\) ? Deduce that \\((X_i - \\overline X)\\) is independent of \\(\\overline X\\) for all \\(i\\).\n\nWe divide \\(N\\) into \\(d\\) blocks: \\(N = N_1 + \\dots + N_d\\). We write \\((X_1, \\dots, X_n) = ((X_{11}, \\dots, X_{N_11}), (X_{12}, \\dots X_{N_2 2}), \\dots, (X_{N_d 1}, \\dots, X_{N_d d}))\\).\n\nWhat is the orthogonal projection on \\(E_k =((0, \\dots, 0), \\dots, (0, \\dots, 0),(1, \\dots, 1), (0,\\dots, 0) ,\\dots, (0, \\dots, 0))\\) ? (\\(0\\) everywhere except on block \\(k\\) where we put ones everywhere)\nGive the orthogonal projection \\(P\\) on \\(\\mathrm{Span}(E_1, \\dots, E_d)\\). Explicit \\((I-P)(I-P_0)X\\) and \\(P(I-P_0)X\\).\nDeduce the distribution of \\(\\frac{d-1}{n-d}\\frac{\\|(I-P)(I-P_0)X\\|^2}{\\|P(I-P_0)X\\|^2}\\) and of the ANOVA Test Statistic under \\(H_0\\)."
  },
  {
    "objectID": "teaching/glossary.html",
    "href": "teaching/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "General\n\n\n\nEnglish\n\nincreasing function\nnondecreasing function\ninterval \\([a,b)\\)\n\n\n\n\nFrench\n\nfonction strictement croissante\nfonction croissante\nintervalle \\([a,b[\\)\n\n\n\n\n\n\nProbability/Statistics\n\n\n\nEnglish\n\nprobability distribution\nexpectation\nlikelihood\ndistribution tail\nCDF (Cumul. Distrib. Func.)\nPDF (Proba. Density Func.)\nCLT (Central Limit Theorem)\nLLN (Law of Large Numbers)\n\n\n\n\nFrench\n\nloi de probabilité\nespérance\nvraissemblance\nqueue de distribution\nFonction de Répartition\nDensité d’une distribution\nTLC (Th. de la limite centrale)\nLoi des grands nombres\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nEnglish\n\nA sample\none-sided test\ntwo-sided test\nchi-squared goodness of fit test\nchi-squared homogeneity test\n\n\n\n\nFrench\n\nUn échantillon\ntest unilatéral\ntest bilatéral\ntest d’adéquation du Khi-deux\ntest d’homogénéité du Khi-deux\n\n\n\n\n\n\nProgramming Languages (JupytR)\n\n\n\n\nJulia\nusing Distributions\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\ncdf(Binomial(n, p), x)\nquantile(Binomial(n,p), q)\ncdf(Normal(0,1), x)\nquantile(Normal(0,1), q)\n\ncdf(Chisq(n), x) # Chi-squared\ncdf(TDist(n), x) # Student\n\nn1,n2 = 5,10\ncdf(FDist(n1, n2), x) # Fisher\n\nlmbda = 2\ncdf(Gamma(n, lmbda), x) # Gamma\ncdf(Poisson(lmbda), x) # Poisson\n\n\n\nPython\nfrom scipy.stats import *\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\nbinom.cdf(x, n, p)\nbinom.ppf(q, n, p) # Quantile\nnorm.cdf(x)\nnorm.ppf(q)\n\nchi2.cdf(x, n)\nt.cdf(x, n) # Student\n\nn1,n2 = 5,10\nf.cdf(x, n1, n2) # Fisher\n\nlmbda = 2\ngamma.cdf(x,n,lmbda) # Gamma\npoisson.cdf(x, lmbda) # Poisson\n\n\n\nR\n\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\npbinom(x, n, p)\nqbinom(q, n, p)\npnorm(x)\nqnorm(q)\n\npchisq(x, n)\npt(x, n)\n\nn1,n2 = 5,10\npf(x, n1,n2)\n\nlmbda = 2\npgamma(x,n,lmbda) \nppois(x, lmbda)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#this-class",
    "href": "teaching/glm/slides/counting_models.html#this-class",
    "title": "Counting Models",
    "section": "This Class",
    "text": "This Class"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#poisson-regression-model",
    "href": "teaching/glm/slides/counting_models.html#poisson-regression-model",
    "title": "Counting Models",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#model-assumption",
    "href": "teaching/glm/slides/counting_models.html#model-assumption",
    "title": "Counting Models",
    "section": "Model Assumption",
    "text": "Model Assumption\n\nWe assume that\n\n\\[Y_i|X_i=x_i \\sim P(\\lambda_\\beta(x_i))\\quad \\text{with} \\quad \\lambda_\\beta(x) = \\exp(x^T \\beta)\\]\n\nwith \\(\\lambda_\\beta(x) = \\exp(x^T \\beta)\\).\n\n\nIn particular, \\(\\mathbb E[Y_i|X_i=x_i] = \\lambda_\\beta(x)\\)\n\n\n\n\n\nWarning\n\n\n\\(Y|X\\) follows a Poisson distribution but not \\(Y\\)!\n(We can only say that it is a mixture of Poisson)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#illustration",
    "href": "teaching/glm/slides/counting_models.html#illustration",
    "title": "Counting Models",
    "section": "Illustration",
    "text": "Illustration\n\nExample: binary regressor \\(X\\) (with as many \\(X = 0\\) as \\(X = 1\\))\n\n\n\n\\(Y|(X = 0) \\sim P(2) \\and Y|(X = 1) \\sim P(10)\\)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#rate-ratio",
    "href": "teaching/glm/slides/counting_models.html#rate-ratio",
    "title": "Counting Models",
    "section": "Rate Ratio",
    "text": "Rate Ratio\n\nThere is no notion of OR since we are not estimating a probability (or a probability ratio) but an expectation.\n\n\nThe equivalent notion here is the rate ratio (RR), \\(\\lambda(x)\\) being seen as an average rate of occurrence of \\(Y\\).\n\n\nFor two characteristics \\(x_1\\) and \\(x_2\\), the rate ratio is simply:\n\n\\[RR(x_1, x_2) = \\frac{\\lambda_\\beta(x_1)}{\\lambda_\\beta(x_2)} = \\exp((x_1 - x_2)^T \\beta)\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#particular-cases-for-rate-ratio",
    "href": "teaching/glm/slides/counting_models.html#particular-cases-for-rate-ratio",
    "title": "Counting Models",
    "section": "Particular Cases for Rate Ratio",
    "text": "Particular Cases for Rate Ratio\n\nSingle Regressor Difference: If \\(x_1\\) and \\(x_2\\) differ only by regressor \\(j\\):\n\\(RR(x_1, x_2) = \\exp((x_{1j} - x_{2j})\\beta_j)\\)\n\n\nBinary regressor: (\\(x_{1j} = 1\\) and \\(x_{2j} = 0\\)): \\(RR_j = e^{\\beta_j}\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#log-likelihood",
    "href": "teaching/glm/slides/counting_models.html#log-likelihood",
    "title": "Counting Models",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n\n\n\\[P(Y = k|X = x) = e^{-\\lambda_\\beta(x)} \\frac{\\lambda_\\beta(x)^k}{k!}\\]\n\n\n\nThus the sample likelihood equals \\(\\prod_{i=1}^n e^{-\\lambda_\\beta(x_i)} \\frac{\\lambda_\\beta(x_i)^{y_i}}{y_i!}\\)\n\n\nSince \\(\\lambda_\\beta(x) = \\exp(x^T \\beta)\\), the log-likelihood therefore equals\n\n\\[L = \\sum_{i=1}^n \\left[y_i x_i^T \\beta - e^{x_i^T \\beta} - \\ln(y_i!)\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#estimation-of-beta-by-mle",
    "href": "teaching/glm/slides/counting_models.html#estimation-of-beta-by-mle",
    "title": "Counting Models",
    "section": "Estimation of \\(\\beta\\) by MLE",
    "text": "Estimation of \\(\\beta\\) by MLE\n\nBy setting the gradient with respect to \\(\\beta\\) to zero, we find that the MLE \\(\\hat{\\beta}\\) must verify:\n\n\\[\\sum_{i=1}^n y_i x_i = \\sum_{i=1}^n \\lambda_{\\hat{\\beta}}(x_i) x_i\\]\n\n\n\nThis is a system with \\(p\\) unknowns (recall that \\(x_i \\in \\mathbb R^p\\)) that we solve numerically."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#properties-of-hat-beta",
    "href": "teaching/glm/slides/counting_models.html#properties-of-hat-beta",
    "title": "Counting Models",
    "section": "Properties of \\(\\hat \\beta\\)",
    "text": "Properties of \\(\\hat \\beta\\)\n\nUnder regularity conditions, when \\(n \\to \\infty\\):\n\n\\[\\hat{\\beta} \\sim N(\\beta, (X^T W_{\\hat{\\beta}} X)^{-1})\\]\n\nwhere \\(W_{\\hat{\\beta}} = \\text{diag}(\\lambda_{\\hat{\\beta}}(x_1), \\ldots, \\lambda_{\\hat{\\beta}}(x_n))\\).\n\n\nWe can therefore perform Wald significance tests."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#validation-deviance",
    "href": "teaching/glm/slides/counting_models.html#validation-deviance",
    "title": "Counting Models",
    "section": "Validation: Deviance",
    "text": "Validation: Deviance\n\nThe saturated model (one parameter per different observation) leads to\n\n\\[\\hat{\\lambda}(x) = \\frac{y_x}{n_x}\\]\n\n\n\\(y_x = \\sum_{i:x_i=x} y_i\\) is the total number of \\(Y\\) observed for characteristic \\(x\\) on the sample\n\\(n_x = \\sum_{i:x_i=x} 1\\) is the number of times \\(x\\) was observed."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#saturated-log-likelihood",
    "href": "teaching/glm/slides/counting_models.html#saturated-log-likelihood",
    "title": "Counting Models",
    "section": "Saturated Log-Likelihood",
    "text": "Saturated Log-Likelihood\n\nThe log-likelihood of the saturated model therefore equals:\n\n\\[L_{\\text{sat}} = \\sum_x \\left[y_x \\ln\\left(\\frac{y_x}{n_x}\\right) - y_x\\right] - \\text{cste}\\]\n\nwhere \\(\\text{cste} = \\sum_{i=1}^n \\ln(y_i!)\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#deviance-formula",
    "href": "teaching/glm/slides/counting_models.html#deviance-formula",
    "title": "Counting Models",
    "section": "Deviance Formula",
    "text": "Deviance Formula\n\nThus the deviance equals\n\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}}) = 2\\sum_x y_x \\ln\\left(\\frac{y_x}{n_x \\lambda_{\\hat{\\beta}}(x)}\\right) - (y_x - n_x \\lambda_{\\hat{\\beta}}(x))\\]\n\n\n\n\nIf a constant is in the model (one coordinate of \\(x\\) equals \\(1\\)), we have from the likelihood equations \\(\\sum_x y_x = \\sum_x n_x \\lambda_{\\hat{\\beta}}(x)\\) and then\n\n\n\\[D = 2\\sum_x y_x \\ln\\left(\\frac{y_x}{\\hat{y}_x}\\right)\\]\n\n\nwhere \\(\\hat{y}_x = n_x \\lambda_{\\hat{\\beta}}(x)\\) are the expected theoretical counts."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#validation-deviance-test",
    "href": "teaching/glm/slides/counting_models.html#validation-deviance-test",
    "title": "Counting Models",
    "section": "Validation: Deviance Test",
    "text": "Validation: Deviance Test\n\nAs in logistic regression, we can compare two nested models by a deviance test (or likelihood ratio test)\n\n\nIf model 2 has \\(q\\) fewer parameters compared to model 1, we have under \\(H_0\\): “the \\(q\\) coefficients in question are zero”:\n\n\\[D_2 - D_1 = 2(L_1 - L_2) \\xrightarrow{L} \\chi^2_q\\]\n\nRejection region at level \\(\\alpha\\):\n\n\\[\\text{CR}_\\alpha = \\{D_2 - D_1 &gt; \\chi^2_q(1-\\alpha)\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#global-significance-test",
    "href": "teaching/glm/slides/counting_models.html#global-significance-test",
    "title": "Counting Models",
    "section": "Global Significance Test",
    "text": "Global Significance Test\n\nThe global significance test corresponds to the case where model 2 contains only the constant.\n\n\nIn this case \\(D_2 = D_0\\) and \\(q = p - 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#validation-graphical-inspection",
    "href": "teaching/glm/slides/counting_models.html#validation-graphical-inspection",
    "title": "Counting Models",
    "section": "Validation: Graphical Inspection",
    "text": "Validation: Graphical Inspection\n\nPlot the predicted counts \\(\\hat{y}_x = n_x \\lambda_{\\hat{\\beta}}(x)\\) against the observed counts \\(y_x\\).\n\n\n\n\n\nWarning\n\n\nthe predicted counts \\(\\hat{y}_x\\) represent the expectation of the expected counts given \\(x\\).\nIt is therefore normal that the observed counts \\(y_x\\) are dispersed around the \\(\\hat{y}_x\\)\n\n\n\n\n\nIt is appropriate to have sufficiently large “classes” \\(x\\) (\\(n_x &gt; 5\\)) for the graph to be relevant."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#alternative-graphical-validation",
    "href": "teaching/glm/slides/counting_models.html#alternative-graphical-validation",
    "title": "Counting Models",
    "section": "Alternative Graphical Validation",
    "text": "Alternative Graphical Validation\n\nIdea: empirical distribution of \\(Y\\) VS its predicted distribution.\n\n\nThe empirical distribution of \\(Y\\) is simply given by\n\n\\(p_k = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}_{y_i = k}, \\quad k \\in \\mathbb{N}\\)\n\n\n\nWhile its predicted distribution is given by\n\n\\(\\hat{p}_k = \\frac{1}{n}\\sum_{i=1}^n \\hat{P}(Y = k|X = x_i), \\quad k \\in \\mathbb{N}\\)\n\n\n\nwhere \\(\\hat{P}(Y = k|X = x_i)\\) is the poisson distribution \\(\\mathcal P(\\lambda_{\\hat{\\beta}}(x_i))\\), i.e., \\(\\hat{P}(Y = k|X = x_i) = \\frac{\\lambda_{\\hat{\\beta}}(x_i)^k e^{-\\lambda_{\\hat{\\beta}}(x_i)}}{k!}\\)\nwith \\(\\lambda_{\\hat{\\beta}}(x_i) = \\exp(x_i^T \\hat{\\beta})\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-number-of-plant-species",
    "href": "teaching/glm/slides/counting_models.html#example-number-of-plant-species",
    "title": "Counting Models",
    "section": "Example: Number of Plant Species",
    "text": "Example: Number of Plant Species\n\nNumber of plant species recorded on a plot according to soil pH (Neutral, Acidic or Basic) and biomass collected.\n\n\n\nSpecies\npH\nBiomass\n\n\n\n\n14\nlow\n3.538\n\n\n31\nmid\n0.740\n\n\n36\nhigh\n7.242\n\n\n20\nmid\n3.216\n\n\n…\n…\n…\n\n\n\n\n\nWe want to model \\(Y =\\) “Species” as a function of pH and Biomass."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-r-output",
    "href": "teaching/glm/slides/counting_models.html#example-r-output",
    "title": "Counting Models",
    "section": "Example: R output",
    "text": "Example: R output\n\nglm(Species ∼ pH + Biomass, family=poisson)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n3.84894\n0.05281\n72.885\n&lt; 2e-16\n***\n\n\npHlow\n-1.13639\n0.06720\n-16.910\n&lt; 2e-16\n***\n\n\npHmid\n-0.44516\n0.05486\n-8.114\n4.88e-16\n***\n\n\nBiomass\n-0.12756\n0.01014\n-12.579\n&lt; 2e-16\n***\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n(Dispersion parameter for poisson family taken to be 1)\nNull deviance: 452.346\nResidual deviance: 99.242\nAIC: 526.43"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-number-of-plant-species-1",
    "href": "teaching/glm/slides/counting_models.html#example-number-of-plant-species-1",
    "title": "Counting Models",
    "section": "Example: Number of Plant Species",
    "text": "Example: Number of Plant Species\n\nThus, the average number of species, given pH and Biomass, is estimated as\n\n\\(\\lambda_{\\hat{\\beta}}(\\text{pH}, \\text{Biomass}) = \\exp(3.85 - 1.14 \\mathbf{1}_{\\text{pH=low}} - 0.46 \\mathbf{1}_{\\text{pH=mid}} - 0.13 \\text{Biomass})\\)\n\n\n\nRate Ratio for low pH (acidic) compared to high pH (basic):\n\n\\(RR(\\text{acidic}, \\text{basic}) = \\exp(-1.14) = 0.32\\)\n\n\n\nOn average, there are therefore about 3 times fewer species in acidic soil than in basic soil."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-interraction-ph-biomass",
    "href": "teaching/glm/slides/counting_models.html#example-interraction-ph-biomass",
    "title": "Counting Models",
    "section": "Example: Interraction Ph-Biomass",
    "text": "Example: Interraction Ph-Biomass\n\nWe can try to introduce an interaction between pH and Biomass\nglm(Species ~ pH + Biomass + pH:Biomass, family=poisson)\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n3.76812\n0.06153\n61.240\n&lt; 2e-16\n***\n\n\npHlow\n-0.81557\n0.10284\n-7.931\n2.18e-15\n***\n\n\npHmid\n-0.33146\n0.09217\n-3.596\n0.000323\n***\n\n\nBiomass\n-0.10713\n0.01249\n-8.577\n&lt; 2e-16\n***\n\n\npHlow:Biomass\n-0.15503\n0.04003\n-3.873\n0.000108\n***\n\n\npHmid:Biomass\n-0.03189\n0.02308\n-1.382\n0.166954\n\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n(Dispersion parameter for poisson family taken to be 1)\nNull deviance: 452.346\nResidual deviance: 83.201\nAIC: 514.39"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-model-comparison",
    "href": "teaching/glm/slides/counting_models.html#example-model-comparison",
    "title": "Counting Models",
    "section": "Example: Model Comparison",
    "text": "Example: Model Comparison\n\nThe model with interaction seems preferable (via AIC and deviance test).\n\n\nThe average number of species, given pH and Biomass, is this time:\n\n\n\\[\\begin{aligned}\n\\lambda_{\\hat{\\beta}}(\\text{pH}, \\text{Bm}) &= \\exp(3.77 - 0.82 \\mathbf{1}_{\\text{pH=low}} - 0.33 \\mathbf{1}_{\\text{pH=mid}}) \\\\\n&- 0.11\\text{Bm} - 0.16\\text{Bm}\\mathbf{1}_{\\text{pH=low}} - 0.032\\text{Bm}\\mathbf{1}_{\\text{pH=mid}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-bm-dependent-rr",
    "href": "teaching/glm/slides/counting_models.html#example-bm-dependent-rr",
    "title": "Counting Models",
    "section": "Example: Bm-dependent RR",
    "text": "Example: Bm-dependent RR\n\nThe Rate Ratio for low pH (acidic) compared to high pH (basic) depends on Biomass and equals:\n\n\n\\(RR(\\text{acidic}, \\text{basic}) = \\exp(-0.82 - 0.16\\text{Bm})\\)"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#example-predicted-vs-observed-counts",
    "href": "teaching/glm/slides/counting_models.html#example-predicted-vs-observed-counts",
    "title": "Counting Models",
    "section": "Example: Predicted vs Observed Counts",
    "text": "Example: Predicted vs Observed Counts\n\nPredicted mean counts \\(\\hat{y}_i = \\lambda_{\\hat{\\beta}}(\\text{pH}_i, \\text{Biomass}_i)\\) as a function of observed counts \\(y_i\\)."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#model-predictions-by-ph",
    "href": "teaching/glm/slides/counting_models.html#model-predictions-by-ph",
    "title": "Counting Models",
    "section": "Model Predictions by pH",
    "text": "Model Predictions by pH\n\nLines: predicted mean counts by pH as a function of biomass\nPoints: observed counts by pH as a function of biomass\n\n\n\nBlack: pH=basic; Green: pH=neutral; Red: pH=acidic"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#predicted-distribution",
    "href": "teaching/glm/slides/counting_models.html#predicted-distribution",
    "title": "Counting Models",
    "section": "Predicted Distribution",
    "text": "Predicted Distribution\n\nHistogram: empirical distribution\nPoints: predicted distribution"
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#model-limitations",
    "href": "teaching/glm/slides/counting_models.html#model-limitations",
    "title": "Counting Models",
    "section": "Model Limitations",
    "text": "Model Limitations\n\nWhen we model \\(Y|(X = x) \\sim \\mathcal P(\\lambda(x))\\), we have\n\n\\[\\E(Y|X = x) = \\lambda(x)\\]\n\n\n\nbut also\n\n\\[\\Var(Y|X = x) = \\lambda(x)\\]\n\n\n\nThis constraint is a limitation of the Poisson model."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#overdispersion-1",
    "href": "teaching/glm/slides/counting_models.html#overdispersion-1",
    "title": "Counting Models",
    "section": "Overdispersion",
    "text": "Overdispersion\n\nSome data are overdispersed, in the sense that\n\n\\[\\Var(Y|X = x) &gt; \\E(Y|X = x)\\]\n\n\n\nMore rarely, we can find underdispersed data.\n\n\nIn case of overdispersion, the estimated variance of estimators is underestimated."
  },
  {
    "objectID": "teaching/glm/slides/counting_models.html#how-to-detect-overdispersion",
    "href": "teaching/glm/slides/counting_models.html#how-to-detect-overdispersion",
    "title": "Counting Models",
    "section": "How to Detect Overdispersion?",
    "text": "How to Detect Overdispersion?\n\nBy assuming that \\(\\Var(Y|X = x) = \\phi \\E(Y|X = x)\\) where \\(\\phi &gt; 0\\), we can estimate \\(\\phi\\) by\n\n\\[\\hat{\\phi} = \\frac{1}{n-p} \\sum_{i=1}^n \\frac{(y_i - \\hat{y}_i)^2}{\\hat{y}_i}\\]\n\nand test if \\(\\phi = 1\\) or not (if \\(\\phi = 1\\), \\(\\hat{\\phi} \\sim N(1, 1/n)\\) when \\(n \\to \\infty\\)).\n\n\nWe can fit a negative binomial model (cf the following), and test if it is better than the Poisson model."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#recalling-the-linear-model",
    "href": "teaching/glm/slides/Introduction_glm.html#recalling-the-linear-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Recalling the Linear Model",
    "text": "Recalling the Linear Model\n\nWe observe \\(Y = (Y_1, \\dots, Y_n)\\) and \\(X = (X^{(1)} , . . . , X^{(p)}) \\in \\mathbb R^{n \\times p}\\),\n\n\nIn the Linear Model, We assume that for some unknown \\(\\beta\\)\n\n\\[Y = X\\beta + \\varepsilon\\]\n\n\n\nThe hypothesis can be written in the form \\(\\mathbb E[Y|X] = X\\beta\\)\n\n\nThe OLS estimator of \\(\\beta\\) is \\(\\hat \\beta= (X^TX)^{-1}X^TY\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#when-this-linearity-is-reasonable",
    "href": "teaching/glm/slides/Introduction_glm.html#when-this-linearity-is-reasonable",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When this Linearity is Reasonable",
    "text": "When this Linearity is Reasonable\n\nThe hypothesis \\(\\E(Y|X) = X^T\\beta\\) in linear regression models implies that \\(\\E(Y|X)\\) can take any real value.\n\n\nThis is not a restriction when:\n\n\\(Y|X\\) follows a Gaussian distribution\n\\(Y|X\\) follows any other continuous distribution on \\(\\mathbb{R}\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#when-it-is-not",
    "href": "teaching/glm/slides/Introduction_glm.html#when-it-is-not",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When it is not",
    "text": "When it is not\n\nThe linear assumption is inappropriate for certain variables \\(Y\\), particularly when \\(Y\\) is qualitative or discrete.\n\n\nBinary outcomes (\\(Y = 0\\) or \\(1\\)):\n\nReturn to employment within 3 months\nEffectiveness of a medical treatment\n\nCredit approval for a bank loan applicant"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#when-it-is-not-1",
    "href": "teaching/glm/slides/Introduction_glm.html#when-it-is-not-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "When it is not",
    "text": "When it is not\n\nCategorical outcomes (\\(Y \\in \\{A_1, \\ldots, A_k\\}\\)):\n\nCustomer segmentation into \\(k\\) categories\n\n\n\nCount data (\\(Y \\in \\mathbb{N}\\)):\n\nNumber of traffic accident fatalities per month"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#key-differences-by-response-type",
    "href": "teaching/glm/slides/Introduction_glm.html#key-differences-by-response-type",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Key Differences by Response Type",
    "text": "Key Differences by Response Type\n\nIn all examples, the objective remains to link \\(Y\\) to \\(X = (X^{(1)}, \\ldots, X^{(p)})\\) through modeling \\(\\E(Y|X)\\).\n\n\nHowever, \\(\\E(Y|X)\\) has different interpretations depending on the situation:\n\nBinary \\(Y\\): \\(Y = 0\\) or \\(1\\)\nCategorical \\(Y\\): \\(Y \\in \\{A_1, \\ldots, A_k\\}\\)\n\nCount data: \\(Y \\in \\mathbb{N}\\)\n\n\n\nIn all these cases, the linear model \\(\\E(Y|X) = X^T\\beta\\) is inappropriate."
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#objectives-of-the-glm-1",
    "href": "teaching/glm/slides/Introduction_glm.html#objectives-of-the-glm-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Objectives of the GLM",
    "text": "Objectives of the GLM\n\nWe model \\(\\E(Y|X)\\) differently using generalized linear models.\nAs in linear regression, we focus on:\n\nSpecific effects: The individual effect of a given regressor, (all other things being equal)\nExplanation: Understanding relationships\nPrediction: Forecasting outcomes"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#three-fundamental-cases",
    "href": "teaching/glm/slides/Introduction_glm.html#three-fundamental-cases",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Three Fundamental Cases",
    "text": "Three Fundamental Cases\n\nWe detail the modeling challenges for \\(\\E(Y|X)\\) in three fundamental cases:\n\nCase 1: Binary: \\(Y\\) is binary (takes values 0 or 1)\nCase 2: Categorical: \\(Y \\in \\{A_1, \\ldots, A_k\\}\\) (general qualitative variable)\nCase 3: Count: \\(Y \\in \\mathbb{N}\\) (count variable)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-1-binary-case",
    "href": "teaching/glm/slides/Introduction_glm.html#case-1-binary-case",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 1: Binary Case",
    "text": "Case 1: Binary Case\n\nWithout loss of generality, \\(Y \\in \\{0, 1\\}\\)\n\n\nIf \\(Y\\) models membership in a category \\(A\\), this is equivalent to studying the variable \\(Y = \\mathbf{1}_A\\)\n\n\nThe distribution of \\(Y\\) given \\(X = x\\) is entirely determined by \\(p(x) = P(Y = 1|X = x)\\)\n\n\nWe deduce \\(P(Y = 0|X = x) = 1 - p(x)\\)\n\n\n\\(Y|X = x\\) follows a Bernoulli distribution with parameter \\(p(x)\\)\n\n\n\\(\\E(Y|X = x) = p(x)\\)\n\n\nkey constraint: \\(p(x) \\in [0, 1]\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#modelling-px",
    "href": "teaching/glm/slides/Introduction_glm.html#modelling-px",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Modelling \\(p(x)\\)",
    "text": "Modelling \\(p(x)\\)\n\n\n\\[E(Y|X = x) = P(Y = 1|X = x) = p(x) \\in [0, 1]\\]\n\n\n\nWhat NOT to do: \\(p(x) = x^T\\beta\\) (for some \\(\\beta \\in \\mathbb{R}^p\\) to be estimated)\n\n\nProposed approach: We can propose a model of the type:\n\n\\[p(x) = f(x^T\\beta)\\]\n\nwhere \\(f\\) is a function from \\(\\mathbb{R}\\) to \\([0, 1]\\)\n\n\nBenefits: Coherent model that depends only on \\(\\beta\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-2-categorical-y",
    "href": "teaching/glm/slides/Introduction_glm.html#case-2-categorical-y",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Categorical \\(Y\\)",
    "text": "Case 2: Categorical \\(Y\\)\n\nIf \\(Y\\) represents membership in \\(k\\) different classes \\(A_1, \\ldots, A_k\\), its distribution is determined by the probabilities:\n\n\\[p_j(x) = P(Y \\in A_j|X = x), \\quad \\text{for } j = 1, \\ldots, k\\]\n\n\n\nConstraint: \\(\\sum_{j=1}^{k} p_j(x) = 1\\) (If \\(k = 2\\), this reduces to the previous case)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-2-categorical-y-1",
    "href": "teaching/glm/slides/Introduction_glm.html#case-2-categorical-y-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Categorical \\(Y\\)",
    "text": "Case 2: Categorical \\(Y\\)\n\n\\(Y = (\\mathbf{1}_{A_1}, \\ldots, \\mathbf{1}_{A_k})\\) follows a multinomial distribution and:\n\n\\[\\E(Y|X = x) = \\begin{pmatrix} p_1(x) \\\\ \\vdots \\\\ p_k(x) \\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-2-model-for-categorical-y",
    "href": "teaching/glm/slides/Introduction_glm.html#case-2-model-for-categorical-y",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 2: Model for Categorical \\(Y\\)",
    "text": "Case 2: Model for Categorical \\(Y\\)\n\nTo model \\(\\E(Y|X = x)\\), it suffices to model \\(p_1(x), \\ldots, p_{k-1}(x)\\) since \\(p_k(x) = 1 - \\sum_{j=1}^{k-1} p_j(x)\\)\n\n\nProposed model: As in the binary case, we can propose:\n\n\\[p_j(x) = f(x^T\\beta_j), \\quad j = 1, \\ldots, k-1\\]\n\nwhere \\(f: \\mathbb{R} \\to [0,1]\\)\n\n\nParameters: There will be \\(k-1\\) unknown parameters to estimate, each in \\(\\mathbb{R}^p\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#case-3-count-y---non-negative-integer-values",
    "href": "teaching/glm/slides/Introduction_glm.html#case-3-count-y---non-negative-integer-values",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Case 3: Count Y - Non-negative Integer Values",
    "text": "Case 3: Count Y - Non-negative Integer Values\n\nIf \\(Y\\) takes integer values, we have for all \\(x\\), \\(E(Y|X = x) \\geq 0\\)\n\n\nCoherent choice: A coherent approach is:\n\n\\[\\E(Y|X = x) = f(x^T\\beta)\\]\n\nwhere \\(f\\) is a function from \\(\\mathbb{R}\\) to \\([0, +\\infty)\\)\n\n\nExample of possible choice for f: The exponential function"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#model-formulation",
    "href": "teaching/glm/slides/Introduction_glm.html#model-formulation",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Model Formulation",
    "text": "Model Formulation\n\nLet \\(g\\) be a strictly monotonic function, called the link function\n\n\nA generalized linear model (GLM) establishes a relationship of the type:\n\n\\[g(\\E(Y|X = x)) = x^T\\beta\\]\n\nEquivalently,\n\n\\[\\E(Y|X = x) = g^{-1}(x^T\\beta)\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#remarks-on-model",
    "href": "teaching/glm/slides/Introduction_glm.html#remarks-on-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Remarks on Model",
    "text": "Remarks on Model\n\nIn the previous examples, \\(g^{-1}\\) was denoted \\(f\\)\nWe generally assume that the distribution \\(Y|X\\) belongs to the exponential family, with unknown parameter \\(\\beta\\)\nThis allows us to compute the likelihood and facilitates inference"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#remark-on-the-intercept",
    "href": "teaching/glm/slides/Introduction_glm.html#remark-on-the-intercept",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Remark on the Intercept",
    "text": "Remark on the Intercept\n\nAmong the explanatory variables \\(X^{(1)}, \\ldots, X^{(p)}\\), we often assume that \\(X^{(1)}=\\1\\) to account for the presence of a constant. Thus: \\[X\\beta = \\beta_1 X^{(1)} + \\cdots + \\beta_p X^{(p)} = \\beta_1 + \\beta_2 X^{(2)} + \\cdots + \\beta_p X^{(p)}\\]\n\n\nAlternative notation: Sometimes indexed differently to write \\(\\beta_0 + \\beta_1 X^{(1)} + \\cdots + \\beta_p X^{(p)}\\)"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#example-1-linear-regression-model",
    "href": "teaching/glm/slides/Introduction_glm.html#example-1-linear-regression-model",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 1: Linear Regression Model",
    "text": "Example 1: Linear Regression Model\n\nLink function: We recover linear regression by taking the identity link function \\(g(t) = t\\)\n\n\nExpected value: Then:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) = x^T\\beta\\]\n\n\n\nIn the Gaussian linear model:\n\n\\[Y|X \\sim \\mathcal{N}(X\\beta, \\sigma^2)\\]\n\n\n\nLinear regression is therefore a special case of GLM models!"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#example-2-binary-case-y-in-0-1",
    "href": "teaching/glm/slides/Introduction_glm.html#example-2-binary-case-y-in-0-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)",
    "text": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)\n\nLink function requirement: The link function \\(g\\) must satisfy:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) \\in [0, 1]\\]\n\n\n\nSince \\(Y\\in \\{0,1\\}\\), \\(Y|X\\) follows a Bernoulli distribution\n\n\\[Y|X \\sim \\mathcal{B}(g^{-1}(X^T\\beta))\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#example-2-binary-case-y-in-0-1-1",
    "href": "teaching/glm/slides/Introduction_glm.html#example-2-binary-case-y-in-0-1-1",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)",
    "text": "Example 2: Binary Case \\(Y \\in \\{0, 1\\}\\)\n\n\n\\[Y|X \\sim \\mathcal{B}(g^{-1}(X^T\\beta))\\]\n\nPossible choices for \\(g^{-1}\\): A CDF of a continuous distrib. on \\(\\mathbb{R}\\)\n\n\nStandard choice for \\(g^{-1}\\): The CDF of a logistic distribution:\n\n\\[g^{-1}(t) = \\frac{e^t}{1 + e^t} \\quad \\text{i.e.} \\quad g(t) = \\ln\\left(\\frac{t}{1-t}\\right) = \\text{logit}(t)\\]\n\n\n\nThis leads to the logistic model, the most important model in this chapter"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#example-3-count-data-yin-mathbb-n",
    "href": "teaching/glm/slides/Introduction_glm.html#example-3-count-data-yin-mathbb-n",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Example 3: Count Data \\(Y\\in \\mathbb N\\)",
    "text": "Example 3: Count Data \\(Y\\in \\mathbb N\\)\n\nLink function: \\(g(t) = \\ln(t)\\), \\(g^{-1}(t) = e^t\\) gives:\n\n\\[E(Y|X = x) = g^{-1}(x^T\\beta) = e^{x^T\\beta}\\]\n\n\n\nFor the distribution of \\(Y|X\\), defined on \\(\\mathbb{N}\\), we often assume it follows a Poisson distribution (exp. familily)\n\n\nIn this context:\n\n\\[Y|X \\sim \\mathcal{P}(e^{X^T\\beta})\\]"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#summary",
    "href": "teaching/glm/slides/Introduction_glm.html#summary",
    "title": "Introduction to the Generalized Linear Model",
    "section": "Summary",
    "text": "Summary\nThere are 2 choices to make when setting up a GLM model:\n\nThe distribution of \\(Y|X\\)\nThe link function \\(g\\) defining \\(E(Y|X) = g^{-1}(X^T\\beta)\\)\n\n\nKey insight: The second choice is linked to the first"
  },
  {
    "objectID": "teaching/glm/slides/Introduction_glm.html#the-3-common-cases",
    "href": "teaching/glm/slides/Introduction_glm.html#the-3-common-cases",
    "title": "Introduction to the Generalized Linear Model",
    "section": "The 3 Common Cases",
    "text": "The 3 Common Cases\n\nBinary (\\(Y \\in \\{0, 1\\}\\)):\n→ \\(Y|X\\): it’s a Bernoulli distribution\n→ By default \\(g = \\text{logit}\\) (see later)\n\n\nMulti-category (\\(Y \\in \\{A_1, \\ldots, A_k\\}\\)):\n→ \\(Y|X\\): it’s a multinomial distribution\n→ By default \\(g = \\text{logit}\\)\n\n\nCount (\\(Y \\in \\mathbb{N}\\)):\n→ \\(Y|X\\): Poisson (often) or negative binomial\n→ Choice of \\(g\\): by default \\(g = \\ln\\)"
  },
  {
    "objectID": "teaching/probability/density_likelihood.html",
    "href": "teaching/probability/density_likelihood.html",
    "title": "Density, Likelihood and Radon Nikodym",
    "section": "",
    "text": "Notation\nIn this note, \\((\\Omega,\\mathbb P)\\) denote a common probability measured space for all the random variables introduced in this note. We also write \\(\\mathbb E\\) for the expectation. Let \\(X\\) be a random variable with values in a measurable space \\((\\mathcal X, \\mathcal A)\\). Let say for simplicity that \\(\\mathcal X = \\mathbb R^n\\).\nWe say that \\(X\\) has distribution \\(P\\) if \\(\\mathbb P(X \\in A)=P(A)\\) for any measurable set \\(A\\). For clarity, we sometimes write \\(\\mathbb P_{X \\sim P}(X \\in A)\\), which means “the probability of \\(X\\) being in \\(A\\) if \\(X\\) follows distribution \\(P\\)”. Sometimes, we do the slight abuse of notation by writing that \\(P(A) = P(X \\in A)\\).\n\\(P\\) can be seen as the “pushforward” measure of the common probability measure \\(\\mathbb P\\) by random variable \\(X\\), since by definition, \\(\\mathbb P(X \\in A) = \\mathbb P(\\{\\omega \\in \\Omega, X(\\omega) \\in A\\})= \\mathbb P(X^{-1}(A))\\).\n\n\nContinuous Densities\nA measure \\(P\\) has density \\(p\\) with respect to the Lebesgue measure if for any event \\(A\\) (which is simply a measurable set of \\(\\mathbb R^n\\)), \\[P(A)= \\int_{x \\in \\mathbb R^n}\\mathbf 1\\{x \\in A\\}p(x)dx \\; .\\] \\(p(x)\\) is sometimes called the likelihood of a random variables that has distribution \\(P\\) at point \\(x\\). An equivalent condition is that for any “kind” real valued function \\(f\\) (e.g. continuous with bounded support), \\[\\mathbb E_{X \\sim P}[f(X)] \\stackrel{\\mathrm{def}}{=} \\int_{x \\in \\mathbb R^n}f(x)dP =\\int_{x \\in \\mathbb R^n}f(x)p(x)dx \\; .\\] We write that \\[dP(x) = p(x)dx ~~~ \\text{or}~~~~ \\tfrac{dP}{dx}(x) = p(x) \\; ,\\] and \\(\\tfrac{dP}{dx}\\) is called the Radon-Nikodym derivative of \\(P\\) with respect to the Lebesgue measure. The intuition of this abstract notation is the following. If \\(x \\in \\mathbb R^n\\) and \\(h\\) is a small quantity that goes to \\(0\\), \\(dP\\) represents the measure of the interval \\([x, x+h]\\), with respect to the measure \\(P\\). Then, \\(d P(x) = P([x, x+h]) = \\int_x^{x+h}p(x)dx \\sim p(x)h\\).\n\n\nThe Counting Measure for Discrete Random Variables\nRandom variables in \\(\\mathbb{R}\\) that take on a finite number of values are referred to as discrete random variables, and they do not have a density with respect to the Lebesgue measure. However, this case is much simpler and is handled within measure theory using the counting measure. As its name indicates, the counting measure \\(\\mu\\) on \\(\\mathcal X=\\mathbb R^n\\) counts the elements of a given set \\(A\\): \\[ \\mu(A) = |A| \\enspace .\\] In particular, \\(\\mu(A)\\) is infinite if \\(A\\) is an infinite set.\nLet \\(X\\) be a discrete random variable that takes values in \\(\\{x_1, \\dots, x_N\\}\\), e.g. a Bernoulli, Binomial or Poisson random variable, and let \\(P\\) be its probability distribution. Let \\(p(x_i)\\) be the probability that \\(X = x_i\\), that is \\(p(x_i) = P(\\{x_i\\}) = P(X = x_i) \\in [0,1]\\). In this discrete case, the probability \\(p(x_i)\\) represents the likelihood of the value \\(x_i\\) for the random variable \\(X\\). While \\(X\\) has not a density with respect to the Lebesgue measure, it has density \\(p\\) with respect to the counting measure \\(\\mu\\), that is \\[\\mathbb E_{X \\sim P}[f(X)] =\\int_{x \\in \\mathbb R^n}f(x)p(x)d\\mu, ~~~~~~~ \\frac{dP}{d\\mu}(x) = p(x) \\; .\\] \\[ \\] ### The General Radon Nikodym Theorem\nThe Radon Nikodym Theorem tells us that any probability \\(P\\) admits a density with respect to a given measure \\(\\nu\\) if it is absolutely continuous with respect to \\(\\nu\\), that is \\[ \\nu(A) = 0 \\implies P(A) = 0 \\; .\\] In this case, the density is the Radon Nikodym derivative \\(\\frac{dP}{d\\nu}\\) of \\(P\\) with respect to \\(\\nu\\) and satisfies\n\\[ \\mathbb E_{X \\sim P}[f(X)] =\\int_{x \\in \\mathbb R^n}f(x)\\frac{dP}{d\\nu}(x)d\\nu \\; .\\] Informally, the \\(d\\nu\\) simplify so that \\(\\frac{dP}{d\\nu}d\\nu\\) = \\(dP\\).\n\n\nGeneralized Likelihood Ratio\nIf \\(P\\) and \\(Q\\) are two probability measures such that \\(P\\) is absolutely continuous with respect to \\(Q\\), then the Radon Nikodym derivative \\(\\frac{dP}{d\\nu}\\) of \\(P\\) with respect to \\(Q\\) is a generalized likelihood ratio.\nIf \\(P\\) and \\(Q\\) are both absolutely continuous with respect to another measure \\(\\nu\\) (for example the Lebesgue measure), then the generalized likelihood ratio can be written \\[\n\\frac{dP}{dQ} = \\frac{\\frac{dP}{d\\nu}}{\\frac{dQ}{d\\nu}} \\; .\n\\]\nIn particular, if \\(P\\) and \\(Q\\) have positive densities \\(p\\) and \\(q\\) with respect to the Lebesgue measure, that is \\(\\frac{dP}{dx} = p(x)\\) and \\(\\frac{dQ}{dx} = q(x)\\), then\n\\[\n\\frac{dP}{dQ}(x) = \\frac{p(x)}{q(x)} \\; .\n\\]\nIn particular, the likelihood ratio does not depend on the reference measure (here Lebesgue).\n\n\nChange of Measure\nIf \\(\\mathbb E_{P}\\) (resp. \\(\\mathbb E_{Q}\\)) denotes the expectation when the random variable \\(X\\) follows distribution \\(P\\) (resp. \\(Q\\)), then for any measurable and bounded function \\(f\\),\n\\[\\mathbb E_{P}[f(X)] = \\mathbb E_{Q}\\left[f(X) \\frac{dP}{dQ}(X)\\right] \\; .\\] In other words, we simply replace the real random variable \\(f(X)\\) by the random variable \\(f(X) \\frac{dP}{dQ}(X)\\) when we observe \\(X\\) under \\(Q\\) instead of \\(P\\). This results directly follows from Radon-Nikodym: \\[\n\\int_{x \\in R^n} f(x) dP(x) = \\int_{x \\in R^n} f(x) \\frac{dP}{dQ}(x) dQ(x) \\; .\n\\]",
    "crumbs": [
      "Probability",
      "Density and Likelihood"
    ]
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#context",
    "href": "teaching/glm/slides/logistic_model.html#context",
    "title": "The Logistic Model",
    "section": "Context",
    "text": "Context\n\\(Y\\) is a binary variable \\(Y_k \\in \\{0,1\\}\\)\n\n\\(X = (X^{(1)}, \\ldots, X^{(p)})\\) are \\(p\\) explanatory variables\n\n\n\\(Y|X = x\\) follows a Bernoulli distribution with parameter \\(p(x) = P(Y = 1|X = x)\\). Model:\n\n\n\n\\[p(x) = g^{-1}(x^T\\beta)\\]\n\nwhere \\(g^{-1}\\) is a strictly increasing function with values in \\([0, 1]\\)\n\n\nApproach: We begin by discussing the choice of \\(g^{-1}\\) (or \\(g\\))"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-coronary-heart-disease",
    "href": "teaching/glm/slides/logistic_model.html#example-coronary-heart-disease",
    "title": "The Logistic Model",
    "section": "Example: Coronary Heart Disease",
    "text": "Example: Coronary Heart Disease\nData Description: Presence of chd as a function of age \\(Y = \\text{chd} \\in \\{0,1\\}\\), \\(X = \\text{age}\\)\n\nWe want to estimate \\(p(x) = \\E(Y|X = x) = \\P(\\text{chd} = 1|X = x)\\) for all \\(x\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-simple-idea",
    "href": "teaching/glm/slides/logistic_model.html#example-simple-idea",
    "title": "The Logistic Model",
    "section": "Example: Simple Idea",
    "text": "Example: Simple Idea\n\nGroup the \\(x\\) values by age class\nCalculate the proportion of \\(chd = 1\\) in the class containing \\(x\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-glm-approach",
    "href": "teaching/glm/slides/logistic_model.html#example-glm-approach",
    "title": "The Logistic Model",
    "section": "Example: GLM Approach",
    "text": "Example: GLM Approach\n\nObjective: We want to model \\(p(x) = P(\\text{chd} = 1|X = x)\\) by: \\[p(x) = g^{-1}(\\beta_0 + \\beta_1 x)\\]\n\n\nConstraint: We need \\(g^{-1}\\) to have values in \\([0, 1]\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.html#example-the-logit-model",
    "title": "The Logistic Model",
    "section": "Example: The Logit Model",
    "text": "Example: The Logit Model\n\n\n\n\\[g^{-1}(t) = \\frac{e^t}{1 + e^t} \\quad \\text{i.e.} \\quad g(t) = \\ln\\left(\\frac{t}{1-t}\\right) = \\text{logit}(t)\\]\n\n\n\n\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-the-probit-model",
    "href": "teaching/glm/slides/logistic_model.html#example-the-probit-model",
    "title": "The Logistic Model",
    "section": "Example: The Probit Model",
    "text": "Example: The Probit Model\n\nIf \\(\\Phi\\) is the CDF of a \\(\\mathcal{N}(0, 1)\\) distribution, we take \\(g^{-1}(t) = \\Phi(t)\\)\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-model-cloglog",
    "href": "teaching/glm/slides/logistic_model.html#example-model-cloglog",
    "title": "The Logistic Model",
    "section": "Example: Model cloglog",
    "text": "Example: Model cloglog\n\n\n\\[g^{-1}(t) = 1 - e^{-e^t}\\]\n\n\n\n\n\n\n\n\n\n\n\nResults of \\(g^{-1}(\\hat \\beta_0 + \\hat \\beta_1 x)\\) obtained by MLE"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-comparison-of-the-three-models",
    "href": "teaching/glm/slides/logistic_model.html#example-comparison-of-the-three-models",
    "title": "The Logistic Model",
    "section": "Example: Comparison of the Three Models",
    "text": "Example: Comparison of the Three Models\n\nlogit and probit give approximately the same result cloglog differs slightly and is not “symmetric”"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#which-link-function-to-choose-for-binary-case",
    "href": "teaching/glm/slides/logistic_model.html#which-link-function-to-choose-for-binary-case",
    "title": "The Logistic Model",
    "section": "Which Link Function to Choose for Binary Case?",
    "text": "Which Link Function to Choose for Binary Case?\n\nQuestion: Which link function to choose in practice when \\(Y\\) is binary?\n\n\nDefault choice: By default, we favor the logit model\n\n\nExceptions: Unless there is a good reason to choose something else (probit model, complementary log-log, or log-log)\n\n\nNext steps: We return to the 3 usual choices to justify this preference"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#details-on-the-probit-model",
    "href": "teaching/glm/slides/logistic_model.html#details-on-the-probit-model",
    "title": "The Logistic Model",
    "section": "Details on the Probit Model",
    "text": "Details on the Probit Model\n\nThe probit model is justified when the binary variable \\(Y|X = x\\) comes from thresholding a Gaussian latent variable \\(Z(x)\\):\n\n\\[(Y|X = x) = \\mathbf{1}_{Z(x) \\geq \\tau}\\]\n\nwhere \\(Z(x) \\sim \\mathcal{N}(x^T\\beta, \\sigma^2)\\)\n\n\nIf \\(\\Phi\\) is the CDF of a \\(\\mathcal N(0,1)\\) \\[P(Y = 1|X = x) = P(Z(x) \\geq \\tau) = \\Phi\\left(\\frac{x^T\\beta - \\tau}{\\sigma}\\right)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#examples-in-probit-model",
    "href": "teaching/glm/slides/logistic_model.html#examples-in-probit-model",
    "title": "The Logistic Model",
    "section": "Examples in Probit Model",
    "text": "Examples in Probit Model\n\n\\(Y\\) represents a purchase decision, and \\(Z(x)\\) quantifies the utility of the good\n\\(Y\\) is a declared psychological state (happiness, depression) and \\(Z(x)\\) is a latent, unobserved measure of personal satisfaction"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#probit-vs-logit-current-trends",
    "href": "teaching/glm/slides/logistic_model.html#probit-vs-logit-current-trends",
    "title": "The Logistic Model",
    "section": "Probit vs Logit: Current Trends",
    "text": "Probit vs Logit: Current Trends\n\nEconometricians: The probit model remains relatively popular among econometricians…\n\n\nGeneral trend: but it tends to be replaced by the logistic model\n\n\nAdvantages of logit: The logit model has many advantages that probit does not have:\n\nInterpretation of results\nExplicit formulas\n\n\n\nTheoretical justification: CDF of probit close to CDF of logit"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#remarks-on-cloglog-model",
    "href": "teaching/glm/slides/logistic_model.html#remarks-on-cloglog-model",
    "title": "The Logistic Model",
    "section": "Remarks on cloglog Model",
    "text": "Remarks on cloglog Model\n\nThe modeling approach is \\(p(x) = g^{-1}(x'\\beta)\\) with\n\n\\[g(t) = \\ln(-\\ln(1-t)) \\quad \\text{i.e.} \\quad g^{-1}(t) = 1 - e^{-e^t}\\]\n\n\n\nNot symmetric in the sense that \\(g(t) \\neq -g(1-t)\\).\n\\(p(x)\\) approaches \\(0\\) slowly but \\(1\\) very rapidly\n\n\nIf the opposite is true: take \\(g(t) = -\\ln(-\\ln(t))\\) (loglog model)\n\n\nUseful in survival models (e.g. Cox)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#details-on-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.html#details-on-the-logit-model",
    "title": "The Logistic Model",
    "section": "Details on the Logit Model",
    "text": "Details on the Logit Model\n\nHighly valued interpretation tool: odds-ratios.\nMore “practical” from a theoretical point of view.\nNatural model in many situations."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#theoretical-motivation-of-logit",
    "href": "teaching/glm/slides/logistic_model.html#theoretical-motivation-of-logit",
    "title": "The Logistic Model",
    "section": "Theoretical Motivation of Logit",
    "text": "Theoretical Motivation of Logit\n\nWe will show in exercises that:\nIf the two groups of individuals associated with \\(Y = 0\\) and \\(Y = 1\\) have a Gaussian distribution of \\(X\\) with different means, i.e. for \\(m_0 \\neq m_1\\),\n\n\\[X|(Y = 0) \\sim \\mathcal N(m_0, \\Sigma) \\and X|(Y = 1) \\sim \\mathcal N(m_1, \\Sigma)\\]\n\nthen \\(\\mathbb P(Y = 1|X = x)\\) follows a logistic model.\n\n\nThe previous result remains true for any distribution from the exponential family instead of \\(\\mathcal N\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#summary-for-binary-variables",
    "href": "teaching/glm/slides/logistic_model.html#summary-for-binary-variables",
    "title": "The Logistic Model",
    "section": "Summary for Binary Variables:",
    "text": "Summary for Binary Variables:\n\nIf \\(Y\\) is a binary variable, \\((Y|X = x) \\sim \\mathcal B(p(x))\\).\nIn a GLM model for \\(Y\\), we set \\(p(x) = g^{-1}(x^T\\beta)\\) where \\(g\\) is:\n\nby default the logit function, which is the most natural;\npossibly probit if we have good reasons to justify it (but the results will be similar to logit);\ncloglog (or loglog) if we have good reasons to justify it (strong asymmetry of \\(p(x)\\), connection with a Cox model).\n\n\n\nIn the following, we will focus on the logit model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#outline",
    "href": "teaching/glm/slides/logistic_model.html#outline",
    "title": "The Logistic Model",
    "section": "Outline",
    "text": "Outline\n\ninterpret the model,\nestimate \\(\\beta\\) from a dataset,\nevaluate the quality of estimation,\nexploit it to make predictions/classification."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#interpretation-of-the-logistic-model",
    "href": "teaching/glm/slides/logistic_model.html#interpretation-of-the-logistic-model",
    "title": "The Logistic Model",
    "section": "Interpretation of the Logistic Model",
    "text": "Interpretation of the Logistic Model\nIf \\(x=(x^{(1)}, \\dots, x^{(p)}) \\in \\mathbb R^{p \\times 1}\\)\n\n\\[p(x) = \\text{logit}^{-1}(x^T\\beta) = \\frac{e^{x^T\\beta}}{1 + e^{x^T\\beta}}\\]\n\n\n\\(x^{(j)} \\to p(x)\\) is increasing if \\(\\beta_j &gt; 0\\), decreasing otherwise.\nThe larger \\(|\\beta_j|\\) is, the stronger the discriminatory power of regressor \\(X^{(j)}\\) (a small variation in \\(x^{(j)}\\) can cause a large variation in \\(p(x)\\))."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#shape-of-logit-function",
    "href": "teaching/glm/slides/logistic_model.html#shape-of-logit-function",
    "title": "The Logistic Model",
    "section": "Shape of logit function",
    "text": "Shape of logit function\n\n\n\n\nshape of \\(x^{(j)} \\to p(x)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-bmi-study-french-imc",
    "href": "teaching/glm/slides/logistic_model.html#example-bmi-study-french-imc",
    "title": "The Logistic Model",
    "section": "Example: BMI Study (French: IMC)",
    "text": "Example: BMI Study (French: IMC)\nFor each of the 5300 patients, we observe:\n\n\\(Y\\): 1 if BMI &gt; 35, 0 otherwise\nAGE\nDBP: low pressure (diastolic)\nSEXE: male or female\nACTIV: 1 if intense sports activity, 0 otherwise\nWALK: 1 if walking or cycling to work, 0 otherwise\nMARITAL: marital status (6 categories: married, widowed, divorced, separated, single or cohabiting)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#model-definition",
    "href": "teaching/glm/slides/logistic_model.html#model-definition",
    "title": "The Logistic Model",
    "section": "Model Definition",
    "text": "Model Definition\n\nWe seek to model \\(P(Y = 1|X)\\) where \\(X\\) groups the previous variables (excluding \\(Y\\)).\n\n\nIn R, we use the glm function with the family=binomial option.\nglm(Y ~ AGE + DBP + SEXE + ACTIV + WALK + MARITAL, family=binomial)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#model-results",
    "href": "teaching/glm/slides/logistic_model.html#model-results",
    "title": "The Logistic Model",
    "section": "Model Results",
    "text": "Model Results\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-2.810240\n0.294316\n-9.548\n&lt; 2e-16\n***\n\n\nAGE\n-0.004407\n0.002717\n-1.622\n0.105\n\n\n\nDBP\n0.017581\n0.003283\n5.356\n8.53e-08\n***\n\n\nSEXEFEMME\n0.544916\n0.081261\n6.706\n2.00e-11\n***\n\n\nWALK1\n-0.409344\n0.095972\n-4.265\n2.00e-05\n***\n\n\nACTIV1\n-0.789734\n0.126653\n-6.235\n4.51e-10\n***\n\n\nMARITAL2\n0.070132\n0.149638\n0.469\n0.639\n\n\n\nMARITAL3\n-0.071318\n0.127510\n-0.559\n0.576\n\n\n\nMARITAL4\n0.188228\n0.206598\n0.911\n0.362\n\n\n\nMARITAL5\n0.070613\n0.115928\n0.609\n0.542\n\n\n\nMARITAL6\n-0.150165\n0.157687\n-0.952\n0.341\n\n\n\n\n\n\n\nThe interpretation is similar to that of a linear regression model.\n\n\nWe want to remove the MARITAL variable from the model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#model-results-with-age2",
    "href": "teaching/glm/slides/logistic_model.html#model-results-with-age2",
    "title": "The Logistic Model",
    "section": "Model Results with \\(AGE^2\\)",
    "text": "Model Results with \\(AGE^2\\)\n\nglm(Y ~ AGE + I(AGE^2) + DBP + SEXE + WALK + ACTIV, family=binomial)\nGives\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\nFor someone for which WALK1=0 and ACTIV1=0:\n\n\\(P(Y = 1|\\text{AGE}, \\text{DBP}) = \\text{logit}^{-1}(-3.95 + 0.064 \\times \\text{AGE} - 0.00068 \\times \\text{AGE}^2 + 0.0122 \\times \\text{DBP})\\)\n\n\n\nFor someone for which WALK1=0 and ACTIV1=1:\n\n\\(P(Y = 1|\\text{AGE}, \\text{DBP}) = \\text{logit}^{-1}(-3.95 + 0.064 \\times \\text{AGE} - 0.00068 \\times \\text{AGE}^2 + 0.0122 \\times \\text{DBP} \\color{red}{ - 0.657})\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#odds",
    "href": "teaching/glm/slides/logistic_model.html#odds",
    "title": "The Logistic Model",
    "section": "Odds",
    "text": "Odds\nIf \\(p\\) is the probability of an event \\(A\\), then its odds are:\n\n\\[\\text{odds} = \\frac{p}{1-p}\\]\n\n\nBetting interpretation for example, \\(3\\) to \\(1\\) means that for \\(3\\) people betting on \\(A\\), \\(1\\) person bets on \\(B\\).\n\n\nSo a randomly chosen bettor has a probability of \\(p=3/4\\) of betting on \\(A\\) and \\(1-p=1/4\\) on betting on \\(B\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#odds-given-xx",
    "href": "teaching/glm/slides/logistic_model.html#odds-given-xx",
    "title": "The Logistic Model",
    "section": "Odds given \\(X=x\\)",
    "text": "Odds given \\(X=x\\)\n\nSimilarly, the odds of obtaining \\(Y = 1\\) given \\(X = x\\) is:\n\n\\[\\text{odds}(x) = \\frac{p(x)}{1 - p(x)}\\]\n\n\n\nwhere \\(p(x) = P(Y = 1|X = x)\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#odds-ratio",
    "title": "The Logistic Model",
    "section": "Odds Ratio",
    "text": "Odds Ratio\n\nIf two individuals have characteristics \\(x_1\\) and \\(x_2\\) respectively, we call the odds ratio between \\(x_1\\) and \\(x_2\\):\n\\[OR(x_1, x_2) = \\frac{\\text{odds}(x_1)}{\\text{odds}(x_2)} = \\frac{\\frac{p(x_1)}{1-p(x_1)}}{\\frac{p(x_2)}{1-p(x_2)}}\\]\n\n\n\n\n\nWarning\n\n\nDO NOT CONFUSE ODDS RATIO WITH PROBABILITY RATIO\nOnly possible exception: if \\(p(x_1)\\) and \\(p(x_2)\\) are very small because then \\(1 - p(x_1) \\approx 1\\) and \\(1 - p(x_2) \\approx 1\\), so that \\(OR(x_1, x_2) \\approx p(x_1)/p(x_2)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#link-with-proba.-ratio",
    "href": "teaching/glm/slides/logistic_model.html#link-with-proba.-ratio",
    "title": "The Logistic Model",
    "section": "Link with Proba. Ratio",
    "text": "Link with Proba. Ratio\n\nHowever, it remains that:\n\\[\\begin{aligned}\nOR(x_1, x_2) &gt; 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} &gt; 1\\\\\nOR(x_1, x_2) &lt; 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} &lt; 1\\\\\nOR(x_1, x_2) = 1 &\\Leftrightarrow \\frac{p(x_1)}{p(x_2)} = 1\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#other-property-of-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#other-property-of-odds-ratio",
    "title": "The Logistic Model",
    "section": "Other Property of Odds Ratio",
    "text": "Other Property of Odds Ratio\n\n\\(OR(x_1, x_2)\\) accentuates the differences compared to \\(p(x_1)/p(x_2)\\):\n\\[\\begin{aligned}\nOR(x_1, x_2) &gt; \\frac{p(x_1)}{p(x_2)} &\\text{ when } OR(x_1, x_2) &gt; 1\\\\\nOR(x_1, x_2) &lt; \\frac{p(x_1)}{p(x_2)} &\\text{ when } OR(x_1, x_2) &lt; 1\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#examples-using-odds-ratios",
    "href": "teaching/glm/slides/logistic_model.html#examples-using-odds-ratios",
    "title": "The Logistic Model",
    "section": "Examples Using Odds Ratios",
    "text": "Examples Using Odds Ratios\n\nA logistic regression is most often used to compare the behavior of two individuals with respect to the variable of interest.\n\n\nExamples:\n\nprobability of purchase depending on whether or not one has been the subject of a personalized promotion;\nfor a given vehicle, probability of experiencing a breakdown according to age;\nprobability of recovery according to the treatment used;"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#odds-ratio-in-logistic-regression",
    "href": "teaching/glm/slides/logistic_model.html#odds-ratio-in-logistic-regression",
    "title": "The Logistic Model",
    "section": "Odds Ratio in Logistic Regression",
    "text": "Odds Ratio in Logistic Regression\n\nIt holds that \\(\\text{odds}(x) = \\frac{p(x)}{1 - p(x)} = \\exp(x^T \\beta)\\)\nHence,\n\n\\[OR(x_1, x_2) = \\frac{\\text{odds}(x_1)}{\\text{odds}(x_2)} = \\exp((x_1 - x_2)^T \\beta)\\]\n\n\n\nIf the two individuals differ only by regressor \\(j\\), then\n\\(OR(x_1, x_2) = \\exp(\\beta_j (x_1^{(j)} - x_2^{(j)}))\\)\n\n\nIf regressor \\(j\\) is binary (\\(x_1^{(j)} = 1\\) while \\(x_2^{(j)} = 0\\)):\n\\(OR(x_1, x_2) = \\exp(\\beta_j)\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#key-summary-statement",
    "href": "teaching/glm/slides/logistic_model.html#key-summary-statement",
    "title": "The Logistic Model",
    "section": "Key Summary Statement",
    "text": "Key Summary Statement\n\nIn a logistic regression model, \\(\\beta_j\\) is interpreted as the logarithm of the odds-ratio between two individuals differing by a quantity of \\(1\\) on regressor \\(j\\), all else being equal.\n\n\nIn brief: \\(\\exp(\\beta_j) = OR(x^{(j)} + 1, x^{(j)})\\)\nIf regressor \\(j\\) is binary (absence or presence of a certain characteristic):\n\n\n\\(\\exp(\\beta_j)\\) is simply the OR between the presence or absence of this characteristic, all else being equal."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-1-intense-sports-activity",
    "href": "teaching/glm/slides/logistic_model.html#example-1-intense-sports-activity",
    "title": "The Logistic Model",
    "section": "Example 1: Intense Sports Activity",
    "text": "Example 1: Intense Sports Activity\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\n\n\nThe Odds Ratio corresponding to practicing or not practicing intense sports activity is, all else being equal:\n\n\n\\(\\exp(-0.657) \\approx 0.52\\)\n\n\nThe odds of obesity occurrence therefore decrease by half for individuals practicing intense sports activity.\n(The odds, not the probability!)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-2-diastolic-pressure",
    "href": "teaching/glm/slides/logistic_model.html#example-2-diastolic-pressure",
    "title": "The Logistic Model",
    "section": "Example 2: Diastolic Pressure",
    "text": "Example 2: Diastolic Pressure\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\nThe OR for a diastolic pressure difference of \\(+20\\) is:\n\n\\(\\exp(0.0121546 \\times 20) \\approx 1.28\\)\n\n\nThe odds of obesity occurrence therefore increase by \\(28\\%\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#the-framework",
    "href": "teaching/glm/slides/logistic_model.html#the-framework",
    "title": "The Logistic Model",
    "section": "The Framework",
    "text": "The Framework\n\nWe observe \\(n\\) i.i.d. realizations \\((Y_i, X_i)\\) where \\(Y_i \\in \\{0, 1\\}\\) and \\(X_i \\in \\mathbb{R}^p\\).\n\n\nWe denote \\(p(x_i) = P(Y_i = 1|X_i = x_i)\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#the-logistic-model",
    "href": "teaching/glm/slides/logistic_model.html#the-logistic-model",
    "title": "The Logistic Model",
    "section": "The Logistic Model",
    "text": "The Logistic Model\n\nWe assume the logistic model: for all \\(i\\),\n\n\\[p(x_i) = \\text{logit}^{-1}(x_i^T \\beta) = \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\]\n\n\n\nwhere \\(\\beta = (\\beta_1, \\ldots, \\beta_p)^T\\) and \\(x_i^T \\beta = \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#parameter-estimation",
    "href": "teaching/glm/slides/logistic_model.html#parameter-estimation",
    "title": "The Logistic Model",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\n\n\\[p(x_i) = \\text{logit}^{-1}(x_i^T \\beta) = \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\]\n\nWe will estimate \\(\\beta\\) by maximizing the likelihood.\n\n\nWe will denote \\(p_\\beta(x_i)\\) to emphasize the dependence of \\(p(x_i)\\) on \\(\\beta\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#likelihood-calculation",
    "href": "teaching/glm/slides/logistic_model.html#likelihood-calculation",
    "title": "The Logistic Model",
    "section": "Likelihood Calculation",
    "text": "Likelihood Calculation\n\nFor all \\(i\\), \\(Y_i|(X_i = x_i)\\) follows the distribution \\(B(p_\\beta(x_i))\\). Therefore\n\n\n\\[P(Y_i = y_i|X_i = x_i) = p_\\beta(x_i)^{y_i}(1 - p_\\beta(x_i))^{1-y_i}\\]\n\n\n\n\nfor all \\(y_i \\in \\{0, 1\\}\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#likelihood",
    "href": "teaching/glm/slides/logistic_model.html#likelihood",
    "title": "The Logistic Model",
    "section": "Likelihood",
    "text": "Likelihood\n\nBy independence, we obtain the likelihood\n\n\\[\\ell(\\beta, y_1, \\ldots, y_n, x_1, \\ldots, x_n) = \\prod_{i=1}^n p_\\beta(x_i)^{y_i}(1 - p_\\beta(x_i))^{1-y_i}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#log-likelihood",
    "href": "teaching/glm/slides/logistic_model.html#log-likelihood",
    "title": "The Logistic Model",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n\nTaking the log and replacing \\(p(x_i)\\) by its expression, we obtain the log-likelihood:\n\n\n\\[\\begin{aligned}\nL(\\beta, y_1, \\ldots, y_n, x_1, \\ldots, x_n) &= \\ln(\\ell) \\\\\n&=\\sum_{i=1}^n \\left[y_i x_i^T \\beta - \\ln(1 + e^{x_i^T \\beta})\\right]\n\\end{aligned}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#mle-calculation",
    "href": "teaching/glm/slides/logistic_model.html#mle-calculation",
    "title": "The Logistic Model",
    "section": "MLE Calculation",
    "text": "MLE Calculation\n\nThe MLE \\(\\hat{\\beta}\\), if it exists, cancels the gradient of \\(L\\) with respect to \\(\\beta\\). This gradient equals\n\n\\[\\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^n x_i \\left(y_i - \\frac{e^{x_i^T \\beta}}{1 + e^{x_i^T \\beta}}\\right)\\]\n\nWe therefore need to solve a system of \\(p\\) equations with \\(p\\) unknowns.\n\n\nBut the solution is not explicit: we resort to numerical methods (Newto-Raphso algo)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#remarks",
    "href": "teaching/glm/slides/logistic_model.html#remarks",
    "title": "The Logistic Model",
    "section": "Remarks",
    "text": "Remarks\n\nThis is a classic situation when using advanced statistical models: we often resort to optimization algorithms.\n\n\nDoes the solution exist? Is it unique?"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#mle-uniqueness",
    "href": "teaching/glm/slides/logistic_model.html#mle-uniqueness",
    "title": "The Logistic Model",
    "section": "MLE Uniqueness",
    "text": "MLE Uniqueness\n\nLet \\(X\\) be the design matrix \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\)\n\n\n\n\n\nProposition\n\n\nIf \\(\\text{rank}(X) = p\\), then the MLE, if it exists, is unique."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#proof-of-mle-uniqueness",
    "href": "teaching/glm/slides/logistic_model.html#proof-of-mle-uniqueness",
    "title": "The Logistic Model",
    "section": "Proof of MLE Uniqueness",
    "text": "Proof of MLE Uniqueness\n\nIt suffices to show that \\(L\\) is strictly concave in \\(\\beta\\).\n\n\nHessian Matrix of \\(L\\):\n\\[\\frac{\\partial^2 L}{\\partial \\beta^2} = -\\sum_{i=1}^n p_\\beta(x_i)(1 - p_\\beta(x_i)) x_i x_i^T\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#hessian-properties",
    "href": "teaching/glm/slides/logistic_model.html#hessian-properties",
    "title": "The Logistic Model",
    "section": "Hessian Properties",
    "text": "Hessian Properties\n\nIt is negative semi-definite. Moreover, for all \\(u \\in \\mathbb{R}^p\\),\n\\[\\begin{aligned}\nu^T \\frac{\\partial^2 L}{\\partial \\beta^2} u = 0 &\\Leftrightarrow u^T x_i x_i^T u = 0 \\text{ for all } i\\\\\n&\\Leftrightarrow u^T x_i = 0 \\text{ for all } i\\\\\n&\\Leftrightarrow Xu = 0\\\\\n&\\Leftrightarrow u = 0\n\\end{aligned}\\]\nsince \\(\\text{rank}(X) = p\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#conclusion",
    "href": "teaching/glm/slides/logistic_model.html#conclusion",
    "title": "The Logistic Model",
    "section": "Conclusion",
    "text": "Conclusion\n\nThus, for all \\(u \\neq 0\\),\n\n\\[u^T \\frac{\\partial^2 L}{\\partial \\beta^2} u &lt; 0\\]\n\nThe Hessian matrix is negative definite and therefore \\(L\\) is strictly concave,\nThe MLE is unique"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#about-mle-existence",
    "href": "teaching/glm/slides/logistic_model.html#about-mle-existence",
    "title": "The Logistic Model",
    "section": "About MLE Existence",
    "text": "About MLE Existence\n\nAlthough \\(L\\) is strictly concave, its maximum can occur at infinity (think of the \\(\\ln\\) function), in which case \\(\\hat{\\beta}\\) does not exist.\n\n\nThis occurs if there is non-overlap, i.e., separation by a hyperplane of the \\(x_i\\) for which \\(y_i = 0\\) and those for which \\(y_i = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#non-overlap-situation",
    "href": "teaching/glm/slides/logistic_model.html#non-overlap-situation",
    "title": "The Logistic Model",
    "section": "Non-Overlap Situation",
    "text": "Non-Overlap Situation\n\nMathematically, there is non-overlap if there exists \\(\\alpha \\in \\mathbb{R}^p\\) such that\n\\[\\begin{cases}\n\\text{for all } i \\text{ such that } y_i = 0, & \\alpha^T x_i \\geq 0 \\\\\n\\text{for all } i \\text{ such that } y_i = 1, & \\alpha^T x_i \\leq 0\n\\end{cases}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#illustration-of-non-overlap-situation",
    "href": "teaching/glm/slides/logistic_model.html#illustration-of-non-overlap-situation",
    "title": "The Logistic Model",
    "section": "Illustration of Non-Overlap Situation",
    "text": "Illustration of Non-Overlap Situation"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#non-overlap-and-existence",
    "href": "teaching/glm/slides/logistic_model.html#non-overlap-and-existence",
    "title": "The Logistic Model",
    "section": "Non-Overlap and Existence",
    "text": "Non-Overlap and Existence\n\n\n\n\nProposition (admitted)\n\n\nIn case of non-overlap, the estimator \\(\\hat{\\beta}\\) does not exist, in the sense that \\(L(\\beta)\\) is maximal when \\(\\|\\beta\\| \\to \\infty\\) (in one or several directions).\n\n\n\n\n\nFor all \\(x\\), \\(\\hat{p}(x) = \\in \\{0,1\\}\\), depending on the position of \\(x\\) relative to the separating hyperplane.\n\n\nNevertheless, there is a “dead zone” in the middle of the \\(2\\) point clouds, because the separating hyperplane is not necessarily unique."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#beyond-the-dead-zone",
    "href": "teaching/glm/slides/logistic_model.html#beyond-the-dead-zone",
    "title": "The Logistic Model",
    "section": "Beyond the Dead Zone",
    "text": "Beyond the Dead Zone\n\nBeyond this dead zone, classification is very simple (\\(0\\) or \\(1\\)).\n\n\nBut no interpretation of the model is possible (the OR are worth \\(0\\) or \\(+\\infty\\))."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#existence-and-uniqueness-of-mle",
    "href": "teaching/glm/slides/logistic_model.html#existence-and-uniqueness-of-mle",
    "title": "The Logistic Model",
    "section": "Existence and Uniqueness of MLE",
    "text": "Existence and Uniqueness of MLE\n\nWe say there is overlap when no hyperplane can separate the red points from the blue points.\n\n\n\n\n\nProposition (admitted)\n\n\nIf \\(\\text{rank}(X) = p\\) and there is overlap, then the MLE exists and is unique.\n\n\n\nUnder these conditions, we can therefore search for the MLE using the Newton-Raphson algorithm.\n\nthe maximum exists,\nthe function to optimize is strictly concave and there is therefore no local maximum, only a global maximum."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#fisher-information-recall",
    "href": "teaching/glm/slides/logistic_model.html#fisher-information-recall",
    "title": "The Logistic Model",
    "section": "Fisher Information (Recall)",
    "text": "Fisher Information (Recall)\n\nLet \\(X\\) be the design matrix (whose rows are the vectors \\(x_i\\)).\n\n\nLet \\(J_n(\\beta)\\) be the Fisher information matrix:\n\n\\[J_n(\\beta) = -E\\left[\\frac{\\partial^2 L}{\\partial \\beta^2}(\\beta) \\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#asymptotic-efficiency",
    "href": "teaching/glm/slides/logistic_model.html#asymptotic-efficiency",
    "title": "The Logistic Model",
    "section": "Asymptotic Efficiency",
    "text": "Asymptotic Efficiency\n\n\n\nProposition (admitted)\n\n\nIn the logistic regression model, if\n\nthe distribution of the regressors \\((X_1, \\ldots, X_p)\\) has compact support,\n\\(\\text{rank}(X) = p\\),\nthe smallest eigenvalue of \\(X^T X\\) tends to infinity with \\(n\\),\n\nthen\n\nthe maximum likelihood estimator \\(\\hat{\\beta}\\) is consistent;\n\\(J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\)\n\nwhere \\(I_p\\) is the identity matrix of size \\(p\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#comments",
    "href": "teaching/glm/slides/logistic_model.html#comments",
    "title": "The Logistic Model",
    "section": "Comments",
    "text": "Comments\n\nUnder these conditions, the MLE therefore exists for sufficiently large \\(n\\). In fact, there is necessarily overlap when \\(n\\) is large (cf board).\n\n\nIt is asymptotically efficient (= minimal asymptotic variance)\n\n\nThe Fisher information matrix \\(J_n(\\beta)\\) can be calculated\n\n\nWe will be able to rely on asymptotic normality to perform tests and construct confidence intervals"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#comparison-with-linear-regression",
    "href": "teaching/glm/slides/logistic_model.html#comparison-with-linear-regression",
    "title": "The Logistic Model",
    "section": "Comparison with Linear Regression",
    "text": "Comparison with Linear Regression\n\nThe formula for \\(\\hat{\\beta}\\) is explicit: \\(\\hat{\\beta} = (X^T X)^{-1} X^T Y\\);\n\n\nIts expectation and variance are explicit;\n\n\nIn the Gaussian model (\\(Y|X\\) Gaussian), the distribution of \\(\\hat{\\beta}\\) is explicit, which allows constructing exact tests (Student, Fisher).\n\n\nIf the model is not Gaussian, these tests are valid asymptotically."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#comparison-with-linear-regression-1",
    "href": "teaching/glm/slides/logistic_model.html#comparison-with-linear-regression-1",
    "title": "The Logistic Model",
    "section": "Comparison with Linear Regression",
    "text": "Comparison with Linear Regression\n\nNo explicit formula for \\(\\hat{\\beta}\\), the solution is obtained numerically;\n\n\nWe know neither the bias nor the variance of \\(\\hat{\\beta}\\);\n\n\nThe distribution of \\(Y|X\\) is simple (a Bernoulli), but we don’t know the distribution of \\(\\hat{\\beta}\\).\n\n\nWe only know its asymptotic distribution.\n\n\nWe’ll do asymptotic tests!"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#asymptotic-framework",
    "href": "teaching/glm/slides/logistic_model.html#asymptotic-framework",
    "title": "The Logistic Model",
    "section": "Asymptotic Framework",
    "text": "Asymptotic Framework\n\nUnder “good conditions”,\n\n\\[J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]\n\nwhere \\(J_n(\\beta)\\) is the Fisher information matrix.\n\n\nTo build asymptotic tests, we need to understand \\(J_n(\\beta)\\) and be able to estimate it."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#computation-of-j_nbeta",
    "href": "teaching/glm/slides/logistic_model.html#computation-of-j_nbeta",
    "title": "The Logistic Model",
    "section": "Computation of \\(J_n(\\beta)\\)",
    "text": "Computation of \\(J_n(\\beta)\\)\n\n\n\\[J_n(\\beta) = -E\\left[\\frac{\\partial^2 L}{\\partial \\beta^2}(\\beta) \\bigg| X\\right]\\]\n\nwhere \\(L\\) is the log-likelihood of the model.\n\n\nFrom the proof of existence of MLE,\n\n\\[J_n(\\beta) = \\sum_{i=1}^n p_\\beta(x_i)(1 - p_\\beta(x_i)) x_i x_i^T\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#equivalent-form",
    "href": "teaching/glm/slides/logistic_model.html#equivalent-form",
    "title": "The Logistic Model",
    "section": "Equivalent Form",
    "text": "Equivalent Form\n\nWe can write equivalently\n\n\\[J_n(\\beta) = X^T W_\\beta X\\]\n\nwhere \\(X\\) is the design matrix and \\(W_\\beta\\) is the diagonal matrix\n\n\n\n\n\\[W_\\beta = \\begin{pmatrix}\np_\\beta(x_1)(1 - p_\\beta(x_1)) & 0 & \\cdots & 0 \\\\\n0 & p_\\beta(x_2)(1 - p_\\beta(x_2)) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & p_\\beta(x_n)(1 - p_\\beta(x_n))\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#estimation",
    "href": "teaching/glm/slides/logistic_model.html#estimation",
    "title": "The Logistic Model",
    "section": "Estimation",
    "text": "Estimation\n\nTo estimate \\(J_n(\\beta)\\), we simply replace \\(\\beta\\) by the MLE \\(\\hat{\\beta}\\)\n\n\nUnder the same regularity assumptions, we can show that\n\n\\[J_n(\\hat{\\beta})^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#estimated-variance-of-hat-beta_j",
    "href": "teaching/glm/slides/logistic_model.html#estimated-variance-of-hat-beta_j",
    "title": "The Logistic Model",
    "section": "Estimated Variance of \\(\\hat \\beta_j\\)",
    "text": "Estimated Variance of \\(\\hat \\beta_j\\)\n\n\n\\[J_n(\\hat{\\beta})^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_p)\\]\n\n\n\nDenoting \\(\\hat{\\sigma}_j^2\\) the \\(j\\)-th diagonal element of \\(J_n(\\hat{\\beta})^{-1}\\), we obtain (admitted):\n\n\\[\\frac{\\hat{\\beta}_j - \\beta_j}{\\hat{\\sigma}_j} \\xrightarrow{L} N(0, 1)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#confidence-interval",
    "href": "teaching/glm/slides/logistic_model.html#confidence-interval",
    "title": "The Logistic Model",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\nWe deduce a confidence interval for \\(\\beta_j\\), at asymptotic confidence level \\(1 - \\alpha\\):\n\n\\[\\text{CI}_{1-\\alpha}(\\beta_j) = \\left[\\hat{\\beta}_j - q_{1-\\alpha/2}\\hat{\\sigma}_j \\,;\\, \\hat{\\beta}_j + q_{1-\\alpha/2}\\hat{\\sigma}_j\\right]\\]\n\nwhere \\(q(1 - \\alpha/2)\\) is the quantile of order \\(1 - \\alpha/2\\) of the \\(N(0, 1)\\) distribution.\n\n\nWe verify that we have \\(\\P(\\beta_j \\in \\text{CI}_{1-\\alpha}(\\beta_j)) \\to 1 - \\alpha\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#significance-test-for-one-coefficient",
    "href": "teaching/glm/slides/logistic_model.html#significance-test-for-one-coefficient",
    "title": "The Logistic Model",
    "section": "Significance Test for One Coefficient",
    "text": "Significance Test for One Coefficient\n\nWe want to test \\(H_0: \\beta_j = 0\\) against \\(H_1: \\beta_j \\neq 0\\).\nUnder \\(H_0\\) we know that \\(\\frac{\\hat{\\beta}_j}{\\hat{\\sigma}_j} \\xrightarrow{L} N(0, 1)\\)\nWe deduce a critical region at asymptotic level \\(\\alpha\\):\n\n\\[\\mathcal R_\\alpha = \\left\\{\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha/2}\\right\\}\\]\n\n\n\nIndeed \\(P_{H_0}(\\mathcal R_\\alpha) \\to \\alpha\\).\nThis test is called the Wald test. (As any other test that relies on asymptotic normality)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#p-value",
    "href": "teaching/glm/slides/logistic_model.html#p-value",
    "title": "The Logistic Model",
    "section": "P-value",
    "text": "P-value\n\nDenoting \\(\\Phi\\) the cdf of the \\(\\mathcal N(0, 1)\\) distribution, the p-value of the test equals\n\n\\[p\\text{-value} = 2\\left(1 - \\Phi\\left(\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j}\\right)\\right)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-in-r",
    "href": "teaching/glm/slides/logistic_model.html#example-in-r",
    "title": "The Logistic Model",
    "section": "Example in R",
    "text": "Example in R\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept)\n-3.9564361\n0.3155529\n-12.538\n&lt; 2e-16\n***\n\n\nAGE\n0.0640837\n0.0123960\n5.170\n2.34e-07\n***\n\n\nI(AGE^2)\n-0.0006758\n0.0001260\n-5.364\n8.14e-08\n***\n\n\nDBP\n0.0121546\n0.0033775\n3.599\n0.00032\n***\n\n\nSEXEFEMME\n0.5155651\n0.0776229\n6.642\n3.10e-11\n***\n\n\nWALK1\n-0.4042257\n0.0913195\n-4.426\n9.58e-06\n***\n\n\nACTIV1\n-0.6573558\n0.1150226\n-5.715\n1.10e-08\n***\n\n\n\n\n\n\nEach columns corresponds resp. to\n\n\\(\\hat \\beta_j\\)\n\\(\\hat \\sigma_j\\)\n\\(\\hat \\beta_j/\\hat \\sigma_j\\) (z-score)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#estimation-of-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#estimation-of-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Estimation of an Odds-Ratio",
    "text": "Estimation of an Odds-Ratio\n\nWe consider two individuals \\(1\\) and \\(2\\) who differ only by their regressor \\(j\\). Then,\n\n\\[OR(x_1^{(j)}, x_2^{(j)}) = e^{\\beta_j(x_1^{(j)} - x_2^{(j)})}\\]\n\n\n\nDo we have \\(OR(x_1^{(j)}, x_2^{(j)})\\approx 1\\)?\n\n\nThe estimation of \\(OR(x_1^{(j)}, x_2^{(j)})\\) is simply\n\n\\[\\widehat{OR}(x_1^{(j)}, x_2^{(j)}) = e^{\\hat{\\beta}_j(x_1^{(j)} - x_2^{(j)})}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#important-example",
    "href": "teaching/glm/slides/logistic_model.html#important-example",
    "title": "The Logistic Model",
    "section": "Important Example",
    "text": "Important Example\n\nIf regressor \\(j\\) is binary with \\(x_1^{(j)} = 1\\) and \\(x_2^{(j)} = 0\\), then\n\n\\[\\widehat{OR}(x_1^{(j)}, x_2^{(j)}) = e^{\\hat{\\beta}_j}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#asymptotic-ci-for-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#asymptotic-ci-for-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Asymptotic CI for an Odds-Ratio",
    "text": "Asymptotic CI for an Odds-Ratio\n\nWe have seen that an asymptotic CI at confidence level \\(1 - \\alpha\\) for \\({\\beta}_j\\) is\n\n\\[\\text{CI}_{1-\\alpha}(\\beta_j) = \\left[\\hat{\\beta}_j - q_{1-\\alpha/2}\\hat{\\sigma}_j \\,;\\, \\hat{\\beta}_j + q_{1-\\alpha/2}\\hat{\\sigma}_j\\right]= [l,r]\\]\n\n\n\nIf \\(x_1^{(j)} &gt; x_2^{(j)}\\), an asymptotic CI for \\(OR(x_1^{(j)}, x_2^{(j)})= e^{\\beta_j(x^{(j)}_1 - x^{(j)}_2)}\\) at level \\(1 - \\alpha\\) is\n\n\\[\\text{CI}_{1-\\alpha}(OR(x_1^{(j)}, x_2^{(j)})) = \\left[e^{l(x_1^{(j)} - x_2^{(j)})}, e^{r(x_1^{(j)} - x_2^{(j)})}\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#significance-test-for-an-odds-ratio",
    "href": "teaching/glm/slides/logistic_model.html#significance-test-for-an-odds-ratio",
    "title": "The Logistic Model",
    "section": "Significance Test for an Odds-Ratio",
    "text": "Significance Test for an Odds-Ratio\n\nWe generally want to compare \\(OR(x_1^{(j)}, x_2^{(j)})\\) to \\(1\\).\n\n\\[OR(x_1^{(j)}, x_2^{(j)}) = 1 \\Leftrightarrow e^{\\beta_j(x_1^{(j)} - x_2^{(j)})} = 1 \\Leftrightarrow \\beta_j = 0\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#two-sided-test",
    "href": "teaching/glm/slides/logistic_model.html#two-sided-test",
    "title": "The Logistic Model",
    "section": "Two-Sided Test",
    "text": "Two-Sided Test\n\n\n\\(H_0: OR(x_1^{(j)}, x_2^{(j)}) = 1\\) VS \\(H_1: OR(x_1^{(j)}, x_2^{(j)}) &gt; 1\\)\n\n\n\namounts to testing \\(H_0: \\beta_j = 0\\) against \\(H_1: \\beta_j \\neq 0\\). The Rejection region at level \\(\\alpha\\) is\n\n\n\n\\[\\mathcal R_\\alpha = \\left\\{\\frac{|\\hat{\\beta}_j|}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha/2}\\right\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#one-sided-tests",
    "href": "teaching/glm/slides/logistic_model.html#one-sided-tests",
    "title": "The Logistic Model",
    "section": "One-Sided Tests",
    "text": "One-Sided Tests\n\nNevertheless, for ORs, we often prefer one-sided tests.\n\n\n\n\\(H_0: OR(x_1^{(j)}, x_2^{(j)}) = 1\\) VS \\(H_1: OR(x_1^{(j)}, x_2^{(j)}) &gt; 1\\)\n\n\n\nIf \\(x_1^{(j)} &gt; x_2^{(j)}\\). Since \\(OR(x_1^{(j)}, x_2^{(j)}) \\leq 1 \\Leftrightarrow \\beta_j \\leq 0\\) rejection region at level \\(\\alpha\\) is\n\n\\[\\mathcal R_{\\alpha}=\\left\\{\\frac{\\hat{\\beta}_j}{\\hat{\\sigma}_j} &gt; q_{1-\\alpha}\\right\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#saturated-model",
    "href": "teaching/glm/slides/logistic_model.html#saturated-model",
    "title": "The Logistic Model",
    "section": "Saturated Model",
    "text": "Saturated Model\n\nSuppose we have \\(n\\) observations \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\) (categorical)\n\n\nHere \\(X_k\\) can represent the vector \\(X_k = (X^{(1)}_k, \\dots, X^{(p)}_k)^T\\).\n\n\nAssume that indivudal are iid, with \\(\\mathbb P(Y=1|X_k=x) = p(x)\\).\n\n\nHow do we estimate \\(p(x)\\)?"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#saturated-estimator",
    "href": "teaching/glm/slides/logistic_model.html#saturated-estimator",
    "title": "The Logistic Model",
    "section": "Saturated Estimator",
    "text": "Saturated Estimator\n\nSuppose we have \\(n\\) observations \\((Y_1, dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\)\n\n\n\\(n(x) = |\\{k:~ X_k = x\\}|\\) (number of indiv. \\(k\\) s.t. \\(X_k=x\\))\n\n\n\\(n_1(x) = |\\{k:~ X_k = x ~~\\text{and}~~Y_k=1\\}|\\)\n\n\nThe saturated model is one that estimates \\(p(x)\\), for an observed \\(x\\), by\n\n\\[\\hat{p}_{\\text{sat}}(x) = \\frac{n_1(x)}{n(x)}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#remark",
    "href": "teaching/glm/slides/logistic_model.html#remark",
    "title": "The Logistic Model",
    "section": "Remark",
    "text": "Remark\n\n\n\\[\\hat{p}_{\\text{sat}}(x) = \\frac{n_1(x)}{n(x)}\\]\n\nIf all observations are distinct, i.e., each observed \\(x\\) is only for a single individual, then for an observed \\(x\\):\n\n\n\\(n(x) = 1\\), \\(n_1(x) \\in \\{0,1\\}\\), and \\(\\hat{p}_{\\text{sat}}(x) = 0\\) or \\(1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#remarks-1",
    "href": "teaching/glm/slides/logistic_model.html#remarks-1",
    "title": "The Logistic Model",
    "section": "Remarks",
    "text": "Remarks\n\nThe saturated model is the simplest model to imagine.\n\n\nIt fits the data perfectly.\n\n\nHowever, it has no explanatory power (effect of regressors on \\(Y\\)?).\n\n\nAnd it says nothing about \\(p(x)\\) if \\(x\\) is not observed.\n\n\nIt will serves as a reference for fit"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#likelihood-of-saturated-estimator",
    "href": "teaching/glm/slides/logistic_model.html#likelihood-of-saturated-estimator",
    "title": "The Logistic Model",
    "section": "Likelihood of Saturated Estimator",
    "text": "Likelihood of Saturated Estimator\n\nFor the saturated model with probabilities \\(p(x)\\), the Log-likelihood is:\n\n\n\\[L(y_1, \\ldots, y_n|x_1, \\ldots, x_n) = \\sum_{i=1}^n y_i \\ln(p(x_i)) + (1 - y_i) \\ln(1 - p(x_i))\\]\n\n\n\n\nThe saturated model minimizes this likelihood, and we denote\n\n\n\\[L_{\\text{sat}} = \\sum_{i=1}^n y_i \\ln(\\hat p_{\\text{sat}}(x_i)) + (1 - y_i) \\ln(1 - \\hat p_{\\text{sat}}(x_i))\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#case-1-distinct-observations",
    "href": "teaching/glm/slides/logistic_model.html#case-1-distinct-observations",
    "title": "The Logistic Model",
    "section": "Case 1: Distinct Observations",
    "text": "Case 1: Distinct Observations\n\nIf all observations \\(x_i\\) are distinct, we have \\(\\hat{p}_{\\text{sat}}(x_i) = y_i\\) with \\(y_i \\in \\{0, 1\\}\\). We thus have\n\n\\[L_{\\text{sat}} = 0\\]\n\n\n\nThe saturated estimator has highest possible log-likelihood: it fits the data perfectly (too well)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#case-2-non-distinct-observations",
    "href": "teaching/glm/slides/logistic_model.html#case-2-non-distinct-observations",
    "title": "The Logistic Model",
    "section": "Case 2: Non-Distinct Observations",
    "text": "Case 2: Non-Distinct Observations\n\nIf the observations \\(x_i\\) are not distinct, we obtain\n\n\n\\[L_{\\text{sat}} = \\sum_x \\left[n_1(x) \\ln\\left(\\frac{n_1(x)}{n(x)}\\right) + (n(x) - n_1(x)) \\ln\\left(1 - \\frac{n_1(x)}{n(x)}\\right)\\right]\\]\n\n\nwhere the sum runs over the set of values \\(x\\) taken by the \\(x_i\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#deviance-as-a-substitute-for-ssr",
    "href": "teaching/glm/slides/logistic_model.html#deviance-as-a-substitute-for-ssr",
    "title": "The Logistic Model",
    "section": "Deviance as a Substitute for SSR",
    "text": "Deviance as a Substitute for SSR"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#definition",
    "href": "teaching/glm/slides/logistic_model.html#definition",
    "title": "The Logistic Model",
    "section": "Definition",
    "text": "Definition\n\nThe deviance of a model measures how much this model deviates from the saturated model (the ideal model in terms of likelihood).\n\n\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\]\n\nwhere \\(L_{\\text{mod}}\\) denotes the log-likelihood for the model parameters.\n\n\nWe always have \\(D \\geq 0\\).\n\n\nIf all observations are distinct, \\(L_{\\text{sat}} = 0\\) therefore \\(D = -2L_{\\text{mod}}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#role-of-deviance-and-computation-in-r",
    "href": "teaching/glm/slides/logistic_model.html#role-of-deviance-and-computation-in-r",
    "title": "The Logistic Model",
    "section": "Role of Deviance and Computation in R",
    "text": "Role of Deviance and Computation in R\n\n\n\\[D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\]\n\n\n\nDeviance plays the role of the SSR of a linear model: the higher the deviance, the less well the model is fitted to the data.\n\n\nIn R, The returned deviance is \\(-2L_{\\text{mod}}\\): the term \\(L_{\\text{sat}}\\) is therefore omitted."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#linear-constraint-test",
    "href": "teaching/glm/slides/logistic_model.html#linear-constraint-test",
    "title": "The Logistic Model",
    "section": "Linear Constraint Test",
    "text": "Linear Constraint Test\n\nAs in linear regression, we would like to test \\(H_0: R\\beta = 0\\) VS \\(H_1: R\\beta \\neq 0\\)\nwhere \\(R\\) is a constraint matrix of size \\((q, p)\\) of full rank.\n\n\nFor recall, depending on the choice of \\(R\\) this allows:\n\ntesting the minimum: is there at least one relevant regressor?\ncomparing nested models\nexamining the collective significance of a family of regressors"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#linear-constraint-testing-test-procedures",
    "href": "teaching/glm/slides/logistic_model.html#linear-constraint-testing-test-procedures",
    "title": "The Logistic Model",
    "section": "Linear Constraint Testing: Test Procedures",
    "text": "Linear Constraint Testing: Test Procedures"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#available-procedures",
    "href": "teaching/glm/slides/logistic_model.html#available-procedures",
    "title": "The Logistic Model",
    "section": "Available Procedures",
    "text": "Available Procedures\n\nIn GLM, several test procedures address the problem.\nThe Wald test, based on the asymptotic normality of \\(\\hat{\\beta}\\), which generalizes the one seen for testing \\(\\beta_j = 0\\) against \\(\\beta_j \\neq 0\\).\n\n\nThe likelihood ratio test, called in this context the deviance test.\n\n\nThe score test, based on the behavior of the gradient of the log-likelihood at the critical point.\n\n\nThe most used is the deviance test."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#the-deviance-test-or-likelihood-ratio-test",
    "href": "teaching/glm/slides/logistic_model.html#the-deviance-test-or-likelihood-ratio-test",
    "title": "The Logistic Model",
    "section": "The Deviance Test (or Likelihood Ratio Test)",
    "text": "The Deviance Test (or Likelihood Ratio Test)\n\nTo test \\(H_0: R\\beta = 0\\) against \\(H_1: R\\beta \\neq 0\\), the principle of the test is as follows:\n\n\nWe calculate the MLE in each model to obtain \\(\\hat{\\beta}\\) in the complete model and \\(\\hat{\\beta}_{H_0}\\) in the constrained model.\n\n\nLogic: If \\(H_0\\) is true, the constrained model should be as “likely” as the complete model, so \\(L(\\hat{\\beta})\\) and \\(L(\\hat{\\beta}_{H_0})\\) should be similar."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#deviance-test-statistic",
    "href": "teaching/glm/slides/logistic_model.html#deviance-test-statistic",
    "title": "The Logistic Model",
    "section": "Deviance Test Statistic",
    "text": "Deviance Test Statistic\n\nThe test statistic is the difference of deviances:\n\n\\[D_{H_0} - D_{H_1} = 2\\left(L(\\hat{\\beta}) - L(\\hat{\\beta}_{H_0})\\right)\\]\n\n\n\nUnder \\(H_0\\), denoting \\(q\\) the number of constraints, we have the convergence (admitted):\n\n\\[D_{H_0} - D_{H_1} = 2\\left(L(\\hat{\\beta}) - L(\\hat{\\beta}_{H_0})\\right) \\xrightarrow{L} \\chi^2_q\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#rejection-region-and-p-value",
    "href": "teaching/glm/slides/logistic_model.html#rejection-region-and-p-value",
    "title": "The Logistic Model",
    "section": "Rejection Region and P-value",
    "text": "Rejection Region and P-value\n\nThe asymp. rejection region at asymptotic level \\(\\alpha\\) is therefore\n\n\\[\\mathcal R_\\alpha = \\{D_{H_0} - D_{H_1} &gt; \\chi^2_{q,1-\\alpha}\\}\\]\n\nwhere \\(\\chi^2_{q,1-\\alpha}\\): quantile \\(1 - \\alpha\\) of a \\(\\chi^2_q\\) distribution.\n\n\nThe p-value equals\n\n\\[p\\text{-value} = 1 - F(D_{H_0} - D_{H_1})\\]\n\nwhere \\(F\\) is the cdf of a \\(\\chi^2\\) distribution."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#special-case-1-significance-test",
    "href": "teaching/glm/slides/logistic_model.html#special-case-1-significance-test",
    "title": "The Logistic Model",
    "section": "Special Case 1: Significance Test",
    "text": "Special Case 1: Significance Test\n\nWe want to test if a model (having a constant) is significant\n\n\nWe therefore test \\(H_0\\): all its coefficients are zero except the constant. This corresponds to the special case\n\n\\(R = [0 | I_{p-1}]\\).\n\n\n\nWe compare the deviance of the model to the null deviance \\(D_0\\), corresponding to a model that contains only the constant."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#test-statistic",
    "href": "teaching/glm/slides/logistic_model.html#test-statistic",
    "title": "The Logistic Model",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nThe test statistic is \\(D_0 - D\\). Under \\(H_0\\), when \\(n \\to \\infty\\):\n\n\\[D_0 - D \\sim \\chi^2_{p-1}\\]\n\n\n\nThe model is therefore significant (relative to the null model) if the sample is in the critical region of asymptotic level \\(\\alpha\\):\n\n\\[\\mathcal R_\\alpha = \\{D_0 - D &gt; \\chi^2_{p-1,1-\\alpha/2}\\}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#special-case-2-nested-models",
    "href": "teaching/glm/slides/logistic_model.html#special-case-2-nested-models",
    "title": "The Logistic Model",
    "section": "Special Case 2: Nested Models",
    "text": "Special Case 2: Nested Models\n\nSuppose that model \\(1\\) (with deviance \\(D_1\\)) is a sub-model of model \\(2\\) (with deviance \\(D_2\\))\n\n\nModel \\(1\\) is therefore obtained from model \\(2\\), with parameter \\(\\beta\\), via a constraint of the type \\(R\\beta = 0\\) where \\(R\\) is a \\((q, p)\\) matrix.\n\n\nUnder \\(H_0: R\\beta = 0\\), we have asymptotically \\(D_1 - D_2 \\sim \\chi^2_q\\)\n\n\nHence the asymptotic test: .\n\n\\(\\mathcal R_\\alpha = \\{D_1 - D_2 &gt; \\chi^2_{q,1 - \\alpha}\\}\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#aic-and-bic-criteria",
    "href": "teaching/glm/slides/logistic_model.html#aic-and-bic-criteria",
    "title": "The Logistic Model",
    "section": "AIC and BIC Criteria",
    "text": "AIC and BIC Criteria\n\nThe AIC and BIC criteria are defined similarly to linear regression, i.e.\n\n\\(\\text{AIC} = -2L_{\\text{mod}} + 2p\\)\n\n\n\\(\\text{BIC} = -2L_{\\text{mod}} + \\ln(n)p\\)\n\nwhere \\(L_{\\text{mod}}\\) is the log-likelihood of the estimated model."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#aic-and-bic-criteria-1",
    "href": "teaching/glm/slides/logistic_model.html#aic-and-bic-criteria-1",
    "title": "The Logistic Model",
    "section": "AIC and BIC Criteria",
    "text": "AIC and BIC Criteria\n\nIf we ignore saturated likelihood and set \\(L_{\\text{sat}}=0\\),\n\n\\(\\text{AIC} = D + 2p\\)\n\n\n\\(\\text{BIC} = D + \\ln(n)p\\)\n\nIn practice, we choose the model having the minimal AIC or BIC\n\n\nAs in linear regression, we can use automatic selection procedures (backward, forward, etc)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-obesity-study-in-the-us",
    "href": "teaching/glm/slides/logistic_model.html#example-obesity-study-in-the-us",
    "title": "The Logistic Model",
    "section": "Example: Obesity Study in the US",
    "text": "Example: Obesity Study in the US\n\nmodel=glm(Y~AGE+DBP+SEXE+ACTIV+WALK+MARITAL, family=binomial)\nsummary(model)\n\n\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4459.5\\)\non \\(5290\\)\n\n\nAIC\n\\(4481.5\\)\n\n\n\n\n\n\n\nThe model deviance is therefore \\(D = 4459.5\\).\nSignificance Test: We compare \\(D\\) to the null deviance \\(D_0 = 4610.8\\): \\(D_0 - D = 151.3\\). The p-value of the test equals \\(1 - F_{10}(151.3) \\approx 0\\) where \\(F_{10}\\) is the cdf of a \\(\\chi^2_{10}\\).\n\n\nThe model is significant."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-significance-test-of-marital",
    "href": "teaching/glm/slides/logistic_model.html#example-significance-test-of-marital",
    "title": "The Logistic Model",
    "section": "Example: Significance Test of MARITAL",
    "text": "Example: Significance Test of MARITAL\n\nmodel=glm(Y~AGE+DBP+SEXE+ACTIV+WALK, family=binomial) # We want to test if `MARITAL` is significant\nsummary(model)\n\n\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4462.7\\)\non \\(5295\\)\n\n\nAIC\n\\(4474.7\\)\n\n\n\n\n\n\n\nThe deviance is now \\(D_2 = 4462.7\\). To compare with the previous model, we calculate: \\(D_2 - D = 3.2\\).\n\n\nThe p-value of the test equals \\(1 - F_5(3.2) \\approx 0.67\\), where \\(F_5\\): cdf of a \\(\\chi^2_5\\).\nWe therefore accept \\(H_0\\): the coefficients related to MARITAL are zero. (Also confirmed with AIC)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#example-significance-test-of-age2",
    "href": "teaching/glm/slides/logistic_model.html#example-significance-test-of-age2",
    "title": "The Logistic Model",
    "section": "Example: Significance Test of AGE\\(~^2\\)",
    "text": "Example: Significance Test of AGE\\(~^2\\)\n\nmodel=glm(Y∼AGE+I(AGE2 )+DBP+SEXE+WALK+ACTIV, family=binomial) # We add AGE^2\nsummary(model)\n\n\n\n\nStatistic\nValue\nDegrees of Freedom\n\n\n\n\nNull deviance\n\\(4610.8\\)\non \\(5300\\)\n\n\nResidual deviance\n\\(4439.5\\)\non \\(5294\\)\n\n\nAIC\n\\(4453.5\\)\n\n\n\n\n\n\n\nThe deviance test with the previous model has p-value \\(1 - F_1(4462.7 - 4439.5) = 1 - F_1(23.2) \\approx 10^{-6}\\)\n\n\nThis model is therefore preferable, (confirmed with AIC).\n\n\nHowever, we cannot compare this model with the first one by deviance test because they are not nested.\n\n\nWe can however compare their AIC: this model is preferable."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#setup-for-prevision",
    "href": "teaching/glm/slides/logistic_model.html#setup-for-prevision",
    "title": "The Logistic Model",
    "section": "Setup for Prevision",
    "text": "Setup for Prevision\n\nSuppose we are interested in a new individual for whom\n\nwe know their characteristics \\(x \\in \\mathbb{R}^p\\),\nwe do not know their \\(Y\\).\n\n\n\nWe want to predict \\(Y\\) for this new individual."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#probability-estimation",
    "href": "teaching/glm/slides/logistic_model.html#probability-estimation",
    "title": "The Logistic Model",
    "section": "Probability Estimation",
    "text": "Probability Estimation\n\nIf we have fitted a logistic regression model, we can estimate\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nby\n\n\\[p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta}) = \\frac{e^{x^T \\hat{\\beta}}}{1 + e^{x^T \\hat{\\beta}}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#what-we-will-see",
    "href": "teaching/glm/slides/logistic_model.html#what-we-will-see",
    "title": "The Logistic Model",
    "section": "What We Will See",
    "text": "What We Will See\n\nWe will see:\n\nhow to construct a confidence interval around this estimation;\nhow to exploit this estimation to classify the individual in category \\(Y = 0\\) or \\(Y = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#asymptotic-distribution",
    "href": "teaching/glm/slides/logistic_model.html#asymptotic-distribution",
    "title": "The Logistic Model",
    "section": "Asymptotic Distribution",
    "text": "Asymptotic Distribution\n\nWe know that when \\(n \\to \\infty\\):\n\n\\[\\hat{\\beta} \\sim N(\\beta, (X^T W_{\\hat{\\beta}} X)^{-1})\\]\n\nHence, when \\(n \\to \\infty\\), for any \\(x \\in \\mathbb R^p\\),\n\n\\[x^T \\hat{\\beta} \\sim N(x^T \\beta, x^T (X^T W_{\\hat{\\beta}} X)^{-1} x)\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#ci-for-linear-predictor",
    "href": "teaching/glm/slides/logistic_model.html#ci-for-linear-predictor",
    "title": "The Logistic Model",
    "section": "CI for Linear Predictor",
    "text": "CI for Linear Predictor\n\nWe deduce an asymptotic CI for \\(x^T \\beta\\)\n\n\n\\[\\text{CI}_{1-\\alpha}(x^T \\beta) = \\left[x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right]= [l,r]\\]\n\n\n\n\nSince \\(p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta})\\), applying the increasing function \\(\\text{logit}^{-1}\\), we get\n\n\\[\\text{CI}_{1-\\alpha}(p_\\beta(x)) = \\left[\\text{logit}^{-1}(l), \\text{logit}^{-1}(r)\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#setting-and-objective",
    "href": "teaching/glm/slides/logistic_model.html#setting-and-objective",
    "title": "The Logistic Model",
    "section": "Setting and Objective",
    "text": "Setting and Objective\n\nSuppose we are interested in a new individual for whom\n\nwe know their characteristics \\(x \\in \\mathbb{R}^p\\),\nwe do not know their \\(Y\\).\n\n\n\nWe want to predict \\(Y\\) for this new individual."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#recall-in-the-logit-model",
    "href": "teaching/glm/slides/logistic_model.html#recall-in-the-logit-model",
    "title": "The Logistic Model",
    "section": "Recall in the Logit Model",
    "text": "Recall in the Logit Model\n\nIf we have fitted a logistic regression model, we can estimate\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nby\n\n\n\n\\[p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta}) = \\frac{e^{x^T \\hat{\\beta}}}{1 + e^{x^T \\hat{\\beta}}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#outline-for-prediction",
    "href": "teaching/glm/slides/logistic_model.html#outline-for-prediction",
    "title": "The Logistic Model",
    "section": "Outline for Prediction",
    "text": "Outline for Prediction\n\n\n\\[p_\\beta(x) = P(Y = 1|X = x)\\]\n\nWe will see:\n\nhow to construct a confidence interval around the estimation \\(p_{\\hat{\\beta}}(x)\\);\nhow to exploit this estimation to classify the new individual in category \\(Y = 0\\) or \\(Y = 1\\)."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#ci-asymptotic-distribution-of-p_betax",
    "href": "teaching/glm/slides/logistic_model.html#ci-asymptotic-distribution-of-p_betax",
    "title": "The Logistic Model",
    "section": "CI: Asymptotic Distribution of \\(p_\\beta(x)\\)",
    "text": "CI: Asymptotic Distribution of \\(p_\\beta(x)\\)\n\nWe know that when \\(n \\to \\infty\\):\n\n\\[\\hat{\\beta} \\sim N(\\beta, (X^T W_{\\hat{\\beta}} X)^{-1})\\]\n\n\n\\(X=(X^{(1)}, \\dots, X^{(p)})\\) is the \\(n \\times p\\) design matrix\n\\(W_{\\hat{\\beta}}\\) is the \\(n \\times n\\) diagonal matrix with coefs \\(p_{\\hat \\beta}(x_i)(1-p_{\\hat \\beta}(x_i))\\)"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#ci-for-linear-predictor-1",
    "href": "teaching/glm/slides/logistic_model.html#ci-for-linear-predictor-1",
    "title": "The Logistic Model",
    "section": "CI for Linear Predictor",
    "text": "CI for Linear Predictor\n\nWe deduce that when \\(n \\to +\\infty\\), \\(x^T \\hat{\\beta} \\sim N(x^T \\beta, x^T (X^T W_{\\hat{\\beta}} X)^{-1} x)\\), and the asymptotic CI of \\(x^T\\beta\\):\n\n\n\n\\[\\text{CI}_{1-\\alpha}(x^T \\beta) = \\left[x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#ci-for-probability-p_betax",
    "href": "teaching/glm/slides/logistic_model.html#ci-for-probability-p_betax",
    "title": "The Logistic Model",
    "section": "CI for Probability \\(p_{\\beta}(x)\\)",
    "text": "CI for Probability \\(p_{\\beta}(x)\\)\n\nSince \\(p_{\\hat{\\beta}}(x) = \\text{logit}^{-1}(x^T \\hat{\\beta})\\), we have therefore by application of the increasing function \\(\\text{logit}^{-1}\\), the CI at asymptotic level \\(1 - \\alpha\\):\n\n\n\n\n\\[\\text{CI}_{1-\\alpha}(p_\\beta(x)) = \\left[\\text{logit}^{-1}\\left(x^T \\hat{\\beta} \\pm q_{1-\\alpha/2} \\sqrt{x^T (X^T W_{\\hat{\\beta}} X)^{-1} x}\\right)\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#classification-1",
    "href": "teaching/glm/slides/logistic_model.html#classification-1",
    "title": "The Logistic Model",
    "section": "Classification",
    "text": "Classification\n\nWe have estimated \\(p_\\beta(x) = P(Y = 1|X = x)\\) by \\(p_{\\hat{\\beta}}(x)\\).\n\n\nFor a threshold to choose \\(s \\in [0, 1]\\), we use the rule:\n\n\\[\\begin{cases}\n\\text{if } p_{\\hat{\\beta}}(x) &gt; s, & \\hat{Y} = 1 \\\\\n\\text{if } p_{\\hat{\\beta}}(x) &lt; s, & \\hat{Y} = 0\n\\end{cases}\\]\n\n\n\nThe “natural” choice of threshold is \\(s = 0.5\\) but this choice can be optimized."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#evaluation-of-classification-quality",
    "href": "teaching/glm/slides/logistic_model.html#evaluation-of-classification-quality",
    "title": "The Logistic Model",
    "section": "Evaluation of Classification Quality",
    "text": "Evaluation of Classification Quality\n\nWe proceed by cross-validation:\n\n\nUsing a train sample, predict \\(Y\\) on a test sample and form the confusion matrix.\n\n\n\n\n\n\n\\(Y = 0\\)\n\\(Y = 1\\)\n\n\n\n\n\\(\\hat{Y} = 0\\)\nTN\nFN\n\n\n\\(\\hat{Y} = 1\\)\nFP\nTP\n\n\n\nReading: T: true, F:False, N: Negative, P: Positive.\n\n\nFP: false positives: number of individuals who were classified positive who were actually negative"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#evaluation-of-classification-quality-1",
    "href": "teaching/glm/slides/logistic_model.html#evaluation-of-classification-quality-1",
    "title": "The Logistic Model",
    "section": "Evaluation of Classification Quality",
    "text": "Evaluation of Classification Quality\n\nThe ideal is to have a confusion matrix that is as diagonal as possible.\n\n\nWe generally seek to maximize the following indicators:\n\n\n\n\nThe sensitivity (or recall, or true positive rate) estimates \\(\\P(\\hat{Y} = 1|Y = 1)\\) by\n\n\\[\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\]\n\n\nThe specificity (or selectivity, or true negative rate) estimates \\(\\P(\\hat{Y} = 0|Y = 0)\\)\n\n\\[\\frac{\\text{VN}}{\\text{VN} + \\text{FP}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#other-indicators",
    "href": "teaching/glm/slides/logistic_model.html#other-indicators",
    "title": "The Logistic Model",
    "section": "Other Indicators",
    "text": "Other Indicators\n\nThe precision (or positive predictive value) estimates \\(\\P(Y = 1|\\hat{Y} = 1)\\) by\n\n\\(\\frac{\\text{VP}}{\\text{VP} + \\text{FP}}\\)\n\n\n\nThe \\(F\\)-score is the harmonic mean between sensitivity and precision:\n\n\\[F_1 = 2 \\frac{\\text{precision} \\times \\text{sensitivity}}{\\text{precision} + \\text{sensitivity}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#choice-of-threshold-s",
    "href": "teaching/glm/slides/logistic_model.html#choice-of-threshold-s",
    "title": "The Logistic Model",
    "section": "Choice of Threshold \\(s\\)",
    "text": "Choice of Threshold \\(s\\)\n\nFor each threshold \\(s\\), from a test sample:\n\nwe can form the confusion matrix\ncalculate scores (sensitivity, \\(F\\)-score, etc.)\n\n\n\nWe finally choose the optimal threshold \\(s\\), according to the chosen score."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#choosing-the-score",
    "href": "teaching/glm/slides/logistic_model.html#choosing-the-score",
    "title": "The Logistic Model",
    "section": "Choosing the score",
    "text": "Choosing the score\n\nIt depends on the context of the study\nIt can be much more serious to wrongly predict \\(\\hat{Y} = 0\\) than \\(\\hat{Y} = 1\\)\n\n\n\\(\\hat Y=1\\) (treatment) while the patient is not ill (\\(Y = 0\\))\n\n\n\\(\\hat Y=0\\) (no treatment) while the patient has a serious illness (\\(Y=1\\))"
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#roc-curve",
    "href": "teaching/glm/slides/logistic_model.html#roc-curve",
    "title": "The Logistic Model",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nWe can also plot the ROC curve (TP rate as a function of FP rate for \\(s \\in [0, 1]\\)):\n\n\\[ROC:~~\\frac{TP}{TP+FN} = F\\left(\\frac{FP}{FP+TN}\\right)\\]\n\n\n\nThe AUC (area under the curve) is a quality indicator of the model (\\(0 \\leq \\text{AUC} \\leq 1\\)).\n\n\nOr equivalently, the Gini index: \\(2 \\times \\text{AUC} - 1\\).\n\n\nUse: compare \\(2\\) models by plotting the 2 ROC curves."
  },
  {
    "objectID": "teaching/glm/slides/logistic_model.html#roc-curve-illustration",
    "href": "teaching/glm/slides/logistic_model.html#roc-curve-illustration",
    "title": "The Logistic Model",
    "section": "ROC Curve Illustration",
    "text": "ROC Curve Illustration"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#this-class",
    "href": "teaching/glm/slides/categorical_data.html#this-class",
    "title": "Models for Categorical Data",
    "section": "This Class",
    "text": "This Class\n\nFor each individual \\(i\\), \\(Y_i\\) takes now \\(K\\) values\n\n\nTwo class of models:\n\nNominal: no relation a priori between the \\(K\\) values\nOrdinal: a natural relation exists between these values"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#binary-case",
    "href": "teaching/glm/slides/categorical_data.html#binary-case",
    "title": "Models for Categorical Data",
    "section": "Binary Case",
    "text": "Binary Case\n\nIn the binary case (\\(K = 2\\)), the logistic model assumes that there exists \\(\\beta \\in \\mathbb{R}^p\\) such that:\n\n\n\n\\[\\frac{p^{(1)}(x)}{p^{(0)}(x)} = e^{x^T \\beta}\\]\n\n(because \\(\\frac{p^{(1)}(x)}{p^{(0)}(x)} = \\frac{p^{(1)}(x)}{1-p^{(1)}(x)}\\))\n\n\nCategory “\\(0\\)” can be seen as a reference category."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#general-case",
    "href": "teaching/glm/slides/categorical_data.html#general-case",
    "title": "Models for Categorical Data",
    "section": "General Case",
    "text": "General Case\n\nIn the general case (\\(K\\) arbitrary), the nominal logistic model (or multinomial, or reference category model) similarly assumes for \\(k \\in \\{1, \\ldots, K-1\\}\\):\n\n\\[\\frac{p^{(k)}(x)}{p^{(0)}(x)} = e^{x^T \\beta^{(k)}}\\]\n\n\n\nwhere \\(\\beta^{(k)} \\in \\mathbb{R}^p\\) is the parameter associated with category \\(k\\).\nCategory “\\(0\\)” is the reference category, whose probability is deduced from the others.\n\n\nThere are in total \\((K-1) \\times p\\) unknown parameters."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#probability-formulas",
    "href": "teaching/glm/slides/categorical_data.html#probability-formulas",
    "title": "Models for Categorical Data",
    "section": "Probability Formulas",
    "text": "Probability Formulas\n\nWe deduce that in this model, for all \\(k \\in \\{1, \\ldots, K-1\\}\\),\n\n\\[p^{(k)}(x) = p_\\beta^{(k)}(x) = \\frac{e^{x^T \\beta^{(k)}}}{1 + \\sum_{r=1}^{K-1} e^{x^T \\beta^{(r)}}}\\]\n\n\n\n\n\\[p^{(0)}(x) = p_\\beta^{(0)}(x) = \\frac{1}{1 + \\sum_{r=1}^{K-1} e^{x^T \\beta^{(r)}}}\\]\n\n\n\n(which is consistent with the previous formula by taking \\(\\beta^{(0)} = 0\\))"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#remarks",
    "href": "teaching/glm/slides/categorical_data.html#remarks",
    "title": "Models for Categorical Data",
    "section": "Remarks",
    "text": "Remarks\n\nWe note that each \\(p_\\beta^{(k)}(x)\\) depends on all parameters \\(\\beta = (\\beta^{(1)}, \\ldots, \\beta^{(K-1)})\\) and not only on \\(\\beta^{(k)}\\)\n\n\nHence the notation \\(p_\\beta^{(k)}(x)\\) with the index \\(\\beta\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#odd-ratio-of-yk",
    "href": "teaching/glm/slides/categorical_data.html#odd-ratio-of-yk",
    "title": "Models for Categorical Data",
    "section": "Odd Ratio of \\(Y=k\\)",
    "text": "Odd Ratio of \\(Y=k\\)\n\n\\(\\beta^{(k)} \\in \\mathbb R^p\\) depends on the reference category.\n\n\nWe call “odds” of event \\(Y = k\\), the ratio \\(p_\\beta^{(k)}(x)/p_\\beta^{(0)}(x)\\).\nThe OR of \\(Y = k\\) for two characteristics \\(x_1\\) and \\(x_2\\) is therefore\n\n\\[OR^{(k)}(x_1, x_2) = \\frac{p_\\beta^{(k)}(x_1)/p_\\beta^{(0)}(x_1)}{p_\\beta^{(k)}(x_2)/p_\\beta^{(0)}(x_2)} = e^{(x_1 - x_2)^T \\beta^{(k)}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#interpretation-of-odds-ratio",
    "href": "teaching/glm/slides/categorical_data.html#interpretation-of-odds-ratio",
    "title": "Models for Categorical Data",
    "section": "Interpretation of Odds Ratio",
    "text": "Interpretation of Odds Ratio\n\n\\(OR^{(k)}(x_1, x_2)\\) depends only on \\(\\beta^{(k)}\\), and even, only on \\(\\beta_j^{(k)}\\) if \\(x_1\\) and \\(x_2\\) differ only by regressor \\(X^{(j)}\\).\n\n\nWhile probabilities \\(p_\\beta^{(k)}(x)\\) depend on other \\(\\beta^{(k')}\\), \\(k'\\neq k\\)!!\n\n\nWe find the same interpretation of OR as in logistic regression, except that here the odds is relative to the reference category.\n\n\nIt is therefore important to judiciously choose the reference category for interpretations."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#change-of-reference",
    "href": "teaching/glm/slides/categorical_data.html#change-of-reference",
    "title": "Models for Categorical Data",
    "section": "Change of Reference",
    "text": "Change of Reference\n\nThat said, for two categories \\(k \\neq l\\), the probability ratio\n\n\\[\\frac{p_\\beta^{(k)}(x)}{p_\\beta^{(l)}(x)} = e^{x^T(\\beta^{(k)} - \\beta^{(l)})}\\]\n\ndoes not depend on the chosen reference category.\n\n\nSimilarly, the value of probabilities \\(p_\\beta^{(k)}(x)\\) and their estimation do not depend on the chosen reference category either."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#change-of-reference-justification",
    "href": "teaching/glm/slides/categorical_data.html#change-of-reference-justification",
    "title": "Models for Categorical Data",
    "section": "Change of Reference, Justification",
    "text": "Change of Reference, Justification\n\nIf the reference category is \\(Y = j\\), denoting the associated parameters \\(\\gamma^{(k)}\\), \\(k \\neq j\\), and \\(\\gamma^{(j)} = 0\\),\n\n\nwe have the relation \\(\\gamma^{(k)} = \\beta^{(k)} - \\beta^{(j)}\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#maximum-likelihood-estimation",
    "href": "teaching/glm/slides/categorical_data.html#maximum-likelihood-estimation",
    "title": "Models for Categorical Data",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nFor each individual \\(i\\), \\(Y_i|X_i = x\\) follows a multinomial distribution \\(\\mathrm{Mult}(1,(p_\\beta^{(0)}(x), \\ldots, p_\\beta^{(K-1)}(x)))\\).\n\n\nThe likelihood of a sample \\((Y_1|X_1 = x_1), \\ldots, (Y_n|X_n = x_n)\\) is written:\n\n\\[\\prod_{i=1}^n \\prod_{k=0}^{K-1} \\left(p_\\beta^{(k)}(x_i)\\right)^{\\mathbf{1}\\{Y_i = k\\}}\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#log-likelihood",
    "href": "teaching/glm/slides/categorical_data.html#log-likelihood",
    "title": "Models for Categorical Data",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n\nTherefore the log-likelihood\n\n\\[L = \\sum_{i=1}^n \\sum_{k=0}^{K-1} \\mathbf{1}_{Y_i = k} \\ln\\left(p_\\beta^{(k)}(x_i)\\right)\\]\n\n\n\nIn the case of the nominal logistic model, we deduce (cf board)\n\n\n\\[L = \\sum_{i=1}^n \\left[\\sum_{k=1}^{K-1} x_i^T \\beta^{(k)} \\mathbf{1}_{y_i = k} - \\ln\\left(1 + \\sum_{k=1}^{K-1} e^{x_i^T \\beta^{(k)}}\\right)\\right]\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#maximum-likelihood-estimator-of-beta",
    "href": "teaching/glm/slides/categorical_data.html#maximum-likelihood-estimator-of-beta",
    "title": "Models for Categorical Data",
    "section": "Maximum Likelihood Estimator of \\(\\beta\\)",
    "text": "Maximum Likelihood Estimator of \\(\\beta\\)\n\nBy setting the gradient of \\(L\\) to zero, we obtain that \\(\\hat{\\beta} = (\\hat{\\beta}^{(1)}, \\ldots, \\hat{\\beta}^{(K-1)})\\) must verify, for all \\(k \\in \\{1, \\ldots, K-1\\}\\):\n\n\\[\\sum_{i=1}^n x_i \\mathbf{1}_{y_i = k} = \\sum_{i=1}^n x_i p_{\\hat{\\beta}}^{(k)}(x_i)\\]\n\n\n\\(K-1\\) equations, each with \\(p\\) parameters, i.e. \\((K-1) \\times p\\) equations.\nWe solve it numerically to find \\(\\hat{\\beta}\\) of size \\((K-1) \\times p\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#uniqueness-and-existence",
    "href": "teaching/glm/slides/categorical_data.html#uniqueness-and-existence",
    "title": "Models for Categorical Data",
    "section": "Uniqueness and Existence",
    "text": "Uniqueness and Existence\n\nAs in logistic regression:\nWe can show that \\(L\\) is strictly concave as soon as \\(\\text{rank}(X) = p\\).\n\n\nThis ensures the uniqueness of the MLE (if it exists).\n\n\nExistence is ensured if no category is separated from the others by a hyperplane."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#asymptotic-normality",
    "href": "teaching/glm/slides/categorical_data.html#asymptotic-normality",
    "title": "Models for Categorical Data",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\n\nUnder regularity assumptions similar to the case of logistic regression, we have\n\n\n\n\\[J_n(\\beta)^{1/2}(\\hat{\\beta} - \\beta) \\xrightarrow{L} N(0, I_{(K-1) \\times p})\\]\n\nwhere \\(J_n(\\beta)\\) is the Fisher information matrix and \\(I_{(K-1) \\times p}\\) is the identity matrix of size \\((K-1) \\times p\\).\n\n\n\\(J_n(\\beta)\\), not detailed here, is a matrix of \\((K-1) \\times (K-1)\\) blocks, each having a similar form to the Fisher information matrix of logistic regression."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#inference-tools",
    "href": "teaching/glm/slides/categorical_data.html#inference-tools",
    "title": "Models for Categorical Data",
    "section": "Inference Tools",
    "text": "Inference Tools\n\nThe inference tools are based on the asymptotic distribution of \\(\\hat{\\beta}\\) and are similar to those of logistic regression:\n\n\nThe significance of each coefficient can be tested by an (asymptotic) Wald test.\n\n\nConfidence intervals, for coefficients and OR, are deduced analogously."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#model-selection-criteria",
    "href": "teaching/glm/slides/categorical_data.html#model-selection-criteria",
    "title": "Models for Categorical Data",
    "section": "Model Selection Criteria",
    "text": "Model Selection Criteria\n\nDeviance is defined similarly: \\(D = 2(L_{\\text{sat}} - L_{\\text{mod}})\\).\n\n\nAs we do for logistic regression:\n\nWe can test the significance of the model,\nor compare two nested models.\nUse the AIC, BIC criteria. Since we have \\(p(K-1)\\) parameters:\n\n\n\n\n\n\\(\\text{AIC} = D + 2p(K-1) \\and \\text{BIC} = D + \\ln(n)p(K-1)\\)"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-car-equipment-preference",
    "href": "teaching/glm/slides/categorical_data.html#example-car-equipment-preference",
    "title": "Models for Categorical Data",
    "section": "Example: Car Equipment Preference",
    "text": "Example: Car Equipment Preference\n\nPreference for an equipped car (with air conditioning and power steering), according to age group and gender.\n\n\n\n\n\n\nGender\nAge Category\nNot Important\nImportant\nVery Important\n\n\n\n\nFemale\n18-23\n26\n12\n7\n\n\n\n24-40\n9\n21\n15\n\n\n\n&gt;40\n5\n14\n41\n\n\nMale\n18-23\n40\n17\n8\n\n\n\n24-40\n17\n15\n12\n\n\n\n&gt;40\n8\n15\n18"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#model-setup",
    "href": "teaching/glm/slides/categorical_data.html#model-setup",
    "title": "Models for Categorical Data",
    "section": "Model Setup",
    "text": "Model Setup\n\nWe want to model the variable \\(Y =\\) “importance” (3 categories)\n\n\nThe regressors are gender (2 classes) and age (3 classes).\n\n\nThese are grouped observations: each category gender/age is observed for several individuals"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#r-implementation",
    "href": "teaching/glm/slides/categorical_data.html#r-implementation",
    "title": "Models for Categorical Data",
    "section": "R Implementation",
    "text": "R Implementation\n\nUnder R, we can use the vglm function from the VGAM library:\nvglm(Y ~ age + sexe, family=multinomial) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept):1\n-0.5908\n0.2840\n-2.080\n0.037484\n*\n\n\n(Intercept):2\n-1.0391\n0.3305\n-3.144\n0.001667\n**\n\n\nage2:1\n1.1283\n0.3416\n3.302\n0.000958\n***\n\n\nage2:2\n1.4781\n0.4009\n3.687\n0.000227\n***\n\n\nage3:1\n1.5877\n0.4029\n3.941\n8.12e-05\n***\n\n\nage3:2\n2.9168\n0.4229\n6.897\n5.32e-12\n***\n\n\nsexeM:1\n-0.3881\n0.3005\n-1.292\n0.196510\n\n\n\nsexeM:2\n-0.8130\n0.3210\n-2.532\n0.011326\n*\n\n\n\n\nNames of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#r-output-remarks",
    "href": "teaching/glm/slides/categorical_data.html#r-output-remarks",
    "title": "Models for Categorical Data",
    "section": "R output, remarks",
    "text": "R output, remarks\n\nIn the output \\(Y\\) is encoded 1, 2 or 3 (from “Not important” to “Very important”)\nWe estimate \\(p^{(2)}(x)/p^{(1)}(x)\\) and \\(p^{(3)}(x)/p^{(1)}(x)\\), the first category being the reference.\nThe reference category for \\(Y\\) is \\(Y = 1\\) (“Not important”)\nAge is encoded 1, 2 or 3 (increasing)\n\\((3-1) + (2-1) + 1 = 4\\) parameters for each \\(p^{(2)}(x)/p^{(1)}(x)\\) and \\(p^{(3)}(x)/p^{(1)}(x)\\)\n\\(2\\times 4=8\\) parameters to estimate!"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#output",
    "href": "teaching/glm/slides/categorical_data.html#output",
    "title": "Models for Categorical Data",
    "section": "Output",
    "text": "Output\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept):1\n-0.5908\n0.2840\n-2.080\n0.037484\n*\n\n\n(Intercept):2\n-1.0391\n0.3305\n-3.144\n0.001667\n**\n\n\nage2:1\n1.1283\n0.3416\n3.302\n0.000958\n***\n\n\nage2:2\n1.4781\n0.4009\n3.687\n0.000227\n***\n\n\nage3:1\n1.5877\n0.4029\n3.941\n8.12e-05\n***\n\n\nage3:2\n2.9168\n0.4229\n6.897\n5.32e-12\n***\n\n\nsexeM:1\n-0.3881\n0.3005\n-1.292\n0.196510\n\n\n\nsexeM:2\n-0.8130\n0.3210\n-2.532\n0.011326\n*"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-calculation",
    "href": "teaching/glm/slides/categorical_data.html#example-calculation",
    "title": "Models for Categorical Data",
    "section": "Example Calculation",
    "text": "Example Calculation\n\nFor example, for a woman aged 18 to 23 years (Age cat. and Gender Cat. are zero):\n\n\\(\\frac{P(Y = \\text{\"Important\"}|\\text{Woman 18-23})}{P(Y = \\text{\"Not important\"}|\\text{Woman 18-23})} = \\exp(-0.59) \\approx 0.55\\)\n\n\n\nFor a man of the same age group, this ratio equals \\(\\exp(-0.59 - 0.3881) \\approx 0.38\\)\n\n\nThe OR between a man and a woman for the “Very important” preference relative to “Not important” equals \\(\\exp(-0.813) = 0.44\\)\n\n\nThis odds is therefore more than double among women…"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#context",
    "href": "teaching/glm/slides/categorical_data.html#context",
    "title": "Models for Categorical Data",
    "section": "Context",
    "text": "Context\n\nIf the categories of \\(Y\\) follow a natural order:\n\n\nWe can obviously ignore it and use the previous nominal model: it is very general but has many parameters.\n\n\nBut we can take advantage of this structure to simplify the model (fewer parameters, easier interpretation)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#recalling-nominal-model",
    "href": "teaching/glm/slides/categorical_data.html#recalling-nominal-model",
    "title": "Models for Categorical Data",
    "section": "Recalling Nominal Model",
    "text": "Recalling Nominal Model\n\nIn consistency with the logistic model, we have focused on the “odds”\n\n\\[\\frac{P(Y = k|X = x)}{P(Y = 0|X = x)} = \\frac{p^{(k)}(x)}{p^{(0)}(x)}\\]\n\n\n\nThis OR quantifies how much the “odds” of \\(P(Y = k)\\) is modified between \\(x_1\\) and \\(x_2\\), relative to the reference category \\(Y = 0\\).\n\n\nIn the ordinal case, we will model “odds” that are easier to interpret."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#section",
    "href": "teaching/glm/slides/categorical_data.html#section",
    "title": "Models for Categorical Data",
    "section": "",
    "text": "Which Odds to Model When Categories Are Ordered?"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#other-odds-for-ordered-categories",
    "href": "teaching/glm/slides/categorical_data.html#other-odds-for-ordered-categories",
    "title": "Models for Categorical Data",
    "section": "Other Odds for Ordered Categories",
    "text": "Other Odds for Ordered Categories\n\nOdds for adjacent categories model:\n\n\\(Odds(x)=\\frac{P(Y = k|X = x)}{P(Y = k-1|X = x)}\\)\n\n\n\nOdds for continuous ratio logistic model:\n\n\\(Odds(x)=\\frac{P(Y = k|X = x)}{P(Y \\leq k-1|X = x)}\\)\n\n\n\nOdds for proportional odds model (most used):\n\n\\(Odds(x)=\\frac{P(Y \\leq k|X = x)}{P(Y &gt; k|X = x)}\\)"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#cumulative-model",
    "href": "teaching/glm/slides/categorical_data.html#cumulative-model",
    "title": "Models for Categorical Data",
    "section": "Cumulative Model",
    "text": "Cumulative Model\n\nThe idea is to construct logistic models for the binary variables \\(\\mathbf{1}_{Y \\leq k}\\), for all \\(k \\in \\{0, \\ldots, K-2\\}\\).\n\n\nThis gives in full generality the cumulative model\n\n\\[\\begin{aligned}\n\\text{logit}(\\P(Y \\leq k|X = x)) &= \\ln\\left(\\frac{\\P(Y \\leq k|X = x)}{\\P(Y &gt; k|X = x)}\\right) \\\\\n&= x^T \\beta^{(k)}\\end{aligned}\\]\n\n\n\n\\(p(K-1)\\) parameters but is different from the nominal model."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#proportional-odds-model",
    "href": "teaching/glm/slides/categorical_data.html#proportional-odds-model",
    "title": "Models for Categorical Data",
    "section": "Proportional Odds Model",
    "text": "Proportional Odds Model\n\nAssumption: the effect of regressors (except the constant) is constant regardless of the categories:\n\n\\[\\text{logit}(\\P(Y \\leq k|X = x)) = \\beta_0^{(k)} + \\beta^T X^*\\]\n\n\n\nwhere \\(X^* \\in \\mathbb{R}^{p-1}\\) denotes the vector of regressors other than the constant.\n\n\n\\((K-1) + (p-1)\\) parameters (this is much less)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#constraints-in-proportional-odds",
    "href": "teaching/glm/slides/categorical_data.html#constraints-in-proportional-odds",
    "title": "Models for Categorical Data",
    "section": "Constraints in Proportional Odds",
    "text": "Constraints in Proportional Odds\n\nSince for any \\(k\\),\n\n\n\\(\\text{logit}(P(Y \\leq k|X = x)) \\leq \\text{logit}(P(Y \\leq k+1|X = x))\\)\n\n\n\n\nthe proportional odds model must verify, for all \\(x \\in \\mathbb R^p\\):\n\n\\(\\beta_0^{(0)} \\leq \\dots \\leq \\beta_0^{(K-2)}\\)\n\n\n\nThis constraint is imposed during estimation."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds",
    "href": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds",
    "title": "Models for Categorical Data",
    "section": "Motivation for Proportional Odds",
    "text": "Motivation for Proportional Odds\n\nSuppose that the classes \\(Y = k\\) come from the discretization of a continuous latent variable \\(Z\\):\n\n\n\nfor \\(\\alpha_{-1} = -\\infty\\), \\(\\alpha_0 &lt; \\cdots &lt; \\alpha_{K-1}\\) and \\(k \\in \\{0, \\ldots, K-1\\}\\),\n\n\\[\\1\\{Y=k\\} = \\1\\{\\alpha_{k-1} \\leq Z &lt; \\alpha_k\\}\\]\n\n\n\nExample: \\(Z\\) is a grade, and \\(Y\\) the distinction level"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds-1",
    "href": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds-1",
    "title": "Models for Categorical Data",
    "section": "Motivation for Proportional Odds",
    "text": "Motivation for Proportional Odds\n\n\n\\(\\1\\{Y=k\\} = \\1\\{\\alpha_{k-1} \\leq Z &lt; \\alpha_k\\}\\)\n\n\n\nSuppose there exists a linear relationship between \\(Z\\) and the regressors \\(X\\):\n\n\\[Z = \\beta^T X + \\varepsilon\\]\n\n\n\nwhere \\(\\varepsilon\\) follows a distribution with cdf \\(F\\). Then\n\n\\[P(Y \\leq k) = F(\\alpha_k - \\beta^T X)\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds-2",
    "href": "teaching/glm/slides/categorical_data.html#motivation-for-proportional-odds-2",
    "title": "Models for Categorical Data",
    "section": "Motivation for Proportional Odds",
    "text": "Motivation for Proportional Odds\n\n\n\\[P(Y \\leq k) = F(\\alpha_k - \\beta^T X)\\]\n\n\n\nThe dependence on \\(X\\) does not depend on \\(k\\).\n\n\nIf \\(F = \\text{logit}^{-1}\\), we obtain the proportional odds model.\n\n\nOther choices of \\(F\\) are possible (probit,…) but the OR become less interpretable."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#section-1",
    "href": "teaching/glm/slides/categorical_data.html#section-1",
    "title": "Models for Categorical Data",
    "section": "",
    "text": "Why “Proportional Odds”?"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#model-assumption",
    "href": "teaching/glm/slides/categorical_data.html#model-assumption",
    "title": "Models for Categorical Data",
    "section": "Model Assumption",
    "text": "Model Assumption\n\nThe proportional odds model assumes that for \\(k \\in \\{0, \\ldots, K-2\\}\\),\n\n\\[\\frac{P(Y \\leq k|X = x)}{P(Y &gt; k|X = x)} = e^{\\beta_0^{(k)} + \\beta^T X^*}\\]\n\n\n\nThis is the odds of \\(Y \\leq k\\) given \\(X=x\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#proportional-log-odds-ratio",
    "href": "teaching/glm/slides/categorical_data.html#proportional-log-odds-ratio",
    "title": "Models for Categorical Data",
    "section": "Proportional log Odds Ratio",
    "text": "Proportional log Odds Ratio\n\nThe odds-ratio of \\(Y \\leq k\\) between two individuals with regressors \\(x_1\\) and \\(x_2\\) respectively therefore equals\n\n\\[OR(x_1, x_2) = \\exp(\\beta^T(x_1^* - x_2^*))\\]\n\n\n\nThis OR does not depend on \\(k\\).\n\n\n\\(\\ln(OR(x_1, x_2))\\) is “proportional” to \\((x_1^* - x_2^*)\\), the “constant” of proportionality \\(\\beta\\) (actually a vector) being independent of \\(k\\)."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#equality-of-slopes",
    "href": "teaching/glm/slides/categorical_data.html#equality-of-slopes",
    "title": "Models for Categorical Data",
    "section": "Equality of Slopes",
    "text": "Equality of Slopes\n\n\n\\(\\text{logit}(P(Y \\leq k|X)) = \\beta_0^{(k)} + \\beta^T X^*\\)\n\n\n\nimplies that the following \\(K-1\\) sets\n\n\\(\\{\\text{logit}(P(Y \\leq k|X=x)), ~ x \\in \\mathbb R^p\\}\\)\n\nare parallel hyperplanes.\n\nThey indeed all have the same normal vector \\(\\beta\\).\nThey differ only by the intercept constant \\(\\beta_0^{(k)}\\).\nTo validate the proportional odds model, it is appropriate to test whether this property is true."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#parallel-hyperplane-illustration",
    "href": "teaching/glm/slides/categorical_data.html#parallel-hyperplane-illustration",
    "title": "Models for Categorical Data",
    "section": "Parallel Hyperplane, Illustration",
    "text": "Parallel Hyperplane, Illustration\nEquality of slopes, or not…"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#slope-equality-test-formulation",
    "href": "teaching/glm/slides/categorical_data.html#slope-equality-test-formulation",
    "title": "Models for Categorical Data",
    "section": "Slope Equality Test, Formulation",
    "text": "Slope Equality Test, Formulation\n\nWe start from the general cumulative model\n\n\\(\\frac{P(Y \\leq k|X = x)}{P(Y &gt; k|X = x)} = e^{x^T \\beta^{(k)}}\\)\n\n\n\nWe test if the parameters (except the constant) are equal regardless of \\(k\\). Writing \\(\\beta^{(k)} = (\\beta_0^{(k)}, \\ldots, \\beta_{p-1}^{(k)})\\), \\(\\beta_0^{(k)}\\) being the constant, we test:\n\n\\[H_0: \\begin{cases}\n\\beta_1^{(0)} = \\cdots = \\beta_1^{(K-2)} \\\\\n\\vdots \\\\\n\\beta_{p-1}^{(0)} = \\cdots = \\beta_{p-1}^{(K-2)}\n\\end{cases}\\]"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#section-2",
    "href": "teaching/glm/slides/categorical_data.html#section-2",
    "title": "Models for Categorical Data",
    "section": "",
    "text": "This can be done by a deviance test (likelihood ratio) by comparing the general cumulative model and the proportional odds model."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#inference",
    "href": "teaching/glm/slides/categorical_data.html#inference",
    "title": "Models for Categorical Data",
    "section": "Inference",
    "text": "Inference\n\nRecall: the \\((Y_i|X_i = x_i)\\) being independent and multinomially distributed, the log-likelihood is\n\n\\[L = \\sum_{i=1}^n \\sum_{k=0}^{K-1} \\mathbf{1}\\{Y_i = k\\} \\ln(p_\\beta^{(k)}(x_i))\\]\n\n\n\nFor the cumulative model and the proportional odds model:\n\nwe can deduce the form of \\(p_\\beta^{(k)}\\)\nwe then maximize \\(L\\) in \\(\\beta\\) to obtain \\(\\hat{\\beta}\\) (by numerical methods)"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#tests",
    "href": "teaching/glm/slides/categorical_data.html#tests",
    "title": "Models for Categorical Data",
    "section": "Tests",
    "text": "Tests\n\nAs usual:\n\n\nWe can compare \\(L_{\\text{mod}} = L(\\hat{\\beta})\\) with other nested models to perform a likelihood ratio test (i.e., deviance test).\n\n\n\\(\\hat{\\beta} - \\beta\\) follows asymptotically a \\(N(0, J_n(\\beta)^{-1})\\), where \\(J_n(\\beta)\\) is the negative of the Hessian of \\(L\\).\n\n\nWe can therefore perform Wald tests."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-car-equipment-study",
    "href": "teaching/glm/slides/categorical_data.html#example-car-equipment-study",
    "title": "Models for Categorical Data",
    "section": "Example: Car Equipment Study",
    "text": "Example: Car Equipment Study"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#data-table",
    "href": "teaching/glm/slides/categorical_data.html#data-table",
    "title": "Models for Categorical Data",
    "section": "Data Table",
    "text": "Data Table\n\n\n\n\nGender\nAge Category\nNot Important\nImportant\nVery Important\n\n\n\n\nFemale\n18-23\n26\n12\n7\n\n\n\n24-40\n9\n21\n15\n\n\n\n&gt;40\n5\n14\n41\n\n\nMale\n18-23\n40\n17\n8\n\n\n\n24-40\n17\n15\n12\n\n\n\n&gt;40\n8\n15\n18\n\n\n\n\n\nWe want to model the variable \\(Y =\\) “importance” (3 categories)\n\n\nThe regressors are gender (2 classes) and age (3 classes).\n\n\nWe have already modeled \\(Y\\) using a multinomial model.\n\n\nIn fact \\(Y\\) is an ordinal variable: we will exploit this."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-proportional-odds-model",
    "href": "teaching/glm/slides/categorical_data.html#example-proportional-odds-model",
    "title": "Models for Categorical Data",
    "section": "Example: Proportional Odds Model",
    "text": "Example: Proportional Odds Model\nWe estimate a proportional odds model:\nvglm(Y ~ age + sexe, family=cumulative(parallel=TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz value\nPr(&gt;\nz\n\n\n\n\n(Intercept):1\n0.04354\n0.23030\n0.189\n0.8501\n\n\n\n(Intercept):2\n1.65498\n0.25360\n6.526\n6.76e-11\n***\n\n\nage2\n-1.14710\n0.27727\n-4.137\n3.52e-05\n***\n\n\nage3\n-2.23246\n0.29042\n-7.687\n1.50e-14\n***\n\n\nsexeM\n0.57622\n0.22611\n2.548\n0.0108\n*\n\n\n\nSignif. codes: 0 ’’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nNames of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2])\nResidual deviance: 4.5321 on 7 degrees of freedom\nLog-likelihood: -25.6671 on 7 degrees of freedom"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-underlying-models",
    "href": "teaching/glm/slides/categorical_data.html#example-underlying-models",
    "title": "Models for Categorical Data",
    "section": "Example: Underlying Models",
    "text": "Example: Underlying Models\n\n\\(K-1=2\\) models (because only the intercepts differs)\n\n\nIn total: \\((K-1) + (p-1) = (3-1) + (4-1) = 5\\) parameters (instead of \\(4\\times 2=8\\))\n\nModel 1: odds between not important and (imortant or very important)\nModel 2: odds between (not important or important) and very important"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-interpretation",
    "href": "teaching/glm/slides/categorical_data.html#example-interpretation",
    "title": "Models for Categorical Data",
    "section": "Example: Interpretation",
    "text": "Example: Interpretation\n\nFor a woman aged 18 to 23 years\n\n\\(\\frac{P(\\text{\"Not important\"}|\\text{Woman 18-23})}{P(\\text{\"Important or very important\"}|\\text{Woman 18-23})} = e^{0.043} \\approx 1.04\\)\n\n\n\nFor a woman aged over 40 years\n\n\\(\\frac{P(\\text{\"Not important\"}|\\text{Woman &gt;40})}{P(\\text{\"Important or very important\"}|\\text{Woman &gt;40})} = e^{0.043-2.23} \\approx 0.11\\)\n\n\n\nThe OR \\(e^{0.57622} = 1.78\\) shows that the odds of having a lower preference is \\(1.78\\) times higher for men than for women."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#example-testing-slope-equality",
    "href": "teaching/glm/slides/categorical_data.html#example-testing-slope-equality",
    "title": "Models for Categorical Data",
    "section": "Example: Testing Slope Equality",
    "text": "Example: Testing Slope Equality\n\nWe test between the full cumulative and proportional odds model.\n\n\nvglm(Y ~ age + sexe, family=cumulative) # To fit with the general cumulative model, we use\nand we note the log-likelihood which equals \\(-25.3164\\).\n\n\nThat of the proportional odds model was \\(-25.6671\\).\n\n\nThe deviance test statistic therefore equals \\(2 \\times (25.6671 - 25.3164) = 0.7\\).\n\n\nWe compare to a \\(\\chi^2_{(K-2)(p-1)} = \\chi^2_3\\) distribution: there is no reason to reject \\(H_0\\) and therefore the proportional odds model is preferable to the cumulative model."
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#comparison-with-nominal-model",
    "href": "teaching/glm/slides/categorical_data.html#comparison-with-nominal-model",
    "title": "Models for Categorical Data",
    "section": "Comparison with Nominal Model",
    "text": "Comparison with Nominal Model\n\nTo compare the proportional odds model with the nominal model:\n\n\nWe cannot use a deviance test because the two models are not nested.\n\n\nNevertheless, the AIC and BIC (not reported here) are in favor of the proportional odds model"
  },
  {
    "objectID": "teaching/glm/slides/categorical_data.html#bar-plot-of-estimated-probability",
    "href": "teaching/glm/slides/categorical_data.html#bar-plot-of-estimated-probability",
    "title": "Models for Categorical Data",
    "section": "Bar Plot of Estimated Probability",
    "text": "Bar Plot of Estimated Probability\n\nIn the proportional odds ratio model:"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html",
    "href": "teaching/hypothesis_testing/TDs/TD1.html",
    "title": "TD1: Introduction to statistical models",
    "section": "",
    "text": "We aim to determine whether ENSAI students have any preference for cats or dogs. We assume that, a priori, they have no preference on average. We ask \\(n\\) students what their preferences are, and we let \\(X\\) be the number of “cat” answers.\n\nDefine \\(H_0\\) and \\(H_1\\). Is it a one-sided (unilatéral) or two-sided (bilatéral) test ?\nWe observe \\(10\\) students and \\(X=8\\) “cat” answers. Compute the pvalue in this specific case. Interprete the result.\nWrite the expression of the pvalue in terms of \\(n\\) and \\(X\\) and \\(F\\), the cdf of \\(\\mathrm{Bin}(n,0.5)\\).\nWrite a line of code to compute the pvalue in Julia, Python or R.\nWhat is the pvalue if \\(H_1\\) is instead\n\n“Students prefer cats” or\n“Students prefer dogs”"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-1",
    "title": "TD1: Introduction to statistical models",
    "section": "",
    "text": "We aim to determine whether ENSAI students have any preference for cats or dogs. We assume that, a priori, they have no preference on average. We ask \\(n\\) students what their preferences are, and we let \\(X\\) be the number of “cat” answers.\n\nDefine \\(H_0\\) and \\(H_1\\). Is it a one-sided (unilatéral) or two-sided (bilatéral) test ?\nWe observe \\(10\\) students and \\(X=8\\) “cat” answers. Compute the pvalue in this specific case. Interprete the result.\nWrite the expression of the pvalue in terms of \\(n\\) and \\(X\\) and \\(F\\), the cdf of \\(\\mathrm{Bin}(n,0.5)\\).\nWrite a line of code to compute the pvalue in Julia, Python or R.\nWhat is the pvalue if \\(H_1\\) is instead\n\n“Students prefer cats” or\n“Students prefer dogs”"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-2",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet \\((X_1, X_2, \\ldots, X_n)\\) be iid random variables that follow distribution \\(\\mathcal E(\\lambda)\\). We want to test: \\[\nH_0: \\lambda = \\frac{1}{2} \\quad \\text{vs.} \\quad H_1: \\lambda = 1.\n\\]\n\nShow that if \\(X \\sim E(\\lambda)\\) and \\(Y \\sim \\Gamma(k, \\lambda)\\) are independent, with \\(k \\in \\mathbb{N}^*\\), then \\(X + Y \\sim \\Gamma(k+1, \\lambda)\\). We recall that the density of \\(\\Gamma(\\lambda, k)\\) is given by \\(p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\)\nDeduce that \\(S_n=\\sum_{i=1}^n X_i\\) follows a Gamma distribution \\(\\Gamma(n, \\lambda)\\).\nFor a sample of size \\(n = 10\\), what is the rejection region of \\(S_n\\) for the simple likelihood ratio test with \\(0.05\\) significance level?\nWe admit that a Gamma distribution \\(\\Gamma(n, \\frac{1}{2})\\) is a chi-squared distribution with \\(2n\\) degrees of freedom, \\(\\chi^2(2n)\\). Bonus: Show this fact for \\(n=1\\), using a polar change of variable.\nThe empirical mean is \\(\\bar{x}_{10} = 2.5\\). What can we conclude?\nRecall what a cdf is, and read the p-value on the cdf of the \\(\\chi^2(20)\\) distribution \nCompare the p-value if we use a Gaussian approximation of \\(\\sum X_i\\) with the TCL. We recall that \\(\\mathbb V(X_1) = \\frac{1}{\\lambda^2}\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-3",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet \\(X_1, X_2, \\ldots, X_n\\) be random variables drawn from a normal distribution \\(N(\\theta, 1)\\). To test \\(H_0: \\theta = 5\\) against \\(H_1: \\theta &gt; 5\\), we propose the following test:\n\\[\nT = \\mathbf 1\\{\\bar{x} &gt; 5 + u \\},\n\\]\nwhere \\(\\bar{x}\\) is the empirical mean and \\(u\\) is to be fixed.\n\n\nDerive the function \\(g:~t \\to \\mathbb P(Z \\geq t) - e^{-t^2/2}\\), where \\(Z \\sim \\mathcal N(0,1)\\)\nDeduce that \\(\\mathbb P(Z \\geq t) \\leq e^{-t^2/2}\\) for all \\(t \\geq 0\\).\n\nDeduce a value of \\(u\\) such that the type I error of this test is smaller than a given \\(\\alpha\\). Rewrite the test \\(T\\) in function of \\(\\alpha\\).\nFix \\(\\alpha = 1/e\\) (and \\(u= \\sqrt{2/n}\\)). Compute the power function."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-4",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 4",
    "text": "Exercise 4\nLet the family of Pareto distributions with known parameter \\(a\\) and unknown parameter \\(\\theta\\):\n\\[\nf(x) =\n\\begin{cases}\n\\frac{\\theta}{a} \\left( \\frac{a}{x} \\right)^{\\theta+1}, & \\text{if } x \\geq a, \\\\\n0, & \\text{if } x &lt; a.\n\\end{cases}\n\\]\n\nCompute the mean and variance of \\(X\\), if \\(X\\) follows a Pareto distribution of parameter \\(a\\) and \\(\\theta\\).\nRewrite the density in the form \\(f(x) = a(x)b(\\theta)e^{c(\\theta)d(x)}\\) and identify \\(a\\),\\(b\\),\\(c\\) and \\(d\\).\nDeduce the general form of the uniformly most powerful test \\(UMP_\\alpha\\) for \\(H_0: \\theta \\geq \\theta_0\\) vs. \\(H_1: \\theta &lt; \\theta_0\\).\nFor \\(a = 1\\), construct the test for the null hypothesis: the mean of the distribution is smaller than or equal to 2.\nWhat is the density of \\(d(X_1)\\) ? \\(d\\) is defined in Q.2\nWrite a line of code in Julia, Python or R to compute the rejection region at level \\(\\alpha=0.05\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html",
    "href": "teaching/hypothesis_testing/TDs/TD3.html",
    "title": "TD3: Goodness of Fit",
    "section": "",
    "text": "We want to test if a die is biased. it is rolled \\(1000\\) times, and the number of occurrences for each face is recorded. The data is as follows:\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nCounts\n159\n168\n167\n160\n175\n171\n\n\n\n\nFormulate the hypothesis testing problem\nCompute the expected counts under \\(H_0\\), give the degree of freedom \\(d\\) of the chi-squared test statistic and give the approximated p-value, using the cdf of \\(\\chi^2(d)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-1",
    "title": "TD3: Goodness of Fit",
    "section": "",
    "text": "We want to test if a die is biased. it is rolled \\(1000\\) times, and the number of occurrences for each face is recorded. The data is as follows:\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nCounts\n159\n168\n167\n160\n175\n171\n\n\n\n\nFormulate the hypothesis testing problem\nCompute the expected counts under \\(H_0\\), give the degree of freedom \\(d\\) of the chi-squared test statistic and give the approximated p-value, using the cdf of \\(\\chi^2(d)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-2",
    "title": "TD3: Goodness of Fit",
    "section": "Exercise 2",
    "text": "Exercise 2\nIn a survey of \\(825\\) families with \\(3\\) children, the number of boys was recorded:\n\\[\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{Number of Boys} & 0 & 1 & 2 & 3 & \\text{Total} \\\\\n\\hline\n\\text{Number of Families} & 71 & 297 & 336 & 121 & 825 \\\\\n\\hline\n\\end{array}\n\\]\nWe assume under \\(H_0\\) that the genders of children in successive births within a family are independent categorical variables and that the probability \\(p\\) of having a boy remains constant.\n\nDetermine the distribution of the number of boys in a family with 3 children as a function of \\(p\\).\nEstimate \\(p\\) using a maximum likelihood estimator.\nTest the goodness of fit to the distribution obtained in question 1."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-3",
    "title": "TD3: Goodness of Fit",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe observe X = [0, 1, 0, 0, 0, 0, 0, 0.5, 1, 1, 1, 0.7, 0.9, 1, 1, 1, 1, 0, 0.1, 0, 1] We assume that the entries of \\(X\\) are iid of distribution \\(P\\). We consider the following hypothesis testing problem:\n\\(H_0\\): \\(P= \\mathcal B(0.5)\\) (Bernoulli)\\(\\quad\\) VS \\(\\quad\\) \\(H_1\\): \\(P \\neq \\mathcal B(0.5)\\).\n\nWhat can you say about the assumptions, \\(H_0\\) and \\(H_1\\)?\nDraw on the same graph the CDF of a Bernoulli \\(0.5\\) and the empirical CDF of the observed data \\(X\\).\nApply the Kolmogorov-Smirnov Test at level \\(0.1\\). To do so, use this table.\nComment on the result."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html",
    "href": "teaching/hypothesis_testing/TDs/TD2.html",
    "title": "TD2: Gaussian Populations",
    "section": "",
    "text": "A bread manufacturing factory wants to establish control procedures with the primary goal of reducing overproduction issues, which result in losses for the factory. Here, we focus on the weights of baguettes produced by the factory, with a target weight of \\(250\\) grams. For a sample of \\(n = 30\\) baguettes, the empirical mean is \\(\\bar{X}_n = 256.3\\) and the empirical variance is \\(S^2_n = 82.1\\). A priori, the factory reaches the target weight of \\(250\\) grams. We aim to test at the level of significance \\(\\alpha = 0.01\\) whether there is an overproduction issue.\n\nIs this a one-sided (unilatéral) or two-sided (bilatéral) testing problem?\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWhat is the test statistic to use, and what is its distribution under \\(H_0\\)?\nDetermine the rejection region.\nDoes the factory have an overproduction issue?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-1",
    "title": "TD2: Gaussian Populations",
    "section": "",
    "text": "A bread manufacturing factory wants to establish control procedures with the primary goal of reducing overproduction issues, which result in losses for the factory. Here, we focus on the weights of baguettes produced by the factory, with a target weight of \\(250\\) grams. For a sample of \\(n = 30\\) baguettes, the empirical mean is \\(\\bar{X}_n = 256.3\\) and the empirical variance is \\(S^2_n = 82.1\\). A priori, the factory reaches the target weight of \\(250\\) grams. We aim to test at the level of significance \\(\\alpha = 0.01\\) whether there is an overproduction issue.\n\nIs this a one-sided (unilatéral) or two-sided (bilatéral) testing problem?\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWhat is the test statistic to use, and what is its distribution under \\(H_0\\)?\nDetermine the rejection region.\nDoes the factory have an overproduction issue?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-2",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe want to test the precision of a method for measuring blood alcohol concentration on a blood sample. Precision is defined as twice the standard deviation of the method (assumed to follow a Gaussian distribution). The reference sample is divided into \\(6\\) test tubes, which are subjected to laboratory analysis. The following blood alcohol concentrations were obtained in g/L: \\[\n1.35, \\; 1.26, \\; 1.48, \\; 1.32, \\; 1.50, \\; 1.44.\n\\]\nWe aim to test the hypothesis that the precision is less than or equal to \\(0.1 \\, \\text{g/L}\\).\n\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWrite the test statistic and give its distribution under \\(H_0\\).\nPerform the test at a significance level of \\(\\alpha = 0.05\\).\nShow that the p-value of this test lies between \\(0.001\\) and \\(0.01\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-3",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 3",
    "text": "Exercise 3\nA candidate for the European elections wants to know if their popularity differs between men and women. A survey was conducted with \\(250\\) men, of whom \\(42\\%\\) expressed support for the candidate, and \\(250\\) women, of whom \\(51\\%\\) expressed support.\n\nFormulate the hypothesis testing problem.\nAt a significance level of \\(\\alpha = 0.05\\), can we say that these values indicate a statistically significant difference in popularity?\nGive an approximation of the p-value in terms of \\(F\\) ,the CDF of \\(\\mathcal N(0,1)\\) and read it on the graph below."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-4",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 4",
    "text": "Exercise 4\nWe aim to compare the average daily durations (in hours) of home-to-work commutes in two departments, labeled \\(A\\) and \\(B\\). We randomly surveyed 26 people in \\(A\\) and 22 in \\(B\\). Let \\(X_i\\) of pepople \\(i\\) be the random variable representing the commute duration in department \\(A\\), and \\(Y_j\\) that in department \\(B\\) of people \\(j\\). We assume the samples obtained are i.i.d following a Gaussian distribution: \\[\nX_i \\sim \\mathcal{N}(\\mu_A, \\sigma_A) \\quad \\text{and} \\quad Y_j \\sim \\mathcal{N}(\\mu_B, \\sigma_B).\n\\]\nHere is a summary of the data:\n\n\n\nDepartment A\nDepartment B\n\n\n\n\n\\(n_A= 26\\)\n\\(n_B=22\\)\n\n\n\\(\\sum x_i = 535\\)\n\\(\\sum y_j = 395\\)\n\n\n\\(\\sum x_i^2 = 11400\\)\n\\(\\sum y_j^2 = 7900\\)\n\n\n\n\nFormulate the hypothesis testing problem\nTest the equality of variances at a significance level of \\(\\alpha = 0.1\\)\nTest the equality of mean commute times between the two departments at a significance level of \\(\\alpha = 0.05\\), and conclude\nGive a Gaussian approximation of the test statistic using the CLT and the LLN, and approximate the p-value using the graph of the cdf of \\(\\mathcal N(0,1)\\) given in the previous exercise"
  },
  {
    "objectID": "teaching/hypothesis_testing/glossary.html",
    "href": "teaching/hypothesis_testing/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "General\n\n\n\nEnglish\n\nincreasing function\nnondecreasing function\ninterval \\([a,b)\\)\n\n\n\n\nFrench\n\nfonction strictement croissante\nfonction croissante\nintervalle \\([a,b[\\)\n\n\n\n\n\n\nProbability/Statistics\n\n\n\nEnglish\n\nprobability distribution\nexpectation\nlikelihood\ndistribution tail\nCDF (Cumul. Distrib. Func.)\nPDF (Proba. Density Func.)\nCLT (Central Limit Theorem)\nLLN (Law of Large Numbers)\n\n\n\n\nFrench\n\nloi de probabilité\nespérance\nvraissemblance\nqueue de distribution\nFonction de Répartition\nDensité d’une distribution\nTLC (Th. de la limite centrale)\nLoi des grands nombres\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nEnglish\n\nA sample\none-sided test\ntwo-sided test\nchi-squared goodness of fit test\nchi-squared homogeneity test\n\n\n\n\nFrench\n\nUn échantillon\ntest unilatéral\ntest bilatéral\ntest d’adéquation du Khi-deux\ntest d’homogénéité du Khi-deux\n\n\n\n\n\n\nProgramming Languages (JupytR)\n\n\n\n\nJulia\nusing Distributions\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\ncdf(Binomial(n, p), x)\nquantile(Binomial(n,p), q)\ncdf(Normal(0,1), x)\nquantile(Normal(0,1), q)\n\ncdf(Chisq(n), x) # Chi-squared\ncdf(TDist(n), x) # Student\n\nn1,n2 = 5,10\ncdf(FDist(n1, n2), x) # Fisher\n\nlmbda = 2\ncdf(Gamma(n, lmbda), x) # Gamma\ncdf(Poisson(lmbda), x) # Poisson\n\n\n\nPython\nfrom scipy.stats import *\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\nbinom.cdf(x, n, p)\nbinom.ppf(q, n, p) # Quantile\nnorm.cdf(x)\nnorm.ppf(q)\n\nchi2.cdf(x, n)\nt.cdf(x, n) # Student\n\nn1,n2 = 5,10\nf.cdf(x, n1, n2) # Fisher\n\nlmbda = 2\ngamma.cdf(x,n,lmbda) # Gamma\npoisson.cdf(x, lmbda) # Poisson\n\n\n\nR\n\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\npbinom(x, n, p)\nqbinom(q, n, p)\npnorm(x)\nqnorm(q)\n\npchisq(x, n)\npt(x, n)\n\nn1,n2 = 5,10\npf(x, n1,n2)\n\nlmbda = 2\npgamma(x,n,lmbda) \nppois(x, lmbda)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Correlation Test",
    "text": "Correlation Test\n\n\n\n\n\n\nPearson’s Correlation Test\n\n\n\n\\(r =  \\frac{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline X)^2\\sum_{i=1}^n (Y_i - \\overline Y)^2}}\\)\n\\(\\psi(X,Y) = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\)\nUnder \\(H_0\\), \\(\\psi(X,Y) \\approx \\mathcal T(n-2)\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Correlation Test",
    "text": "Correlation Test\nMonte Carlo Simulation with \\(n=4\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nFor each group \\(k\\) (eg a region), we observe \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) (eg salaries).\nWe assume that the \\(X_{ik}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\).\nEx: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\) [Wooclap]\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\n\nNote\n\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\) [Wooclap]\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\\)\n\\(SS_k \\sim \\sigma^2\\chi^2(N_k-1)\\) under \\(H_0\\)\nEmpirical var of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\n\n\n\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\)\n\\(SSB \\sim \\sigma^2\\chi^2(d-1)\\) under \\(H_0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#interpretation-of-variances-in-anova",
    "href": "teaching/hypothesis_testing/slides/dependency.html#interpretation-of-variances-in-anova",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Interpretation of variances in ANOVA",
    "text": "Interpretation of variances in ANOVA\n\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#chi2-homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#chi2-homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Homogeneity Test",
    "text": "\\(\\chi^2\\) Homogeneity Test\n\n\\(d\\) different bags (or groups), each containing balls of \\(m\\) potential colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix of counts:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Homogeneity Test",
    "text": "Homogeneity Test\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown [Wooclap]\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all color \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/slides/dependency.html#example-soft-drink-preferences",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\) (proportion of Coke lovers)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#computation-of-chi2-stat",
    "href": "teaching/hypothesis_testing/slides/dependency.html#computation-of-chi2-stat",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Computation of Chi2 stat",
    "text": "Computation of Chi2 stat\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#chi2-independence-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#chi2-independence-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Independence Test",
    "text": "\\(\\chi^2\\) Independence Test\n\n\n\n\nObservations: Paired Categorical Variables \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\ne.g. \\(X_i \\in \\{\\mathrm{male}, \\mathrm{female}\\}\\), \\(Y_i \\in \\{\\mathrm{coffee}, \\mathrm{tea}\\}\\) [Wooclap]\nIdea: regroup the data into bags, e.g. \\(\\{\\mathrm{male}, \\mathrm{female}\\}\\)\nBuild the contingency table\nPerform a chi-square homogeneity test\n\n\n\n\n\n\n\nExample of contingency table:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n30\n20\n50\n\n\nTea\n28\n22\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\nExpected counts:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n29\n21\n50\n\n\nTea\n29\n21\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\n\n\n\\(N_1 \\hat p_1 = 58 \\cdot 50/100  = 29\\), Degree of freedom \\(= 1\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\nSymetrization\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t) &=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy = \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]\n\\(X\\) indep of \\(Y\\) \\(\\implies\\) \\(X-Y\\) symmetric \\(\\implies\\) \\(\\mathrm{median}(X-Y) = 0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/slides/dependency.html#dependency-problem-for-paired-data",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The median of \\((X_i - Y_i)\\) is \\(0\\) for all \\(i\\)\n\\(H_1:\\) The median of \\((X_i - Y_i)\\) is not \\(0\\) for some \\(i\\) [Wooclap]\n\n\n\n\n\n\n\n\nGenerality of \\(H_0\\)\n\n\n\nIf \\(X_i-Y_i\\) is symmetric \\(i\\), then we are under \\(H_0\\).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#section",
    "href": "teaching/hypothesis_testing/slides/dependency.html#section",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "Warning\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\))."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where [Wooclap] \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#properties-on-the-signed-ranks",
    "href": "teaching/hypothesis_testing/slides/dependency.html#properties-on-the-signed-ranks",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Properties on the Signed Ranks",
    "text": "Properties on the Signed Ranks\nUnder \\(H_0\\),\n\nThe signs \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\nProperties on the Signed Ranks\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &gt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\nif \\(H_1\\): \\(\\mathrm{median}(D_i) &gt; 0\\): left-tailed test on \\(W_-\\).\n\n\n\n\n\n\nTo generate a \\(W_-\\) under \\(H_0\\):\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#section-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#section-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "This approximation fits well the exact distribution. Monte-Carlo simulation:"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#effect-of-drug-on-blood-pressure",
    "href": "teaching/hypothesis_testing/slides/dependency.html#effect-of-drug-on-blood-pressure",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Effect of Drug on Blood Pressure",
    "text": "Effect of Drug on Blood Pressure\n\n\\(H_0\\): the drug has no effect. \\(H_1\\): it lowers the blood pressure\n\n\n\n\n\n\nPatient\n\\(X_i\\) (Before)\n\\(Y_i\\)​ (After)\n\\(D_i = X_i-Y_i\\)​\n\\(R_i\\)\n\n\n\n\n1\n150\n140\n10\n6 (+)\n\n\n2\n135\n130\n5\n5 (+)\n\n\n3\n160\n162\n-2\n2 (-)\n\n\n4\n145\n146\n-1\n1 (-)\n\n\n5\n154\n150\n4\n4 (+)\n\n\n6\n171\n160\n11\n7 (+)\n\n\n7\n141\n138\n3\n3 (+)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#numerical-appli",
    "href": "teaching/hypothesis_testing/slides/dependency.html#numerical-appli",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Numerical Appli",
    "text": "Numerical Appli\n\n\\(W_- = 1+2 = 3\\).\nFrom a simulation, we approx \\(\\mathbb P(W_-=i)\\), for \\(i \\in \\{0, 1, 2, 3, 4, 5,6\\}\\) under \\(H_0\\) by\n[0.00784066, 0.00781442, 0.00781534, 0.01563892, 0.01562184, 0.02343478]\nFrom a simulation, \\(p_{value}=\\mathbb P(W_- \\leq 3) \\approx 0.039 &lt; 0.05\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#organization",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#organization",
    "title": "Hypothesis Testing",
    "section": "Organization",
    "text": "Organization\n\n15h of lectures, 18h of TD\n\\(12^{th}\\) may: Exam (2h)\nLecture notes and slides on the website\nAbout english and programming languages\nWooclap sessions [Test]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#objective",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#objective",
    "title": "Hypothesis Testing",
    "section": "Objective",
    "text": "Objective\n\nGiven a general decision problem\n\nIntroduce precise notations to describe the pb\nformulate mathematically hypotheses \\(H_0\\) (a priori) and \\(H_1\\) (alternative)\n\nChoose a statistic adapted to the problem\nCompute this statistic and its pvalue (or an approx.)\nConclude and make a decision"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "General Principles",
    "text": "General Principles\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect Data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "Good and Bad Decisions",
    "text": "Good and Bad Decisions\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "title": "Hypothesis Testing",
    "section": "Dice Biased Toward \\(6\\)",
    "text": "Dice Biased Toward \\(6\\)\n\nObjective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-dice",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-dice",
    "title": "Hypothesis Testing",
    "section": "Fairness of Dice",
    "text": "Fairness of Dice\n\nWe observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\)\n\n\n\n\n\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_0\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_1\\) is “unlikely”)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "title": "Hypothesis Testing",
    "section": "Medical Test",
    "text": "Medical Test\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\n\n\n\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "title": "Hypothesis Testing",
    "section": "Recall of Proba",
    "text": "Recall of Proba\nConsider a probability measure \\(P\\) on \\(\\mathbb R\\).\n\nCDF (Cumulative Distribution Function): \\[x \\to P(~(-\\infty,x]~) = \\mathbb P(X \\leq x) ~~~~\\text{(if $X \\sim P$ under $\\mathbb P$)}\\]\n\n\n\nContinous Measures\n\ndensity wrp to Lebesgue: \\(\\mathbb P(X\\in[x,x+dx])=dP(x) = p(x)dx\\)\nPDF (Proba Density Function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha}) = \\alpha\\)\n\n\n\nDiscrete Measures\n\ndensity wrp to counting measure: \\(\\mathbb P(X=x) = P(\\{x\\})=p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nExponential \\(\\mathcal E(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-gammapoisson",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-gammapoisson",
    "title": "Hypothesis Testing",
    "section": "Examples Gamma/Poisson",
    "text": "Examples Gamma/Poisson\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\]\n\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\lambda\\)\n\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\n\nNumb. of tics before time \\(1\\) of clock \\(\\lambda\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_0\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]\nQuestion Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\nTest"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple\n\n\nTest Model\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_0\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\nMultiple VS Multiple"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nparametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nnon-parametric: Otherwise\n\n\nExample of Multiple VS Multiple Parametric Problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)\n\n\n\nSimple VS Multiple Non-Parametric Problems"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\n\nDecision Rule\n\n\n\nA Decision Rule or Test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\\(T(x) = 0\\) (or \\(1\\)) for all \\(x\\) is the trivial decision rule. Question: Decision Rule\n\n\n\n\n\n\n\n\n\n\n\n\nTest Statistic\n\n\n\na Test Statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter. Question: Test Statistic"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nExample of \\(\\mathcal R\\), for a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) \\not \\in [t_1, t_2]\\}:~~~~~~\\mathcal R = (-\\infty,t_1)\\cup (t_2, +\\infty)\\;\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "title": "Hypothesis Testing",
    "section": "Simple VS Simple Problem",
    "text": "Simple VS Simple Problem\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\nFor a given test \\(T\\) we define:\n\nlevel of \\(T\\): \\(\\alpha = P(T(X)=1)\\) (also: type-1 error)\npower of \\(T\\): \\(\\beta = Q(T(X)=1) = 1-Q(T(X)=0)\\) (1-\\(\\beta\\) is the type-2 error)\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta \\geq \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) ! But \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\n\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\) ?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\n\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta &gt; \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) !\nBut \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson’s Theorem",
    "text": "Neyman Pearson’s Theorem\n\n\n\n\n\n\n\n\n\n\nNeyman Pearson’s Theorem\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\n\n\n\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nWhere, if \\(X\\sim P\\), \\[ P(T^*(X)=1)=\\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\nEquivalent to Log-Likelihood Ratio Test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "title": "Hypothesis Testing",
    "section": "Example with Gaussians",
    "text": "Example with Gaussians\n\n\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\n\n\n\nLog-Likelihood Ratio Test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\nDistrib of \\(\\overline X\\) ?\n\\(t_{\\alpha}\\) ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "title": "Hypothesis Testing",
    "section": "Generalization: Exponential Families",
    "text": "Generalization: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood Ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\nLikelihood Ratio Test: (Q: Select Exp. Families) \\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "title": "Hypothesis Testing",
    "section": "Example: Radioactive Source",
    "text": "Example: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) &gt; 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) &gt; 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): Reject if \\(N \\geq 19\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#definitions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#definitions",
    "title": "Hypothesis Testing",
    "section": "Definitions",
    "text": "Definitions\n\n\n\n\\(H_0 = \\mathcal P_0=\\{P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\n\n\n\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(right-tailed: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(left-tailed: unilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(simple/multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( multiple/multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-tailed test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (right-tailed test)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\nHere, \\(\\Theta_0\\) is not necessarily a singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\n\nPivotal Test Statistic\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\n\n\n\n\n\n\nP-value: definition\n\n\nWe define \\(p_{value}(x_{\\mathrm{obs}}) =\\mathbb P(\\psi(X) \\geq x_{\\mathrm{obs}})\\) for a right-tailed test.\nFor a two-tailed test, \\(p_{value}(x_{\\mathrm{obs}}) =2\\min(\\mathbb P(\\psi(X) \\geq x_{\\mathrm{obs}}),\\mathbb P(\\psi(X) \\leq x_{\\mathrm{obs}}))\\)\n\n\n\n\n\n\n\n\n\n\n\nP-value under \\(H_0\\)\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(p_{value}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\n\n\n\n\n\nIn practice: reject if \\(p_{value}(x_{\\mathrm{obs}}) \\leq \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html",
    "href": "teaching/hypothesis_testing/annals/test_2025.html",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "",
    "text": "Duration: 2 hours, no document allowed. Special attention will be given to clarity of writing."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-1-testing-a-preference-for-renewable-or-non-renewable-energy-sources",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-1-testing-a-preference-for-renewable-or-non-renewable-energy-sources",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 1: Testing a Preference for Renewable or Non-Renewable Energy Sources",
    "text": "Exercise 1: Testing a Preference for Renewable or Non-Renewable Energy Sources\nWe aim to determine whether citizens in a region have any preference for renewable energy sources (e.g., solar, wind) or non-renewable energy sources (e.g., coal, natural gas). We assume that, a priori, there is no preference on average. We survey \\(n\\) individuals, and let \\(X\\) be the number of respondents who prefer renewable energy.\n\nQuestions:\n\nFormalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\). Indicate whether this test is one-tailed or two-tailed.\nWe survey \\(n = 100\\) individuals, and \\(X = 58\\) prefer renewable energy sources. Write the \\(p_{value}\\) in function of \\(F\\), the cdf of \\(\\text{Bin}(100, 0.5)\\) (binomial distribution with parameter \\(p = 0.5\\)).\nWrite a line of code that would compute the exact p-value in Julia, Python, or R.\nGive an approximation of the p-value using a Gaussian approximation and the graph of the cdf of \\(\\mathcal N(0,1)\\) given bellow. What do you conclude?\nRedefine the alternative hypothesis \\(H_1\\) and compute an approximated p-value if we aim to determine whether citizens have a preference for:\n\nrenewable energy sources.\nCitizens prefer non-renewable energy sources.\n\nWhat do you conclude for these two other problems?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-2-environmental-monitoring-of-river-pollution",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-2-environmental-monitoring-of-river-pollution",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 2: Environmental Monitoring of River Pollution",
    "text": "Exercise 2: Environmental Monitoring of River Pollution\nAn environmental agency is monitoring the pollution levels of a river to determine whether a nearby factory is causing an increase in harmful chemical concentration. The target concentration for a specific chemical is \\(15 \\, \\text{ppm}\\) (parts per million), which is considered safe for aquatic life. For a sample of \\(n = 20\\) water samples taken downstream from the factory, the empirical mean concentration is \\(\\bar{X}_n = 16.3 \\, \\text{ppm}\\), and the empirical variance is \\(S^2_n = 2.4 \\, \\text{ppm}^2\\).\nA priori, the river is assumed to meet the safe pollution threshold of \\(15 \\, \\text{ppm}\\).\nWe aim to test at the level of significance \\(\\alpha = 0.05\\) whether the chemical concentration downstream exceeds the safe threshold, indicating pollution from the factory.\n\nQuestions:\n\nUsing a Gaussian assumption, formalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\). Is this a one-tailed or two-tailed testing problem? Precise what the unknown parameters are.\nDefine the test statistic. What is its distribution under \\(H_0\\)?\nDetermine the rejection region. You can use a Gaussian approximation and the cdf of Exercise 1.\nWrite a line of code do compute the exact rejection threshold.\nDoes the river exhibit an increased chemical concentration that could indicate pollution from the factory?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-3-bird-migration-habitat-distribution-analysis",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-3-bird-migration-habitat-distribution-analysis",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 3: Bird Migration Habitat Distribution Analysis",
    "text": "Exercise 3: Bird Migration Habitat Distribution Analysis\nA wildlife researcher is studying the behavior of a certain species of birds that migrate to a nature reserve. The researcher has a hypothesis about how the birds distribute themselves across different types of habitats in the reserve. The expected distribution, based on historical data, is as follows:\n\nGrassland: 40%\nWetlands: 30%\nForests: 20%\nRocky Areas: 10%\n\nTo test this hypothesis, the researcher surveys 200 birds and records their habitat preferences. The observed counts are as follows:\n\n\n\nHabitat\nGrassland\nWetlands\nForests\nRocky Areas\n\n\n\n\nObserved\n90\n60\n30\n20\n\n\n\n\nQuestions:\n\nFormalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\).\nCompute the expected counts.\nCompute the chi-square statistic.\nDetermine the degree of freedom \\(df\\) of the chi-square statistic, and read the p-value on the following graph of the cdf.\nWhat do you conclude?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nEmployee Productivity Across Departments\nA company wants to evaluate whether a new management style has had a consistent effect on employee productivity across five departments. Each department has adopted a specific variation of the management style for three months, and the company has recorded the average number of tasks completed per employee during that period.\nData:\n\n\n\nDepartment\n1\n2\n3\n4\n5\n\n\n\n\nNumber of employees\n12\n10\n8\n9\n11\n\n\nAverage tasks completed\n72.4\n68.9\n75.6\n74.3\n69.7\n\n\nVariance of tasks\n8.5\n9.2\n10.1\n7.8\n9.6\n\n\n\nThe company seeks to understand whether productivity levels vary significantly across departments, indicating that the management styles might have different impacts.\nLet \\(d=5\\) be the number of departments and \\(N_{\\text{tot}} = 50\\) the total number of employees. For any department \\(j\\), we denote \\(N_k\\) the number of employees in department \\(k\\), and \\(P_{ik}\\) the number of tasks completed by employee \\(i\\) in department \\(k\\). We assume that the \\(P_{ik}\\)’s are independent and normally distributed with mean \\(\\mu_k\\) and variance \\(\\sigma^2\\).\nWe write \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDefine the hypotheses of the problem to test whether the management styles had a uniform impact on productivity.\nGive a brief interpretation of each one of the quantities \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\n\nProve the analysis of variance formula \\(V_T = V_W + V_B\\)\nCalculate \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), and \\(V_T\\).\n\nExpress the ANOVA test statistic in terms of \\(V_W\\) and \\(V_B\\).\n\nWhat are the distributions of \\(N_k V_k\\) and of \\(N_{\\mathrm{tot}}V_W\\) under \\(H_0\\)? Do they change under \\(H_1\\)?\nRecall the definition of ANOVA test statistic, and perform the ANOVA test at significance level \\(\\alpha =0.05\\). We give the \\(0.05\\) and \\(0.95\\)-quantiles of \\(\\mathcal D\\) which are approximately \\(0.18\\) and \\(2.58\\).\nConclude whether productivity differs significantly between departments."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#course-questions",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#course-questions",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Course Questions",
    "text": "Course Questions\n\nRecall the definition of a test statistic \\(\\psi\\) and a test (or decision rule) \\(T\\).\nWhat are the two types of errors that we can commit?\nFor a given test statistic \\(\\psi\\), recall the definition of the p-value in the context of a two-sided test.\nState the Neyman-Pearson theorem."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables",
    "text": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables\nNous cherchons à déterminer si les citoyens d’une région ont une quelconque préférence pour les sources d’énergie renouvelables (par exemple, solaire, éolienne) ou les sources d’énergie non renouvelables (par exemple, charbon, gaz naturel). Nous supposons que, a priori, il n’y a aucune préférence en moyenne. Nous interrogeons \\(n\\) individus, et nous notons \\(X\\) le nombre de répondants qui préfèrent les énergies renouvelables."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#questions-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#questions-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions :",
    "text": "Questions :\n\nFormalisez le problème de test d’hypothèse, et définissez \\(H_0\\) et \\(H_1\\)​. Indiquez si ce test est unilatéral ou bilatéral.\nNous interrogeons \\(n=100\\) individus, et \\(X=58\\) préfèrent les sources d’énergie renouvelables. Écrivez la p-valeur en fonction de \\(F\\), la fonction de répartition d’une distribution Binomiale Bin\\((100,0.5)\\).\nÉcrivez une ligne de code qui calculerait la p-valeur exacte en Julia, Python, ou R.\nDonnez une approximation de la p-valeur en utilisant une approximation gaussienne et le graphique de la fonction de répartition de \\(\\mathcal N(0,1)\\) fourni dans le sujet. Quelle est votre conclusion ?\nRedéfinissez l’hypothèse alternative \\(H_1\\)​ et calculez une p-valeur approximative si nous cherchons à déterminer si les citoyens ont une préférence pour :\n\nles sources d’énergie renouvelables.\nles sources d’énergie non renouvelables. Quelles sont vos conclusions pour ces deux autres problèmes ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 2 : Surveillance environnementale de la pollution fluviale",
    "text": "Exercice 2 : Surveillance environnementale de la pollution fluviale\nUne agence environnementale surveille les niveaux de pollution d’une rivière pour déterminer si une usine à proximité provoque une augmentation de la concentration de produits chimiques nocifs. La concentration cible pour un produit chimique spécifique est de \\(15 \\, \\text{ppm}\\) (parties par million), ce qui est considéré comme sûr pour la vie aquatique. Pour un échantillon de \\(n = 20\\) prélèvements d’eau effectués en aval de l’usine, la concentration moyenne empirique est \\(\\bar{X}_n = 16,3 \\, \\text{ppm}\\), et la variance empirique est \\(S^2_n = 2,4 \\, \\text{ppm}^2\\).\nA priori, on suppose que la rivière respecte le seuil de pollution sans danger de \\(15 \\, \\text{ppm}\\).\nNous visons à tester, avec un niveau de signification \\(\\alpha = 0,05\\), si la concentration chimique en aval dépasse le seuil de sécurité, indiquant une pollution provenant de l’usine.\n\nQuestions :\n\nEn utilisant une hypothèse Gaussienne, formalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\). S’agit-il d’un test unilatéral ou bilatéral ?\nDéfinissez la statistique de test. Quelle est sa distribution sous \\(H_0\\) ?\nDéterminez la zone de rejet. Vous pouvez utiliser une approximation Gaussienne et le graphe de l’exercice 1.\nEcrivez une ligne de code permettant de calculer le seuil de rejet exact.\nLa rivière présente-t-elle une concentration chimique accrue qui pourrait indiquer une pollution provenant de l’usine ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux",
    "text": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux\nUn chercheur en faune sauvage étudie le comportement d’une certaine espèce d’oiseaux qui migrent vers une réserve naturelle. Le chercheur a une hypothèse sur la façon dont les oiseaux se répartissent entre différents types d’habitats dans la réserve. La distribution attendue, basée sur des données historiques, est la suivante :\n\nPrairie : 40%\nZones humides : 30%\nForêts : 20%\nZones rocheuses : 10%\n\nPour tester cette hypothèse, le chercheur observe 200 oiseaux et enregistre leurs préférences d’habitat. Les comptages observés sont les suivants :\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé\n90\n60\n30\n20\n\n\n\n\nQuestions :\n\nFormalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\).\nCalculez les effectifs attendus.\nCalculez la statistique du chi-deux.\nDéterminez le degré de liberté \\(df\\) de la statistique du chi-deux, et lisez la p-value sur le graphique suivant de la fonction de répartition.\nQuelle est votre conclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 4",
    "text": "Exercice 4\n\nProductivité des employés entre départements\nUne entreprise souhaite évaluer si un nouveau style de management a eu un effet uniforme sur la productivité des employés dans cinq départements. Chaque département a adopté une variation spécifique du style de management pendant trois mois, et l’entreprise a enregistré le nombre moyen de tâches accomplies par employé durant cette période.\nDonnées :\n\n\n\nDépartement\n1\n2\n3\n4\n5\n\n\n\n\nNombre d’employés\n12\n10\n8\n9\n11\n\n\nMoyenne des tâches accomplies\n72,4\n68,9\n75,6\n74,3\n69,7\n\n\nVariance des tâches\n8,5\n9,2\n10,1\n7,8\n9,6\n\n\n\nL’entreprise cherche à comprendre si les niveaux de productivité varient significativement entre les départements, indiquant que les styles de management pourraient avoir des impacts différents.\nSoit \\(d=5\\) le nombre de départements et \\(N_{\\text{tot}} = 50\\) le nombre total d’employés. Pour tout département \\(j\\), nous notons \\(N_k\\) le nombre d’employés dans le département \\(k\\), et \\(P_{ik}\\) le nombre de tâches accomplies par l’employé \\(i\\) dans le département \\(k\\). Nous supposons que les \\(P_{ik}\\) sont indépendants et suivent une distribution normale de moyenne \\(\\mu_k\\) et de variance \\(\\sigma^2\\).\nNous écrivons \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDéfinissez les hypothèses du problème pour tester si les styles de management ont eu un impact uniforme sur la productivité.\nDonnez une brève interprétation de chacune des quantités \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\nDémontrez la formule d’analyse de la variance : \\(V_T = V_W + V_B\\)\nCalculez \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), et \\(V_T\\).\nExprimez la statistique de test ANOVA en termes de \\(V_W\\) et \\(V_B\\).\nQuelles sont les distributions de \\(N_k V_k\\) et de \\(N_{\\mathrm{tot}}V_W\\) sous \\(H_0\\) ? Changent-elles sous \\(H_1\\) ?\nRappelez la définition de la statistique de test ANOVA, donnez sa distribution \\(\\mathcal D\\) sous \\(H_0\\) et effectuez le test ANOVA au niveau \\(\\alpha =0,05\\). On donne les quantiles \\(0.05\\) et \\(0.95\\) de \\(\\mathcal D\\): \\(0.18\\) and \\(2.58\\). Concluez si la productivité diffère significativement entre les départements."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#questions-de-cours",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#questions-de-cours",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions de cours",
    "text": "Questions de cours\n\nRappelez la définition d’une statistique de test \\(\\psi\\) et d’un test (ou règle de décision) \\(T\\).\nQuels sont les deux types d’erreur que nous pouvons commettre ?\nPour une statistique de test \\(\\psi\\) donnée et un problème de test bilatéral, rappelez la définition de la p-valeur.\nÉnoncez le théorème de Neyman-Pearson."
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html",
    "title": "Goodness of Fit Tests",
    "section": "",
    "text": "Binomial distribution\n\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting.",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#multinomials",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#multinomials",
    "title": "Goodness of Fit Tests",
    "section": "",
    "text": "Binomial distribution\n\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting.",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit Test",
    "text": "\\chi^2 Goodness of Fit Test\n\nWe observe (X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q). This corresponds to n counts: X_1 + \\dots + X_m = n\nq = (q_1, \\dots, q_m) corresponds to probabilities of getting color 1, \\dots, m\nLet p = (p_1, \\dots, p_m) be a known vector s.t. p_1 + \\dots + p_m = 1.\nH_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\n\n\n\n\n\n\n\n\\chi^2 Goodness of Fit Test (Adéquation)\n\n\n\n\nChi-squared test statistic: \\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; , where n_i = np_i = \\mathbb E[X_i] is the expected number of counts for color i.\nWhen np_i = n_i are large, under H_0, we approximate the distribution of \\psi(X) as \\psi(X) \\sim \\chi^2(m-1)\nReject if \\psi(X) &gt; t_{1- \\alpha} (right-tail of \\chi^2(m-1))",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "title": "Goodness of Fit Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\n\n\n\n\n\n\nWe observe a bag of sweets containing n=100 sweets of m=3 different colors: red, green, and yellow.\nManufacturer: p_1= 40\\% red, p_2=35\\% green, and p_3=25\\% yellow.\nH_0: q=p (manufacturer’s claim is correct)\nH_1: q\\neq p (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\nColor\nObserved Counts\nExpected Counts\n\n\n\n\nRed\nX_1=50\nn_1=40\n\n\nGreen\nX_2=30\nn_2=35\n\n\nYellow\nX_3=20\nn_3=25\n\n\n\n\n\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\n\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878 (p_{value} \\approx 0.222)\nConclusion: do not reject H_0",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#histograms",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#histograms",
    "title": "Goodness of Fit Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nHistogram\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n\n\n\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\n\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\n\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\nwhere I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\n\n\n\n\n\n\n\n\n\nNormalization\n\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\nLaw of large numbers, Monte Carlo (informal)\n\n\n\n\nAssume that (X_1, \\dots, X_n) are iid of distrib P, and that a,b, k are fixed\nThe histogram \\mathrm{hist}(a,b,k) converges to the histogram of the density P",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit to a given distribution",
    "text": "\\chi^2 Goodness of Fit to a given distribution\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n, iid with unknown distrib P\nH_0: P = P_0, where P_0 is known\nH_1: P \\neq P_0\nIdea: under H_0, the counts in disjoint intervals I_1, I_2, \\dots, I_k follow a multinomial distribution \\mathrm{Mult}(n, (p_1, \\dots, p_{k})) where p_1 = P_0(I_1), \\dots, p_{k} = P_0(I_k)\n\n\n\n\n\n\n\n\n\n\nReduction to chi-squared test statistic\n\n\n\n\nCount the number of data (c_1, \\dots, c_k) in I_1, \\dots, I_k\nTheoretical counts nP_0(I_1) \\dots, nP_0(I_k)\nChi-squared statistic: \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_j)}\nDecide using an \\alpha-quantile of a \\chi^2(k-1) distribution\n\n\n\n\n\n\n\n\n\nCorrected \\chi^2 Test\n\n\n\n\nIf P_0 is unknown\nIn a class \\mathcal P parameterized by \\ell parameters\nThen estimate the \\ell parameters\nCompute theoretical counts with \\hat P_{\\hat \\theta}\nBut decide with \\chi^2(k - 1 - \\ell)",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Goodness of Fit Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\nH_0: X_i iid \\mathcal P(2)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n0\n1\n2 \n\\geq 3\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\n\n\n\n\n\n\nTo get 9.7, we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\gtrsim \\frac{(16-4)^2}{4} = 36\n(1-cdf(Chisq(3),36)) is very small: Reject\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nIf H_0 is X_i iid \\mathcal P(\\lambda) with unknown \\lambda\nNot \\chi^2(3) but \\chi^2(2)\n(1-cdf(Chisq(2),36)) is even smaller: (Still Reject)",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "title": "Goodness of Fit Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)}\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\n\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with y=x",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "title": "Goodness of Fit Tests",
    "section": "Kolmogorov-Smirnov Test",
    "text": "Kolmogorov-Smirnov Test\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)}\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\nEmpirical CDF: \\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf 1\\{X_i \\leq x\\}\nIdea: Max distance between empirical and true CDF\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\n\\psi(X) = \\sup_{x}|\\hat F(x) - F_0(x)|\nApprox: \\mathbb P_0(\\psi(X) &gt;c/\\sqrt{n}) \\to 2\\sum_{r=1}^{+\\infty}(-1)^{r-1}\\exp(-2c^2r^2) when n \\to +\\infty\nIn practice, use Julia, Python or R for KS Tests",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X = (X_1, \\dots, X_n), iid with distribution \\mathcal N(\\mu, \\sigma). We assume that \\mu is unknown but that \\sigma is known.\n\n\n\n\n\n\nTest problems\n\n\n\n\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\nTest Statistic:  \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\n\\psi(X) \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\nTests\n\n\n\n\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFisher’s Quote\n\n\n\nThe value for which p=0.05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.\n\n\n\n\n\n\nMultiple VS Multiple Test Problem: \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} no longer test statistic.\nIdea: replace \\sigma by its estimator  \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\nThis gives \n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\nIs \\psi(X) pivotal under H_0 ? What is its distribution ?\n\n\n\n\n\n\n\n\n\n\nChi-Squared Distribution \\chi^2(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\sum_{i=1}^k Z_i^2\nwhere the Z_i’s are iid \\mathcal N(0,1).\n\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\n\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1) when k \\to +\\infty\n\n\n\n\n\n\n\n\n\nStudent distribution \\mathcal T(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\tfrac{Z}{\\sqrt{U/k}}\nZ, U are independent and follow resp. \\mathcal N(0,1) and a \\chi^2(k)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume X_i are iid \\mathcal N(\\mu_0, \\sigma).\n\nThe test statistic \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} pivotal (indep. of \\sigma).\nIt follows a Student distribution \\mathcal T(n-1).\n\n\n\nSketch of Proof.\nRemark that, the orthogonal projection of (X_1, \\dots, X_n) on (1, \\dots, 1) is equal to \\overline X \\cdot (1, \\dots, 1). This is precisely because the empirical mean minimizes the quantity \\tfrac{1}{n}\\sum (X_i - \\overline X)^2. Hence, the two vectors \\overline X \\cdot (1, \\dots, 1) and (X_1 - \\overline X, \\dots, X_n - \\overline X) are orthogonal. From the Cochran’s theorem, Orthogonality is equivalent to independence for Gaussian random variables, and we deduce that \\overline X is independent of \\hat \\sigma^2. \n\\tag*{$\\blacksquare$}\n\nThe student tests are the same as Gaussian tests with known variance, except that we replace the quantiles of the Gaussian the quantiles of the Student distribution.\n\nThe quantiles of the Student distribution are close to the quantile of the Standard Gaussian when the degree of freedom k is large:\nThe quantiles are slightly larger when k is small. The Student Distribution has a slightly heavier tail.\n\n\n\n\n\nMultiple VS multiple test problem X=(X_1, \\dots, X_n): \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n(Student) T-test statistic: \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe X=(X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu, \\sigma). \\mu, \\sigma are unknown. \\sigma_0 is fixed.\n\n\n\n\n\n\n\n\n\n\nRight-tailed test\n\n\n\n\nH_0: \\sigma \\leq \\sigma_0, H_1: \\sigma &gt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\nq_{1-\\alpha}: quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\n\n\n\n\n\nLeft-tailed test\n\n\n\n\nH_0: \\sigma \\geq \\sigma_0, H_1: \\sigma &lt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\nq_{\\alpha}: quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X = (X_1, \\dots, X_n), iid with distribution \\mathcal N(\\mu, \\sigma). We assume that \\mu is unknown but that \\sigma is known.\n\n\n\n\n\n\nTest problems\n\n\n\n\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\nTest Statistic:  \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\n\\psi(X) \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\nTests\n\n\n\n\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Fisher’s Quote\n\n\n\nThe value for which p=0.05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Multiple VS Multiple Test Problem: \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} no longer test statistic.\nIdea: replace \\sigma by its estimator  \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\nThis gives \n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\nIs \\psi(X) pivotal under H_0 ? What is its distribution ?",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Chi-Squared Distribution \\chi^2(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\sum_{i=1}^k Z_i^2\nwhere the Z_i’s are iid \\mathcal N(0,1).\n\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\n\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1) when k \\to +\\infty\n\n\n\n\n\n\n\n\n\nStudent distribution \\mathcal T(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\tfrac{Z}{\\sqrt{U/k}}\nZ, U are independent and follow resp. \\mathcal N(0,1) and a \\chi^2(k)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume X_i are iid \\mathcal N(\\mu_0, \\sigma).\n\nThe test statistic \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} pivotal (indep. of \\sigma).\nIt follows a Student distribution \\mathcal T(n-1).\n\n\n\nSketch of Proof.\nRemark that, the orthogonal projection of (X_1, \\dots, X_n) on (1, \\dots, 1) is equal to \\overline X \\cdot (1, \\dots, 1). This is precisely because the empirical mean minimizes the quantity \\tfrac{1}{n}\\sum (X_i - \\overline X)^2. Hence, the two vectors \\overline X \\cdot (1, \\dots, 1) and (X_1 - \\overline X, \\dots, X_n - \\overline X) are orthogonal. From the Cochran’s theorem, Orthogonality is equivalent to independence for Gaussian random variables, and we deduce that \\overline X is independent of \\hat \\sigma^2. \n\\tag*{$\\blacksquare$}\n\nThe student tests are the same as Gaussian tests with known variance, except that we replace the quantiles of the Gaussian the quantiles of the Student distribution.\n\nThe quantiles of the Student distribution are close to the quantile of the Standard Gaussian when the degree of freedom k is large:\nThe quantiles are slightly larger when k is small. The Student Distribution has a slightly heavier tail.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Multiple VS multiple test problem X=(X_1, \\dots, X_n): \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n(Student) T-test statistic: \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variance-unknown-mean",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variance-unknown-mean",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X=(X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu, \\sigma). \\mu, \\sigma are unknown. \\sigma_0 is fixed.\n\n\n\n\n\n\n\n\n\n\nRight-tailed test\n\n\n\n\nH_0: \\sigma \\leq \\sigma_0, H_1: \\sigma &gt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\nq_{1-\\alpha}: quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\n\n\n\n\n\nLeft-tailed test\n\n\n\n\nH_0: \\sigma \\geq \\sigma_0, H_1: \\sigma &lt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\nq_{\\alpha}: quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1^2) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_1, \\sigma_1^2).\n\\sigma_1, \\sigma_2 are known, \\mu_1, \\mu_2 are unknown\nTest Problem: H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\nIdea: Normalize \\overline X - \\overline Y: \n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\nTwo-Tailed Test for Testing Variance: \nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\nt_{1-\\alpha/2} is the (1-\\alpha/2)-quantile of a Gaussian distribution",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown\nVariance Testing Problem: \nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\nF-Test Statistic of the Variances (ANOVA) \n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher Distribution \\mathcal F(k_1,k_2)\n\n\n\n\n(k_1, k_2): degrees of freedom\nDistribution of \\frac{U_1/k_1}{U_2/k_2}\nWhere U_1, U_2 are indep. and follow \\chi^2(k_1), \\chi^2(k_2). wiki\n\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right) when k_1,k_2 \\to +\\infty\nExample: \\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1) if Z_i \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\n\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} is independent of \\mu_1, \\mu_2, \\sigma_1, \\sigma_2. It is pivotal\nIt follow distribution \\mathcal F(n_1-1, n_2-1)\n\n\n\n\n\nTwo-tailed test:  \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown, but we know that \\sigma_1=\\sigma_2\nEquality of Mean Testing Problem: \nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\nFormally, H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}.\n\n\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\n\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\nNormalize \\overline X - \\overline Y:\n\n\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\n\n\\psi(X,Y) is pivotal because \\sigma_1 = \\sigma_2.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown\nEquality of Mean Testing Problem: \nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\nFormally, H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}.\n\n\n\n\n\n\n\nStudent Welch Test Statistic\n\n\n\n\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\n\n\\psi(X,Y) is not pivotal\nGaussian approximation: \\psi(X,Y) \\approx \\mathcal N(0,1) when n_1, n_2 \\to \\infty\nBetter approximation: Student Welch",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\n\nCLT\n\n\n\n\nLet S_n = \\sum_{i=1}^n X_i with (X_1, \\dots, X_n) iid (L^2) then  \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \nEquality when X_i’s are \\mathcal N(\\mu, \\sigma)\nRule of thumb: n \\geq 30\n\n\n\n\n\n\n\n\n\nExample: Binomials\n\n\n\n\nIf p \\in (0,1)\n\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1) when n \\to \\infty\nn should be \\gg \\frac{1}{p}\n\n\n\nGood Approx for (n=100, p=0.2)\n\nBad Approx for (n=100, p=0.01)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe X \\sim Bin(n_1, p_1) and Y \\sim Bin(n_2, p_2).\nn_1, n_2 are known but p_1, p_2 are unknown in (0,1)\nH_0: p_1 = p_2 or H_1: p_1 \\neq p_2\n\n\n\n\n\n\n\nTest Statistic\n\n\n\n \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\n\n\\hat p_1 = X/n_1, \\hat p_2 = X/n_2\n\\hat p = \\frac{X+Y}{n_1+n_2}\nIf np_1, np_2 \\gg 1: \\psi(X) \\sim \\mathcal N(0,1)\nWe reject if |\\psi(X,Y)| \\geq t_{1-\\alpha/2} (gaussian quantile)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion: “should we raise taxes on cigarettes to pay for a healthcare reform ?”\np_1, p_2: proportion of non-smokers or smokers willing to raise taxes\nH_0: p_1=p_2 or H_1: p_1 &gt; p_2\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\hat p_1 \\approx 0.58, \\hat p_2 \\approx 0.21.\n\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\n\\mathbb P(\\psi(X,Y) &gt; 8.99) = 1-cdf(Normal(0,1), 8.99)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#practical-question",
    "href": "teaching/linear_model/slides/selection.html#practical-question",
    "title": "Selection",
    "section": "Practical Question",
    "text": "Practical Question\n\nIn practice, we often hesitate between several models:\n\nWhich variables to include in the model?\nHow to choose between one model and another?\nIdeally: How to select the “best” model among all possible sub-models of a large linear regression model?"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#selection-criteria",
    "href": "teaching/linear_model/slides/selection.html#selection-criteria",
    "title": "Selection",
    "section": "Selection Criteria",
    "text": "Selection Criteria\n\nSeveral criteria exist. The main ones:\n\n\\(R_a^2\\): Adjusted \\(R^2\\) (already seen)\nFisher test for nested models (already seen)\n\nMallows’ \\(C_p\\)\nAIC criterion\nBIC criterion"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#setup",
    "href": "teaching/linear_model/slides/selection.html#setup",
    "title": "Selection",
    "section": "Setup",
    "text": "Setup\n\nSuppose we have \\(p_{\\max}\\) explanatory variables, forming the “maximal” design matrix \\(X_{\\max}\\).\n\n\nTrue model (unknown):\n\n\\[Y = X^*\\beta^* + \\varepsilon\\]\n\nwhere \\(X^*\\) is a sub-matrix of \\(X_{\\max}\\) formed by \\(p^* \\leq p_{\\max}\\) columns.\n\n\nWe don’t know \\(p^*\\) nor which variables are involved.\nGoal: Select the correct matrix \\(X^*\\) and estimate \\(\\beta^*\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#practice",
    "href": "teaching/linear_model/slides/selection.html#practice",
    "title": "Selection",
    "section": "Practice",
    "text": "Practice\n\nWe regress \\(Y\\) on \\(p \\leq p_{\\max}\\) variables, assuming: \\[Y = X\\beta + \\varepsilon\\] where \\(X\\): sub-matrix of \\(X_{\\max}\\) containing the \\(p\\) chosen columns (yielding \\(\\hat{\\beta}\\)).\n\n\nThis model is potentially wrong (bad choice of variables).\n\n\nObjective: Calculate a quality score for this submodel."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#adjusted-r2---reminder",
    "href": "teaching/linear_model/slides/selection.html#adjusted-r2---reminder",
    "title": "Selection",
    "section": "Adjusted \\(R^2\\) - Reminder",
    "text": "Adjusted \\(R^2\\) - Reminder\n\nFor a model with constant:\n\n\\[R_a^2 = 1 - \\frac{n-1}{n-p} \\cdot \\frac{SSR}{SST}\\]\n\n\n\\(SST = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\) (independent of chosen model)\n\\(SSR = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) (specific to considered model)\n\n\n\nSelection rule: Between two models, prefer highest \\(R_a^2\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#fisher-test-for-nested-models",
    "href": "teaching/linear_model/slides/selection.html#fisher-test-for-nested-models",
    "title": "Selection",
    "section": "Fisher Test for Nested Models",
    "text": "Fisher Test for Nested Models\n\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR}\\]\n\n\n\\(SSR\\): residual sum of squares of the larger model\n\\(SSR_c\\): SSR of the sub-model (fewer variables)\n\\(p\\): number of variables in the larger model\n\\(q\\): number of constraints (\\(p-q\\) variables in sub-model)\n\n\nIf \\(F &lt; f_{q,n-p}(1-\\alpha)\\): prefer sub-model (\\(H_0\\) at level \\(\\alpha\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#mallows-c_p",
    "href": "teaching/linear_model/slides/selection.html#mallows-c_p",
    "title": "Selection",
    "section": "Mallows’ \\(C_p\\)",
    "text": "Mallows’ \\(C_p\\)\n\nTrue model (unknown): \\(Y = X^*\\beta^* + \\varepsilon\\)\n\nTested model (possibly wrong): \\(Y = X\\beta + \\varepsilon\\) with OLS estimate \\(\\hat{\\beta}\\)\n\n\nMallows’ \\(C_p\\) aims to estimate the prediction risk: \\[\\E(\\|\\tilde{Y} - X\\hat{\\beta}\\|^2)\\]\nwhere \\(\\tilde{Y}\\) follows the same distribution as \\(Y\\) but is independent."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#formula-for-mallows-c_p",
    "href": "teaching/linear_model/slides/selection.html#formula-for-mallows-c_p",
    "title": "Selection",
    "section": "Formula for Mallows’ \\(C_p\\)",
    "text": "Formula for Mallows’ \\(C_p\\)\n\n\n\\[C_p = \\frac{SSR}{\\hat{\\sigma}^2} - n + 2p\\]\n\n\n\\(p\\): number of variables in the considered model\n\\(SSR\\): residual sum of squares of the considered model\n\n\\(\\hat{\\sigma}^2\\): estimation of \\(\\sigma^2\\) in the largest model\nSame for all tested models\n\n\n\nSelection rule: Among all tested models, choose the one with lowest \\(C_p\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#aic-criterion",
    "href": "teaching/linear_model/slides/selection.html#aic-criterion",
    "title": "Selection",
    "section": "AIC Criterion",
    "text": "AIC Criterion\n\nAIC (Akaike Information Criterion) is motivated like \\(C_p\\).\nIt also focuses on prediction error \\(\\tilde{Y} - X\\hat{\\beta}\\), but Kullback distance instead of Quadratic distance.\n\n\n\n\\[AIC = n \\ln\\left(\\frac{SSR}{n}\\right) + 2(p+1)\\]\n\n\n\nSelection rule: choose model with lowest AIC.\nIn practice, AIC and \\(C_p\\) are very close (choose same model)"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#bic-criterion",
    "href": "teaching/linear_model/slides/selection.html#bic-criterion",
    "title": "Selection",
    "section": "BIC Criterion",
    "text": "BIC Criterion\n\nBIC (Bayesian Information Criterion) seeks the “most probable” model in a Bayesian formalism.\n\n\\[BIC = n \\ln\\left(\\frac{SSR}{n}\\right) + (p+1) \\ln n\\]\n\n\n\nSelection rule: choose the one with lowest BIC.\n\nKey difference: The “2” in front of \\((p+1)\\) is replaced by \\(\\ln n\\)\nThis difference frequently leads to a different model choice between AIC and BIC"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#relationship-between-criteria",
    "href": "teaching/linear_model/slides/selection.html#relationship-between-criteria",
    "title": "Selection",
    "section": "Relationship Between Criteria",
    "text": "Relationship Between Criteria\n\n“Large” model: low \\(SSR\\), but high number of variables \\(p\\)\n(if too large: overfitting)\n“Small” model: high \\(SSR\\), but low number of variables \\(p\\)   (if two small: underfitting)\n\n\nAll previous criteria try to find a compromise between:\n\nGood fit to data (low \\(SSR\\))\nSmall model size (low \\(p\\))\n\n\n\nThis is a permanent trade-off in statistics (not just in regression)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#general-form",
    "href": "teaching/linear_model/slides/selection.html#general-form",
    "title": "Selection",
    "section": "General Form",
    "text": "General Form\n\n\\(C_p\\), AIC, and BIC consist of minimizing an expression of the form:\n\n\\[f(SSR) + c(n) \\cdot p\\]\n\n\n\nBIC: \\(c(n) = \\ln n\\) \\(\\quad\\quad\\) AIC: \\(c(n) = 2\\)\n\n\\(f\\) is an increasing function of \\(SSR\\)\n\\(c(n) \\cdot p\\) is a term penalizing models with many variables\n\n\n\n(Other criteria exist built on the same principle.)"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#relationship-between-criteria-1",
    "href": "teaching/linear_model/slides/selection.html#relationship-between-criteria-1",
    "title": "Selection",
    "section": "Relationship Between Criteria",
    "text": "Relationship Between Criteria\n\n\\[f(SSR) + c(n) \\cdot p\\]\n\n\nWhen \\(\\ln n &gt; 2\\), BIC penalizes large models more than AIC.\n\n\nOrdering criteria by their propensity to select the most sparse model:\n\\[BIC \\leq F\\text{ test} \\leq C_p \\approx AIC \\leq R_a^2\\]\n\nBIC will favor a smaller model than \\(C_p\\) or AIC\n\\(R_a^2\\) will tend to favor an even larger model"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#theoretical-aspects",
    "href": "teaching/linear_model/slides/selection.html#theoretical-aspects",
    "title": "Selection",
    "section": "Theoretical Aspects",
    "text": "Theoretical Aspects\n\n\n\n\n\n\n\n\nProbability as \\(n \\to \\infty\\)                                                             \nBIC\n\\(C_p\\), AIC, \\(R_a^2\\)\n\n\n\n\n\\(\\mathbb{P}\\)(selects model smaller than true)\n\\(\\to 0\\)\n\\(\\to 0\\)\n\n\n\\(\\mathbb{P}\\)(selects model larger than true)\n\\(\\to 0\\)\n\\(\\not\\to 0\\)\n\n\n\\(\\mathbb{P}\\)(selects correct model)\n\\(\\to 1\\)\n\\(\\not\\to 1\\)\n\n\n\n\nBIC is asymptotically consistent, while other criteria tend to overfit."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#exhaustive-search",
    "href": "teaching/linear_model/slides/selection.html#exhaustive-search",
    "title": "Selection",
    "section": "Exhaustive Search",
    "text": "Exhaustive Search\n\nGiven \\(p_{\\max}\\) available explanatory variables:\n\nTempting approach: Test all possible sub-models\nSelection: Keep the one with lowest BIC (or other criterion)\nComputational cost: \\(2^{p_{\\max}}\\) models to test (that’s a lot!)\n\n\n\nIf \\(p_{\\max}\\) is not too large, this remains feasible.\nR function: regsubsets from leaps library"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#exhaustive-search-1",
    "href": "teaching/linear_model/slides/selection.html#exhaustive-search-1",
    "title": "Selection",
    "section": "Exhaustive Search",
    "text": "Exhaustive Search\n\n\n\n\nImportant Warning\n\n\nAutomatic selection does not guarantee that the selected model is good.\nIt’s simply the best model according to the chosen criterion.\nThe selected model may be bad in terms of:\n\nExplanatory power\nMulticollinearity problems\n\nHeteroscedasticity issues\nAuto-correlation problems"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#stepwise-procedures",
    "href": "teaching/linear_model/slides/selection.html#stepwise-procedures",
    "title": "Selection",
    "section": "Stepwise Procedures",
    "text": "Stepwise Procedures\n\nIf \\(p_{\\max}\\) is too large for exhaustive search:\n\n\nStepwise Backward (according to chosen criterion, e.g., BIC):\n\nStart with largest model (\\(p_{\\max}\\) variables)\nRemove least significant variable\nRepeat: remove remaining least significant variable\nStop when no removal improves the model"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#stepwise-procedures-1",
    "href": "teaching/linear_model/slides/selection.html#stepwise-procedures-1",
    "title": "Selection",
    "section": "Stepwise Procedures",
    "text": "Stepwise Procedures\nIf \\(p_{\\max}\\) is too large for exhaustive search:\n\nStepwise Forward:\n\nStart with smallest model (constant only)\nAdd most significant variable at each step\n\n\n\nStepwise Backward (or Forward) Hybrid:\n\nLike backward (or forward), but also try adding (or removing) a variable at each step"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#limitations-and-characteristics",
    "href": "teaching/linear_model/slides/selection.html#limitations-and-characteristics",
    "title": "Selection",
    "section": "Limitations and Characteristics",
    "text": "Limitations and Characteristics\n\nStepwise procedures do not explore all possible sub-models:\n\nMay miss the best model\n\n\n\nSpeed comparison:\n\nForward: fastest (small models are quicker to estimate)\nHybrid procedures: slower, but explore more possible models"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#r-implementation",
    "href": "teaching/linear_model/slides/selection.html#r-implementation",
    "title": "Selection",
    "section": "R Implementation",
    "text": "R Implementation\n\nFunction: step with option direction:\n\n\"backward\" or \"forward\" or \"both\"\nDefault criterion: AIC (k = 2)\nFor BIC: use k = ln(n)\n\n\n\nThe option k corresponds to the penalty \\(c(n)\\) introduced earlier."
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#general-setting",
    "href": "teaching/linear_model/slides/linear_model.html#general-setting",
    "title": "Definition of the Linear Model",
    "section": "General Setting",
    "text": "General Setting\n\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\n\nIn other words, we observe\n\\(Y= (Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\n\n\\(X^{(1)} = (X^{(1)}_1, \\dots, X^{(1)}_n) \\in \\mathbb R^n\\)\n\\(X^{(2)} = (X^{(2)}_1, \\dots, X^{(2)}_n) \\in \\mathbb R^n\\)\n…\n\\(X^{(p)} = (X^{(p)}_1, \\dots, X^{(p)}_n)\\in \\mathbb R^n\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#non-parametric-model",
    "href": "teaching/linear_model/slides/linear_model.html#non-parametric-model",
    "title": "Definition of the Linear Model",
    "section": "Non Parametric Model",
    "text": "Non Parametric Model\n\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\n\nWe assume that\n\n\\[Y_i = F(X^{(1)}_i, X^{(2)}_i, \\dots, X^{(p)}_i, \\varepsilon_i)\\]\n\n\n\nwhere \\(\\varepsilon = (\\varepsilon_1, \\dots, \\varepsilon_n) \\in \\mathbb R^n\\) are iid random noise\n\n\n\n\\(\\varepsilon\\) is not observed\n\n\n\\(F\\) is unknown\n–&gt; Too ambitious, risk of overfitting"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-1",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-1",
    "title": "Definition of the Linear Model",
    "section": "Linear Model",
    "text": "Linear Model\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\nWe assume that\n\n\\[Y = \\beta_1 X^{(1)}+ \\beta_2 X^{(2)}+ \\dots+ \\beta_p X^{(p)}+ \\varepsilon\\]\n\n\n\nThat is, we know that \\(F\\) is of the form \\(F(x_1, \\dots, x_p, \\varepsilon) = \\beta_1 x_1+ \\beta_2 x_2+ \\dots+ \\beta_p x_p+ \\varepsilon\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-2",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-2",
    "title": "Definition of the Linear Model",
    "section": "Linear Model",
    "text": "Linear Model\n\\(Y\\) and the \\(X^{(k)}\\)’s are vectors in \\(\\mathbb R^n\\).\n\nFor all \\(i\\),\n\n\\[Y_i = \\beta_1 X^{(1)}_i+ \\beta_2 X^{(2)}_i+ \\dots+ \\beta_p X^{(p)}_i+ \\varepsilon_i\\]\n\nWe assume that\n\n\\(X^{(k)}\\) are known and deterministic (otherwise we condition on \\(X^{(k)}\\)’s)\n\\(\\mathbb E[\\varepsilon_i] = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-with-intercept",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-with-intercept",
    "title": "Definition of the Linear Model",
    "section": "Linear Model with Intercept",
    "text": "Linear Model with Intercept\n\\(Y\\) and the \\(X^{(k)}\\)’s are vectors in \\(\\mathbb R^n\\).\n\nIf we set \\(X^{(1)}= (1, \\dots, 1)\\), then the model rewrites\n\n\\[Y_i = \\beta_1+ \\beta_2 X^{(2)}_i+ \\dots+ \\beta_p X^{(p)}_i+ \\varepsilon_i\\]\n\n\n\n\nThe model is linear in \\(X^{(2)}, \\dots, X^{(p)}\\)\n\\(\\beta_1\\) is then called the intercept"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation",
    "href": "teaching/linear_model/slides/linear_model.html#notation",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\nWe write \\(Y = (Y_1, \\dots, Y_n)\\) and \\(X^{(k)} = (X^{(k)}_1, \\dots, X^{(k)}_n)\\) as columns:\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\) \\(\\newcommand{\\E}{\\mathbb E}\\) \\(\\newcommand{\\P}{\\mathbb P}\\) \\(\\newcommand{\\Var}{\\mathbb V}\\)\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X^{(k)}=\\begin{pmatrix}\nX^{(k)}_1 \\\\\n\\vdots \\\\\nX^{(k)}_n\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation-1",
    "href": "teaching/linear_model/slides/linear_model.html#notation-1",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\n\nTo get a matrix form, we write \\(X_{ik} = X^{(k)}_i\\). Then:\n\n\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X^{(k)}=\\begin{pmatrix}\nX_{1k} \\\\\n\\vdots \\\\\nX_{n,k}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation-2",
    "href": "teaching/linear_model/slides/linear_model.html#notation-2",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\n\nTo get a matrix form, we write \\(X\\) for the matrix \\((X_{ik}) \\in \\mathbb R^{n \\times p}\\)\n\n\nThat is, \\(X = (X^{(1)}, \\dots, X^{(p)})\\)\n\n\nAnd:\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X=\\begin{pmatrix}\n&X_{1,1} &\\dots &X_{1,p} \\\\\n&\\vdots &~ &\\vdots \\\\\n&X_{n,1} &\\dots &X_{n,p}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#model-matrix-form",
    "href": "teaching/linear_model/slides/linear_model.html#model-matrix-form",
    "title": "Definition of the Linear Model",
    "section": "Model, Matrix Form",
    "text": "Model, Matrix Form\n\nLet \\(\\beta = (\\beta_1, \\dots, \\beta_p) \\in \\mathbb R^p\\) be unknown parameters, and \\(\\varepsilon = (\\varepsilon_1, \\dots, \\varepsilon_n)\\) be iid noise.\n\n\nIn column notation:\n\n\\[\\beta = \\begin{pmatrix}\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{pmatrix} \\and \\varepsilon=\\begin{pmatrix}\n\\varepsilon_{1} \\\\\n\\vdots \\\\\n\\varepsilon_{n}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-matrix-form",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-matrix-form",
    "title": "Definition of the Linear Model",
    "section": "Linear Model, Matrix Form",
    "text": "Linear Model, Matrix Form\n\nWe observe \\(Y = (Y_1, \\dots, Y_n) \\in \\mathbb R^n\\) and \\(X \\in \\mathbb R^{n \\times p}\\)\n\n\nWe assume that\n\n\\[Y = X \\beta + \\varepsilon\\]\n\nwhere\n\n\\(X\\) is known,\n\\(\\beta \\in \\mathbb R^p\\) is unknown\n\\(\\varepsilon \\in \\mathbb R^n\\) is a vector of iid random noise with \\(\\mathbb E[\\varepsilon_i] = 0\\) and \\(\\mathbb V(\\varepsilon_i) = \\sigma^2\\)(unknown)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#remarks",
    "href": "teaching/linear_model/slides/linear_model.html#remarks",
    "title": "Definition of the Linear Model",
    "section": "Remarks",
    "text": "Remarks\n\n\\((\\varepsilon_1, \\dots, \\varepsilon_n)\\) independent implies no correlation between individuals\n\\(\\mathbb V(\\varepsilon_i)= \\sigma^2\\) does not depend on \\(i\\): this is called homoscedasticity assumption\nWe can write \\(\\mathbb V(\\varepsilon) = \\sigma^2 I_n\\) (covariance matrix of \\(\\varepsilon\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#identifiability-condition",
    "href": "teaching/linear_model/slides/linear_model.html#identifiability-condition",
    "title": "Definition of the Linear Model",
    "section": "Identifiability Condition",
    "text": "Identifiability Condition\n\nRecall that \\(X \\in \\mathbb R^{n \\times p}\\)\n\n\nWe assume that \\(rk(X)=p\\).\n\n\nThis implies \\(p \\leq n\\)\n\n\nIf this condition is not satisfied:\n\n\nIt means that there is a linear relation between the \\(X^{(k)}\\)!\n\n\nIt means that \\(X\\alpha=\\alpha_1X^{(1)} + \\dots + \\alpha_p X^{(p)}=0\\) for some \\(\\alpha \\in \\mathbb R^p\\setminus \\{0\\}\\)\n\n\nWe can take infinitely many possible \\(\\beta\\), since for \\(t \\in \\mathbb R\\),\n\\[\nX(\\beta + t\\alpha) = X\\beta\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#course-structure",
    "href": "teaching/linear_model/slides/introduction.html#course-structure",
    "title": "Introduction",
    "section": "Course Structure",
    "text": "Course Structure\n\n8 lecture sessions\nCourse materials and slides (both evolving) available on Moodle\n8 TD/TP sessions (tutorial/practical work)\nContinuous assessment: November 6 (date to be confirmed)\nFinal exam: December 18 (date to be confirmed)\nAttention: Some practical sessions may take place in the tutorial room with your personal computer (not the first session)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#objectives-of-a-regression-model",
    "href": "teaching/linear_model/slides/introduction.html#objectives-of-a-regression-model",
    "title": "Introduction",
    "section": "Objectives of a Regression Model",
    "text": "Objectives of a Regression Model\n\nExplain a quantity \\(Y\\) based on \\(p\\) quantities \\(X^{(1)}, ..., X^{(p)}\\) (explanatory variables, or regressors).\n\n\nFor this purpose, we have \\(n\\) observations of each quantity from \\(n\\) individuals."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#examples",
    "href": "teaching/linear_model/slides/introduction.html#examples",
    "title": "Introduction",
    "section": "Examples:",
    "text": "Examples:\n\n\\(Y\\): daily electricity consumption in France\n\n\\(X= X^{(1)}\\): average daily temperature\n\n\n\nThe data consists of a history of \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\) over \\(n\\) days\n\n\nQuestion: Do we have \\(Y \\approx f(X)\\) for a certain function f?\nSimplifying: Do we have \\(Y ≈ aX + b\\) for certain values \\(a\\) and \\(b\\)?\nIf yes, what is \\(a\\)? What is \\(b\\)? Is the relationship “reliable”?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#examples-1",
    "href": "teaching/linear_model/slides/introduction.html#examples-1",
    "title": "Introduction",
    "section": "Examples",
    "text": "Examples\n\n\\(Y \\in \\{0,1\\}\\): customer quality (\\(1\\): good; \\(0\\): not good)\n\n\\(X^{(1)}\\): customer income\n\n\\(X^{(2)}\\): socio-professional category (6-7 possibilities)\n\n\\(X^{(3)}\\): age\n\n\n\nData: n customers.\nIn this case, we model \\(p = P(Y = 1)\\).\nDo we have \\(p \\approx f(X^{(1)}, X^{(2)}, X^{(3)})\\) for a function f with values in \\([0, 1]\\)?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#predictivedescriptive-model",
    "href": "teaching/linear_model/slides/introduction.html#predictivedescriptive-model",
    "title": "Introduction",
    "section": "Predictive/Descriptive Model",
    "text": "Predictive/Descriptive Model\n\nThe “approximate” relationship we’re trying to establish between \\(Y\\) and \\(X^{(1)}\\), …, \\(X^{(p)}\\) is a model.\n\n\nWhy seek to establish such a model? Two main reasons:\n\n\nDescriptive objective: quantify the marginal effect of each variable. For example, if \\(X^{(1)}\\) increases by 10%, how does \\(Y\\) change?\n\n\nPredictive objective: given new values for \\(X^{(1)}\\), …, \\(X^{(p)}\\), we can deduce the (approximate) associated \\(Y\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#course-outline",
    "href": "teaching/linear_model/slides/introduction.html#course-outline",
    "title": "Introduction",
    "section": "Course Outline",
    "text": "Course Outline\n\nIntroduction → Bivariate analysis (review): relationship between 2 variables → General aspects of modeling\nLinear Regression → Quantitative \\(Y\\) as a function of quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)\nAnalysis of Variance and Covariance → Quantitative \\(Y\\) as a function of qualitative and/or quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)\nGeneralized Linear Regression → Qualitative or quantitative \\(Y\\) as a function of qualitative and/or quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#two-types-of-variables",
    "href": "teaching/linear_model/slides/introduction.html#two-types-of-variables",
    "title": "Introduction",
    "section": "Two Types of Variables",
    "text": "Two Types of Variables\nWe are interested in the relationship between \\(2\\) variables \\(X\\) and \\(Y\\). We distinguish two main categories, each divided into two types."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#quantitative-variables",
    "href": "teaching/linear_model/slides/introduction.html#quantitative-variables",
    "title": "Introduction",
    "section": "Quantitative Variables",
    "text": "Quantitative Variables\n\nA variable whose observation is a measured quantity. Examples: age, salary, number of infractions, etc.\nWe distinguish between:\n\nDiscrete quantitative variables whose possible values are finite or countable (Examples: number of children, number of infractions, etc.)\nContinuous quantitative variables which can take any value within an interval (Examples: height, salary, etc.)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#qualitative-variables-or-factors",
    "href": "teaching/linear_model/slides/introduction.html#qualitative-variables-or-factors",
    "title": "Introduction",
    "section": "Qualitative Variables (or Factors)",
    "text": "Qualitative Variables (or Factors)\n\nA variable whose observation results in a category or code. The possible observations are called the modalities of the qualitative variable. Examples: gender, socio-professional category, nationality, high school honors, etc.\nWe distinguish between:\n\nordinal qualitative variable: a natural order appears in the modalities (Examples: high school honors, etc.).\nnominal qualitative variable otherwise (Examples: gender, socio-professional category, etc.)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-of-the-pottery-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-of-the-pottery-dataset",
    "title": "Introduction",
    "section": "Example of the “Pottery” Dataset",
    "text": "Example of the “Pottery” Dataset\n\nData: chemical composition of pottery found at different archaeological sites in the United Kingdom\n\n\n\n\n\nSite\nAl\nFe\nMg\nCa\nNa\n\n\n\n\n1\nLlanedyrn\n14.4\n7.00\n4.30\n0.15\n0.51\n\n\n2\nLlanedyrn\n13.8\n7.08\n3.43\n0.12\n0.17\n\n\n3\nLlanedyrn\n14.6\n7.09\n3.88\n0.13\n0.20\n\n\n4\nLlanedyrn\n10.9\n6.26\n3.47\n0.17\n0.22\n\n\n5\nCaldicot\n11.8\n5.44\n3.94\n0.30\n0.04\n\n\n6\nCaldicot\n11.6\n5.39\n3.77\n0.29\n0.06\n\n\n7\nIsleThorns\n18.3\n1.28\n0.67\n0.03\n0.03\n\n\n8\nIsleThorns\n15.8\n2.39\n0.63\n0.01\n0.04\n\n\n9\nIsleThorns\n18.0\n1.88\n0.68\n0.01\n0.04\n\n\n10\nIsleThorns\n20.8\n1.51\n0.72\n0.07\n0.10\n\n\n11\nAshleyRails\n17.7\n1.12\n0.56\n0.06\n0.06\n\n\n12\nAshleyRails\n18.3\n1.14\n0.67\n0.06\n0.05\n\n\n13\nAshleyRails\n16.7\n0.92\n0.53\n0.01\n0.05\n\n\n\n\n\nIndividuals: pottery numbered from 1 to 13\n\nVariables: the archaeological site (factor with 4 modalities) and different chemical compounds (quantitative)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-of-the-no2traffic",
    "href": "teaching/linear_model/slides/introduction.html#example-of-the-no2traffic",
    "title": "Introduction",
    "section": "Example of the “NO2traffic”",
    "text": "Example of the “NO2traffic”\n\nData: NO2 concentration inside cars in Paris, type of road, (P, T, A, V or U) and traffic fluidity (A to D).\n\n\n\n\n\nNO2\nType\nFluidity\n\n\n\n\n1\n378.94\nP\nA\n\n\n2\n806.67\nT\nD\n\n\n3\n634.58\nA\nD\n\n\n4\n673.35\nT\nC\n\n\n5\n589.75\nP\nA\n\n\n…\n…\n…\n…\n\n\n283\n184.16\nP\nB\n\n\n284\n121.88\nV\nD\n\n\n285\n152.39\nU\nA\n\n\n286\n129.12\nU\nC\n\n\n\n\n\nIndividuals: vehicles numbered from 1 to 286\n\nVariables: NO2 (quantitative), type (factor with 5 modalities) and fluidity (ordinal factor with 4 modalities)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#pairwise-scatter-plots",
    "href": "teaching/linear_model/slides/introduction.html#pairwise-scatter-plots",
    "title": "Introduction",
    "section": "Pairwise Scatter Plots",
    "text": "Pairwise Scatter Plots\n\nWe observe\n\\(X=(X_1, \\ldots, X_n) \\in \\mathbb R^n\\) and \\(Y=(Y_1, \\ldots, Y_n) \\in \\mathbb R^n\\), (quantitative variables)\n\n\nRelationship between \\(X\\) and \\(Y\\): scatter plot of points \\((X_i, Y_i)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-pottery-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-pottery-dataset",
    "title": "Introduction",
    "section": "Example: Pottery Dataset",
    "text": "Example: Pottery Dataset"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-plot",
    "href": "teaching/linear_model/slides/introduction.html#correlation-plot",
    "title": "Introduction",
    "section": "Correlation Plot",
    "text": "Correlation Plot\n\nplot_cor=@df pottery_num corrplot(cols(1:4),grid=false, compact=true) #Julia"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-empirical-correlation",
    "href": "teaching/linear_model/slides/introduction.html#linear-empirical-correlation",
    "title": "Introduction",
    "section": "Linear Empirical Correlation",
    "text": "Linear Empirical Correlation\n\nThe linear relationship is quantified by Pearson’s linear correlation: \\(\\DeclareMathOperator{\\cov}{cov}\\) \\(\\DeclareMathOperator{\\var}{var}\\)\n\n\\[\\hat \\rho = \\frac{\\hat\\cov(X,Y)}{\\sqrt{\\hat \\var(X)\\hat \\var(Y)}}\\]\n\n\n\nwhere \\(\\hat \\var\\) and \\(\\hat \\cov\\) denote the empirical variance and covariance:\n\n\\(\\hat \\cov(X,Y)= \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\overline X)(Y_i - \\overline Y)\\)\n\\(\\hat \\var(X) = \\hat \\cov(X,X)\\), \\(\\hat \\var(Y)=\\hat\\cov(Y,Y)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#properties-of-empirical-correlation",
    "href": "teaching/linear_model/slides/introduction.html#properties-of-empirical-correlation",
    "title": "Introduction",
    "section": "Properties of Empirical Correlation",
    "text": "Properties of Empirical Correlation\n\nFrom the Cauchy-Schwarz inequality, we deduce that:\n\n\nThe correlation \\(\\hat \\rho\\) is always between \\(-1\\) and \\(1\\):\n\nIf \\(\\hat \\rho = 1\\): for all \\(i\\), \\(Y_i = aX_i + b\\) for some \\(a &gt; 0\\)\nIf \\(\\hat \\rho = -1\\): for all \\(i\\), \\(Y_i = aX_i + b\\) for some \\(a &lt; 0\\)\nIf \\(\\hat \\rho = 0\\): no linear relationship. notebook"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-test",
    "href": "teaching/linear_model/slides/introduction.html#correlation-test",
    "title": "Introduction",
    "section": "Correlation test",
    "text": "Correlation test\n\n\\(\\hat \\rho(X, Y)\\) is an estimator of the unknown theoretical correlation \\(\\rho\\) between \\(X\\) and \\(Y\\) defined by \\[\\rho = \\frac{\\mathbb E[(X - \\mathbb E(X))(Y - \\mathbb E(Y))]}{\\sqrt{\\mathbb V(X)\\mathbb V(Y)}}\\]\n\n\nCorrelation test problem:\n\\[H_0: \\rho = 0 \\quad \\text{VS}\\quad  H_1: \\rho \\neq 0\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-test-1",
    "href": "teaching/linear_model/slides/introduction.html#correlation-test-1",
    "title": "Introduction",
    "section": "Correlation test",
    "text": "Correlation test\n\nTest statistic (here we use \\(\\psi\\) for test statistics and \\(T\\) for tests) \\[\\psi(X,Y) = \\frac{\\hat \\rho\\sqrt{n-2}}{\\sqrt{1-\\hat \\rho^2}}\\]\n\n\nTest\nUnder \\(H_0\\), if \\((X,Y)\\) is Gaussian, \\(\\psi(X,Y) \\sim \\mathcal T(n-2)\\) (Student distribution of degree of freedom \\(n-2\\))\n\n\n\\[T(X,Y) = \\mathbf{1}\\{|\\psi(X,Y)| &gt; t_{1-\\alpha/2}\\}\\]\nIn R: cor.test"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#least-square-p1",
    "href": "teaching/linear_model/slides/introduction.html#least-square-p1",
    "title": "Introduction",
    "section": "Least Square \\((p=1)\\)",
    "text": "Least Square \\((p=1)\\)\n\nGiven observations \\((X_i, Y_i)\\), we consider \\(\\hat \\alpha\\), \\(\\hat \\mu\\) that minimize, over all \\((\\alpha, \\mu) \\in \\mathbb R^2\\):\n\n\\[\nL(\\alpha, \\mu) = \\sum_{i=1}^n (Y_i - \\alpha X_i - \\mu)^2\n\\]\n\n\n\nSolution: (check homogeneity!)\n\n\\[\\hat \\alpha = \\hat \\cov(X,Y)  \\quad \\text{and} \\quad \\hat \\mu = \\overline Y - \\hat a \\overline X\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#contingency-table-and-notation",
    "href": "teaching/linear_model/slides/introduction.html#contingency-table-and-notation",
    "title": "Introduction",
    "section": "Contingency Table and Notation",
    "text": "Contingency Table and Notation\n\nWe observe \\(X=(X_1, \\dots, X_n)\\) and \\(Y=(Y_1, \\dots, Y_n)\\), where\n\n\\(X_k \\in \\{1, \\dots, I\\}\\) (factor with \\(I\\) categories, “colors”)\n\\(Y_k \\in \\{1, \\dots, J\\}\\) (factor with \\(J\\) categories, “bags”)\n\n\n\n\n\n\nCategory X/Y\nBag 1\nBag 2\nBag 3\nTotals\n\n\n\n\nCol 1\n\\(n_{11}\\)\n\\(n_{12}\\)\n\\(n_{13}\\)\n\\(R_1\\)\n\n\nCol 2\n\\(n_{21}\\)\n\\(n_{22}\\)\n\\(n_{23}\\)\n\\(R_2\\)\n\n\nTotals\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\\(n_{ij}\\): number of individuals having category \\(i\\) for \\(X\\) and \\(j\\) for \\(Y\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-no2trafic-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-no2trafic-dataset",
    "title": "Introduction",
    "section": "Example: NO2trafic dataset",
    "text": "Example: NO2trafic dataset\ncontingency table of variable “Type” and “Fluidity”\n\n\n\nFluidity/Type\nP\nU\nA\nT\nV\n\n\n\n\nA\n21\n21\n19\n9\n9\n\n\nB\n20\n17\n16\n8\n7\n\n\nC\n17\n17\n16\n8\n7\n\n\nD\n20\n20\n18\n8\n8\n\n\n\nIn R: table(X,Y)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#in-julia",
    "href": "teaching/linear_model/slides/introduction.html#in-julia",
    "title": "Introduction",
    "section": "In Julia",
    "text": "In Julia\n\nfluidity_types = [\"A\", \"B\", \"C\", \"D\"]\ntype_p = [21, 20, 17, 20]\ntype_u = [21, 17, 17, 20]\ntype_a = [19, 16, 16, 18]\ntype_t = [9, 8, 8, 8]\ntype_v = [9, 7, 7, 8]\n\n# Create a matrix for the grouped bar plot\n# Each row represents a fluidity type, each column represents a measurement type\ndata_matrix = hcat(type_p, type_u, type_a, type_t, type_v)\n\n# Create a grouped bar plot\np1 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Dodge (Beside)\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:dodge,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\np2 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Stack\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:stack,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\nplot(p1,p2)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-problem",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-problem",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test Problem",
    "text": "\\(\\chi^2\\) Dependency Test Problem\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)\n\nWe observe\n\\(X=(X_1, \\dots, X_n) \\in \\{1, \\dots, I\\}^n\\) and \\(Y=(Y_1, \\dots, Y_n) \\in \\{1, \\dots, J\\}^n\\)\n\n\nAssumptions: \\((X_k,Y_k)\\) are independent, each pair has unknown distribution \\(P_{XY}\\)\n\n\ndependency test problem:\n\n\\[H_0: P_{XY}=P_{X}P_Y \\VS H_1: P_{XY} \\neq P_{X}P_{Y}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#definitions",
    "href": "teaching/linear_model/slides/introduction.html#definitions",
    "title": "Introduction",
    "section": "Definitions",
    "text": "Definitions\n\nEntries of the table:\n\n\\[n_{ij} = \\sum_{k=1}^n \\mathbf 1\\{X_{k} = i\\}\\mathbf 1\\{Y_k=j\\}\\]\n\n\n\nTotal proportion of individuals \\(k\\) of color \\(X_k =i\\):\n\n\n\\(\\hat p_{i}=\\frac{R_i}{N}\\) \\(= \\tfrac{1}{N}\\sum_{j=1}^{J}n_{ij}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test",
    "text": "\\(\\chi^2\\) Dependency Test\n\nChi-squared statistic, or chi-squared distance:\n\n\\[\\psi(X,Y) = \\sum_{i=1}^I\\sum_{j=1}^J \\frac{(n_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\]\n\n\nApproximation: \\(\\psi(X,Y) \\sim \\chi^2((I-1)(J-1))\\) when \\(n \\to \\infty\\)\nTest: \\(T=\\mathbf 1\\{\\psi(X,Y) \\geq t_{1-\\alpha/2}\\}\\), where\n\\(t_{0.975}\\) = quantile(Chisq(I-1,J-1), 0.975)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#setting",
    "href": "teaching/linear_model/slides/introduction.html#setting",
    "title": "Introduction",
    "section": "Setting",
    "text": "Setting\n\nWe observe \\(X=(X_1, \\dots, X_n)\\) and \\(Y=(Y_1, \\dots, Y_n)\\), where\n\n\\(X_k \\in \\{1, \\dots, I\\}\\) (Quali)\n\\(Y_k \\in \\mathbb R\\) (Quanti)\n\n\n\nBoxplot: represents \\(0, 25, 75\\) and \\(100\\) percentiles."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#singer-dataset-julia-statsplots",
    "href": "teaching/linear_model/slides/introduction.html#singer-dataset-julia-statsplots",
    "title": "Introduction",
    "section": "Singer Dataset (Julia StatsPlots)",
    "text": "Singer Dataset (Julia StatsPlots)\n\n\\(X\\): Height (in inches), \\(Y\\): Type of singer"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#definitions-1",
    "href": "teaching/linear_model/slides/introduction.html#definitions-1",
    "title": "Introduction",
    "section": "Definitions",
    "text": "Definitions\n\n\\(X=(X_1, \\dots, X_n) \\in \\{1, \\dots, I\\}^n\\), \\(Y=(Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\nif \\(i \\in \\{1, \\dots, I\\}\\), we define partial means as\n\n\n\\[N_i = \\sum_{k=1}^n \\mathbf 1\\{X_k=i\\} \\and \\overline Y_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\mathbf 1\\{X_k=i\\}\\]\n\n\n\n\nTotal mean:\n\\(Y_i = \\frac{1}{N}\\sum_{k=1}^n Y_k = \\frac{1}{N}\\sum_{i=1}^I\\sum N_i \\overline Y_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#variance-decomposition",
    "href": "teaching/linear_model/slides/introduction.html#variance-decomposition",
    "title": "Introduction",
    "section": "Variance Decomposition",
    "text": "Variance Decomposition\n\n\n\n\\[\\frac{1}{n}\\underbrace{\\sum_{k=1}^n(Y_k - \\overline Y)^2}_{SST} =\n\\frac{1}{n}\\underbrace{\\sum_{i=1}^IN_i(\\overline Y_i - \\overline Y)^2}_{SSB}\n+ \\frac{1}{n}\\underbrace{\\sum_{k=1}^n\\mathbf 1\\{X_k=i\\}(Y_k - \\overline Y_i)^2}_{SSW}\\]\n\n\n\n\ncorrelation ratio:\n\n\\[ \\hat \\eta^2 = \\frac{SSB}{SST}  \\in [0,1]\\]\n\n\n\nThis is an estimator of unknown \\(\\eta = \\frac{\\mathbb V(\\mathbb E[Y|X])}{\\mathbb V(Y)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#anova-test",
    "href": "teaching/linear_model/slides/introduction.html#anova-test",
    "title": "Introduction",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\\((X_1, \\dots, X_n) \\in \\{1, \\dots I\\}^n\\)\n\\((Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\nAssumption: \\(Y_k\\) are independent, Gaussian of same variance. \\(\\mu_i = \\mathbb E[Y|X=i]=\\frac{\\mathbb E[Y\\mathbf 1\\{X=i\\}]}{\\mathbb P(X=i)}\\) (unknown)\n\n\nProblem:\n\n\\[H_0: \\mu_1=\\dots \\mu_I \\VS H_1: \\mu_i \\neq \\mu_j \\text{ for some $i,j$}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#anova-test-1",
    "href": "teaching/linear_model/slides/introduction.html#anova-test-1",
    "title": "Introduction",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\nTest Statistic\n\n\\[\\psi(X,Y) = \\frac{SSB/(I-1)}{SSW/(N-I)}\\]\n\n\n\n\\(\\psi(X,Y) \\sim \\mathcal F(I-1, N-I)\\) under \\(H_0\\)\n\n\npvalue:\n1-cdf(FDist(I-1, N-1), psiobs)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#context",
    "href": "teaching/linear_model/slides/introduction.html#context",
    "title": "Introduction",
    "section": "Context",
    "text": "Context\n\n\\(n\\) individuals, \\(p\\) explanatory variables \\(X=(X^{(1)}, \\dots, X^{(p)})\\).\nGoal: Explain/Predict \\(Y\\) in function of \\(X\\)\n\n\nWe observe\n\\[Y=\\begin{pmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X=\\begin{pmatrix}\nX^{(1)}_1 & \\cdots & X^{(p)}_1 \\\\\nX^{(1)}_2 & \\cdots & X^{(p)}_1 \\\\\n\\vdots & & \\vdots \\\\\nX^{(1)}_n & \\cdots & X^{(p)}_1\n\\end{pmatrix}\\]\nEach individual \\(k\\) correspond to \\(Y_k\\) a row \\((X^{(1)}_k, \\dots, X^{(p)}_k)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#where-is-randomness",
    "href": "teaching/linear_model/slides/introduction.html#where-is-randomness",
    "title": "Introduction",
    "section": "Where is Randomness?",
    "text": "Where is Randomness?\nGenerally, we don’t know any values a priori. Example:\n\nindividual characteristics of a customer\n\n\n\\(Y\\) and \\(X^{(1)}, \\ldots, X^{(p)}\\) are random variables.\n\n\nWe observe realizations the \\(Y\\)’s and \\(X\\)’s\n\n\nSometimes \\(X = (X^{(1)}, \\ldots, X^{(p)})\\) is chosen a priori. Example:\n\n\\(X\\): medication dosages (and \\(Y\\): a physiological response)\n\n\n\nIn this context, \\(Y\\) is random, but \\(X^{(1)}, \\ldots, X^{(p)}\\) are not."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#summary",
    "href": "teaching/linear_model/slides/introduction.html#summary",
    "title": "Introduction",
    "section": "Summary:",
    "text": "Summary:\n\n\\(Y\\) is always viewed as a random variable\n\n\n\\(X^{(1)}, \\ldots, X^{(p)}\\) are viewed as random variables or deterministic variables, depending on the context"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#general-model",
    "href": "teaching/linear_model/slides/introduction.html#general-model",
    "title": "Introduction",
    "section": "General Model",
    "text": "General Model\n\n\\(Y=(Y_1, \\dots, Y_n)\\)\n\\(X^{(k)} = (X^{(k)}_1, \\dots, X^{(k)}_n)\\), \\(k= 1, \\dots, p\\) (row notation)\n\n\nGeneral model:\n\n\\[Y_i = F(X_i^{(1)}, \\dots, X_i^{(p)}, \\varepsilon_i)\\]\n\n\n\\(F\\) is an unknown and deterministic function of \\(p\\) variables.\n\\(\\varepsilon_i\\) are iid random representing external independent noise\n\n\n\nNonparametric problem: space of all \\(F\\) is of infinite dimension!"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-model",
    "href": "teaching/linear_model/slides/introduction.html#linear-model",
    "title": "Introduction",
    "section": "Linear Model",
    "text": "Linear Model\n\nIdea: reduce to a smaller class of function \\(F \\in \\mathcal F\\).\n\n\nLinear Model:\n\n\\[\nY_i = \\mu + \\beta_1 X^{(1)}_i + \\beta_2 X^{(2)}_i + \\dots + \\beta_p X^{(p)}_i + \\sigma \\varepsilon_i\n\\]\n\n\n\nSpace of affine function:\n\\[\n\\mathcal F = \\{F:~ F(x, \\varepsilon) = \\mu + \\beta^T x + \\sigma \\varepsilon, (\\mu, \\beta, \\sigma) \\in \\mathbb R^{p+2}\\}\n\\]\n\n\n\\(\\dim(\\mathcal F) = p+2\\) (number of unknown parameters)\n\n\nmuch easier to estimate \\(f\\) (and perhaps less overfitting)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#wait-what-is",
    "href": "teaching/linear_model/slides/introduction.html#wait-what-is",
    "title": "Introduction",
    "section": "Wait, what is + ?",
    "text": "Wait, what is + ?\n\nIf the \\(X^{(k)}\\) are qualitative factors,\n\n\nWhat is the meaning of\n\\[\nY_i = \\mu + \\beta_1 X^{(1)}_i + \\beta_2 X^{(2)}_i + \\dots + \\beta_p X^{(p)}_i + \\sigma \\varepsilon_i\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#case-of-categorical-variables",
    "href": "teaching/linear_model/slides/introduction.html#case-of-categorical-variables",
    "title": "Introduction",
    "section": "Case of categorical variables",
    "text": "Case of categorical variables\n\nEncode each category\n\n\nIf \\(Y \\in \\{A, B\\}\\) has \\(2\\) categories, we encode \\[\\widetilde Y = \\mathbf 1\\{Y = A\\}\\]\n\n\nIf \\(Y\\in \\{A_1, \\dots, A_k\\}\\), we use one hot encoding:\n\\[\\widetilde{Y}_k = \\mathbf 1\\{Y=A_k\\}\\]\n\nAlso encode \\(X^{(1)}, \\ldots, X^{(p)}\\) if needed.\nsee also the chapter on ANOVA and ANCOVA."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x",
    "href": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x",
    "title": "Introduction",
    "section": "From random to deterministic \\(X\\)",
    "text": "From random to deterministic \\(X\\)\n\nIf \\(X^{(1)}, \\ldots, X^{(p)}\\) are random,\n\n\nThen for all deterministic \\(x^{(1)}, \\dots, x^{(p)}\\)\n\n\nConditionnally to \\((X^{(1)}=x^{(1)}, \\dots, X^{(p)} =x^{(p)})\\), we have the general model\n\\[Y = F(x^{(1)},\\dots, x^{(p)}, \\varepsilon)\\]\n\nBecause \\(\\varepsilon\\) is independent of \\(X\\)\nReplace \\(X^{(k)}\\) by their observations \\(x^{(k)}\\).\nThe only randomness is now in \\(\\varepsilon\\)!"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html",
    "href": "teaching/linear_model/notes/cramer-rao.html",
    "title": "Cramér-Rao Bound",
    "section": "",
    "text": "AI was used to assist with the formatting and writing of the proofs on this page."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#setup",
    "href": "teaching/linear_model/notes/cramer-rao.html#setup",
    "title": "Cramér-Rao Bound",
    "section": "Setup",
    "text": "Setup\nLet:\n\n\\(\\beta \\in \\mathbb R^p\\) be a vector of parameters\n\\(X=(X_1, \\dots, X_n) \\in \\mathbb R^n\\) be observations with joint pdf \\(f\\)\n\\(\\tilde \\beta\\) be an unbiased estimator of \\(\\beta\\), so \\(\\mathbb E[\\tilde\\beta]= \\mathbb E_{X\\sim f}[\\tilde\\beta] = \\beta\\)\n\\(s(x; \\beta) = \\nabla_{\\beta} \\log f(x; \\beta)\\) be the derivative of the log-likelihood"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#key-definitions",
    "href": "teaching/linear_model/notes/cramer-rao.html#key-definitions",
    "title": "Cramér-Rao Bound",
    "section": "Key Definitions",
    "text": "Key Definitions\nThe Fisher Information Matrix is: \\[I(\\beta) = E[s(x; \\beta)s(x; \\beta)^T]\\]\nUnder regularity conditions, this equals: \\[I(\\beta) = -E\\left[\\frac{\\partial^2 \\log f(x; \\beta)}{\\partial \\beta \\partial \\beta^T}\\right]\\]\n\n\n\n\n\n\nCramér-Rao (vector version)\n\n\n\nIn this context, it holds that \\[[I(\\beta)]^{-1} \\preceq \\mathbb V(\\tilde \\beta) \\; .\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\\(I(\\beta)\\) and \\(\\mathbb V(\\tilde \\beta)\\) are matrices\n\\(I(\\beta)\\) does not depend on the estimator, unlike \\(\\mathbb V(\\tilde \\beta)\\)."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#matrix-cauchy-schwarz-inequality",
    "href": "teaching/linear_model/notes/cramer-rao.html#matrix-cauchy-schwarz-inequality",
    "title": "Cramér-Rao Bound",
    "section": "Matrix Cauchy-Schwarz Inequality",
    "text": "Matrix Cauchy-Schwarz Inequality\nFor random vectors \\(U \\in \\mathbb R^p\\) and \\(V \\in \\mathbb R^q\\), the covariance satisfies: \\[\\text{Cov}(U, V)^T [\\text{Var}(V)]^{-1} \\text{Cov}(U, V) \\preceq \\text{Var}(U)\\]\nwhere \\(A\\preceq B\\) means \\(B-A\\) is positive semidefinite."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#proof-of-cramér-rao-bound",
    "href": "teaching/linear_model/notes/cramer-rao.html#proof-of-cramér-rao-bound",
    "title": "Cramér-Rao Bound",
    "section": "Proof of Cramér-Rao Bound",
    "text": "Proof of Cramér-Rao Bound\nSince \\(\\tilde \\beta\\) is unbiased, \\(\\mathbb E[\\tilde\\beta]\\) = \\(\\beta\\). Differentiating both sides with respect to \\(\\beta\\): \\[\\frac{\\partial}{\\partial \\beta} \\int \\tilde \\beta(x) f(x; \\beta) dx = I_p\\]\nwhere \\(I_p\\) is the p×p identity matrix.\nBy interchanging differentiation and integration (under regularity conditions): \\[\\int \\tilde \\beta(x) \\big(\\nabla_{\\beta}f(x; \\beta)\\big)^T dx = I_p\\]\nUsing the identity \\(\\nabla_{\\beta} f(x;\\beta)= f(x;\\beta)\\cdot\\nabla_{\\beta} \\log(f(x;\\beta))\\): \\[\\int \\tilde \\beta(x) f(x; \\beta) s(x; \\beta)^T dx = I_p\\]\nThis gives us: \\[E[\\tilde \\beta s^T] = I_p\\]\nSince \\(\\mathbb E[s]=0\\) (under regularity conditions), we have: \\[\\text{Cov}(\\tilde \\beta, s) = E[\\tilde \\beta s^T] - \\mathbb E[\\tilde\\beta]E[s]^T = I_p\\]\nApply the matrix Cauchy-Schwarz inequality with \\(U=\\tilde \\beta\\) and \\(V = s\\): \\[\\text{Cov}(\\tilde \\beta, s)^T [\\text{Var}(s)]^{-1} \\text{Cov}(\\tilde \\beta, s) \\preceq \\text{Var}(\\tilde \\beta)\\]\nSubstituting our results:\n\n\\(\\mathrm{Cov}(\\tilde \\beta, s) = I_p\\)\n\\(\\mathbb V(s) = I(\\beta)\\) (the Fisher Information Matrix)\n\nWe get: \\[I_p^T [I(\\beta)]^{-1} I_p \\preceq \\mathbb V(\\tilde \\beta)\\]\nSimplifying: \\[[I(\\beta)]^{-1} \\preceq \\text{Var}(\\tilde \\beta)\\]\nThis is the Cramér-Rao Lower Bound for vector parameters."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#interpretation",
    "href": "teaching/linear_model/notes/cramer-rao.html#interpretation",
    "title": "Cramér-Rao Bound",
    "section": "Interpretation",
    "text": "Interpretation\n\nFor any unbiased estimator \\(\\tilde \\beta\\) of \\(\\beta\\), its covariance matrix is bounded below by the inverse of the Fisher Information Matrix.\nFor a scalar function \\(c^T \\beta\\), we have: \\(\\mathbb V(c^T \\tilde \\beta) \\succeq c^T[I(\\beta)]^{-1}c\\)\n\nThis generalizes the scalar Cramér-Rao bound to the multivariate case using matrix inequalities."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#theorem",
    "href": "teaching/linear_model/notes/cramer-rao.html#theorem",
    "title": "Cramér-Rao Bound",
    "section": "Theorem",
    "text": "Theorem\nFor random vectors \\(U\\in \\mathbb R^p\\) and \\(V \\in \\mathbb R^q\\) with finite second moments, if \\(\\mathbb V(V)\\) is invertible, then: \\[\\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\text{Cov}(V, U) \\preceq \\text{Var}(U)\\]"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#proof",
    "href": "teaching/linear_model/notes/cramer-rao.html#proof",
    "title": "Cramér-Rao Bound",
    "section": "Proof",
    "text": "Proof\nFor any matrix \\(A \\in \\mathbb R^{p \\times q}\\), consider: \\[\\text{Var}(U - AV) = \\text{Var}(U) - A\\text{Cov}(V, U) - \\text{Cov}(U, V)A^T + A\\text{Var}(V)A^T\\]\nTo minimize this quadratic form in A, take the derivative and set to zero: \\[\\frac{\\partial}{\\partial A} = -2\\text{Cov}(U, V) + 2A\\text{Var}(V) = 0\\]\nSolving gives: \\[A^* = \\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\]\nAt this minimum: \\[\\text{Var}(U - A^*V) = \\text{Var}(U) - \\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\text{Cov}(V, U) \\succeq 0\\]\nThis gives the generalized CS inequality."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#connection-to-scalar-case",
    "href": "teaching/linear_model/notes/cramer-rao.html#connection-to-scalar-case",
    "title": "Cramér-Rao Bound",
    "section": "Connection to Scalar Case",
    "text": "Connection to Scalar Case\nWhen U and V are scalars, this reduces to: \\[\\frac{[\\text{Cov}(U,V)]^2}{\\text{Var}(V)} \\leq \\text{Var}(U)\\]\nWhich is equivalent to the familiar form: \\[[\\text{Cov}(U,V)]^2 \\leq \\text{Var}(U)\\text{Var}(V)\\]\nThe matrix version generalizes this to higher dimensions using positive semidefiniteness instead of simple inequality."
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html",
    "href": "teaching/linear_model/lectures/validation.html",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "If \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R) = q\\), and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR} \\sim F(q, n-p)\\]\nwhere:\n\n\\(SSR\\): sum of squares of residuals in the unconstrained model\n\\(SSR_c\\): sum of squares of residuals in the constrained model satisfying \\(R\\beta = 0\\)\n\n\n\n\n\n\nDefine the following spaces:\n\n\\(V = [X] \\subseteq \\mathbb{R}^n\\) (column space of \\(X\\))\n\\(V_0 = \\{X\\beta : R\\beta = 0\\} = X(\\text{Ker}(R))\\) (constrained subspace)\n\nSince \\(\\text{rank}(X) = p\\) and \\(\\text{rank}(R) = q\\):\n\nBy rank-nullity theorem: \\(\\dim(\\text{Ker}(R)) = p - q\\)\nSince \\(X\\) has full column rank, the map \\(\\beta \\mapsto X\\beta\\) is injective\nTherefore: \\(\\dim(V_0) = \\dim(X(\\text{Ker}(R))) = \\dim(\\text{Ker}(R)) = p - q\\)\nAlso: \\(\\dim(V) = p\\)\n\n\n\n\nSince \\(V_0 \\subseteq V\\), we can decompose \\(V\\) as: \\[V = V_0 \\oplus V_0^{\\perp_V}\\]\nwhere \\(V_0^{\\perp_V}\\) is the orthogonal complement of \\(V_0\\) within \\(V\\), with: \\[\\dim(V_0^{\\perp_V}) = \\dim(V) - \\dim(V_0) = p - (p-q) = q\\]\n\n\n\n\n\\(\\hat{y} = P_V y\\) is the projection of \\(y\\) onto \\(V\\) (unconstrained fit)\n\\(\\hat{y}_c = P_{V_0} y\\) is the projection of \\(y\\) onto \\(V_0\\) (constrained fit)\n\n\n\n\nSince \\(V_0 \\subseteq V\\), we have \\(\\hat{y}_c \\in V\\), and by the Pythagorean theorem: \\[\\|y - \\hat{y}_c\\|^2 = \\|y - \\hat{y}\\|^2 + \\|\\hat{y} - \\hat{y}_c\\|^2\\]\nThis gives us: \\[SSR_c = SSR + \\|\\hat{y} - \\hat{y}_c\\|^2\\]\nSince \\(\\hat{y} - \\hat{y}_c = P_V y - P_{V_0} y = P_{V_0^{\\perp_V}} y\\): \\[SSR_c - SSR = \\|P_{V_0^{\\perp_V}} y\\|^2\\]\n\n\n\nUnder \\(H_0: R\\beta = 0\\), we have \\(\\mathbb{E}[y] = X\\beta \\in V_0\\), which implies: - \\(P_{V_0} \\mathbb{E}[y] = X\\beta\\) - \\(P_{V_0^{\\perp_V}} \\mathbb{E}[y] = 0\\)\nSince \\(y = X\\beta + \\varepsilon\\) with \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\):\nFor the numerator: \\[P_{V_0^{\\perp_V}} y = P_{V_0^{\\perp_V}} \\varepsilon \\sim N(0, \\sigma^2 P_{V_0^{\\perp_V}})\\]\nSince \\(P_{V_0^{\\perp_V}}\\) is a projection onto a \\(q\\)-dimensional space: \\[\\frac{1}{\\sigma^2}\\|P_{V_0^{\\perp_V}} y\\|^2 = \\frac{SSR_c - SSR}{\\sigma^2} \\sim \\chi^2_q\\]\nFor the denominator: \\[P_{V^{\\perp}} y = P_{V^{\\perp}} \\varepsilon \\sim N(0, \\sigma^2 P_{V^{\\perp}})\\]\nSince \\(P_{V^{\\perp}}\\) is a projection onto an \\((n-p)\\)-dimensional space: \\[\\frac{1}{\\sigma^2}\\|P_{V^{\\perp}} y\\|^2 = \\frac{SSR}{\\sigma^2} \\sim \\chi^2_{n-p}\\]\n\n\n\nThe projections \\(P_{V_0^{\\perp_V}}\\) and \\(P_{V^{\\perp}}\\) are orthogonal because:\n\n\\(V_0^{\\perp_V} \\subseteq V\\)\n\\(V \\perp V^{\\perp}\\)\nTherefore \\(V_0^{\\perp_V} \\perp V^{\\perp}\\)\n\nThis implies \\(P_{V_0^{\\perp_V}} \\varepsilon\\) and \\(P_{V^{\\perp}} \\varepsilon\\) are independent.\n\n\n\nThe F-statistic is: \\[F = \\frac{(SSR_c - SSR)/q}{SSR/(n-p)} = \\frac{\\chi^2_q/q}{\\chi^2_{n-p}/(n-p)}\\]\nSince this is the ratio of two independent chi-squared random variables divided by their respective degrees of freedom, we have: \\[F \\sim F(q, n-p)\\]\n\n\n\n\nThe F-statistic measures the relative magnitude of:\n\nThe projection onto \\(V_0^{\\perp_V}\\) (the constraint violation space within the model)\nThe projection onto \\(V^{\\perp}\\) (the residual space)\n\nUnder \\(H_0\\), both projections capture only noise, leading to the F-distribution. Large values of \\(F\\) suggest the constraint \\(R\\beta = 0\\) is violated."
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html#theorem",
    "href": "teaching/linear_model/lectures/validation.html#theorem",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "If \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R) = q\\), and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR} \\sim F(q, n-p)\\]\nwhere:\n\n\\(SSR\\): sum of squares of residuals in the unconstrained model\n\\(SSR_c\\): sum of squares of residuals in the constrained model satisfying \\(R\\beta = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html#proof",
    "href": "teaching/linear_model/lectures/validation.html#proof",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "Define the following spaces:\n\n\\(V = [X] \\subseteq \\mathbb{R}^n\\) (column space of \\(X\\))\n\\(V_0 = \\{X\\beta : R\\beta = 0\\} = X(\\text{Ker}(R))\\) (constrained subspace)\n\nSince \\(\\text{rank}(X) = p\\) and \\(\\text{rank}(R) = q\\):\n\nBy rank-nullity theorem: \\(\\dim(\\text{Ker}(R)) = p - q\\)\nSince \\(X\\) has full column rank, the map \\(\\beta \\mapsto X\\beta\\) is injective\nTherefore: \\(\\dim(V_0) = \\dim(X(\\text{Ker}(R))) = \\dim(\\text{Ker}(R)) = p - q\\)\nAlso: \\(\\dim(V) = p\\)\n\n\n\n\nSince \\(V_0 \\subseteq V\\), we can decompose \\(V\\) as: \\[V = V_0 \\oplus V_0^{\\perp_V}\\]\nwhere \\(V_0^{\\perp_V}\\) is the orthogonal complement of \\(V_0\\) within \\(V\\), with: \\[\\dim(V_0^{\\perp_V}) = \\dim(V) - \\dim(V_0) = p - (p-q) = q\\]\n\n\n\n\n\\(\\hat{y} = P_V y\\) is the projection of \\(y\\) onto \\(V\\) (unconstrained fit)\n\\(\\hat{y}_c = P_{V_0} y\\) is the projection of \\(y\\) onto \\(V_0\\) (constrained fit)\n\n\n\n\nSince \\(V_0 \\subseteq V\\), we have \\(\\hat{y}_c \\in V\\), and by the Pythagorean theorem: \\[\\|y - \\hat{y}_c\\|^2 = \\|y - \\hat{y}\\|^2 + \\|\\hat{y} - \\hat{y}_c\\|^2\\]\nThis gives us: \\[SSR_c = SSR + \\|\\hat{y} - \\hat{y}_c\\|^2\\]\nSince \\(\\hat{y} - \\hat{y}_c = P_V y - P_{V_0} y = P_{V_0^{\\perp_V}} y\\): \\[SSR_c - SSR = \\|P_{V_0^{\\perp_V}} y\\|^2\\]\n\n\n\nUnder \\(H_0: R\\beta = 0\\), we have \\(\\mathbb{E}[y] = X\\beta \\in V_0\\), which implies: - \\(P_{V_0} \\mathbb{E}[y] = X\\beta\\) - \\(P_{V_0^{\\perp_V}} \\mathbb{E}[y] = 0\\)\nSince \\(y = X\\beta + \\varepsilon\\) with \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\):\nFor the numerator: \\[P_{V_0^{\\perp_V}} y = P_{V_0^{\\perp_V}} \\varepsilon \\sim N(0, \\sigma^2 P_{V_0^{\\perp_V}})\\]\nSince \\(P_{V_0^{\\perp_V}}\\) is a projection onto a \\(q\\)-dimensional space: \\[\\frac{1}{\\sigma^2}\\|P_{V_0^{\\perp_V}} y\\|^2 = \\frac{SSR_c - SSR}{\\sigma^2} \\sim \\chi^2_q\\]\nFor the denominator: \\[P_{V^{\\perp}} y = P_{V^{\\perp}} \\varepsilon \\sim N(0, \\sigma^2 P_{V^{\\perp}})\\]\nSince \\(P_{V^{\\perp}}\\) is a projection onto an \\((n-p)\\)-dimensional space: \\[\\frac{1}{\\sigma^2}\\|P_{V^{\\perp}} y\\|^2 = \\frac{SSR}{\\sigma^2} \\sim \\chi^2_{n-p}\\]\n\n\n\nThe projections \\(P_{V_0^{\\perp_V}}\\) and \\(P_{V^{\\perp}}\\) are orthogonal because:\n\n\\(V_0^{\\perp_V} \\subseteq V\\)\n\\(V \\perp V^{\\perp}\\)\nTherefore \\(V_0^{\\perp_V} \\perp V^{\\perp}\\)\n\nThis implies \\(P_{V_0^{\\perp_V}} \\varepsilon\\) and \\(P_{V^{\\perp}} \\varepsilon\\) are independent.\n\n\n\nThe F-statistic is: \\[F = \\frac{(SSR_c - SSR)/q}{SSR/(n-p)} = \\frac{\\chi^2_q/q}{\\chi^2_{n-p}/(n-p)}\\]\nSince this is the ratio of two independent chi-squared random variables divided by their respective degrees of freedom, we have: \\[F \\sim F(q, n-p)\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html#geometric-interpretation",
    "href": "teaching/linear_model/lectures/validation.html#geometric-interpretation",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "The F-statistic measures the relative magnitude of:\n\nThe projection onto \\(V_0^{\\perp_V}\\) (the constraint violation space within the model)\nThe projection onto \\(V^{\\perp}\\) (the residual space)\n\nUnder \\(H_0\\), both projections capture only noise, leading to the F-distribution. Large values of \\(F\\) suggest the constraint \\(R\\beta = 0\\) is violated."
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#introduction",
    "href": "research/presentations/crowdsourcing/presentation_long.html#introduction",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Introduction",
    "text": "Introduction\n\nIt’s a great privilege to address you today and present my latest research on crowdsourcing problems.\nFor context, I conducted this research primarily at ENS Lyon as a postdoctoral researcher, building on my previous PhD work in crowdsourcing.\nImagine a group of workers assigned binary classification tasks.\nThey must provide binary responses: YES or NO.\nThis scenario applies to many practical situations\n\nworkers might be paid to perform image classification, text moderation, sentiment analysis, or data verification tasks.\n\n\n\nWorkers are given binary tasks to which they have to give a response: YES or NO\nExamples\n\nImage Classification: “Does this image contain a dog?”\nText Moderation: “Is this comment toxic or offensive?”\nSentiment Analysis: “Does this review express positive sentiment?”\nData Verification: “Is this information factually correct?”"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#objectives",
    "href": "research/presentations/crowdsourcing/presentation_long.html#objectives",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Objectives",
    "text": "Objectives\n\nGiven their responses, three natural objectives emerge\n\nwhat are the actual true labels?\nCan we compare workers to identify which perform better or worse?\nHow well do workers perform on a given task?\n\nI will touch on all these issues in my talk. However, my primary focus will be on the main question of recovering the true labels.\n\n\nGiven their responses, we have 3 objectives\n\nrecover the true label\nrank the workers\nestimate their abilities\n\n\n\nGiven \\(n\\) workers and \\(d\\) binary tasks\n\n\nA proportion \\(\\lambda\\) of observations\n\n\nMain Quetion: How can we accurately recover the labels?"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#this-talk",
    "href": "research/presentations/crowdsourcing/presentation_long.html#this-talk",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "This Talk",
    "text": "This Talk\n\nMy talk will break down this problem into three parts.\nI’ll begin by introducing the non-parametric isotonic model, since it offers a framework for addressing all three problems.\nNext, I’ll review existing methods, as I believe understanding these approaches is essential to grasp the key insights behind the method I introduce in my paper.\nThe final section will focus on my contribution - the new method I’ve developed and some of the key ideas that emerge from it.\n\n\nIntroducing the non-parametric isotonic model\nPresenting already existing algorithms\nIterative Spectral Voting (ISV) algo and Key Insights"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#illustration",
    "href": "research/presentations/crowdsourcing/presentation_long.html#illustration",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration",
    "text": "Illustration\n\nLet me start with a concrete example to illustrate the type of data we’re working with.\nConsider binary tasks where labels are either \\(-1\\) or \\(+1\\). If we have \\(9\\) tasks, there is a vector \\(x^*\\) of true labels with length \\(9\\).\nWe observe a matrix \\(Y\\) where each entry \\((i,k)\\) represents the response of worker \\(i\\) to task \\(k\\).\nWe model partial observations with rate \\(\\lambda \\in [0,1]\\) — when a worker doesn’t respond to a particular task, we simply put \\(0\\) in that matrix position.\n\n\nTwo binary tasks with labels in \\(\\{\\color{green}{-1},\\color{blue}{+1}\\}\\)\nUnknown vector \\(x^*\\) of labels with \\(d=9\\) tasks \\(x^*= (\\color{blue}{+1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1})\\)\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1}\n\\end{array}\\right)\n\\]\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{red}{0} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0}\n\\end{array}\\right)\n\\]\n\n\n\n\nRate of observations: \\(\\color{red}{\\lambda= 0.72}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#observation-model",
    "href": "research/presentations/crowdsourcing/presentation_long.html#observation-model",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nThe underlying statistical model can be described as follows. Given a worker \\(i\\) and a task \\(k\\), we observe their response \\(Y_{ik}\\), and we assume this relation:\n\n\\(x^*\\) is the vector of unknown true labels\n\\(M_{ik} \\in [0,1]\\) represents the unknown ability of worker \\(i\\) on task \\(k\\). For the best experts or easiest tasks, we have \\(M_{ik} = 1\\)\n\\(E_{ik}\\) is standard sub-Gaussian noise\n\\(B_{ik}\\) is a \\(0\\)-\\(1\\) coefficient that indicates whether we observe \\((i,k)\\) or not\n\n\n\nGiven workers \\(i\\in\\{1, \\dots, n\\}\\) and tasks \\(k\\in\\{1, \\dots, d\\}\\)\n\n\nWe observe \\(Y_{ik} \\in \\{-1,0,1\\}\\)\n\n\\[\nY_{ik} = B_{ik}(M_{ik}x_k^* + E_{ik}) \\enspace\n\\]\n\n\n\\(x_k^*\\in \\{-1,1\\}\\) true response of task \\(k\\)\n\\(M_{ik} \\in [0,1]\\) represents the unknown ability of worker \\(i\\) to task \\(k\\)\n\\(E_{ik}\\) are independent and \\(1\\)-subGaussian noise\n\\(B_{ik}\\) are iid Bernoulli \\(\\lambda \\in [0,1]\\). \\(=1\\) if \\((i,k)\\) is observed"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#alernative-formulation",
    "href": "research/presentations/crowdsourcing/presentation_long.html#alernative-formulation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Alernative Formulation",
    "text": "Alernative Formulation\n\nIn matrix form, we can rewrite the model as follows […] where \\(\\odot\\) is the coordinate-wise product between matrices.\nThis is equivalent to what I said before, except here \\(M\\), \\(E\\), and \\(B\\) are matrices. \\(B\\) can be thought of as a mask matrix.\n\n\nWe observe\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\nwhere \\(\\odot\\) is the Hadamard product.\n\n\n\n\\(M \\in [0,1]^{n \\times d}\\) is the ability matrix\n\\(x^*\\) is the vector of unknown labels\n\\(E\\) is a matrix of independent noise\n\\(B\\) is a Bernoulli “mask” matrix, modelling partial observations"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#bernoulli-observation-submodel",
    "href": "research/presentations/crowdsourcing/presentation_long.html#bernoulli-observation-submodel",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Bernoulli Observation Submodel",
    "text": "Bernoulli Observation Submodel\n\nAn important example is the Bernoulli Observation Model.\nThis is the general model I just presented, and this is the Bernoulli Model, introduced by Shah et al. in 2020.\n\nIt assumes that each worker has probability \\((1+M_{ik})/2\\) of being correct on task \\(k\\)\nWe observe each response with probability \\(\\lambda\\)\n\nThis is a submodel because Bernoulli variables are indeed \\(1\\)-subgaussian.\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\n\n\n\nBernoulli submodel (Shah et al., 2020)\n\n\n\\[\n\\begin{aligned}\\label{eq:bernoulli_model}\n    Y_{ik} = \\begin{cases}\n        x^*_k \\text{ with proba } \\lambda\\left(\\frac{1+M_{ik}}{2}\\right)\n        \\\\\n        -x^*_k \\text{ with proba } \\lambda\\left(\\frac{1-M_{ik}}{2}\\right)\\\\\n        0 \\text{ with proba } 1-\\lambda\n        \\end{cases}\n\\end{aligned}\n\\]\n\n\n\n\n\n\\(\\frac{1+M_{ik}}{2}\\) is the proba that \\(i\\) answers correctly to task \\(k\\).\n\n\n\\(\\lambda \\in[0,1]\\) is the probability of observing worker/task pair \\((i,k)\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#shape-constraints",
    "href": "research/presentations/crowdsourcing/presentation_long.html#shape-constraints",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nUntil now, I have’nt assumed anything on the ability matrix \\(M\\) except that it has coef. in \\([0,1]\\).\nFrom now on, we assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) of its rows, that is,\nit has increasing columns, up to an unknown permutation \\(\\pi^*\\)\n\n\n\\(M \\in [0,1]^{n \\times d}\\) represents the ability matrix of worker/task pairs\nWe assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) , that is\n\n\n\n\\[\nM_{\\pi^*}=\\left(\\begin{array}{ccccccccc}\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8} \\\\\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6}\n\\end{array}\\right)\n\\]\n\n\n\\[\nM_{\\phantom{{\\pi^*}}}=\\left(\\begin{array}{ccccccccc}\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6} \\\\\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#shape-constraints-1",
    "href": "research/presentations/crowdsourcing/presentation_long.html#shape-constraints-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nMore formally, the isotonicity constraint can be written as this set of inequalities.\nIt means that the rows of \\(M\\) are uniformly increasing.\nFor any two workers \\(i\\) and \\(j\\), either \\(i\\) or \\(j\\) is uniformly better than the other one.\n\n\nFor all \\(k = 1, \\dots, d\\),\n\n\n\n\\[\nM_{\\pi^*(1), k}\\geq \\dots \\geq M_{\\pi^*(n),k}\n\\]\n\n\nIt means that the rows are uniformly increasing\nA worker \\(i\\) is better on average than \\(j\\), if it is better on all tasks on average"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#illustration-when-x-is-known",
    "href": "research/presentations/crowdsourcing/presentation_long.html#illustration-when-x-is-known",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration when \\(x^*\\) is Known",
    "text": "Illustration when \\(x^*\\) is Known\n\nLet me illustrate the isotonicity assumption with this picture. This is an isotonic matrix represented in a scale of grays. This is a permuted isotonic matrix, and this is what we might observe.\nNow I’m mostly interested in the statistical limit for finding labels, that is, where the noise is of the same scale as the coefficients of \\(M\\), and when the picture becomes more blurry."
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#square-norm-loss-vs-hamming-loss",
    "href": "research/presentations/crowdsourcing/presentation_long.html#square-norm-loss-vs-hamming-loss",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Square Norm Loss VS Hamming Loss",
    "text": "Square Norm Loss VS Hamming Loss\n\nNow let move on to the definition of the Square norm loss for finding \\(x^*\\).\nTo motivate its definition, let me recall that the Hamming loss consists simply in summing up all the mistakes we made in the estimation of the true labels.\nFor the square norm loss, we rather take the frobenius norm of \\(M\\), restricted to the column corresponding to tasks on which we did a mistake estimating \\(x^*_k\\).\n\nIf workers are bad (\\(M\\) close to \\(0\\)), square norm loss is small but Hamming Loss can be of order \\(d\\)\nThe square norm loss is better for my purpose, because it evaluates the quality of the estimator \\(\\hat x\\) instead of the performance of the workers!\n\n\n\n\n\n\nHamming Loss: \\[ \\sum_{k=1}^d \\mathbf 1\\{\\hat x_k \\neq x_k^*\\}\\]\n\n\n\n\n\nSquare Norm Loss: \\[ \\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\]\n\n\n\n\nIf workers are bad, \\(M\\) is small but Hamming Loss is large\nIf \\(M \\sim 0\\), Hamming loss \\(\\sim d\\)\nSquare norm loss evaluates the quality of \\(\\hat x\\) rather than workers"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#crowdsourcing-problems",
    "href": "research/presentations/crowdsourcing/presentation_long.html#crowdsourcing-problems",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\nA good feature of the isotonic model is that in addition to recovering labels, we can define two other objectives: ranking the workers and estimating their abilities. Each objective corresponds to a similar squre norm loss.\nThese three objectives are closely connected in practice.\n\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\n\nObjectives\n\n\nRecovering the labels (\\(x^*\\))\n\n\nRanking the workers (\\(\\pi^*\\))\n\n\nEstimating their abilities (\\(M\\))\n\n\n\nLosses\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\)\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2\\)\n\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M - \\hat M\\|_F^2\\)\n\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#crowdsourcing-problems-1",
    "href": "research/presentations/crowdsourcing/presentation_long.html#crowdsourcing-problems-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\nObjectives\nRecovering the labels (\\(x^*\\))\n\nRanking the workers (\\(\\pi^*\\))\n\nEstimating their abilities (\\(M\\))\n\nRisks\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#minimax-risk-for-recovering-labels",
    "href": "research/presentations/crowdsourcing/presentation_long.html#minimax-risk-for-recovering-labels",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "MiniMax Risk for Recovering Labels",
    "text": "MiniMax Risk for Recovering Labels\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\n\nmaximize squre norm loss on all \\(M\\), \\(\\pi^*\\), \\(x^*\\) such that \\(M_{\\pi^*}\\) is isotonic\nminimize on all estimator \\(\\hat x\\)\nSimilarly, we can define minimax ranking risk and estimation risks"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#short-story",
    "href": "research/presentations/crowdsourcing/presentation_long.html#short-story",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Short Story",
    "text": "Short Story\n\n(Shah et al., 2020): recovering \\(x^*\\) optimally using a least square method, conjectured NP hard (\\(x^*\\) unknown, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic).\n\n\n(Mao et al., 2020): estimating abilities \\(M\\) of workers optimally with least square method (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Liu & Moitra, 2020): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): improve state of the art poly. time (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Pilliat et al., 2024): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): achieves rates of Liu & Moitra (2020) without bi-isotonic assumption (\\(x^*\\) known, \\(M_{\\pi^*}\\) isotonic)\n\n\nThis paper: recovering \\(x^*\\), ranking \\(\\pi^*\\) and estimating abilities \\(M\\) in poly. time when \\(n=d\\) (\\(x^*\\) unknown, \\(M_{\\pi^*}\\) isotonic)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#main-results",
    "href": "research/presentations/crowdsourcing/presentation_long.html#main-results",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Results",
    "text": "Main Results\n\n\n\n\nTheorem\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), there exists a poly. time method \\(\\hat x\\) achieving \\(\\mathcal R^*_{\\mathrm{reco}}\\) up to polylog factors, i.e. \\[\n\\mathcal R_{\\mathrm{reco}}(n,d,\\lambda, \\hat x) \\lesssim \\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\enspace .\n\\]\nMoreover, up to polylogs,\n\\[\n\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\asymp \\frac{d}{\\lambda} \\enspace .\n\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#majority-vote",
    "href": "research/presentations/crowdsourcing/presentation_long.html#majority-vote",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Majority Vote",
    "text": "Majority Vote\n\n\\[ \\hat x^{(maj)}_k = \\mathrm{sign} \\left( \\sum_{i=1}^n Y_{ik} \\right) \\enspace .\\]\n\n\n\n\n\nMax risk of majority vote\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n, d, \\lambda, \\hat x^{(maj)}) \\asymp \\tfrac{d \\sqrt{n}}{\\lambda}\\]\n\n\n\n\n\nWorst case (\\(\\lambda=1\\)): \\(M \\asymp \\frac{1}{\\sqrt{n}}(\\mathbf 1_{n\\times d})\\)\n\n\nIn this case, \\(\\hat x^{(maj)}\\) is not much better than random labelling and \\(\\|M\\mathrm{diag}(\\hat x \\neq x^*)\\|_F^2 \\asymp d\\sqrt{n}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#obi-wan-shah2020permutation",
    "href": "research/presentations/crowdsourcing/presentation_long.html#obi-wan-shah2020permutation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "OBI-WAN (Shah et al., 2020)",
    "text": "OBI-WAN (Shah et al., 2020)\n\nIdea: Population term \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T\\) is independent of \\(x^*\\)\n\n\nPCA Step:\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]\n\n\n\n(Shah et al., 2020) sort \\(|\\hat v|\\) to get a partial ranking\n\n\nAggregation: Majority vote on \\(k\\) top experts according to \\(|\\hat v|\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#result-on-obi-wan-method",
    "href": "research/presentations/crowdsourcing/presentation_long.html#result-on-obi-wan-method",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Result on Obi-Wan Method",
    "text": "Result on Obi-Wan Method\n\n\n\n\n\nTheorem 1 (Shah et al., 2020)\n\n\nIn submodel where \\(\\mathrm{rk}(M)= 1\\), \\(\\hat x^{(\\mathrm{Obi-Wan})}\\) achieves minimax risk up to polylogs: \\[\\mathbb E\\|M\\mathrm{diag}(\\hat x^{\\text{Obi-Wan}} \\neq x^*)\\|_F^2 \\lesssim \\frac{d}{\\lambda} \\quad \\textbf{(minimax)}\\]\n\n\n\n\n\n\n\n\n\n\nTheorem 2 (Shah et al., 2020)\n\n\nIn the model where \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic up to polylogs: \\[\\mathbb E\\|M\\mathrm{diag}(\\hat x^{\\text{Obi-Wan}} \\neq x^*)\\|_F^2 \\lesssim \\frac{\\sqrt{n}d}{\\lambda} \\quad \\textbf{(not minimax)}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#pca-step",
    "href": "research/presentations/crowdsourcing/presentation_long.html#pca-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "PCA Step",
    "text": "PCA Step\n\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y^{(1)}\\|^2 \\quad \\text{and} \\quad \\tilde v = \\hat v \\land \\sqrt{\\lambda / T}\\]\n\n\n\nMain idea: if \\(M\\) is isotonic, then up to a polylog\n\n\\[\\|MM^T\\|_{\\mathrm{op}} \\gtrsim \\|M\\|_F^2\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#voting-step",
    "href": "research/presentations/crowdsourcing/presentation_long.html#voting-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Voting Step",
    "text": "Voting Step\n\nDefine the weighted vote vector\n\n\\[\\hat w = \\tilde v^T Y^{(2)}\\]\n\n\n\nDefine the estimated label as\n\n\\[\\hat x_k^{(1)} = \\mathrm{sign}(\\hat w_k)\\mathbf{1}\\bigg\\{|\\hat w_k| \\gg \\sqrt{\\sum_{i=1}^n \\tilde v_i B_{ik}^{(2)}}\\bigg\\}\\]"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#iterate",
    "href": "research/presentations/crowdsourcing/presentation_long.html#iterate",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Iterate",
    "text": "Iterate\n\nKeep certain labels: if \\(\\hat x^{(t)}\\neq 0\\), set \\(\\hat x^{(t+1)}= \\hat x^{(t)}\\).\n\n\nRestrict columns of \\(Y\\) to uncertain labels \\(\\hat x^{(t)}=0\\)\n\n\nRepeat PCA Step + Voting Step on \\(Y \\mathrm{diag}(\\hat x^{(t)} \\neq 0)\\) a polylogarithmic number of times.\n\n\nOutput last estimator \\(\\hat x^{(T)}\\)\n\\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#proof-idea",
    "href": "research/presentations/crowdsourcing/presentation_long.html#proof-idea",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Proof Idea",
    "text": "Proof Idea\n\nLet \\(M(t) = M\\mathrm{diag}(\\hat x^{(t-1)} = 0)\\)\n\n\nWhile \\(M(t) \\gg d/\\lambda\\), we prove that\n\n\n\\[\\|\\tilde v^TM(t)\\|_2^2 \\gtrsim \\|M\\|_F^2 \\and \\|\\tilde v^TM(t+1)\\|_2^2 \\lesssim d/\\lambda\\]\n\n\n\n\nBy Pythagoeran Theorem, we have\n\n\n\\[\\|M(t)\\|_F^2 - \\|M(t+1)\\|_F^2 \\geq \\|\\tilde v M(t)\\|_2^2 - \\|\\tilde v^TM(t+1)\\|_2^2\\]\n\n\n\n\nThis leads to exponential decay of \\(\\|M(t)\\|_F^2\\) until \\(M(t) \\leq d/\\lambda\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#summary",
    "href": "research/presentations/crowdsourcing/presentation_long.html#summary",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Summary",
    "text": "Summary\n\nThe core of my presentation can be captured in three key points.\n\nFirst, the isotonic model is a very flexible, non-parametric framework for describing crowdsourcing data and tackling tasks such as recovering labels and ranking workers.\nSecondly, ISV method is computationally feasible and achieves minimax rates in most interesting regimes.\nLastly, and this is a surprising result: not knowing the true labels does not make the problem of ranking workers any harder from a statistical perspective\n\n\n\nNon parametric isotonic model very flexible in crowdsourcing problems\nMinimax and polynomial time method ISV for recovering labels, ranking and estimating abiliti (at least when \\(n=d\\))\nNot knowing the labels is not harder than knowing them for ranking workers"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#main-insights",
    "href": "research/presentations/crowdsourcing/presentation_long.html#main-insights",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Insights",
    "text": "Main Insights\n\nSpectral method because \\(\\|M\\mathrm{diag}(x^*)(M\\mathrm{diag}(x^*))^T\\|_{\\mathrm{op}}=\\|MM^T\\|_{\\mathrm{op}}\\) does not depend on \\(x^*\\)\nIterate to reduce remaining square norm loss \\(\\|M\\mathrm{diag}(x^* \\neq \\hat x^{(t)})\\|_F^2\\)\nBecause \\(\\|M\\|^2_{\\mathrm{op}}\\gtrsim\\|M\\|_F^2\\)"
  },
  {
    "objectID": "research/presentations/crowdsourcing/presentation_long.html#whats-next",
    "href": "research/presentations/crowdsourcing/presentation_long.html#whats-next",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "What’s Next?",
    "text": "What’s Next?\n\n\nCan we do better if we are allowed to select worker task pairs \\((i,k)\\) based on past information?"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problem-with-binary-tasks",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problem-with-binary-tasks",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problem with Binary Tasks",
    "text": "Crowdsourcing Problem with Binary Tasks\n\nImage Classification: “Does this image contain a dog?”\nText Moderation: “Is this comment toxic or offensive?”\nSentiment Analysis: “Does this review express positive sentiment?”\nData Verification: “Is this information factually correct?”"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#main-question",
    "href": "research/crowdsourcing/presentation.html#main-question",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Question",
    "text": "Main Question\n\nGiven \\(n\\) workers and \\(d\\) binary tasks\n\n\nA proportion \\(\\lambda\\) of observations\n\n\nHow can we accurately recover the labels?"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#illustration",
    "href": "research/crowdsourcing/presentation.html#illustration",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration",
    "text": "Illustration\n\nTwo binary tasks with labels in \\(\\{\\color{green}{-1},\\color{blue}{+1}\\}\\)\nUnknown vector \\(x^*\\) of labels with \\(d=9\\) tasks \\(x^*= (\\color{blue}{+1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1})\\)\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1}\n\\end{array}\\right)\n\\]\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{red}{0} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0}\n\\end{array}\\right)\n\\]\n\n\n\n\nRate of observations: \\(\\color{red}{\\lambda= 0.72}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#observation-model",
    "href": "research/crowdsourcing/presentation.html#observation-model",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nGiven workers \\(i\\in\\{1, \\dots, n\\}\\) and tasks \\(k\\in\\{1, \\dots, d\\}\\)\n\n\nWe observe\n\n\\[\nY_{ik} = B_{ik}(M_{ik}x_k^* + E_{ik}) \\enspace\n\\]\n\n\nWhere:\n\n\\(M_{ik} \\in [0,1]\\)\n\\(E_{ik}\\) are independent and \\(1\\)-subGaussian\n\\(B_{ik}\\) are iid Bernoulli \\(\\lambda \\in [0,1]\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#observation-model-1",
    "href": "research/crowdsourcing/presentation.html#observation-model-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nWe observe\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\nwhere \\(\\odot\\) is the Hadamard product.\n\n\\(M \\in [0,1]^{n \\times d}\\) is the ability matrix\n\\(x^*\\) is the vector of unknown labels\n\\(E\\) is a matrix of independent noise\n\\(B\\) is a Bernoulli “mask” matrix, modelling partial observations"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#bernoulli-observation-submodel",
    "href": "research/crowdsourcing/presentation.html#bernoulli-observation-submodel",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Bernoulli Observation SubModel",
    "text": "Bernoulli Observation SubModel\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\n\n\n\nBernoulli submodel (Shah et al., 2020)\n\n\n\\[\n\\begin{aligned}\\label{eq:bernoulli_model}\n    Y_{ik} = \\begin{cases}\n        x^*_k \\text{ with proba } \\lambda\\left(\\frac{1+M_{ik}}{2}\\right)\n        \\\\\n        -x^*_k \\text{ with proba } \\lambda\\left(\\frac{1-M_{ik}}{2}\\right)\\\\\n        0 \\text{ with proba } 1-\\lambda\n        \\end{cases}\n\\end{aligned}\n\\]\n\n\n\n\n\n\\(\\frac{1+M_{ik}}{2}\\) is the proba that \\(i\\) answers correctly to task \\(k\\).\n\n\n\\(\\lambda\\) is the probability of observing worker/task pair \\((i,k)\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#shape-constraints",
    "href": "research/crowdsourcing/presentation.html#shape-constraints",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\n\\(M \\in [0,1]^{n \\times d}\\) represents the ability matrix of worker/task pairs\nWe assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) , that is\n\n\n\n\\[\nM_{\\pi^*}=\\left(\\begin{array}{ccccccccc}\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8} \\\\\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6}\n\\end{array}\\right)\n\\]\n\n\n\\[\nM_{\\phantom{{\\pi^*}}}=\\left(\\begin{array}{ccccccccc}\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6} \\\\\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#shape-constraints-1",
    "href": "research/crowdsourcing/presentation.html#shape-constraints-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nFor all \\(k = 1, \\dots, d\\), \\[\nM_{\\pi^*(1), k}\\geq \\dots \\geq M_{\\pi^*(n),k}\n\\]\nIt means that the rows are uniformly increasing\nA worker \\(i\\) is better on average than \\(j\\), if it is better on all tasks on average"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#illustration-when-x-is-known",
    "href": "research/crowdsourcing/presentation.html#illustration-when-x-is-known",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration when \\(x^*\\) is Known",
    "text": "Illustration when \\(x^*\\) is Known"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problems",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problems",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\n\nObjectives\n\n\nRecovering the labels\n\n\nRanking the workers\n\n\nEstimating their abilities\n\n\n\nLosses\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\)\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2\\)\n\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M - \\hat M\\|_F^2\\)\n\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\nObjectives\nRecovering the labels\n\nRanking the workers\n\nEstimating their abilities\n\nRisks\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "href": "research/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Square Norm Loss VS Hamming Loss",
    "text": "Square Norm Loss VS Hamming Loss\n\n\n\n\nHamming Loss: \\[ \\sum_{k=1}^d \\mathbf 1\\{\\hat x_k \\neq x_k^*\\}\\]\n\n\n\n\n\nSquare Norm Loss: \\[ \\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\]\n\n\n\n\nIf workers are bad, \\(M\\) is small but Hamming Loss is large\nIf \\(M \\sim 0\\), Hamming loss \\(\\sim d\\)\nSquare norm loss evaluates the quality of \\(\\hat x\\) rather than workers"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#other-parametric-models",
    "href": "research/crowdsourcing/presentation.html#other-parametric-models",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Other Parametric Models",
    "text": "Other Parametric Models\n\nDS: \\(M_{ik} = q_i\\) (Dawid & Skene, 1979), (Shah et al., 2020)\n\nThe abilities are independent of the tasks\n\nBTL: \\(M_{ik}=\\phi(a_i-b_k)\\) (Bradley & Terry, 1952)\n\n\\(a_i\\): abilities of the workers\n\\(b_i\\): difficulties of the tasks\n\n\n\nThese parametric models often fail to fit data well"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#non-parametric-models",
    "href": "research/crowdsourcing/presentation.html#non-parametric-models",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Non-Parametric Models",
    "text": "Non-Parametric Models\n\nknown labels \\(x^*\\), \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic (Mao et al., 2020) (Liu & Moitra, 2020)\nknown labels \\(x^*\\), \\(M_{\\pi^*}\\) is isotonic (Flammarion et al., 2019) (Pilliat et al., 2024)\nunknown labels \\(x^*\\), \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic (Shah et al., 2020)\nunknown labels \\(x^*\\), \\(M_{\\pi^*}\\) is isotonic (this paper)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#risks-recap",
    "href": "research/crowdsourcing/presentation.html#risks-recap",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Risks Recap",
    "text": "Risks Recap\n\n\n\n\nObjectives\n\n\nRecovering the labels\n\nRanking the workers\n\nEstimating their abilities\n\nRisks\n\n\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#minimax-recovering-risk",
    "href": "research/crowdsourcing/presentation.html#minimax-recovering-risk",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "MiniMax Recovering Risk",
    "text": "MiniMax Recovering Risk\n\n\n\n\nMax risk for recovering labels\n\n\n\\[\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\nmaximize on all \\(M\\), \\(\\pi^*\\), \\(x^*\\) such that \\(M_{\\pi^*}\\) is isotonic\n\n\n\n\n\n\n\n\n\nMiniMax risk for recovering labels\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\nminimize on all estimator \\(\\hat x\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#minimax-risks",
    "href": "research/crowdsourcing/presentation.html#minimax-risks",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Minimax Risks",
    "text": "Minimax Risks\n\n\n\n\nRecovering labels\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\n\n\n\n\n\n\n\nRanking workers\n\n\n\\[\\mathcal R^*_{\\mathrm{rk}}(n,d,\\lambda)=\\min_{\\hat \\pi}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\]\n\n\n\n\n\n\n\n\nEstimating abilities\n\n\n\\[\\mathcal R^*_{\\mathrm{est}}(n,d,\\lambda)=\\min_{\\hat M}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M- \\hat M\\|_F^2]\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#short-story",
    "href": "research/crowdsourcing/presentation.html#short-story",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Short Story",
    "text": "Short Story\n\n(Shah et al., 2020): recovering \\(x^*\\) optimally using a least square method, conjectured NP hard (\\(x^*\\) unknown, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic).\n\n\n(Mao et al., 2020): estimating abilities \\(M\\) of workers optimally with least square method (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Liu & Moitra, 2020): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): improve state of the art poly. time (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Pilliat et al., 2024): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): achieves rates of Liu & Moitra (2020) without bi-isotonic assumption (\\(x^*\\) known, \\(M_{\\pi^*}\\) isotonic)\n\n\nThis paper: recovering \\(x^*\\), ranking \\(\\pi^*\\) and estimating abilities \\(M\\) in poly. time when \\(n=d\\) (\\(x^*\\) unknown, \\(M_{\\pi^*}\\) isotonic)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#main-results",
    "href": "research/crowdsourcing/presentation.html#main-results",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Results",
    "text": "Main Results\n\n\n\n\nOptimal poly. time method\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), there exists a poly. time method \\(\\hat x\\) achieving \\(\\mathcal R^*_{\\mathrm{reco}}\\) up to polylog factors, i.e. \\[\n\\mathcal R_{\\mathrm{reco}}(n,d,\\lambda, \\hat x) \\lesssim \\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)\n\\]\n\n\n\n\n\n\n\n\nMinimax Risk\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), up to polylogs, \\[\n\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\asymp \\frac{d}{\\lambda}\n\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#majority-vote",
    "href": "research/crowdsourcing/presentation.html#majority-vote",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Majority Vote",
    "text": "Majority Vote\n\n\\[ \\hat x^{(maj)}_k = \\mathrm{sign} \\left( \\sum_{i=1}^n Y_{ik} \\right) \\enspace .\\]\n\n\n\n\n\nMax risk of majority vote\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n, d, \\lambda, \\hat x^{(maj)}) \\asymp \\tfrac{d \\sqrt{n}}{\\lambda}\\]\n\n\n\n\n\nWorst case (\\(\\lambda=1\\)): \\(M \\asymp \\frac{1}{\\sqrt{n}}(\\mathbf 1_{n\\times d})\\)\n\n\nIn this case, \\(\\hat x^{(maj)}\\) is no better than random labelling and \\(\\|M\\mathrm{diag}(\\hat x \\neq x^*)\\|_F^2 \\asymp d\\sqrt{n}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#least-square-conjectured-np-hard",
    "href": "research/crowdsourcing/presentation.html#least-square-conjectured-np-hard",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Least square (conjectured NP Hard)",
    "text": "Least square (conjectured NP Hard)\n\nMinimize\n\n\\[\\|Y- \\lambda M' \\mathrm{diag}(x)\\|_F^2\\]\n\n\nover all \\(x \\in \\{-1, 1\\}^d\\)\nand \\(M'\\) isotonic, up to a permutation \\(\\pi^*\\)\n\n\n\nThe set of isotonic matrices is convex…\n\n\n\nBut not isotonic matrices up to a permutation \\(\\pi^*\\)\n\n\nIt is minimax optimal (Shah et al., 2020)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "href": "research/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "OBI-WAN (Shah et al., 2020)",
    "text": "OBI-WAN (Shah et al., 2020)\n\nIdea: Population term \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T\\) is independent of \\(x^*\\)\n\n\nPCA Step:\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]\n\n\n\n(Shah et al., 2020) sort \\(|\\hat v|\\) to get a partial ranking\n\n\nAggregation: Majority vote on \\(k\\) top experts according to \\(|\\hat v|\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "href": "research/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Result on Obi-Wan Method",
    "text": "Result on Obi-Wan Method\n\n\n\n\n\nTheorem 1 (Shah et al., 2020)\n\n\nIn submodel where \\(\\mathrm{rk}(M)= 1\\), \\(\\hat x^{(\\mathrm{Obi-Wan})}\\) achieves minimax risk up to polylogs: \\[\\mathcal R(n,d, \\lambda, \\hat x^{(\\mathrm{Obi-Wan})}) \\lesssim \\frac{d}{\\lambda} \\quad \\textbf{(minimax)}\\]\n\n\n\n\n\n\n\n\n\n\nTheorem 2 (Shah et al., 2020)\n\n\nIn the model where \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic up to polylogs: \\[\\mathcal R(n,d, \\lambda, \\hat x^{(\\mathrm{Obi-Wan})}) \\lesssim \\frac{\\sqrt{n}d}{\\lambda} \\quad \\textbf{(not minimax)}\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#subsampling",
    "href": "research/crowdsourcing/presentation.html#subsampling",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Subsampling",
    "text": "Subsampling\n\nLet \\(T \\geq 1\\). We generate \\(T\\) samples \\((Y^{(1)}, \\dots, Y^{(T)})\\) from \\(Y\\).\n\n\nPut \\(Y_{ik}\\) uniformly at random into one of the \\(Y^{(s)}\\).\n\n\\(Y_{ik}^{(s)}= 0\\) for all \\(s\\) except one, which is \\(Y_{ik}\\)\nThe \\((Y^{(s)})\\)’s are not independent!\nTechnical trick: condition on the sampling scheme"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#pca-step",
    "href": "research/crowdsourcing/presentation.html#pca-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "PCA Step",
    "text": "PCA Step\n\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y^{(1)}\\|^2 \\quad \\text{and} \\quad \\tilde v = \\hat v \\land \\sqrt{\\lambda / T}\\]\n\n\n\nMain idea: if \\(M\\) is isotonic, then up to a polylog\n\n\\[\\|MM^T\\|_{\\mathrm{op}} \\gtrsim \\|M\\|_F^2\\]\n\n\n\nIdea for the proof: if \\(\\|M\\|_F^2 \\gg \\frac{d}{\\lambda}\\), then \\(\\|\\tilde v^T M\\| \\gtrsim \\|M\\|_F^2\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#voting-step",
    "href": "research/crowdsourcing/presentation.html#voting-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Voting Step",
    "text": "Voting Step\n\nDefine the weighted vote vector\n\n\\[\\hat w = \\tilde v^T Y^{(2)}\\]\n\n\n\nDefine the estimated label as\n\n\\[\\hat x_k^{(1)} = \\mathrm{sign}(\\hat w_k)\\mathbf{1}\\bigg\\{|\\hat w_k| \\gg \\sqrt{\\sum_{i=1}^n \\tilde v_i B_{ik}^{(2)}}\\bigg\\}\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#iterate",
    "href": "research/crowdsourcing/presentation.html#iterate",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Iterate",
    "text": "Iterate\n\nRepeat PCA Step + Voting Step on \\(Y \\mathrm{diag}(\\hat x \\neq 0)\\) a polylogarithmic number of times.\n\n\nWe get \\(\\hat x^{(1)}, \\hat x^{(2)}, \\dots, \\hat x^{(T)}\\)\n\n\nOutput \\(\\hat x^{(T)}\\)\n\\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#proof-idea",
    "href": "research/crowdsourcing/presentation.html#proof-idea",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Proof Idea",
    "text": "Proof Idea\n\nLet \\(M(t) = M\\mathrm{diag}(x^{(t-1)} = 0)\\)\n\n\nWhile \\(M(t) \\gg d/\\lambda\\), we prove that\n\n\\[\\|\\tilde v^TM(t)\\|_2^2 \\gtrsim \\|M\\|_F^2 \\and \\|\\tilde v^TM(t+1)\\|_2^2 \\lesssim d/\\lambda\\]\n\n\n\nBy Pythagoeran Theorem, we have\n\n\\[\\|M(t)\\|_F^2 - \\|M(t+1)\\|_F^2 \\geq \\|\\tilde v M(t)\\|_2^2 - \\|\\tilde v^TM(t+1)\\|_2^2\\]\n\n\n\nThis leads to exponential decay of \\(\\|M(t)\\|_F^2\\) until \\(M(t) \\leq d/\\lambda\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#synthetic-data",
    "href": "research/crowdsourcing/presentation.html#synthetic-data",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Synthetic Data",
    "text": "Synthetic Data\n\n\n\n\n\n\n\n\n\n \n\nBlack: \\(M_{ik}=0\\)\nBlue: \\(M_{ik} = h\\)"
  }
]