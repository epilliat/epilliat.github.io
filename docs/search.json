[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentation",
    "section": "",
    "text": "I am an assistant professor in statistics at ENSAI (Rennes) working on topics related to machine learning and high-dimensional statistics. My PhD centered on two areas in modern statistics: change-point detection and ranking problems.\nI am interested in GPU parallel computing. I’m currently developing a Julia library focused on optimizing fundamental functions like mapreduce (for operations such as sum) and accumulate (for operations like prefix sum). You can explore my experimental work at my GitHub repository Luma for more technical details.\ncontact: firstname.lastname@ensai.fr"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Presentation",
    "section": "Publications",
    "text": "Publications\n\nE. Pilliat, Recovering Labels from Crowdsourced Data: An Optimal and Polynomial-Time Method (2025) COLT [presentation]\nA. Garivier, E. Pilliat, On Sparsity and Sub-Gaussianity in the Johnson-Lindenstrauss Lemma (2024) [arxiv]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal rates for ranking a permuted isotonic matrix in polynomial time (2024) SODA\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal permutation estimation in crowd-sourcing problems (2023) Annals of Statistics. [arxiv], [presentation],[poster]\nE. Pilliat, A. Carpentier, N. Verzelen, Optimal multiple change-point detection for high-dimensional data (2023) EJS [arxiv]"
  },
  {
    "objectID": "index.html#vitae",
    "href": "index.html#vitae",
    "title": "Presentation",
    "section": "Vitae",
    "text": "Vitae\n\nResearch\n\nSep 2024 - Present: assistant professor in statistics at ENSAI\n2024: Postdoctoral researcher at ENS Lyon\nFeb 2020 - Apr 2022 and Oct 2022 - Dec 2023: PhD Student at Université de Montpellier and INRAE\n2019: Research Project at Ecole Normale Supérieure Paris Saclay with Vianney Perchet on Optimal Order Selection for an Online Reward Maximization problem\nApr 2019: Research Project at OVGU Magdeburg with Alexandra Carpentier on Signal Detection and Change-Point Detection\n2018: Research Internship at the University of Cambridge on Gaussian Free Field\n\nExperience\n\nApr 2022 - Oct 2022: Quantitative Intern at QRT\n\nEducation\n\n2016-2020: Student at Ecole Normale Supérieure de Lyon\n2013-2016: Preparatory School of Mathematics and Physics (CPGE MPSI/MP*) in Strasbourg\n\nTeaching\n\n2020-2022: Teaching assistant at Université de Montpellier."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I currently am a teaching three topics in statistics at ENSAI Rennes: Hypothesis Testing, Linear and Generalized Linear Model and Times Series (TD 2024).\nSee the Glossary for English/French translations. See also the note recalling some notion on Density and Likelihood."
  },
  {
    "objectID": "teaching.html#slides",
    "href": "teaching.html#slides",
    "title": "Teaching",
    "section": "Slides",
    "text": "Slides\n\nTesting Models\nClassical Tests on Gaussian Populations\nGoodness of Fit and Homogeneity Tests\nTests for Dependency"
  },
  {
    "objectID": "teaching.html#lectures",
    "href": "teaching.html#lectures",
    "title": "Teaching",
    "section": "Lectures",
    "text": "Lectures\n\nTesting Models\n\nClassical Tests on Gaussian Populations\nGoodness of Fit and Homogeneity Tests\nTests for Dependency"
  },
  {
    "objectID": "teaching.html#pluto-notebooks",
    "href": "teaching.html#pluto-notebooks",
    "title": "Teaching",
    "section": "Pluto Notebooks",
    "text": "Pluto Notebooks\n\nProba Basics\nApproximation of Distributions\nIllustration of pvalue"
  },
  {
    "objectID": "teaching.html#td-tp",
    "href": "teaching.html#td-tp",
    "title": "Teaching",
    "section": "TD-TP",
    "text": "TD-TP\n\nTD1\nTD2\nTD3\nTD4\nTP"
  },
  {
    "objectID": "teaching.html#annals",
    "href": "teaching.html#annals",
    "title": "Teaching",
    "section": "Annals",
    "text": "Annals\n\nExam 2025\nCorrection 2025"
  },
  {
    "objectID": "teaching.html#slides-1",
    "href": "teaching.html#slides-1",
    "title": "Teaching",
    "section": "Slides",
    "text": "Slides\n\nLinear Model\n\nIntroduction\nDefinition of the Linear Model\nValidation\nModel Selection\n\n\n\nGeneralized Linear Model"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html",
    "href": "teaching/linear_model/lectures/linear_model.html",
    "title": "Linear Regression Model",
    "section": "",
    "text": "AI was used to assist with the formatting and writing of the proofs on this page."
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#proof-of-the-gauss-markov-theorem",
    "href": "teaching/linear_model/lectures/linear_model.html#proof-of-the-gauss-markov-theorem",
    "title": "Linear Regression Model",
    "section": "Proof of the Gauss-Markov Theorem",
    "text": "Proof of the Gauss-Markov Theorem\n\nSetup\nLet \\(\\tilde{\\beta} = CY\\) be any linear unbiased estimator of \\(\\beta\\), where \\(C\\) is an \\(n \\times p\\) matrix of constants.\n\n\nStep 1: Unbiasedness Constraint\nSince \\(\\tilde{\\beta}\\) is unbiased: \\(\\E[\\tilde{\\beta}] = \\beta\\) for all \\(\\beta\\)\n\\[\\E[\\tilde{\\beta}] = \\E[CY] = \\E[C(X\\beta + \\varepsilon)]\\]\n\\[= CX\\beta + C\\E[\\varepsilon] = CX\\beta\\]\nFor unbiasedness: \\(CX\\beta = \\beta\\) for all \\(\\beta\\)\nTherefore: \\(CX = I\\) (the \\(p \\times p\\) identity matrix)\n\n\nStep 2: Express Any Linear Unbiased Estimator\nSince \\(CX = I\\), we can write: \\[C = (X^TX)^{-1}X^T + D\\]\nwhere \\(D\\) is any matrix satisfying \\(DX = 0\\).\nVerification: \\((X^TX)^{-1}X^TX + DX = I + 0 = I\\) ✓\n\n\nStep 3: Express the Estimator\n\\[\\tilde{\\beta} = CY = [(X^TX)^{-1}X^T + D]Y\\]\n\\[= (X^TX)^{-1}X^TY + DY\\]\n\\[= \\hat{\\beta} + DY\\]\n\n\nStep 4: Calculate Variance\n\\[\\text{Var}(\\tilde{\\beta}) = \\text{Var}(\\hat{\\beta} + DY)\\]\n\\[= \\text{Var}(\\hat{\\beta}) + \\text{Var}(DY) + 2\\text{Cov}(\\hat{\\beta}, DY)\\]\n\n\nStep 5: Show Covariance Term is Zero\n\\[\\text{Cov}(\\hat{\\beta}, DY) = \\text{Cov}((X^TX)^{-1}X^TY, DY)\\]\n\\[= (X^TX)^{-1}X^T \\text{Cov}(Y, Y) D^T\\]\n\\[= (X^TX)^{-1}X^T (\\sigma^2I) D^T\\]\n\\[= \\sigma^2(X^TX)^{-1}X^TD^T\\]\nSince \\(DX = 0\\), we have \\(X^TD^T = 0\\), therefore: \\[\\text{Cov}(\\hat{\\beta}, DY) = 0\\]\n\n\nStep 6: Final Comparison\n\\[\\text{Var}(\\tilde{\\beta}) = \\text{Var}(\\hat{\\beta}) + \\text{Var}(DY)\\]\n\\[= \\sigma^2(X^TX)^{-1} + \\sigma^2DD^T\\]\nSince \\(DD^T \\succeq 0\\) (positive semidefinite), we have:\n\\[\\text{Var}(\\tilde{\\beta}) - \\text{Var}(\\hat{\\beta}) = \\sigma^2DD^T \\succeq 0\\]\n\n\nConclusion\nThis proves that \\(\\text{Var}(\\hat{\\beta}) \\preceq \\text{Var}(\\tilde{\\beta})\\) in the matrix sense, establishing that the OLS estimator \\(\\hat{\\beta}\\) has minimum variance among all linear unbiased estimators. □"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#maximum-likelihood-estimator",
    "href": "teaching/linear_model/lectures/linear_model.html#maximum-likelihood-estimator",
    "title": "Linear Regression Model",
    "section": "Maximum Likelihood Estimator",
    "text": "Maximum Likelihood Estimator\n\n\n\n\n\n\nMLE\n\n\n\nLet \\(\\hat \\beta_{MLE}\\) and \\(\\hat \\sigma_{MLE}^2\\) be the MLE of \\(\\beta\\) and \\(\\sigma^2\\), respectively.\n\n\\(\\hat{\\beta}_{MLE} = \\hat{\\beta}\\) et \\(\\hat{\\sigma}^2_{MLE} = \\frac{SCR}{n} = \\frac{n-p}{n} \\hat{\\sigma}^2\\).\n\\(\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\\).\n\\(\\frac{n-p}{\\sigma^2} \\hat{\\sigma}^2 = \\frac{n}{\\sigma^2} \\hat{\\sigma}^2_{MLE} \\sim \\chi^2(n - p)\\).\n\\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#proof-mle",
    "href": "teaching/linear_model/lectures/linear_model.html#proof-mle",
    "title": "Linear Regression Model",
    "section": "Proof",
    "text": "Proof\n\nSetup\nModel: \\(Y = X\\beta + \\varepsilon\\) where \\(\\varepsilon \\sim N(0, \\sigma^2 I)\\)\nThis means: \\(Y \\sim N(X\\beta, \\sigma^2 I)\\)\n\n\nLikelihood Function\nFor \\(n\\) observations, the likelihood function is:\n\\[L(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} (Y - X\\beta)^T(Y - X\\beta)\\right)\\]\n\n\nLog-Likelihood Function\n\\[\\ell(\\beta, \\sigma^2) = \\log L(\\beta, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} (Y - X\\beta)^T(Y - X\\beta)\\]\n\\[\\ell(\\beta, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} (Y - X\\beta)^T(Y - X\\beta)\\]\n\n\nFinding MLE for \\(\\beta\\)\nTaking the partial derivative with respect to \\(\\beta\\):\n\\[\\frac{\\partial \\ell}{\\partial \\beta} = \\frac{1}{\\sigma^2} X^T(Y - X\\beta)\\]\nSetting equal to zero: \\[X^T(Y - X\\hat{\\beta}_{MLE}) = 0\\]\n\\[X^TY - X^TX\\hat{\\beta}_{MLE} = 0\\]\n\\[X^TX\\hat{\\beta}_{MLE} = X^Ty\\]\nTherefore: \\[\\hat{\\beta}_{MLE} = (X^TX)^{-1}X^Ty\\]\n\n\nFinding MLE for \\(\\sigma^2\\)\nTaking the partial derivative with respect to \\(\\sigma^2\\):\n\\[\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} (Y - X\\beta)^T(Y - X\\beta)\\]\nSetting equal to zero: \\[-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} (Y - X\\hat{\\beta}_{MLE})^T(Y - X\\hat{\\beta}_{MLE}) = 0\\]\nMultiplying by \\(2\\sigma^4\\): \\[-n\\sigma^2 + (y - X\\hat{\\beta}_{MLE})^T(y - X\\hat{\\beta}_{MLE}) = 0\\]\nTherefore: \\[\\hat{\\sigma}^2 = \\frac{1}{n} (y - X\\hat{\\beta}_{MLE})^T(y - X\\hat{\\beta}_{MLE}) = \\frac{SSR}{n}\\]\n\n\nVerification (Second-Order Conditions)\nThe Hessian matrix has:\n\\[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta'} = -\\frac{1}{\\sigma^2} X^TX\\]\nThis is negative definite (assuming \\(X^TX\\) is invertible), confirming \\(\\hat{\\beta}\\) is a maximum.\n\\[\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\frac{n}{2\\sigma^4} - \\frac{1}{\\sigma^6} (y - X\\beta)^T(y - X\\beta)\\]\nAt the MLE: \\(\\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2}\\bigg|_{\\hat{\\sigma}^2} = \\frac{n}{2\\sigma^4} - \\frac{n}{\\sigma^4} = -\\frac{n}{2\\sigma^4} &lt; 0\\)\nThis confirms \\(\\hat{\\sigma}^2\\) is a maximum.\n\n\nKey Properties\n\nConsistency: Both estimators are consistent\nBias: \\(\\hat{\\beta}\\) is unbiased, but \\(\\hat{\\sigma}^2\\) is biased (divides by \\(n\\) instead of \\(n-k\\))\nEfficiency: Under normality, these MLEs achieve the Cramér-Rao lower bound\nRelationship to OLS: \\(\\hat{\\beta}_{MLE} = \\hat{\\beta}_{OLS}\\) under normality assumption"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#setup-2",
    "href": "teaching/linear_model/lectures/linear_model.html#setup-2",
    "title": "Linear Regression Model",
    "section": "Setup",
    "text": "Setup\nConsider the linear regression model: \\[Y = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2 I)\\]\nWe want to prove that \\(\\hat \\beta = (X^TX)^{-1}X^TY\\) is efficient."
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#definition-of-efficiency",
    "href": "teaching/linear_model/lectures/linear_model.html#definition-of-efficiency",
    "title": "Linear Regression Model",
    "section": "Definition of Efficiency",
    "text": "Definition of Efficiency\nAn unbiased estimator is efficient if it achieves the Cramér-Rao lower bound: \\[\\text{Var}(\\hat \\beta) = [I(\\beta)]^{-1}\\] where \\(I(\\beta)\\) is the Fisher Information Matrix. This comes from the Cramér-Rao lower bound"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#step-1-fisher-information-matrix",
    "href": "teaching/linear_model/lectures/linear_model.html#step-1-fisher-information-matrix",
    "title": "Linear Regression Model",
    "section": "Step 1: Fisher Information Matrix",
    "text": "Step 1: Fisher Information Matrix\nThe log-likelihood function is: \\[\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}(Y - X\\beta)^T(Y - X\\beta)\\]\nFirst derivative with respect to \\(\\beta\\): \\[\\frac{\\partial \\ell}{\\partial \\beta} = \\frac{1}{\\sigma^2}X^T(Y - X\\beta)\\]\nSecond derivative: \\[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^T} = -\\frac{1}{\\sigma^2}X^TX\\]\nFisher Information Matrix for \\(\\beta\\): \\[I(\\beta) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta^T}\\right] = \\frac{1}{\\sigma^2}X^TX\\]\nCramér-Rao lower bound: \\[[I(\\beta)]^{-1} = \\sigma^2(X^TX)^{-1}\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#step-2-variance-of-hat-beta",
    "href": "teaching/linear_model/lectures/linear_model.html#step-2-variance-of-hat-beta",
    "title": "Linear Regression Model",
    "section": "Step 2: Variance of \\(\\hat \\beta\\)",
    "text": "Step 2: Variance of \\(\\hat \\beta\\)\n\\[\\hat \\beta = (X^TX)^{-1}X^TY = (X^TX)^{-1}X^T(X\\beta + \\varepsilon) = \\beta + (X^TX)^{-1}X^T\\varepsilon\\]\nSince \\(\\varepsilon \\sim N(0, \\sigma^2 I)\\): \\[\\text{Var}(\\hat \\beta) = \\text{Var}((X^TX)^{-1}X^T\\varepsilon)\\]\n\\[= (X^TX)^{-1}X^T \\cdot \\text{Var}(\\varepsilon) \\cdot X(X^TX)^{-1}\\]\n\\[= (X^TX)^{-1}X^T \\cdot \\sigma^2 I \\cdot X(X^TX)^{-1}\\]\n\\[= \\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}\\]\n\\[= \\sigma^2(X^TX)^{-1}\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#step-3-verification-of-efficiency",
    "href": "teaching/linear_model/lectures/linear_model.html#step-3-verification-of-efficiency",
    "title": "Linear Regression Model",
    "section": "Step 3: Verification of Efficiency",
    "text": "Step 3: Verification of Efficiency\nWe have shown: - Cramér-Rao bound: \\([I(\\beta)]^{-1} = \\sigma^2(X^TX)^{-1}\\) - Variance of \\(\\hat \\beta\\): \\(\\text{Var}(\\hat \\beta) = \\sigma^2(X^TX)^{-1}\\)\nSince: \\[\\text{Var}(\\hat \\beta) = [I(\\beta)]^{-1}\\]\nThe estimator \\(\\hat \\beta\\) achieves the Cramér-Rao lower bound."
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#conclusion-1",
    "href": "teaching/linear_model/lectures/linear_model.html#conclusion-1",
    "title": "Linear Regression Model",
    "section": "Conclusion",
    "text": "Conclusion\nTherefore, \\(\\hat \\beta = (X^TX)^{-1}X^TY\\) is an efficient estimator of \\(\\beta\\) in the Gaussian linear regression model."
  },
  {
    "objectID": "teaching/linear_model/lectures/linear_model.html#additional-notes",
    "href": "teaching/linear_model/lectures/linear_model.html#additional-notes",
    "title": "Linear Regression Model",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nThis efficiency holds specifically under the normality assumption\n\\(\\hat \\beta\\) is also the Best Linear Unbiased Estimator (BLUE) by the Gauss-Markov theorem\nUnder normality, \\(\\hat \\beta\\) is the Best Unbiased Estimator (BUE) among all estimators, not just linear ones"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#ordinary-least-square-estimator-ols",
    "href": "teaching/linear_model/slides/inference.html#ordinary-least-square-estimator-ols",
    "title": "Inference",
    "section": "Ordinary Least Square Estimator (OLS)",
    "text": "Ordinary Least Square Estimator (OLS)\n\nWe observe \\(Y \\in \\mathbb R^{n\\times 1}\\) and \\(X \\in \\mathbb R^{n \\times p}\\).\n\n\nWhat is the best estimator \\(\\hat \\beta\\) of \\(\\beta\\) such that \\(Y = X\\beta + \\varepsilon\\)?\n\n\nOrdinary Least Square (OLS) Estimator:\n\\(\\newcommand{\\argmin}{\\mathrm{argmin}}\\)\n\n\\[\\hat \\beta = \\argmin_{\\beta'}\\|Y-X\\beta'\\|^2\\]\n\nHere, \\(\\|Y-X\\beta'\\|^2 = \\sum_{i=1}^n(Y_i-\\beta_1X_{i}^{(1)}- \\dots - \\beta_pX_i^{(p)})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\n\nIf \\(\\mathrm{rk}(X) = p\\), it holds that\n\n\\[\\hat \\beta = (X^TX)^{-1}X^TY\\]\n\n\n\n\\(X\\hat \\beta = X(X^TX)^{-1}X^TY\\) is the projection of \\(Y\\) on the space generated by columns of \\(X\\): \\[[X]=\\mathrm{Im}(X)=\\mathrm{Span}(X^{(1)},\\dots, X^{(p)})=\\{X\\alpha, \\alpha \\in \\mathbb R^p\\}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-1",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-1",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\n\n\n\\[P_{[X]} = X(X^TX)^{-1}X^T \\and \\widehat{Y} = P_{[X]}Y = X\\hat \\beta\\]\n\n\n\nCheck that \\(P_{[X]}\\) is the orthogonal projector on \\([X]\\)\n\n\nThat is \\(P_{[X]}^2 = P_{[X]}\\), \\(P_{[X]}=P_{[X]}^T\\) and \\(\\mathrm{Im} (P_{[X]}) = [X]\\)\n\n\nOr, without computation:\n\\(P_{[X]}Y\\) minimizes \\(\\|Y-Y'\\|^2\\) over all \\(Y' \\in [X]\\).\n\n\nSo \\(P_{[X]}Y\\) must be the orthogonal projection by Pythagorean theorem"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-2",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-2",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\nWe can decompose\n\n\\[Y = \\underbrace{P_{[X]}Y}_{\\widehat Y} + \\underbrace{(I-P_{[X]})Y}_{\"residuals\"}\\]\n\n\nNotice that \\(I-P_{[X]}= P_{[X]^{\\perp}}\\) is the orthogonal projection on \\([X]^{\\perp} = \\{\\alpha\\in \\mathbb R^p:~ X\\alpha =0\\}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-3",
    "href": "teaching/linear_model/slides/inference.html#least-squares-as-an-l2-projection-3",
    "title": "Inference",
    "section": "Least squares as an L2 projection",
    "text": "Least squares as an L2 projection\n\n(image: elements of statistical learning. In yellow: \\([X]\\) with \\(p=2\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-on-hat-beta",
    "href": "teaching/linear_model/slides/inference.html#properties-on-hat-beta",
    "title": "Inference",
    "section": "Properties on \\(\\hat \\beta\\)",
    "text": "Properties on \\(\\hat \\beta\\)\n\n\n\n\nExpectation and variance\n\n\nAssume that \\(rk(X)=p\\), \\(\\mathbb E[\\varepsilon] = 0\\) and \\(\\mathbb V(\\varepsilon) = \\sigma^2I_n\\). Then,\n\n\\(\\hat \\beta = (X^TX)^{-1}X Y\\) is a linear estimator\n\\(\\mathbb E[\\hat \\beta] = \\beta\\) unbiased estimator\n\\(\\mathbb V(\\hat \\beta) = \\sigma^2(X^TX)^{-1}\\)\n\n\n\n\n\n\n\n\n\nGauss-Markov Theorem\n\n\nUnder the same assumptions, if \\(\\tilde \\beta\\) is another linear and unbiased estimator then \\[\\mathbb V(\\hat \\beta) \\preceq \\mathbb V(\\tilde \\beta),\\]\nwhere \\(A\\preceq B\\) means that \\(B-A\\) is a symmetric positive semidefinite matrix Elements of proof"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#residuals",
    "href": "teaching/linear_model/slides/inference.html#residuals",
    "title": "Inference",
    "section": "Residuals",
    "text": "Residuals\n\nWe define the residuals \\(\\hat \\varepsilon\\) as\n\n\\[\n\\hat \\varepsilon = Y- \\hat Y = Y-X\\hat \\beta\n\\]\n\n\n\nIt can be computed from the data.\n\n\nIt is the orthogonal projection of \\(Y\\) on \\([X]^{\\perp}\\):\n\n\n\n\\[Y = \\underbrace{P_{[X]}Y}_{\\widehat Y} + \\underbrace{(I-P_{[X]})Y}_{\\hat \\varepsilon=\"residuals\"}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#residuals-1",
    "href": "teaching/linear_model/slides/inference.html#residuals-1",
    "title": "Inference",
    "section": "Residuals",
    "text": "Residuals\n\n\\[\n\\begin{aligned}\nY &= X\\beta + \\varepsilon & \\quad \\text{(model)} \\\\\nY &= \\widehat Y + \\hat \\varepsilon & \\quad \\text{(estimation)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-on-residuals",
    "href": "teaching/linear_model/slides/inference.html#properties-on-residuals",
    "title": "Inference",
    "section": "Properties on residuals",
    "text": "Properties on residuals\n\n\n\nExpectation and Variance of \\(\\hat\\varepsilon\\)\n\n\nIf \\(rk(X)=p\\), \\(\\mathbb E[\\varepsilon] = 0\\) and \\(\\mathbb V(\\varepsilon)=\\sigma^2I_n\\), then\n\n\\(\\mathbb E[\\hat\\varepsilon]=0\\)\n\\(\\mathbb V(\\hat \\varepsilon) = \\sigma^2P_{[X]^\\perp} = \\sigma^2(I_n - X(X^TX)^{-1}X^T)\\)\n\n\n\n\n\nRemark: if a constant vector is in \\([X]\\), e.g. \\(\\forall i,X_i^{(1)}=1\\), then \\(\\hat \\varepsilon \\perp \\mathbf 1\\) and\n\\[\n\\overline{\\hat\\varepsilon} = \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i = 0 \\and \\overline{\\widehat Y} = \\overline Y\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#estimation-of-sigma2-1",
    "href": "teaching/linear_model/slides/inference.html#estimation-of-sigma2-1",
    "title": "Inference",
    "section": "Estimation of \\(\\sigma^2\\)",
    "text": "Estimation of \\(\\sigma^2\\)\n\nRecall that in the model \\(Y=X\\beta + \\varepsilon\\), both \\(\\beta\\) and \\(\\sigma^2=\\mathbb E[\\varepsilon_i^2]\\) are unknown (\\(p+1\\) parameters)\n\n\n\n\\[\n\\hat\\varepsilon = Y- \\hat Y = P_{[X]^\\perp}\\varepsilon \\and dim([X]^{\\perp}) = =n-p\n\\]\n\n\n\nWe estimate \\(\\sigma^2\\) with\n\n\\[\\hat \\sigma^2 = \\frac{1}{n-p}\\sum_{i=1}^n \\hat \\varepsilon_i^2\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#properties-of-hat-sigma2",
    "href": "teaching/linear_model/slides/inference.html#properties-of-hat-sigma2",
    "title": "Inference",
    "section": "Properties of \\(\\hat \\sigma^2\\)",
    "text": "Properties of \\(\\hat \\sigma^2\\)\n\n\n\nProposition\n\n\nIf \\(rk(X)=p\\), \\(\\E[\\varepsilon]=0\\) and \\(\\Var(\\varepsilon)= \\sigma^2I_n\\), then\n\\[\\hat \\sigma^2 = \\frac{1}{n-p}\\sum_{i=1}^n \\hat \\varepsilon_i^2=\\frac{\\mathrm{SSR}}{n-p}\\]\nis an unbiased estimator of \\(\\sigma^2\\). If moreover the \\(\\varepsilon_i\\)’s are iid, then \\(\\hat \\sigma^2\\) is a consistent estimator.\n\n\n\n\nunbiased: \\(\\E[\\hat \\sigma^2]= \\sigma^2\\)\nconsistent: \\(\\hat \\sigma^2 \\to \\sigma^2\\) a.s. as \\(n\\to +\\infty\\)\nSSR: Sum of squared residuals"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#gaussian-model",
    "href": "teaching/linear_model/slides/inference.html#gaussian-model",
    "title": "Inference",
    "section": "Gaussian Model",
    "text": "Gaussian Model\n\nUntil then, we assumed that \\(Y=X\\beta+\\varepsilon\\), where \\(\\E[\\varepsilon]=0\\) and \\(\\Var(\\varepsilon)= \\sigma^2I_n\\).\n\n\nThe \\(\\varepsilon_i\\) are uncorrelated but there can be dependency\n\n\nNo assumption was made on the distribution of \\(\\varepsilon\\)\n\n\nNow (Gaussian Model):\n\n\\[\\varepsilon \\sim \\mathcal N(0, \\sigma^2I_n) \\quad \\text{i.e.} \\quad Y \\sim \\mathcal N(X\\beta, \\sigma^2I_n)\\]\n\n\n\nEquivalently we assume that the \\(\\varepsilon_i\\)’s are iid \\(\\mathcal N(0, \\sigma^2)\\).\n\n\nIn this simpler model, we can do maximum likelihood estimation (MLE)!"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#maximum-likelihood-estimation",
    "href": "teaching/linear_model/slides/inference.html#maximum-likelihood-estimation",
    "title": "Inference",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\n\n\n\nMLE\n\n\nLet \\(\\hat \\beta_{MLE}\\) and \\(\\hat \\sigma_{MLE}^2\\) be the MLE of \\(\\beta\\) and \\(\\sigma^2\\), respectively.\n\n\\(\\hat{\\beta}_{MLE} = \\hat{\\beta}\\) et \\(\\hat{\\sigma}^2_{MLE} = \\frac{SCR}{n} = \\frac{n-p}{n} \\hat{\\sigma}^2\\).\n\\(\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\\).\n\\(\\frac{n-p}{\\sigma^2} \\hat{\\sigma}^2 = \\frac{n}{\\sigma^2} \\hat{\\sigma}^2_{MLE} \\sim \\chi^2(n - p)\\).\n\\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent\n\nElements of proof"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#efficient-estimator",
    "href": "teaching/linear_model/slides/inference.html#efficient-estimator",
    "title": "Inference",
    "section": "Efficient Estimator",
    "text": "Efficient Estimator\n\n\n\n\nTheorem\n\n\nIn the Gaussian Model, \\(\\hat \\beta\\) is an efficient estimator of \\(\\hat \\beta\\). This means that \\[\n\\Var(\\hat \\beta) \\preceq \\Var(\\tilde \\beta)\\; ,\n\\] for any estimator \\(\\tilde \\beta\\). See Elements of proof\n\n\n\n\nThis is stronger than Gauss-Markov\nBetter than any \\(\\tilde \\beta\\), not only linear \\(\\tilde  \\beta\\). \\(\\hat \\beta\\) is “BUE” (Best Unbiased Estimator)\nIf \\(n\\) is large, most of the result in Gaussian case remains valid."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#pivotal-distribution-in-gaussian-case",
    "href": "teaching/linear_model/slides/inference.html#pivotal-distribution-in-gaussian-case",
    "title": "Inference",
    "section": "Pivotal Distribution in Gaussian Case",
    "text": "Pivotal Distribution in Gaussian Case\n\nRecall that \\(\\hat \\sigma^2 = \\frac{1}{n-p}\\|\\hat \\varepsilon\\|^2\\)\n\n\n\n\n\nProperty\n\n\nIn the Gaussian model,\n\\[ \\frac{\\hat \\beta_j - \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) \\]\n(Student Distribution of degree \\(n-p\\), \\((X^TX)^{-1}_{jj}\\) is the \\(j^{th}\\) element of the matrix \\((X^TX)^{-1}\\))\n\n\n\n\n\n\\(\\mathbb V(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\\) implies that \\(\\mathbb V(\\hat{\\beta}_j) = \\sigma^2 (X^T X)^{-1}_{jj}\\). \\(\\hat{\\sigma}^2_{\\hat{\\beta}_j}:=\\hat\\sigma^2 (X^T X)^{-1}\\) is an estimator of \\(\\sqrt{\\mathbb V(\\hat{\\beta}_j)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#student-signifactivity-test",
    "href": "teaching/linear_model/slides/inference.html#student-signifactivity-test",
    "title": "Inference",
    "section": "Student Signifactivity Test",
    "text": "Student Signifactivity Test\n\nWe observe \\(Y = X\\beta+\\varepsilon\\), where \\(\\beta \\in \\mathbb R^p\\) is unknown.\n\n\nWe want to test whether the \\(j^{th}\\) feature \\(X^{(j)}\\) is significant in the LM, that is:\n\n\\(H_0: \\beta_j =0 \\VS H_1: \\beta_j \\neq 0\\).\n\nWe use the test statistic\n\n\n\\[\\psi_j(X,Y)= \\frac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}} = \\frac{\\hat \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) ~~\\text{(under $H_0$)}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#student-signifactivity-test-1",
    "href": "teaching/linear_model/slides/inference.html#student-signifactivity-test-1",
    "title": "Inference",
    "section": "Student Signifactivity Test",
    "text": "Student Signifactivity Test\n\n\n\n\\[\\psi_j(X,Y)= \\frac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}} = \\frac{\\hat \\beta_j}{\\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}} \\sim \\mathcal T(n-p) ~~\\text{(under $H_0$)}\\]\n\n\n\n\nWe reject if \\(|\\psi(X,Y)| \\geq t_{1-\\alpha/2}\\), where \\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of \\(\\mathcal T(n-p)\\).\n\n\n\n\\[p_{value}=2\\min(F(\\psi_j(X,Y)), 1-F(\\psi_j(X,Y)))\\]\n\nwhere \\(F\\) is the cdf of \\(\\mathcal T(n-p)\\).\n\n\nIf we get \\(\\beta_j=0\\), we can remove \\(X^{(j)}\\) from the model."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#confidence-interval",
    "href": "teaching/linear_model/slides/inference.html#confidence-interval",
    "title": "Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nConfidence interval with proba \\(1-\\alpha\\) around \\(\\beta_j\\):\n\n\\[CI_{1-\\alpha} = [\\hat \\beta_j \\pm t\\hat \\sigma_{\\hat \\beta_j}]\\]\n\n\nHere, \\(t\\) is the \\(1-\\alpha/2\\) quantile of \\(\\mathcal T(n-p)\\)\nRecall that \\(\\hat \\sigma_{\\hat \\beta_j} = \\hat \\sigma \\sqrt{(X^T X)^{-1}_{jj}}\\)\n\n\nWe check that\n\\[\n\\P(\\beta \\in CI_{1-\\alpha}) = 1- \\alpha\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#remarks",
    "href": "teaching/linear_model/slides/inference.html#remarks",
    "title": "Inference",
    "section": "Remarks",
    "text": "Remarks\n\nThis is what is computed on \\(R\\)\nThis is valid in Gaussian case, but is also true when \\(n\\) is large for other distributions"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-setting",
    "href": "teaching/linear_model/slides/inference.html#prediction-setting",
    "title": "Inference",
    "section": "Prediction Setting",
    "text": "Prediction Setting\n\nConsider the LM \\(Y = X\\beta + \\varepsilon\\).\n\n\nFrom previous slides, we estimate \\((\\beta, \\sigma)\\) with \\((\\hat \\beta, \\hat \\sigma)\\) from observations \\(Y\\) and matrix \\(X=(X^{(1)}, \\dots, X^{(p)})\\).\n\n\nWe observe a new individual \\(o\\), with unknown \\(Y_o\\) and vector \\(X_o=X_{o,\\cdot}=(X^{(1)}_o, \\dots, X^{(p)}_o)\\), and independent noise \\(\\varepsilon_o\\).\n\n\nwith this definition, \\(X_o\\) is a row vector and\n\n\\[Y_o = X_{o}\\beta + \\varepsilon_o = \\beta_1X^{(1)}_o + \\dots + \\beta_p X^{(p)}_o+\\varepsilon_o\\]\n\nHere, \\(\\mathbb E[\\varepsilon_o] = 0\\) and \\(\\mathbb V(\\varepsilon_o)=\\sigma^2\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-1",
    "href": "teaching/linear_model/slides/inference.html#prediction-1",
    "title": "Inference",
    "section": "Prediction",
    "text": "Prediction\n\nWe want predict \\(Y_o\\) (unknown) from \\(X_o\\) (known). Natural predictor:\n\n\\(\\hat Y_o = X_o \\hat \\beta + \\varepsilon_o\\)\n\n\n\nPrediction error \\(Y_o - \\hat Y_o\\) decomposes in two terms:\n\n\\[Y_o - \\hat Y_o = \\underbrace{X_o(\\beta - \\hat \\beta)}_{\\text{Estimation error}} + \\underbrace{\\varepsilon_o}_{\\text{Random error}}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#error-decomposition",
    "href": "teaching/linear_model/slides/inference.html#error-decomposition",
    "title": "Inference",
    "section": "Error Decomposition",
    "text": "Error Decomposition\n\n\n\n\\[Y_o - \\hat Y_o = \\underbrace{X_o(\\beta - \\hat \\beta)}_{\\text{Estimation error}} + \\underbrace{\\varepsilon_o}_{\\text{Random error}}\\]\n\n\n\n\n\\(\\E(Y_o - \\hat Y_o)\\): prediction error is \\(0\\) on average\n\n\n\\[\\begin{aligned}\n\\Var(Y_o - \\hat Y_o) &= \\color{blue}{ \\Var(X_o(\\beta - \\hat \\beta))} + \\color{red}{\\Var(\\varepsilon_o)} \\\\\n&=\\color{blue}{\\sigma^2X_o(X^TX)^{-1}X_o} + \\color{red}{\\sigma^2} \\\\\n\\end{aligned}\\]\n\n\n\\(\\color{blue}{\\Var(X_o(\\beta - \\hat \\beta))\\to 0}\\) when \\(n \\to +\\infty\\) but \\(\\color{red}{\\Var(\\varepsilon_o) =\\sigma^2}\\)\n\n\nEstimation error is negligible when \\(n \\to +\\infty\\) but random error is incompressible."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-interval",
    "href": "teaching/linear_model/slides/inference.html#prediction-interval",
    "title": "Inference",
    "section": "Prediction Interval",
    "text": "Prediction Interval\n\nIn the Gaussian model, \\(Y_o - \\hat Y_o \\sim \\mathcal N(0,  \\sigma^2X_o(X^TX)^{-1}X_o^T+\\sigma^2)\\).\n\n\nSince \\(\\hat \\sigma\\) is indep of \\(\\hat Y_o\\) (check this with projections!),\n\n\n\\[\\frac{Y_o - \\hat Y_o}{\\hat \\sigma\\sqrt{X_o(X^TX)^{-1}X_o^T+1}} \\sim \\mathcal T(n-p)\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#prediction-interval-1",
    "href": "teaching/linear_model/slides/inference.html#prediction-interval-1",
    "title": "Inference",
    "section": "Prediction Interval",
    "text": "Prediction Interval\n\nWe deduce the prediction interval\n\n\n\\[PI_{1-\\alpha}(Y_o)=\\left[\\hat Y_o \\pm \\sqrt{\\color{blue}{\\hat \\sigma^2X_o(X^TX)^{-1}X_o^T} + \\color{red}{\\hat \\sigma^2}}\\right]\\]\n\n\nsuch that \\(\\P(Y_o \\in PI_{1-\\alpha})= 1-\\alpha\\)\n\n\nIf we only want to estimate \\(\\mathbb E[Y_o]=X_o \\beta\\), (point on the hyperplane), we get the confidence interval\n\n\n\\[CI_{1-\\alpha}(X_o\\beta)=\\left[\\hat Y_o \\pm \\sqrt{\\color{blue}{\\hat \\sigma^2X_o(X^TX)^{-1}X_o^T}}\\right]\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#example",
    "href": "teaching/linear_model/slides/inference.html#example",
    "title": "Inference",
    "section": "Example",
    "text": "Example\nFor 100 trees, we record their volume and their girth."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#model",
    "href": "teaching/linear_model/slides/inference.html#model",
    "title": "Inference",
    "section": "Model",
    "text": "Model\n\nVolume = \\(\\beta_1\\) + \\(\\beta_2\\) Girth + \\(\\varepsilon\\)\n# In R\nreg=lm(Volume ~ Girth, data=trees)\nsummary(reg)\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#results",
    "href": "teaching/linear_model/slides/inference.html#results",
    "title": "Inference",
    "section": "Results",
    "text": "Results\n\nEstimation:\n\n\n\\[\\hat \\beta_1=-36.9 \\and \\hat \\beta_2=5.07\\] \\[\\hat \\sigma_{\\hat \\beta_1} =3.4 \\and \\hat \\sigma_{\\hat \\beta_2} =0.25\\]\n\n\n\n\nStatistics:\n\n\n\\[\\frac{\\hat \\beta_1}{\\hat \\sigma_{\\hat \\beta_1}}=-10.98 \\and \\frac{\\hat \\beta_2}{\\hat \\sigma_{\\hat \\beta_2}}=20.48\\]\n\n\n\n\n\\(Pr(&gt;|t|)\\): pvalues of the student tests. Last row: \\(\\hat \\sigma=4.25\\) and df."
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#illustration",
    "href": "teaching/linear_model/slides/inference.html#illustration",
    "title": "Inference",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "teaching/linear_model/slides/inference.html#r-code",
    "href": "teaching/linear_model/slides/inference.html#r-code",
    "title": "Inference",
    "section": "R code",
    "text": "R code\nrequire(stats); require(graphics)\npairs(trees, main = \"trees data\")\ntrees[,c(\"Girth\", \"Volume\")]\nreg &lt;- lm(Volume ~ Girth, data = trees)\n# Create sequence of x values for smooth curves\nx_seq &lt;- seq(min(trees$Girth), max(trees$Girth), length.out = 100)\n\n# Calculate confidence and prediction intervals\nconf_int &lt;- predict(reg, newdata = data.frame(Girth = x_seq), \n                   interval = \"confidence\", level = 0.95)\npred_int &lt;- predict(reg, newdata = data.frame(Girth = x_seq), \n                   interval = \"prediction\", level = 0.95)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#pythagorean-decomposition",
    "href": "teaching/linear_model/slides/validation.html#pythagorean-decomposition",
    "title": "Validation",
    "section": "Pythagorean Decomposition",
    "text": "Pythagorean Decomposition\n\nLet \\(\\mathbf{1}\\) be the constant column vector in \\(\\mathbb R^{n\\times 1}\\).\n\n\nIf \\(\\mathbf{1} \\in [X]\\) (eg if we consider an intercept) \\[ \\underbrace{\\|Y-\\overline Y \\1\\|^2}_{SST} = \\underbrace{\\|Y-\\widehat Y\\|^2}_{SSR}+\\underbrace{\\|\\widehat Y-\\overline Y \\1\\|^2}_{SSE}\\]\nIn the general case,\n\n\n\\[\\|Y\\|^2 = \\|Y-\\widehat Y\\|^2 + \\|\\widehat Y\\|^2\\]\nGood model if sum of squares of residuals \\(SSR \\ll 1\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#r2",
    "href": "teaching/linear_model/slides/validation.html#r2",
    "title": "Validation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\in [X]\\)\n\n\n\\[R^2 = \\frac{SSE}{SST} = 1-\\frac{SSR}{SST}\\]\n\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\not \\in [X]\\)\n\n\n\\[R^2 = \\frac{\\|\\widehat Y\\|^2}{\\|Y\\|^2} = 1 - \\frac{SCR}{\\|Y\\|^2}\\]\n\n\n\n\n\n\\(0 \\leq R^2 \\leq 1\\). Better model if \\(R^2\\) close to \\(1\\)\nTwo definitions of \\(R^2\\) when \\(\\1 \\in [X]\\) or not\nIn simple linear regression \\((Y_i = \\beta_1+\\beta_2X_i+\\varepsilon_i)\\): \\(R^2 = \\hat \\rho^2\\) is the square empirical correlation between \\(Y\\) and \\(X\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#adjusted-r2",
    "href": "teaching/linear_model/slides/validation.html#adjusted-r2",
    "title": "Validation",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nMain flaw of \\(R^2\\): adding a new variables decreases \\(R^2\\) (because \\([X]\\) is a bigger projection space)\n\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\in [X]\\)\n\n\n\\[R^2_a = 1-\\frac{n-1}{n-p}\\frac{SSR}{SST}\\]\n\n\n\n\n\n\n\n\\(R^2\\) if \\(\\1 \\not \\in [X]\\)\n\n\n\\[R^2_a = 1 - \\frac{n}{n-p}\\frac{SCR}{\\|Y\\|^2}\\]\n\n\n\n\n\n\nWith a new variable, \\(SCR\\) decreases but \\(p \\to p+1\\)\n\n\n\\(R_a^2\\)​ only decreases when adding a new variable if that variable significantly reduces the residual sum of squares. ()"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#r-output-interpretation",
    "href": "teaching/linear_model/slides/validation.html#r-output-interpretation",
    "title": "Validation",
    "section": "R output interpretation",
    "text": "R output interpretation\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nHere, \\(R^2=0.9353\\) and \\(R^2_a=0.9331\\).\n\n\\(\\approx 93\\%\\) of the variability is explained by the model."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linear-constraints",
    "href": "teaching/linear_model/slides/validation.html#linear-constraints",
    "title": "Validation",
    "section": "Linear Constraints",
    "text": "Linear Constraints\n\nWe want to test q linear constraints on the coefficient vector \\(\\beta \\in \\mathbb R^p\\).\n\n\nThis is formulated as: \\[H_0: R\\beta = 0 \\quad \\text{vs} \\quad H_1: R\\beta \\neq 0\\]\nwhere R is a (q × p) constraint matrix encoding the restrictions, with \\(q \\leq p\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#common-test-types",
    "href": "teaching/linear_model/slides/validation.html#common-test-types",
    "title": "Validation",
    "section": "Common Test Types",
    "text": "Common Test Types\n\\[H_0: R\\beta = 0 \\quad \\text{vs} \\quad H_1: R\\beta \\neq 0\\]\n\nStudent’s t-test: is variable \\(j\\) significant?\n\\(H_0: \\beta_j = 0\\) vs \\(H_1: \\beta_j \\neq 0\\)\nGlobal F-test: is any variable significant? Identity matrix excluding intercept\n\\(H_0: \\beta_2 = \\cdots = \\beta_p = 0\\) vs \\(H_1: \\exists j \\in \\{2,\\ldots,p\\}\\) s.t. \\(\\beta_j \\neq 0\\)\nNested model test: are q variables jointly significant?\n\\(H_0: \\beta_{p-q+1} = \\cdots = \\beta_p = 0\\) vs \\(H_1\\): the contrary"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#key-applications",
    "href": "teaching/linear_model/slides/validation.html#key-applications",
    "title": "Validation",
    "section": "Key Applications",
    "text": "Key Applications\n\nIndividual significance: Testing if a single predictor matters\nOverall model significance: Testing if the model explains anything beyond the intercept\n\nVariable subset significance: Testing if a group of variables contributes to the model"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#fisher-test",
    "href": "teaching/linear_model/slides/validation.html#fisher-test",
    "title": "Validation",
    "section": "Fisher Test",
    "text": "Fisher Test\n\n\n\nTheorem\n\n\n\n\\(SSR\\): sum of squares of residuals in the unconstrained regression model\n\\(SSR_c\\): sum of squares of residuals in the constrained regression model, i.e., in the sub-model satisfying \\(R\\beta = 0\\)\n\n\nIf \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R)=q\\) and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{{SSR}_c - {SSR}}{{SSR}} \\sim F(q, n-p)\\]\nwhere \\(F(q, n-p)\\) denotes the Fisher distribution with \\((q, n-p)\\) degrees of freedom. Elements of proof"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#rejection-region",
    "href": "teaching/linear_model/slides/validation.html#rejection-region",
    "title": "Validation",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nKey argument: \\(SSR_c - SSR\\) is equal to \\(\\|P_{V}Y\\|^2\\), where \\(V=X(Ker(R))^{\\perp} \\cap [X]\\) and \\(\\text{dim}(V)=q\\).\n\n\nTherefore, the critical region at significance level \\(\\alpha\\) for testing \\(H_0: R\\beta = 0\\) against \\(H_1: R\\beta \\neq 0\\) is:\n\n\\[RC_\\alpha = \\{F &gt; f_{q,n-p}(1-\\alpha)\\}\\]\n\nwhere \\(f_{q,n-p}(1-\\alpha)\\) denotes the \\((1-\\alpha)\\)-quantile of an \\(F(q, n-p)\\) distribution."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-1-student-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-1-student-test",
    "title": "Validation",
    "section": "Particular Case 1: Student Test",
    "text": "Particular Case 1: Student Test\n\nFix some variable \\(j\\) and consider\n\n\\(H_0: \\beta_j=0\\) VS \\(H_1:\\beta_j\\neq 0\\)\n\n\n\nOnly one constraint: \\(q=1\\), so that\n\n\\[ F = (n-p) \\frac{SCR_c-SCR}{SCR} \\sim \\mathcal F(1,n-p) \\sim \\mathcal T^2(n-p)\\]\n\n\n\nIn fact, Here \\(F = \\big(\\tfrac{\\hat \\beta_j}{\\hat \\sigma_{\\hat \\beta_j}}\\big)^2\\) so \\(F\\) is the student test presented before"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-2-global-fisher-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-2-global-fisher-test",
    "title": "Validation",
    "section": "Particular Case 2: Global Fisher Test",
    "text": "Particular Case 2: Global Fisher Test\n\n\n\\(H_0: \\beta_2= \\dots = \\beta_p=0\\) VS \\(H_1\\): contrary\n\n\\(q = p-1\\) in this case . . .\n\n\\[F = \\frac{n-p}{p-1}\\frac{SSE}{SSR} = \\frac{n-p}{p-1}\\frac{R^2}{1-R^2} \\sim \\mathcal F(p-1, n-p)\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#particular-case-3-nested-fisher-test",
    "href": "teaching/linear_model/slides/validation.html#particular-case-3-nested-fisher-test",
    "title": "Validation",
    "section": "Particular Case 3: Nested Fisher Test",
    "text": "Particular Case 3: Nested Fisher Test\n\n\n\\(H_0: \\beta_{p-q+1}= \\dots = \\beta_p=0\\) VS \\(H_1\\): contrary\n\n\n\n\n\\[F = \\frac{n-p}{q}\\frac{SCR_c - SCR}{SCR} \\sim \\mathcal F(q, n-p)\\]\n\n\n\nInterpretation:\nIf \\(F \\geq f_{q,n-p}(1-\\alpha)\\) (\\(1-\\alpha\\)-quantile of Fisher dist.) then constraints are not satisfied. We do not accept the submodel with respect to larger model."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#example-of-r-output",
    "href": "teaching/linear_model/slides/validation.html#example-of-r-output",
    "title": "Validation",
    "section": "Example of R Output",
    "text": "Example of R Output\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\nHere, Fisher global significance test statistic is \\(F=419.4\\), \\(q=1\\) and \\(n-p=29\\) (\\(n=31\\)). pvalue is negligible\n\n\nHere, \\(q=1\\) and \\(Global Fisher test\\) is a Student Test."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#model-assumptions",
    "href": "teaching/linear_model/slides/validation.html#model-assumptions",
    "title": "Validation",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nThe linear regression model relies on the following key assumptions:\n\nModel specification: \\(Y = X\\beta + \\varepsilon\\) (linear relationship)\nFull rank design: \\(\\text{rank}(X) = p\\) (no perfect multicollinearity)\nZero mean errors: \\(\\mathbb{E}(\\varepsilon) = 0\\)\nHomoscedastic errors: \\(\\text{Var}(\\varepsilon) = \\sigma^2 I_n\\) (constant variance and uncorrelated errors)\n\n\nNow: diagnostic tools to verify each assumption and remedial strategies when they fail."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linearity-assumption",
    "href": "teaching/linear_model/slides/validation.html#linearity-assumption",
    "title": "Validation",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\nThe linearity assumption \\(\\mathbb{E}(Y) = X\\beta\\) is the fundamental hypothesis of linear regression.\nDiagnostic Tools:\n\nPre-modeling: Scatter plots \\((X^{(j)}, Y)\\) and empirical correlations for each predictor\nPost-modeling: Residual analysis - non-linearity manifests as patterns in \\(\\hat{\\varepsilon}\\)\nAdvanced: Partial residual plots (not covered here)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#linearity-assumption-1",
    "href": "teaching/linear_model/slides/validation.html#linearity-assumption-1",
    "title": "Validation",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\nThe linearity assumption \\(\\mathbb{E}(Y) = X\\beta\\) is the fundamental hypothesis of linear regression.\nRemedial Strategies:\n\nTransformations: Apply transformations to \\(Y\\) and/or predictors \\(X^{(j)}\\) to achieve linearity\nAlternative models: If transformations fail, consider nonlinear regression models"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#full-rank-assumption",
    "href": "teaching/linear_model/slides/validation.html#full-rank-assumption",
    "title": "Validation",
    "section": "Full Rank Assumption",
    "text": "Full Rank Assumption\nThe condition \\(\\text{rank}(X) = p\\) ensures no predictor \\(X^{(j)}\\) is a linear combination of others.\n\nWhy it matters:\n\nIdentifiability: Without full rank, \\(\\beta\\) is not uniquely defined\nEstimation: \\((X^TX)\\) becomes non-invertible, making \\(\\hat{\\beta} = (X^TX)^{-1}X^TY\\) undefined\nInfinitely many solutions satisfy \\(X^TX\\hat{\\beta} = X^TY\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#full-rank-assumption-1",
    "href": "teaching/linear_model/slides/validation.html#full-rank-assumption-1",
    "title": "Validation",
    "section": "Full Rank Assumption",
    "text": "Full Rank Assumption\nThe condition \\(\\text{rank}(X) = p\\) ensures no predictor \\(X^{(j)}\\) is a linear combination of others.\nIn practice:\n\nPerfect collinearity is rare, so \\(\\text{rank}(X) = p\\) usually holds\nNear-collinearity is the real concern - when predictors are “almost” linearly dependent"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#near-collinearity-issues",
    "href": "teaching/linear_model/slides/validation.html#near-collinearity-issues",
    "title": "Validation",
    "section": "Near-Collinearity Issues",
    "text": "Near-Collinearity Issues\nWhen a variable is highly correlated with others (correlation close to but not exactly \\(\\pm 1\\)):\n\nMathematical consequences:\n\n\\(X'X\\) remains invertible, but its smallest eigenvalue approaches zero\n\\((X'X)^{-1}\\) becomes numerically unstable"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#near-collinearity-issues-1",
    "href": "teaching/linear_model/slides/validation.html#near-collinearity-issues-1",
    "title": "Validation",
    "section": "Near-Collinearity Issues",
    "text": "Near-Collinearity Issues\n\nStatistical implications:\n\nInstability: Adding/removing a single observation can drastically change \\((X'X)^{-1}\\)\nUnreliable estimates: \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\) becomes highly unstable\nInflated variance: \\(\\text{Var}(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\) becomes very large\n\n\n\nThis is undesirable from a statistical point of view"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#detecting-collinearity-vif",
    "href": "teaching/linear_model/slides/validation.html#detecting-collinearity-vif",
    "title": "Validation",
    "section": "Detecting Collinearity: VIF",
    "text": "Detecting Collinearity: VIF\nCompute the VIF (Variance Inflation Factor) for each \\(X^{(j)}\\):\n\nRegress \\(X^{(j)}\\) on all other \\(X^{(k)}\\) (where \\(k \\neq j\\))\nCompute \\(R_j^2\\) from this regression\nCalculate: \\(\\text{VIF}_j = \\frac{1}{1 - R_j^2}\\)\n\n\nProperties:\n\n\\(\\text{VIF}_j \\geq 1\\) always\nHigh VIF indicates collinearity. Common threshold: \\(\\text{VIF}_j \\geq 5\\). In R: vif() from car package"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#remedies-for-multicollinearity",
    "href": "teaching/linear_model/slides/validation.html#remedies-for-multicollinearity",
    "title": "Validation",
    "section": "Remedies for Multicollinearity",
    "text": "Remedies for Multicollinearity\n\nVariable removal: Drop variables with high VIF\nPreferably remove those least correlated with \\(Y\\)\nPenalized regression: Ridge, LASSO, or elastic net methods (not covered here)\n\n\nImportant Distinction on Multicollinearity\n\nParameter estimation: Multicollinearity severely affects \\(\\hat{\\beta}\\) reliability\nPrediction: Not problematic: \\(\\hat{Y}\\) remains well-defined and stable since projection on \\([X]\\) is still unique"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#analysis-of-the-residuals",
    "href": "teaching/linear_model/slides/validation.html#analysis-of-the-residuals",
    "title": "Validation",
    "section": "Analysis of the Residuals",
    "text": "Analysis of the Residuals\n\nRecall that \\(\\hat \\varepsilon = Y - \\widehat Y = P_{[X]^{\\perp}} \\varepsilon\\)\n\n\nResidual Properties\n\n\\(\\E(\\hat{\\varepsilon}) = 0\\)\n\\(\\Var(\\hat{\\varepsilon}) = \\sigma^2P_{[X]}^{\\perp}\\)\n\\(\\text{Cov}(\\hat{\\varepsilon}, \\hat{Y}) = 0\\)\nIf \\(\\1 \\in [X]\\): \\(\\bar{\\hat{\\varepsilon}} = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#diagnostic-tools",
    "href": "teaching/linear_model/slides/validation.html#diagnostic-tools",
    "title": "Validation",
    "section": "Diagnostic Tools",
    "text": "Diagnostic Tools\n\nGraphical Assessment: Visual evaluation of model quality\nHomoscedasticity Test: Test: \\(\\Var(\\varepsilon_i) = \\sigma^2\\) for all \\(i\\) (constant variance)\nNon-correlation Test:\nTest: \\(\\Var(\\varepsilon)\\) is diagonal (uncorrelated errors)\nNormality Test: Examine normality of residuals"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#residual-vs.-fitted-plot",
    "href": "teaching/linear_model/slides/validation.html#residual-vs.-fitted-plot",
    "title": "Validation",
    "section": "Residual vs. Fitted Plot",
    "text": "Residual vs. Fitted Plot\n\nThe scatter plot between \\(\\hat{Y}\\) and \\(\\hat{\\varepsilon}\\) is informative.\nSince \\(\\text{Cov}(\\hat{\\varepsilon}, \\hat{Y}) = 0\\), no structure should appear.\nIf patterns emerge, this may indicate violations of:\n\nLinearity assumption\nHomoscedasticity assumption\n\nNon-correlation assumption\nOr a combination of these…"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#homosced.-test-breusch-pagan",
    "href": "teaching/linear_model/slides/validation.html#homosced.-test-breusch-pagan",
    "title": "Validation",
    "section": "Homosced. Test (Breusch-Pagan)",
    "text": "Homosced. Test (Breusch-Pagan)\n\nWe want to test whether \\(\\Var(\\varepsilon_i)=\\sigma^2, ~~\\forall i\\)\n\n\nPrinciple: Assume \\(\\varepsilon_i\\) has variance \\(\\sigma_i^2 = \\sigma^2 + z_i^T\\gamma\\) where:\n\n\\(z_i\\) is a \\(k\\)-vector of variables that might explain heteroscedasticity (known)\nDefault in R: \\(z_i = (X_i^{(1)}, \\ldots, X_i^{(p)})\\), so \\(k = p\\)\n\\(\\gamma\\) is an unknown \\(k\\)-dimensional parameter\n\n\n\n\\(H_0: \\gamma = 0\\) (homosced.) VS \\(H_1: \\gamma \\neq 0\\) (heterosced.)\n\n\nR function: bptest from lmtest library"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#consequences-of-heteroscedasticity",
    "href": "teaching/linear_model/slides/validation.html#consequences-of-heteroscedasticity",
    "title": "Validation",
    "section": "Consequences of Heteroscedasticity",
    "text": "Consequences of Heteroscedasticity\n\nWhat happens:\n\nOLS estimation of \\(\\beta\\) is no longer optimal but remains consistent\nInference tools (tests, confidence intervals) become invalid because they rely on \\(\\hat{\\sigma}^2\\) estimation"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#solutions-for-heteroscedasticity",
    "href": "teaching/linear_model/slides/validation.html#solutions-for-heteroscedasticity",
    "title": "Validation",
    "section": "Solutions for Heteroscedasticity",
    "text": "Solutions for Heteroscedasticity\n\nTransformation: Transform \\(Y\\) (e.g., log transformation) to stabilize variance\nModeling: Model heteroscedasticity explicitly and account for it in estimation\nGLS: Use Generalized Least Squares (not detailed here)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#non-correlation-test",
    "href": "teaching/linear_model/slides/validation.html#non-correlation-test",
    "title": "Validation",
    "section": "3. Non-correlation Test",
    "text": "3. Non-correlation Test\n\nPurpose: Test if \\(\\Var(\\varepsilon)\\) is diagonal (uncorrelated errors)\n\n\nCorrelation between \\(\\varepsilon_i\\) often occurs with temporal data (index \\(i\\) represents time)\n\n\nAuto-correlation Model of order \\(r\\):\n\n\\[\\varepsilon_i = \\rho_1 \\varepsilon_{i-1} + \\dots + \\rho_r \\varepsilon_{i-r} + \\eta_i\\] where \\(\\eta_i \\sim \\text{iid } N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#tests-for-correlation",
    "href": "teaching/linear_model/slides/validation.html#tests-for-correlation",
    "title": "Validation",
    "section": "Tests for Correlation",
    "text": "Tests for Correlation\n\nIn the auto-correlation model \\(\\varepsilon_i = \\rho_1 \\varepsilon_{i-1} + \\dots + \\rho_r \\varepsilon_{i-r} + \\eta_i\\):\n\n\nDurbin-Watson Test (for \\(r = 1\\) only):\n\n\\(H_0: \\rho_1 = 0\\) VS \\(H_1: \\rho_1 \\neq 0\\)\nR function: dwtest from lmtest\n\n\n\nBreusch-Godfrey Test (for any \\(r\\)):\n\n\\(H_0: \\rho_1 = \\cdots = \\rho_r = 0\\) VS \\(H_1:\\) at least one \\(\\rho_j \\neq 0\\)\nUser chooses order \\(r\\) (default \\(r = 1\\))\nR function: bgtest from lmtest"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#consequences-of-auto-correlation",
    "href": "teaching/linear_model/slides/validation.html#consequences-of-auto-correlation",
    "title": "Validation",
    "section": "Consequences of Auto-correlation",
    "text": "Consequences of Auto-correlation\n\nOLS estimation of \\(\\beta\\) is no longer optimal but remains consistent\nInference tools (tests, confidence intervals) become invalid\n\n\nSolutions:\n\nGLS modeling: Model the dependence structure (complex, risky if wrong)\nModel improvement: Exploit the dependence to enhance the model\nEx: Explain \\(Y_i\\) using \\(Y_{i-1}\\) in addition to \\((X_i^{(1)}, \\dots, X_i^{(p)})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#normality-test",
    "href": "teaching/linear_model/slides/validation.html#normality-test",
    "title": "Validation",
    "section": "4. Normality Test",
    "text": "4. Normality Test\n\nPurpose: Examine normality of residuals \\(\\hat \\varepsilon\\)\nReminder on Normality Assumption\n\nNot essential when \\(n\\) is large\nAll tests remain asymptotically valid\nOnly prediction intervals truly require normality\n\n\n\nWhy examine it anyway?\nIf \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\) then \\(\\hat{\\varepsilon} \\sim N(0, \\sigma^2 P_{[X]}^{\\perp})\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#diagnostic-tools-for-normality-of-hat-varepsilon",
    "href": "teaching/linear_model/slides/validation.html#diagnostic-tools-for-normality-of-hat-varepsilon",
    "title": "Validation",
    "section": "Diagnostic Tools for Normality of \\(\\hat \\varepsilon\\)",
    "text": "Diagnostic Tools for Normality of \\(\\hat \\varepsilon\\)\n\nQ-Q Plot (Henry’s line):\n\nPlot theoretical vs. sample quantiles of \\(\\hat{\\varepsilon}\\)\nR function: qqnorm\n\n\n\nShapiro-Wilk, \\(\\chi^2\\) or KS Tests:\n\nFormal test of normality for \\(\\hat{\\varepsilon}\\)\n\\(H_0\\): residuals are normally distributed\n\\(H_1\\): residuals are not normally distributed\nR function: shapiro.test"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#outlier-analysis",
    "href": "teaching/linear_model/slides/validation.html#outlier-analysis",
    "title": "Validation",
    "section": "Outlier Analysis",
    "text": "Outlier Analysis\n\nAn individual is atypical when:\n\nPoorly explained by the model, and/or\nHeavily influences coefficient estimation\n\n\n\nIdentify these individuals to:\n\nUnderstand the reason for this particularity\nPotentially modify the model accordingly\n\nPotentially exclude the individual from the study"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#poorly-explained-individuals",
    "href": "teaching/linear_model/slides/validation.html#poorly-explained-individuals",
    "title": "Validation",
    "section": "Poorly Explained Individuals",
    "text": "Poorly Explained Individuals\n\nIndividual \\(i\\) is poorly explained if its residual \\(\\hat{\\varepsilon}_i\\) is “abnormally” large.\n\n\nHow to quantify “abnormally”?\nLet \\(h_{ij}\\) be elements of matrix \\(P_{[X]}\\) (hat matrix).\nFor a Gaussian model: \\(\\hat{\\varepsilon}_i \\sim N(0, (1-h_{ii})\\sigma^2)\\)\n\n\nStandardized Residuals\n\n\\[t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#poorly-explained-individuals-1",
    "href": "teaching/linear_model/slides/validation.html#poorly-explained-individuals-1",
    "title": "Validation",
    "section": "Poorly Explained Individuals",
    "text": "Poorly Explained Individuals\n\n\n\\[h_{ij} = (P_{[X]})_{ij} \\and t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\]\n\n\n\nWe expect \\(t_i \\sim St(n-p)\\) (not strictly true since \\(\\hat{\\varepsilon}_i \\not\\perp \\hat{\\sigma}^2\\))\n\n\n\nDefinition\n\n\nIndividual \\(i\\) is considered poorly explained by the model if: \\[|t_i| &gt; t_{n-p}(1-\\alpha/2)\\] for predetermined \\(\\alpha\\), typically \\(\\alpha = 0.05\\), giving \\(t_{n-p}(1-\\alpha/2) \\approx 2\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#leverage-points",
    "href": "teaching/linear_model/slides/validation.html#leverage-points",
    "title": "Validation",
    "section": "Leverage Points",
    "text": "Leverage Points\n\nA point is influential if it contributes significantly to \\(\\hat{\\beta}\\) estimation.\n\n\nLeverage value: \\(h_{ii}\\) corresponds to the weight of \\(Y_i\\) on its own estimation \\(\\hat{Y}_i\\)\n\n\nWe know that: \\(\\sum_{i=1}^n h_{ii} = \\text{tr}(P_{[X]}) = p\\)\n\n\nTherefore, on average: \\(h_{ii} \\approx p/n\\)\n\n\n\n\n\nDefinition\n\n\nIndividual \\(i\\) is called a leverage point if \\(h_{ii} \\gg p/n\\)\nTypically: \\(h_{ii} &gt; 2p/n\\) or \\(h_{ii} &gt; 3p/n\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#outlier-analysis-cooks-distance",
    "href": "teaching/linear_model/slides/validation.html#outlier-analysis-cooks-distance",
    "title": "Validation",
    "section": "Outlier Analysis: Cook’s Distance",
    "text": "Outlier Analysis: Cook’s Distance\n\nCook’s Distance\nQuantifies the influence of individual \\(i\\) on \\(\\hat{Y}\\):\n\n\\[C_i = \\frac{\\|\\hat{Y} - \\hat{Y}_{(-i)}\\|^2}{p\\hat{\\sigma}^2}\\]\n\nwhere \\(\\hat{Y}_{(-i)} = X\\hat{\\beta}_{(-i)}\\) with \\(\\hat{\\beta}_{(-i)}\\): estimation of \\(\\beta\\) without individual \\(i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/validation.html#cooks-distance-alternative-formula",
    "href": "teaching/linear_model/slides/validation.html#cooks-distance-alternative-formula",
    "title": "Validation",
    "section": "Cook’s Distance, Alternative Formula",
    "text": "Cook’s Distance, Alternative Formula\n\n\n\\[C_i = \\frac{1}{p} \\cdot \\frac{h_{ii}}{1-h_{ii}} \\cdot t_i^2,\\]\n\nwhere \\(t_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\).\nThis formula shows that Cook’s distance \\(C_i\\) combines:\n\nAberrant effect of individual (through \\(t_i\\))\nLeverage effect (through \\(h_{ii}\\))\n\n\n\nR functions: cooks.distance and last plot of plot.lm"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-lectures",
    "href": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-lectures",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Link with Previous Lectures",
    "text": "Link with Previous Lectures\n\nPreviously, we considered that:\n\nResponse variable \\(Y\\) is quantitative\nExplanatory variables \\(X^{(j)}\\) are quantitative\n\n\n\nWe still assume \\(Y\\) is quantitative, but explanatory variables can be qualitative and/or quantitative."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#terminology",
    "href": "teaching/linear_model/slides/anova_ancova.html#terminology",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Terminology",
    "text": "Terminology\n\nANOVA (Analysis of Variance): All explanatory variables \\(X^{(j)}\\) are qualitative\nANCOVA (Analysis of Covariance): Explanatory variables mix both quantitative and qualitative variables\n\n\nWe’ll see that these situations reduce to the case of the previous chapter."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#context-notations",
    "href": "teaching/linear_model/slides/anova_ancova.html#context-notations",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Context, Notations",
    "text": "Context, Notations\n\nWe seek to explain \\(Y\\) using a single qualitative variable \\(A\\).\nWe observe \\((Y, X)\\), and define\n\n\n\n\n\\[N_i = \\sum_{k=1}^n \\mathbf 1\\{X_k=i\\} \\and \\overline Y_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\mathbf 1\\{X_k=i\\}\\]\n\n\n\n\nTotal mean:\n\\(Y_i = \\frac{1}{N}\\sum_{k=1}^n Y_k = \\frac{1}{N}\\sum_{i=1}^I\\sum N_i \\overline Y_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#setting",
    "href": "teaching/linear_model/slides/anova_ancova.html#setting",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Setting",
    "text": "Setting\n\nFor \\(k = 1, \\ldots, n\\)\n\n\\[Y_k = \\sum_{i=1}^I \\mu_i \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\]\n\nwhere \\(\\E[\\varepsilon_k]=0\\), \\(\\Cov(\\varepsilon_k,\\varepsilon_l)=\\sigma^2\\1\\{k=l\\}\\)\n\n\\(Y_k\\) is random and has expectation \\(\\mu_i\\) if \\(X_k=A_i\\)\n\\(\\Var(Y_k)=\\sigma^2\\) is the same regardless of modality \\(A_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#problem",
    "href": "teaching/linear_model/slides/anova_ancova.html#problem",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Problem",
    "text": "Problem\n\nDo we have \\(\\mu_1 = \\mu_2 \\ldots = \\mu_I\\)? (Does factor \\(A\\) influences \\(Y\\)?)\nDoes a \\(\\mu_i\\) has more influence on \\(Y\\)?\nHow do we estimate the \\(\\mu_i\\)’s?"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#one-hot-encoding",
    "href": "teaching/linear_model/slides/anova_ancova.html#one-hot-encoding",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "One Hot Encoding",
    "text": "One Hot Encoding\n\nWe use one-hot encoding: \\(X_{ki}=\\1\\{X_k=A_i\\}\\)\n\n\nThat is, for individual \\(k\\), \\(X_{k\\cdot} = (\\1\\{X_k=A_1\\}, \\dots, \\1\\{X_k=A_I\\}) \\in \\{0,1\\}^I\\)\n\n\nOr, using previous notations, \\(X = (X^{(1)}, \\dots, X^{(I)})\\) where\n\\[X^{(i)}= \\begin{pmatrix}\nX^{(i)}_1 \\\\\n\\vdots  \\\\\nX^{(i)}_n\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#in-other-words",
    "href": "teaching/linear_model/slides/anova_ancova.html#in-other-words",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "In Other Words",
    "text": "In Other Words\n\nIf there are \\(3\\) categories e.g. blue, orange, green, we replace the column \\(X\\) by \\(3\\) columns\nExample with \\(I=3\\) categories and \\(n=5\\) individuals \\[\n\\begin{pmatrix}\n\\color{blue}{\\mathrm{blue}} \\\\\n\\mathrm{green} \\\\\n\\color{blue}{\\mathrm{blue}} \\\\\n\\mathrm{orange} \\\\\n\\mathrm{orange} \\\\\n\\end{pmatrix} \\quad  \\text{becomes} \\quad\nX=\\begin{pmatrix}\n\\color{blue}{\\mathrm{1}} & \\mathrm{0} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{0} & \\mathrm{1}\\\\\n\\color{blue}{\\mathrm{1}} & \\mathrm{0} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{1} & \\mathrm{0}\\\\\n\\mathrm{0} & \\mathrm{1} & \\mathrm{0}\\\\\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-matrix-form",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-matrix-form",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Model, Matrix Form",
    "text": "Model, Matrix Form\n\nWe rewrite the model \\(Y_k = \\sum_{i=1}^I \\mu_i \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\) as\n\n\\[Y = X\\mu + \\varepsilon\\]\n\n\nHere, \\(X_{k\\cdot} = (\\1\\{X_k=A_1\\}, \\dots, \\1\\{X_k=A_I\\}) \\in \\{0,1\\}^I\\)\nThere is no constant in this model"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model-with-constant",
    "href": "teaching/linear_model/slides/anova_ancova.html#model-with-constant",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Model with Constant",
    "text": "Model with Constant\n\nIf we want to model the constant (intercept) and \\(I-1\\) modalities, we assume\n\n\\[Y_k = \\mu_1+\\sum_{i=2}^I (\\mu_i-\\mu_1) \\mathbf{1}\\{X_k=A_i\\} + \\varepsilon_k\\]\n\n\n\nWhy \\(\\mu_i - \\mu_1\\) and not just \\(\\mu_i\\)?\n\n\\(\\E[Y_k | X_k=A_i] = \\mu_1 + \\mu_i - \\mu_1 = \\mu_i\\)\n\\(\\sum_{i=2}^{I}\\{X_k = A_i\\} = 1\\) (collinearity problem)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#in-r",
    "href": "teaching/linear_model/slides/anova_ancova.html#in-r",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "In R",
    "text": "In R\n\n\nwith intercept (default)\n\nlm(Y~A) #with intercept\n\n\nInterpretation: gives expectation \\(\\E[Y_k] = \\mu_1\\) and coefficients \\(\\alpha_i= \\mu_i - \\mu_1\\)\n \n\n\n\n\nwithout intercept\n\nlm(Y~A-1) #without intercept\n\n\nInterpretation: gives coefficients \\(\\alpha_i= \\mu_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#estimation-of-mu",
    "href": "teaching/linear_model/slides/anova_ancova.html#estimation-of-mu",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Estimation of \\(\\mu\\)",
    "text": "Estimation of \\(\\mu\\)\n\nWhichever the model we choose (with constant or not), estimation of \\(\\mu_i=\\E[Y_k|X_k=A_i]\\) is the same. Same for \\(\\Var(\\varepsilon_k)=\\sigma^2\\).\n\n\n\n\n\nProposition\n\n\nIn category \\(i\\), OLS estimation of \\(\\mu_i\\) leads to:\n\n\\(\\hat{\\mu}_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\1\\{X_k=A_i\\} = \\overline{Y}_i\\)\n\nAn unbiased estimator of \\(\\sigma^2\\) is\n\n\\(\\hat{\\sigma}^2 = \\frac{1}{n-I} \\sum_{k=1}^{n}\\sum_{i=1}^I (Y_{k} - \\overline{Y}_i)^2\\1\\{X_k=A_i\\}\\)\n\n\n\n\n\n\nProof (OLS): derive \\(\\sum_{k=1}^n (Y_k - \\mu'_i)^2\\) with respect to \\(\\mu'_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-for-factor-effect-anova-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-for-factor-effect-anova-test",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Testing for Factor Effect (ANOVA Test)",
    "text": "Testing for Factor Effect (ANOVA Test)\n\nWe want to test \\(H_0: \\mu_1 = \\cdots = \\mu_I\\).\nThis is a linear constraints test! (See previous chapters)\n\n\n\n\n\nProposition\n\n\nIf \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: \\mu_1 = \\cdots = \\mu_I\\):\n\\[F = \\frac{SSB/(I-1)}{SSW/(n-I)} \\sim F(I-1, n-I)\\]\n\n\\(SSB = \\sum_{i=1}^I N_i (\\overline{Y}_i - \\overline{Y})^2\\)\n\\(SSW = \\sum_{i=1}^I \\sum_{k=1}^{n} (Y_{k} - \\overline{Y}_i)^2\\1\\{X_k=A_i\\}\\)\n\nCritical region at level \\(\\alpha\\): \\(RC_\\alpha = \\{F &gt; f_{I-1,n-I}(1-\\alpha)\\}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-chapter",
    "href": "teaching/linear_model/slides/anova_ancova.html#link-with-previous-chapter",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Link with previous chapter",
    "text": "Link with previous chapter\nIn \\(\\mu_1= \\ldots = \\mu_I\\), there are \\(I-1\\) constraints to test.\n\n\\[F = \\frac{n-I}{I-1} \\cdot \\frac{SSR_c - SSR}{SSR}\\]\n\n\nWe show that\n\n\\(SSR = SSW\\)\n\\(SSR_c = SST = \\sum_{k=1}^n(Y_k - \\overline Y)^2\\)\n\\(SST = SSB + SSW\\).\n\n\n\nIn R: anova(lm(Y~A))"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#factor-significance-analysis-of-variance-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#factor-significance-analysis-of-variance-test",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Factor Significance: Analysis of Variance Test",
    "text": "Factor Significance: Analysis of Variance Test\n\n\n\n\nWarning\n\n\nThe previous analysis of variance test tests equality of means between modalities not equality of variances\n\n\n\n\n\nIt is valid under the assumption \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\).\n\nGaussian assumption: Not critical if \\(n\\) is large\nHomoscedasticity: Important assumption"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#homoscedasticity-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#homoscedasticity-tests",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Homoscedasticity Tests",
    "text": "Homoscedasticity Tests\n\nHow to test equality of variances in each modality:\n\nLevene test\nBartlett test\n\n\n\nIn R: leveneTest or bartlett.test from car library"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis-multiple-testing-problem",
    "href": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis-multiple-testing-problem",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Post-hoc Analysis: Multiple Testing Problem",
    "text": "Post-hoc Analysis: Multiple Testing Problem\n\nIf factor \\(A\\) is significant, we want to know more:\nwhich modality(ies) differs from others?\n\n\nWe want to perform all tests:\n\n\\[H_{0}^{i,j}: \\mu_i = \\mu_j \\quad \\text{vs} \\quad H_{1}^{i,j}: \\mu_i \\neq \\mu_j\\]\n\n\n\nfor all \\(i \\neq j\\) in \\(\\{1, \\ldots, I\\}\\), corresponding to \\(I(I-1)/2\\) tests."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-naive-approach",
    "href": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-naive-approach",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Multiple Testing: Naive Approach",
    "text": "Multiple Testing: Naive Approach\n\nPerform all Student’s t-tests for mean comparison (1 constraint), each at level \\(\\alpha\\).\nProblem: Given the number of tests, this would lead to many false positives.\n\n\nFalse positives are a well-known problem in multiple testing.\n\n\nSolution: Apply a correction to the decision rule, e.g.\n\nBonferroni correction\nBenjamini-Hochberg correction\n\n\n\nFor one-way ANOVA: Tukey’s test addresses the problem."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-tukeys-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#multiple-testing-tukeys-test",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Multiple Testing: Tukey’s Test",
    "text": "Multiple Testing: Tukey’s Test\n\n\n\\[Q = \\max_{(i,j)} \\frac{|\\overline{Y}_i - \\overline{Y}_j|}{\\hat{\\sigma}\\sqrt{\\frac{1}{N_i} + \\frac{1}{N_j}}}\\]\n\n\n\n\n\n\nDistribution of Tukey’s Test Statistic\n\n\nUnder \\(H_0: \\mu_1 = \\cdots = \\mu_I\\) and assuming \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\):\n\\[Q \\sim Q_{I,n-I}\\]\nwhere \\(Q_{I,n-I}\\) denotes the Tukey distribution with \\((I, n-I)\\) degrees of freedom.\nNote: This is exact if all \\(n_i\\) are equal, otherwise the distribution is approximately Tukey."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#individual-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#individual-tests",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Individual Tests",
    "text": "Individual Tests\nTo test each \\(H_0^{i,j}: \\mu_i = \\mu_j\\), we use the critical regions:\n\n\n\n\\[RC_\\alpha^{i,j} = \\left\\{|\\overline{Y}_i - \\overline{Y}_j| &gt; \\frac{\\hat{\\sigma}}{\\sqrt{2}} \\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}} \\cdot Q_{I,n-I}(1-\\alpha)\\right\\}\\]\n\n\nwhere \\(Q_{I,n-I}(1-\\alpha)\\) denotes the \\((1-\\alpha)\\) quantile of a tukey distribution \\(Q(I,n-I)\\) of degrees \\((I, n-I)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#key-properties-of-tukeys-test",
    "href": "teaching/linear_model/slides/anova_ancova.html#key-properties-of-tukeys-test",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Key Properties of Tukey’s Test",
    "text": "Key Properties of Tukey’s Test\nThe form of the previous \\(RC_\\alpha^{i,j}\\) ensures that: \\[\\mathbb{P}_{\\mu_1 = \\cdots = \\mu_I}\\left(\\bigcup_{(i,j)} H_1^{i,j}\\right) = \\alpha\\]\nInterpretation: If all null hypotheses \\(H_0^{i,j}\\) are true (\\(\\mu_1 = \\cdots = \\mu_I\\)), then the probability of concluding at least one \\(H_1^{i,j}\\) equals \\(\\alpha\\).\n\nThis is the simultaneous Type I error rate equals \\(\\alpha\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#comparison-with-students-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#comparison-with-students-tests",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Comparison with Student’s tests",
    "text": "Comparison with Student’s tests\n\nIndividual Student’s t-tests at level \\(\\alpha\\): only guarantee \\(\\mathbb{P}_{\\mu_i = \\mu_j}(H_1^{i,j}) = \\alpha\\)\nWhen cumulated: \\(\\mathbb{P}_{\\mu_1 = \\cdots = \\mu_I}\\left[\\cup_{(i,j)} H_1^{i,j}\\right] \\approx 1\\) → false positives\n\n\nAdvantage\nWith Tukey’s test, two significantly different means are truly different, not just due to false positives.\nIn R: TukeyHSD"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-of-r-output",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-of-r-output",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Example of R output",
    "text": "Example of R output\n\nHomoscedasticity Test:\nleveneTest(Loss∼Exercise)\nWe get\nLevene’s Test for Homogeneity of Variance (center = median)\nDf F value Pr(&gt;F)\ngroup 3 0.6527 0.584\n68\n\n\nHomoscedasticity is ok. ANOVA?\nreg=lm(Loss∼Exercise) # Exercise has 4 categories\nanova(reg)\n\n\nWe get\nResponse: Loss\n         Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nExercise   3 712.56  237.519  20.657 1.269e-09 ***\nResiduals 68 781.89   11.498"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#results-interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#results-interpretation",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Results Interpretation",
    "text": "Results Interpretation\n\nThe means are significantly different. We read in particular:\n\n\\(I - 1 = 3\\), \\(n - I = 68\\)\n\\(SSB = 712.56\\), \\(SSW = 781.89\\)\n\\(F = \\frac{SSB/(I-1)}{SSW/(n-I)} = 20.657\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#r-example-post-hoc-analysis",
    "href": "teaching/linear_model/slides/anova_ancova.html#r-example-post-hoc-analysis",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "R Example: Post-hoc Analysis",
    "text": "R Example: Post-hoc Analysis\n\nWe finish by analyzing the mean differences more precisely:\n\n\nTukeyHSD(aov(Loss~Exercise))\n\n\nExercise\n     diff        lwr        upr     p adj\n2-1  7.1666667   4.1897551 10.1435782 0.0000001\n3-1  3.8888889   0.9119773  6.8658005 0.0053823\n4-1 -0.6111111  -3.5880227  2.3658005 0.9487355\n3-2 -3.2777778  -6.2546894 -0.3008662 0.0252761\n4-2 -7.7777778 -10.7546894 -4.8008662 0.0000000\n4-3 -4.5000000  -7.4769116 -1.5230884 0.0009537\nConclusion: All differences are significant, except between exercise 4 and exercise 1."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#setting-1",
    "href": "teaching/linear_model/slides/anova_ancova.html#setting-1",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Setting",
    "text": "Setting\n\nWe observe \\(Y\\) and explanatory variables \\(X^{(1)}, X^{(2)}\\)\n\n\\(X^{(1)}_k \\in \\{A_1, \\dots, A_I\\}\\) (\\(I\\) modalities)\n\\(X^{(2)}_k \\in \\{B_1, \\dots, B_J\\}\\) (\\(J\\) modalities)\n\\(N_{ij} = \\sum_{k=1}^n \\1\\{X^{(1)}_k \\in A_i\\}\\1\\{X_k^{(2)} \\in B_j\\}\\) individuals in modality \\(A_i\\) and \\(B_j\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#model",
    "href": "teaching/linear_model/slides/anova_ancova.html#model",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Model",
    "text": "Model\n\n\n\n\\[\\begin{aligned}\nY_k &= m + \\alpha_i\\sum_{i=1}^I\\1\\{X^{(1)}_k \\in A_i\\} +\\beta_i\\sum_{j=1}^J\\1\\{X^{(2)}_k \\in B_j\\} \\\\\n&+ \\gamma_{ij}\\sum_{i=1}^I\\sum_{j=1}^J\\1\\{X^{(1)}_k \\in A_i\\}\\1\\{X^{(2)}_k \\in B_i\\} + \\varepsilon_k\n\\end{aligned}\\]\n\n\n\n\nIn other words, in modality \\(A_i\\) and \\(B_j\\),\n\n\n\n\\(Y_k =m+\\alpha_i + \\beta_j + \\gamma_{ij}+ \\varepsilon_k\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#interpretation",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Interpretation",
    "text": "Interpretation\n\nThis is a model of the type \\(Y_k = \\mu_{ij} + \\varepsilon_k\\).\n\n\nIn modalities \\((i,j)\\), \\(\\E[Y_k] = \\mu_{ij} = m + \\alpha_i + \\beta_j + \\gamma_{ij}\\)\n\n\\(m\\): the average effect of \\(Y\\) (without considering \\(A\\) and \\(B\\))\n\\(\\alpha_i = \\mu_{i.} - m\\): the marginal effect due to \\(A\\)\n\\(\\beta_j = \\mu_{.j} - m\\): the marginal effect due to \\(B\\)\n\n\\(\\gamma_{ij} = \\mu_{ij} - m - \\alpha_i - \\beta_j\\): the remaining effect, due to interaction between \\(A\\) and \\(B\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-1",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-1",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Example 1",
    "text": "Example 1\n\n\\(Y\\): employee satisfaction\n\\(A\\): schedule type (flexible or fixed)\n\\(B\\): training level (basic or advanced)\n\n\nWe can imagine:\n\nEffect due to \\(A\\): satisfaction is greater with flexible schedules\nEffect due to \\(B\\): satisfaction is higher with advanced training\nNo particular interaction between A and B"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#example-2",
    "href": "teaching/linear_model/slides/anova_ancova.html#example-2",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Example 2",
    "text": "Example 2\n\n\\(Y\\): plant yield\n\\(A\\): fertilizer type (1 or 2)\n\\(B\\): water quantity (low, medium, high)\nWe can imagine:\n\nEffect due to \\(A\\): yield differs according to fertilizer used\nEffect due to \\(B\\): yield is better when there is lots of water\nInteraction: fertilizer 1 is better with water, and vice versa for fertilizer 2\n\n\n\nMaybe interaction is so strong that the effect due to A seems absent!"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#constraints-on-the-parameters",
    "href": "teaching/linear_model/slides/anova_ancova.html#constraints-on-the-parameters",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Constraints on the Parameters",
    "text": "Constraints on the Parameters\n\nIn modalities \\((i,j)\\), \\(\\E[Y_k] = \\mu_{ij} = m + \\alpha_i + \\beta_j + \\gamma_{ij}\\)\n\n\nInitial two-factor ANOVA problem: \\(I \\times J\\) parameters \\(\\mu_{ij}\\).\n\n\nNow: \\(1 + I + J + IJ\\) parameters (\\(m\\), \\(\\alpha_i\\), \\(\\beta_j\\), \\(\\gamma_{ij}\\)).\n\n\nTherefore, we need \\(1 + I + J\\) constraints for identifiability:\n\n\n\n\\(\\sum_{i=1}^{I} \\alpha_i = 0 \\and \\sum_{j=1}^{J} \\beta_j = 0\\)\n\n\n\n\n\\(\\sum_{i=1}^{I} \\gamma_{ij} = 0 \\and \\sum_{j=1}^{J} \\gamma_{ij} = 0\\)\n\n\n\nThese constraints ensure model identifiability by removing the redundant parameters that cause multicollinearity issues."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#implementation-in-r",
    "href": "teaching/linear_model/slides/anova_ancova.html#implementation-in-r",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Implementation in R",
    "text": "Implementation in R\n\nThe complete model (with interaction) is launched with the command:\nlm(Y ~ A + B + A:B) ## or equivalently, lm(Y ~ A*B)\n\n\nWith these constraints, the parameter interpretation is as follows:\nIntercept: \\(m = \\mu_{11}\\)\nMain effect A: \\(\\alpha_i = \\mu_{i1} - \\mu_{11}\\)\nMain effect B: \\(\\beta_j = \\mu_{1j} - \\mu_{11}\\)\nInteraction: \\(\\gamma_{ij} = \\mu_{ij} - \\mu_{i1} - \\mu_{1j} + \\mu_{11}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#important-note-on-interpretation",
    "href": "teaching/linear_model/slides/anova_ancova.html#important-note-on-interpretation",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Important Note on Interpretation",
    "text": "Important Note on Interpretation\n\nFrom\nlm(Y ~ A + B + A:B) ## or equivalently, lm(Y ~ A*B)\n\n\nIntercept: \\(m = \\mu_{11}\\)\nMain effect A: \\(\\alpha_i = \\mu_{i1} - \\mu_{11}\\)\nMain effect B: \\(\\beta_j = \\mu_{1j} - \\mu_{11}\\)\nInteraction: \\(\\gamma_{ij} = \\mu_{ij} - \\mu_{i1} - \\mu_{1j} + \\mu_{11}\\)\n\n\n\n\n\n\nBe Cautious on the Interpretation of the Coefficients\n\n\nTo impose the constraints from the previous approach (sum-to-zero constraints):\nlm(Y ~ A*B, contrasts = list(A = contr.sum, B = contr.sum))"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#estimation",
    "href": "teaching/linear_model/slides/anova_ancova.html#estimation",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Estimation",
    "text": "Estimation\n\nThe choice of constraints does not affect the estimation of the expectation of \\(Y\\) in each crossed modality \\(A_i \\cap B_j\\).\n\n\n\n\n\nProposition\n\n\nWhatever the linear constraints chosen, the OLS leads to, for all \\(i = 1, \\ldots, I\\), \\(j = 1, \\ldots, J\\) and \\(k = 1, \\ldots, N_{ij}\\), if \\(X^{(1)}_{k}=A_i\\) and \\(X_k^{(2)}=B_j\\):\n\\[\\widehat{Y}_{k} = \\overline{Y}_{ij}:= \\frac{1}{N_{ij}} \\sum_{k=1}^{n} Y_{k}\\1\\{X_k^{(1)}=A_i \\and X_k^{(2)}=B_j\\}\\]\nand to the estimation of the residual variance:\n\\[\\hat{\\sigma}^2 = \\frac{1}{n - IJ} \\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^n (Y_{k} - \\overline{Y}_{ij})^2\\1\\{X_k^{(1)}=A_i \\and X_k^{(2)}=B_j\\}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#significance-tests-for-effects",
    "href": "teaching/linear_model/slides/anova_ancova.html#significance-tests-for-effects",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Significance Tests for Effects",
    "text": "Significance Tests for Effects\n\n\nIs the effect due to the interaction between A and B significant?\nIs the marginal effect due to A significant?\nIs the marginal effect due to B significant?\n\n\n\nFirst, in the additive model with interaction, \\(Y_{ijk} = m + \\alpha_i + \\beta_j + \\varepsilon_{ijk}\\)\n\n\nDo we have \\(\\gamma_{ij} = 0\\) for all \\(i, j\\)?"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#interpretation-on-plots",
    "href": "teaching/linear_model/slides/anova_ancova.html#interpretation-on-plots",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Interpretation on Plots:",
    "text": "Interpretation on Plots:\n\nPlot \\(\\overline Y_{ij}\\) in function of modalities \\((i,j)\\)\n\n\n\nwithout interactions, lines should be almost parallel\n\n\n\ninteraction.plot(A,B,Y)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#plot-presence-of-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#plot-presence-of-interaction",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Plot: Presence of Interaction",
    "text": "Plot: Presence of Interaction\nLines cross in presence of interaction"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#significance-tests",
    "href": "teaching/linear_model/slides/anova_ancova.html#significance-tests",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Significance Tests",
    "text": "Significance Tests\n\nWe want to test for the presence of an interaction:\n\n\\[H_0^{(AB)}: \\gamma_{ij} = 0 \\text{ for all } i, j\\]\n\n\n\nIf we conclude \\(H_0^{(AB)}\\) (accept the null hypothesis), we then want to test the marginal effects:\n\n\\[H_0^{(A)}: \\alpha_i = 0 \\text{ for all } i \\and H_0^{(B)}: \\beta_j = 0 \\text{ for all } j\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#presence-of-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#presence-of-interaction",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Presence of Interaction",
    "text": "Presence of Interaction\n\n\n\\[H_0^{(AB)}: \\gamma_{ij} = 0 \\text{ for all } i, j\\]\n\n\n\n\n\n\nWarning\n\n\nIf we reject \\(H_0^{(AB)}\\), it makes no sense to test whether A or B have an effect: they have one through their interaction.\n\n\n\n\n\nThese tests reduce to constraint tests in the regression model."
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#balanced-design-analysis-of-variance",
    "href": "teaching/linear_model/slides/anova_ancova.html#balanced-design-analysis-of-variance",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Balanced Design Analysis of Variance",
    "text": "Balanced Design Analysis of Variance\n\nWe assume that “the design is balanced”: this means that \\(N_{ij}:=N\\) does not depend on \\(i\\) or \\(j\\). (Otherwise, everything becomes complicated).\n\n\nIn this case, we have the analysis of variance formula:"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#anova-formula",
    "href": "teaching/linear_model/slides/anova_ancova.html#anova-formula",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "ANOVA Formula",
    "text": "ANOVA Formula\n\n\n\n\\[S_T^2 = S_A^2 + S_B^2 + S_{AB}^2 + S_R^2\\]\n\n\n\\(S_T^2 = \\sum_{k=1}^{n} (Y_{k} - \\overline{Y})^2\\): total sum of squares\n\\(S_A^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{i.} - \\overline{Y})^2\\): \\(S^2_{between}\\) in the case of one-factor ANOVA where the factor is \\(A\\)\n\\(S_B^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{.j} - \\overline{Y})^2\\): \\(S^2_{between}\\) in the case of one-factor ANOVA where the factor is \\(B\\)\n\\(S_{AB}^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} N_{ij} (\\overline{Y}_{ij} - \\overline{Y}_{i.} - \\overline{Y}_{.j} + \\overline{Y})^2\\) quantifies the interaction\n\\(S_R^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{N_{ij}} (Y_{k} - \\overline{Y}_{ij})^2\\1\\{X^{(1)}_k=A_i \\and X^{(2)}_k=B_j\\}\\): \\(S_{within}\\) in one-factor ANOVA"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-interaction",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-interaction",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Testing the Interaction",
    "text": "Testing the Interaction\n\nFor testing the interaction, \\(H_0^{(AB)}: \\gamma_{ij} = 0\\) for all \\(i, j\\)\n\n\nWe the linear constraint test statistic \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\gamma_{ij}=0\\) for all \\((i,j)\\).\n\n\n\n\\[F^{(AB)} = \\frac{S_{AB}^2/(I-1)(J-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal N(0, \\sigma^2 I_n)\\), \\(F^{(AB)} \\sim \\mathcal F((I-1)(J-1), n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-a",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-a",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Testing the Effect of \\(A\\)",
    "text": "Testing the Effect of \\(A\\)\n\nFor testing the main effect of \\(A\\), \\(H_0^{(A)}: \\alpha_i = 0\\) for all \\(i\\):\nWe use \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\alpha_i=0\\) for all \\(i\\).\n\n\n\n\\[F^{(A)} = \\frac{S_A^2/(I-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\), \\(F^{(A)} \\sim \\mathcal{F}(I-1, n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-b",
    "href": "teaching/linear_model/slides/anova_ancova.html#testing-the-effect-of-b",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Testing the Effect of \\(B\\)",
    "text": "Testing the Effect of \\(B\\)\n\nFor testing the main effect of \\(B\\), \\(H_0^{(B)}: \\beta_j = 0\\) for all \\(j\\):\nWe use \\(F = \\frac{n-p}{q} \\frac{(SSR_c - SSR)}{SSR}\\)\nwhere \\(SSR_c\\) corresponds to the sum of squares of the residuals in the space \\(\\beta_j=0\\) for all \\(j\\).\n\n\n\n\\[F^{(B)} = \\frac{S_B^2/(J-1)}{S_R^2/(n-IJ)}\\]\n\nWhen \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\), \\(F^{(B)} \\sim \\mathcal{F}(J-1, n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#r-outputs",
    "href": "teaching/linear_model/slides/anova_ancova.html#r-outputs",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "R outputs",
    "text": "R outputs\n\nIn software, these tests are summarized in a table as shown below.\nIn R: anova(lm(Y ~ A*B))\n\n\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\n\\(A\\)\n\\(I-1\\)\n\\(S_A^2\\)\n\\(S_A^2/(I-1)\\)\n\\(F^{(A)}\\)\n…\n\n\n\\(B\\)\n\\(J-1\\)\n\\(S_B^2\\)\n\\(S_B^2/(J-1)\\)\n\\(F^{(B)}\\)\n…\n\n\n\\(A:B\\)\n\\((I-1)(J-1)\\)\n\\(S_{AB}^2\\)\n\\(S_{AB}^2/((I-1)(J-1))\\)\n\\(F^{(AB)}\\)\n…\n\n\nResiduals\n\\(n-IJ\\)\n\\(S_R^2\\)\n\\(S_R^2/(n-IJ)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#assumptions",
    "href": "teaching/linear_model/slides/anova_ancova.html#assumptions",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Assumptions",
    "text": "Assumptions\n\nFisher tests are based on the assumption \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\)\n\nNormality is not critical, but homoscedasticity is\nWe can test equality of variances in each modality of A (or B), or in each crossed modality if the \\(n_{ij}\\) are sufficiently large\nThis can be done with Levene’s test or Bartlett’s test"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#practical-procedure",
    "href": "teaching/linear_model/slides/anova_ancova.html#practical-procedure",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Practical Procedure",
    "text": "Practical Procedure\n\nTest equality of variances\nForm the ANOVA table (independent of chosen constraints)\nIf the \\(AB\\) interaction is significant: don’t change anything\nIf the interaction is not significant: analyze the marginal effects of A and B\n\n\nIf they are significant: the model is additive: lm(Y ~ A + B)\nOtherwise: we can remove \\(A\\) (or \\(B\\)) from the model"
  },
  {
    "objectID": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis",
    "href": "teaching/linear_model/slides/anova_ancova.html#post-hoc-analysis",
    "title": "Analysis of Variance (ANOVA) and of Covariance (ANCOVA)",
    "section": "Post-Hoc Analysis",
    "text": "Post-Hoc Analysis\nOnce effects are identified: perform post-hoc analysis by examining differences between (crossed) modalities more closely, using Tukey’s test as in one-factor ANOVA"
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Tests provide the theoretical basis for decision-making based on data. For example, doctors diagnose diseases using specific biological markers, industrial quality engineers evaluate the quality of a production batch, and climate scientists determine whether there are significant changes in measurements compared to the pre-industrial era.\n\n\nHypothesis testing is a key statistical method that enables professionals to make informed decisions. The process involves setting up two competing hypotheses:\n\nthe null hypothesis \\(H_0\\), which assumes no effect or no difference. This is usually an a priori on the data.\nthe alternative hypothesis \\(H_1\\), which suggests that there is a significant effect or difference.\n\nLet’s illustrate this with the example of a doctor diagnosing a disease, such as diabetes, using blood glucose levels:\n\nThe doctor begins by assuming the null hypothesis \\(H_0\\): the patient’s blood glucose levels are normal, indicating no diabetes.\nThe alternative hypothesis (\\(H_1\\)) would suggest that the patient’s glucose levels are abnormally high, indicating diabetes.\n\nAfter collecting the patient’s blood glucose data, the doctor compares it to a standard threshold for diagnosing diabetes. A hypothesis test is then performed to evaluate whether the observed glucose level is significantly higher than the threshold, or whether the difference could simply be due to random variation.\nThe testing process can be summarized as follows:\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests?\n\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)\n\n\n\n\nIn general, observing \\(T=1\\) is just one piece of evidence supporting \\(H_1\\). It’s possible that we were simply unlucky and the data led the test to reject \\(H_0\\). Additionally, the sample used might not be representative, or the elevated glucose level could be caused by another condition, not diabetes. Therefore, we cannot blindly accept \\(H_1\\) based on this result alone. Further investigation is essential when \\(T=1\\) to minimize the risk of making a Type I error.\nThe same applies when \\(T=0\\). A result of \\(T=0\\) simply indicates that the test provides no evidence in favor of \\(H_1\\), but it doesn’t mean that \\(H_1\\) is false. It is possible that the patient exhibits other symptoms that could still suggest a diagnosis of diabetes, despite the lack of evidence from the test.\n\n\n\n\n\n\nObjective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?\n\nIn the above example, we formally observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\) Imagine that instead of ten \\(6\\), we observed \\(600\\) tosses leading to the following counts:\n\nThe number of \\(6\\) is really close to \\(100 = 600/6\\), which implies that we would not reject \\(H_0\\) in the previous example. We would reject however if we wanted to test whether the dice is fair, since it is highly biased toward \\(2\\).\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_1\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_0\\) is “unlikely”)\n\n\n\n\n\n\n\n\nSee also the (Pluto notebook: illustration of pvalue)\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\) (“\\(95^{th}\\) percentile”)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Hypothesis testing is a key statistical method that enables professionals to make informed decisions. The process involves setting up two competing hypotheses:\n\nthe null hypothesis \\(H_0\\), which assumes no effect or no difference. This is usually an a priori on the data.\nthe alternative hypothesis \\(H_1\\), which suggests that there is a significant effect or difference.\n\nLet’s illustrate this with the example of a doctor diagnosing a disease, such as diabetes, using blood glucose levels:\n\nThe doctor begins by assuming the null hypothesis \\(H_0\\): the patient’s blood glucose levels are normal, indicating no diabetes.\nThe alternative hypothesis (\\(H_1\\)) would suggest that the patient’s glucose levels are abnormally high, indicating diabetes.\n\nAfter collecting the patient’s blood glucose data, the doctor compares it to a standard threshold for diagnosing diabetes. A hypothesis test is then performed to evaluate whether the observed glucose level is significantly higher than the threshold, or whether the difference could simply be due to random variation.\nThe testing process can be summarized as follows:\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Decision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)\n\n\n\n\nIn general, observing \\(T=1\\) is just one piece of evidence supporting \\(H_1\\). It’s possible that we were simply unlucky and the data led the test to reject \\(H_0\\). Additionally, the sample used might not be representative, or the elevated glucose level could be caused by another condition, not diabetes. Therefore, we cannot blindly accept \\(H_1\\) based on this result alone. Further investigation is essential when \\(T=1\\) to minimize the risk of making a Type I error.\nThe same applies when \\(T=0\\). A result of \\(T=0\\) simply indicates that the test provides no evidence in favor of \\(H_1\\), but it doesn’t mean that \\(H_1\\) is false. It is possible that the patient exhibits other symptoms that could still suggest a diagnosis of diabetes, despite the lack of evidence from the test.",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Objective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?\n\nIn the above example, we formally observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\) Imagine that instead of ten \\(6\\), we observed \\(600\\) tosses leading to the following counts:\n\nThe number of \\(6\\) is really close to \\(100 = 600/6\\), which implies that we would not reject \\(H_0\\) in the previous example. We would reject however if we wanted to test whether the dice is fair, since it is highly biased toward \\(2\\).\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_1\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_0\\) is “unlikely”)\n\n\n\n\n\n\n\n\nSee also the (Pluto notebook: illustration of pvalue)\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\) (“\\(95^{th}\\) percentile”)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: Calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion?",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#section",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\n\n\nTest",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#type-of-problems",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#type-of-problems",
    "title": "Hypothesis Testing",
    "section": "Type of Problems",
    "text": "Type of Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\n\n\nMultiple VS Multiple",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nParametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nNon-parametric: otherwise\n\n\nExample of multiple VS multiple parametric problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\nDecision rule\n\n\n\n\nA decision rule or test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\n\n\n\n\n\n\n\n\nTest statistic\n\n\n\n\na test statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nExample of \\(\\mathcal R\\), for a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) \\not \\in [t_1, t_2]\\}:~~~~~~\\mathcal R = (-\\infty,t_1)\\cup (t_2, +\\infty)\\;\n\\end{aligned}\n\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#continous-measures",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#continous-measures",
    "title": "Hypothesis Testing",
    "section": "Continous Measures",
    "text": "Continous Measures\n\ndensity wrp to Lebesgue: \\(dP(x) = p(x)dx\\)\nPDF (proba density function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha} = \\alpha)\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#discrete-measures",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#discrete-measures",
    "title": "Hypothesis Testing",
    "section": "Discrete Measures",
    "text": "Discrete Measures\n\ndensity wrp to counting measure: \\(P(X=x) = p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\nGaussian/Bernoulli\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)\n\n\n\n\n\nExponential/Geometric\n\n\n\nExponential \\(\\mathcal E(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)\n\n\n\n\n\nGamma/Poisson\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\]\n\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\lambda\\)\n\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\n\nNumber of tics before time \\(1\\) of an atomic clock of rate \\(\\lambda\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\)?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nEquivalently, we can consider the log-likelihood ratio test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#neyman-pearson",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#neyman-pearson",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson",
    "text": "Neyman Pearson\n\n\n\n\n\n\nNeyman Pearson’s theorem\n\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\nProof of Neyman Pearson’s theorem\nWe prove the theorem in the case where \\(P\\) and \\(Q\\) each have a density \\(p\\) and \\(q\\) on \\(\\mathbb R^n\\). For any \\(t &gt; 0\\), define \\[I_t(P, Q) = \\int_{x \\in \\mathbb R^n} |q(x) - tp(x)|dx \\; .\\]\nWhen \\(t=1\\), this quantity is equal to the total-variation distance between \\(P\\) and \\(Q\\). For any event A in \\(\\mathbb R^n\\) , it holds that\n\\[\n\\begin{split}\n  I_t(P, Q) &= \\int_{x \\in \\mathbb R^n} |q(x) - tp(x)|dx \\\\\n  &= \\int_{q(x)&gt; tp(x)} q(x) - tp(x)dx + \\int_{q(x)&lt; tp(x)} tp(x) - q(x)dx\\\\\n  &=  2\\int_{x \\in \\mathbb R^n} \\mathbf1_{q(x)&gt; tp(x)}(q(x) - tp(x))dx + t-1 \\\\\n  &\\geq 2\\int_{x \\in \\mathbb R^n} \\mathbf 1_A \\mathbf1_{q(x)&gt; tp(x)}(q(x) - tp(x))dx + t-1\\\\\n  &\\geq t-1 + 2(Q(A) - tP(A)) \\; .\n\\end{split}\n\\]\nIf \\(A  = \\{x \\in \\mathbb R^n : q(x) &gt; tp(x)\\}\\), then the two last inequalities are equalities. In particular, \\[I_t(P, Q) = t-1 + 2\\sup_{A \\subset \\mathbb R^n}(Q(A) - tP(A))) \\; .\\]\nAssume that the type-1 error of \\(T\\) is smaller than \\(\\alpha\\): \\(P(T=1) \\leq \\alpha\\).\nThe power of \\(T\\), \\(Q(T = 1)\\), is upper-bounded as follows: \\[\n\\begin{split}\n  Q(T=1) &\\leq Q(T=1) + t(\\alpha - P(T=1))\\\\\n  &= \\alpha t + Q(T=1) - tP(T=1) \\\\\n  &\\leq \\alpha t + \\frac{t-1}{2} + \\frac{1}{2}I_t \\; .\n\\end{split}\n\\]\n\nThere is equality in the second inequality if \\(T=1\\) is the event \\(\\{ \\frac{q(X)}{p(X)} &gt; t \\}\\).\nThere is equality in the first inequality if \\(P(T=1) = \\alpha\\).\n\nLet \\(t_{\\alpha}\\) be such that \\(P(\\frac{q(X)}{p(X)}&gt; t_{\\alpha}) = \\alpha\\). The test \\(T^*(X) = \\mathbf1 \\{\\frac{q(X)}{p(X)}&gt; t_{\\alpha}\\}\\) satisfies the two above points, since its rejection event is exactly \\(\\{T^*(X) = 1\\} = \\{\\frac{q(X)}{p(X)}&gt; t_{\\alpha}\\}\\) and since \\(P(T^* = 1) = \\alpha.\\) Hence, for any test \\(T\\) of type 1 error smaller than \\(\\alpha\\), it holds that \\(Q(T = 1) \\leq Q(T^* = 1)\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#examples-2",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#examples-2",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\nExample (Gaussians)\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\nLog-likelihood ratio test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\n\n\n\nExample: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\n\n\\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]\n\n\nExample: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) \\leq 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): reject if \\(N \\geq 19\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#generalities",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#generalities",
    "title": "Hypothesis Testing",
    "section": "Generalities",
    "text": "Generalities\n\n\\(H_0 = \\{ \\mathcal P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(right-tailed: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(left-tailed: unilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(simple/multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( multiple/multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-tailed test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (right-tailed test).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#pivotal-test-statistic",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#pivotal-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic",
    "text": "Pivotal Test Statistic\n\nConsider \\(\\Theta_0\\) not singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\nPivotal test statistic\n\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/testing_models.html#p-value",
    "href": "teaching/hypothesis_testing/lectures/testing_models.html#p-value",
    "title": "Hypothesis Testing",
    "section": "P-value",
    "text": "P-value\nSee the (Pluto notebook: Illustration of pvalue)\n\n\n\n\n\n\nP-value: definition\n\n\n\nWe define \\(p_{value}(x_{\\mathrm{obs}}) =\\mathbb P(\\psi(X) &gt; x_{\\mathrm{obs}})\\) for a right-tailed test.\nFor a two-tailed test, \\(p_{value}(x_{\\mathrm{obs}}) =2\\min(\\mathbb P(\\psi(X) &gt; x_{\\mathrm{obs}}),\\mathbb P(\\psi(X) &lt; x_{\\mathrm{obs}}))\\)\n\n\n\n\n\n\n\n\nP-value under \\(H_0\\)\n\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(p_{value}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\nProof.\nThe p-value is a probability, so it belongs to \\([0,1]\\). Let \\(F_{\\psi}\\) be the cumulative distribution function of a random variable \\(\\psi(X)\\) when \\(X\\) follows distribution \\(P\\), that is \\(G_{\\psi}(t) = \\mathbb P(\\psi(X) &gt; t)\\). It holds that \\[\n\\mathbb P(\\psi(X') &gt; \\psi(X) ~|~ \\psi(X)) = F_{\\psi}(\\psi(X))\n\\] If \\(u \\in [0,1]\\), then \\[\n\\begin{aligned}\n\\mathbb P(\\mathrm{pvalue}(X) &gt; u) &= \\mathbb P(F_{\\psi}(\\psi(X)) &gt; u) \\\\\n&= \\mathbb P(\\psi(X) &gt; F_{\\psi}^{-1}(u)) \\\\\n&= 1- F_{\\psi}(F_{\\psi}^{-1}(u)) = 1-u\n\\end{aligned}\n\\] Hence, \\(\\mathrm{pvalue}(X)\\) is uniform when \\(X\\) follows distribution \\(\\mathbb P\\). \\[\\tag*{$\\blacksquare$}\\]\n\n\n\nIn practice: reject if \\(p_{value}(X) &lt; \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\) under \\(H_0\\):",
    "crumbs": [
      "Hypothesis Testing",
      "Testing Models"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html",
    "href": "teaching/hypothesis_testing/lectures/dependency.html",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "We observe iid paired data \\((X_1, Y_1), \\dots, (X_n,Y_n)\\) of unknown mean \\(\\mu_X, \\mu_Y\\) and cov matrix \\(\\Sigma\\).\nCov matrix: \\(\\Sigma =\n\\left(\\begin{matrix}\n\\sigma_X^2 & \\mathrm{Cov(X,Y)} \\\\\n\\mathrm{Cov(X,Y)} & \\sigma_Y^2 \\\\\n\\end{matrix}\\right)\\)\n\\(H_0: \\mathrm{Cov}(X,Y)=0\\) or \\(H_1: \\mathrm{Cov}(X,Y)\\neq 0\\)\n\\(\\mathrm{Cov(X,Y)} = \\mathbb E[(X- \\mathbb E[X])(Y- \\mathbb E[Y])]\\)\n\\(\\sigma_X^2 = \\mathrm{Cov(X,X)}\\)\n\\(\\sigma_Y^2 = \\mathrm{Cov(Y,Y)}\\)\n\\(\\mathrm{Cor(X,Y)} = \\frac{\\mathrm{Cov(X,Y)}}{\\sigma_X \\sigma_Y}\\)\n\n\n\n\n\n\n\n\n\n\nPearson’s Correlation Test\n\n\n\n\n\\(r =  \\frac{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline X)^2\\sum_{i=1}^n (Y_i - \\overline Y)^2}}\\)\n\\(\\psi(X,Y) = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\)\nUnder \\(H_0\\), \\(\\psi(X,Y) \\approx \\mathcal T(n-2)\\)\n\n\n\n\n\n\nMonte Carlo Simulation with \\(n=4\\):\n\n\n\n\n\n\n\n\n\n\n\n\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nGroup \\(k\\): \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\). Ex: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\)\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2 \\sim \\sigma^2\\chi^2(N_k-1)\\) (under \\(H_0\\))\nVar of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\sim\\sigma^2\\chi^2(d-1)\\) (under \\(H_0\\))\n\n\n\n\n\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}{\\tfrac{1}{d-1}SSB}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\).\n\n\n\n\n\n\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#correlation-test-1",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#correlation-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "Monte Carlo Simulation with \\(n=4\\):",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#anova-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#anova-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nGroup \\(k\\): \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\). Ex: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\)\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2 \\sim \\sigma^2\\chi^2(N_k-1)\\) (under \\(H_0\\))\nVar of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\sim\\sigma^2\\chi^2(d-1)\\) (under \\(H_0\\))\n\n\n\n\n\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}{\\tfrac{1}{d-1}SSB}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#interpretation-of-variances-in-anova",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#interpretation-of-variances-in-anova",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#chi2-homogeneity-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#chi2-homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Homogeneity Test",
    "text": "\\(\\chi^2\\) Homogeneity Test\n\n\\(d\\) different bags (or groups), each containing balls of \\(m\\) potential colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix of counts:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\n\n\n\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all color \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#example-soft-drink-preferences",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\)\n\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#chi2-independence-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#chi2-independence-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Independence Test",
    "text": "\\(\\chi^2\\) Independence Test\n\n\n\n\n\n\n\nObservations: Paired Categorical Variables \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\ne.g. \\(X_i \\in \\{\\mathrm{male}, \\mathrm{female}\\}\\), \\(Y_i \\in \\{\\mathrm{coffee}, \\mathrm{tea}\\}\\)\nIdea: regroup the data into bags, e.g. \\(\\{\\mathrm{male}, \\mathrm{female}\\}\\)\nBuild the contingency table\nPerform a chi-square homogeneity test\n\n\n\n\nExample of contingency table:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n30\n20\n50\n\n\nTea\n28\n22\n50\n\n\nTotal\n58\n42\n100\n\n\n\nExpected counts:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n29\n21\n50\n\n\nTea\n29\n21\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\nDegree of freedom: \\((2-1)(2-1) = 1\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#symetric-random-variable",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)\n\n\n\n\n\n\n\n\n\n\n\nSymetrization\n\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t)\n&=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy \\\\\n&= \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]\n\\(X\\) indep of \\(Y\\) \\(\\implies\\) \\(X-Y\\) symmetric \\(\\implies\\) \\(\\mathrm{median}(X-Y) = 0\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#dependency-problem-for-paired-data",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The median of \\((X_i - Y_i)\\) is \\(0\\) for all \\(i\\)\n\\(H_1:\\) The median of \\((X_i - Y_i)\\) is not \\(0\\) for some \\(i\\)\n\n\n\n\n\n\n\n\n\n\nGenerality of \\(H_0\\)\n\n\n\n\nIf \\(X_i-Y_i\\) is symmetric \\(i\\), then we are under \\(H_0\\).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\).\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\)).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\n\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]\n\n\n\n\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\n\nUnder \\(H_0\\),\n\nSigns of the \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\).",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#wilcoxons-signed-rank-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\n\n\nProperties on the Signed Ranks\n\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &gt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\nif \\(H_1\\): \\(\\mathrm{median}(D_i) &gt; 0\\): left-one sided test on \\(W_-\\).\n\n\n\nThis approximation fits well the exact distribution. Monte-Carlo simulation:\n\nTo generate a \\(W_-\\) under \\(H_0\\) in Julia:\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/dependency.html#effect-of-drug-on-blood-pressure",
    "href": "teaching/hypothesis_testing/lectures/dependency.html#effect-of-drug-on-blood-pressure",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Effect of Drug on Blood Pressure",
    "text": "Effect of Drug on Blood Pressure\n\n\\(H_0\\): the drug has no effect. \\(H_1\\): it lowers the blood pressure\n\n\n\n\nPatient\n\\(X_i\\) (Before)\n\\(Y_i\\)​ (After)\n\\(D_i = X_i-Y_i\\)​\n\\(R_i\\)\n\n\n\n\n1\n150\n140\n10\n6 (+)\n\n\n2\n135\n130\n5\n5 (+)\n\n\n3\n160\n162\n-2\n2 (-)\n\n\n4\n145\n146\n-1\n1 (-)\n\n\n5\n154\n150\n4\n4 (+)\n\n\n6\n171\n160\n11\n7 (+)\n\n\n7\n141\n138\n3\n3 (+)\n\n\n\n\n\\(W_- = 1+2 = 3\\).\nFrom a simulation, we approx \\(\\mathbb P(W_-=i)\\), for \\(i \\in \\{0, 1, 2, 3, 4, 5,6\\}\\) under \\(H_0\\) by\n\n[0.00784066, 0.00781442, 0.00781534, 0.01563892, 0.01562184, 0.02343478]\n\nFrom a simulation, \\(p_{value}=\\mathbb P(W_- \\leq 3) \\approx 0.039 &lt; 0.05\\)",
    "crumbs": [
      "Hypothesis Testing",
      "Dependency"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "",
    "text": "Note sur la correction\n\n\n\nComme répété à plusieurs reprises en cours, la qualité de la rédaction ainsi que les détails d’explications sont grandement pris en compte dans l’évaluation, surtout pour les introductions des modèles (premières questions des exercices)."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables",
    "text": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables\nNous cherchons à déterminer si les citoyens d’une région ont une quelconque préférence pour les sources d’énergie renouvelables (par exemple, solaire, éolienne) ou les sources d’énergie non renouvelables (par exemple, charbon, gaz naturel). Nous supposons que, a priori, il n’y a aucune préférence en moyenne. Nous interrogeons \\(n\\) individus, et nous notons \\(X\\) le nombre de répondants qui préfèrent les énergies renouvelables."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#questions",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#questions",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions :",
    "text": "Questions :\n\nFormalisez le problème de test d’hypothèse, et définissez \\(H_0\\) et \\(H_1\\)​. Indiquez si ce test est unilatéral ou bilatéral.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\(X\\) le nombre d’individus qui préfèrent les ENR.\nOn suppose que \\(X\\) suit une loi \\(Bin(n,p)\\), où \\(p\\) est un paramètre inconnu.\nOn considère le problème de test: \\(H_0: p=1/2\\) VS \\(H_1: p\\neq 1/2\\)\n\nC’est un problème de test bilatéral\n\n\n\nNous interrogeons \\(n=100\\) individus, et \\(X=58\\) préfèrent les sources d’énergie renouvelables. Écrivez la p-valeur en fonction de \\(F\\), la fonction de répartition d’une distribution Binomiale Bin\\((100,0.5)\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\nSous \\(H_0\\), \\(X\\) suit une loi \\(Bin(100, 0.5)\\) de cdf \\(F\\). Ainsi, \\(p_{valeur} = 2\\min(\\mathbb P(X \\leq 58), \\mathbb P(X \\geq 58)) =  2\\mathbb P(X \\geq 58) = 2(1- F(57))\\).\nd’où \\(p_{valeur} = 2(1- F(57))\\).\n(58 accepté aussi car \\(\\mathbb P(X=58)\\) est petit)\n\n\n\nÉcrivez une ligne de code qui calculerait la p-valeur exacte en Julia, Python, ou R.\n\n2*(1-cdf(Binomial(100, 0.5), 57)) # Julia\n2*(1-pbinom(57,100,0.5)) # R\n# Résultat: 0.133\n\nDonnez une approximation de la p-valeur en utilisant une approximation gaussienne et le graphique de la fonction de répartition de \\(\\mathcal N(0,1)\\) fourni dans le sujet. Quelle est votre conclusion ?\n\n\n\n\n\n\n\nNote\n\n\n\nSous \\(H_0\\), \\(\\mathbb E_0[X]=n/2\\) et \\(\\mathbb V_0(X)=n\\frac{1}{2}(1-\\frac{1}{2})=n/4\\)\n\nOn renormalise \\(X\\) pour obtenir la statistique de test suivante: \\[\\psi(X) = \\frac{X-\\mathbb E_0[X]}{\\sqrt{\\mathbb V_0(X)}}=\\frac{X-n/2}{\\sqrt{n/4}}\\]\nLorsque \\(n \\to \\infty\\), \\(\\psi(X)\\) converge en loi vers \\(\\mathcal N(0,1)\\). Ainsi, on obtient suite à cette approximation Gaussienne (\\(n/2\\) est assez grand):\n\\[\np_{valeur} = 2\\mathbb P(\\psi(X) \\geq \\psi(X_{obs})) \\asymp 2\\mathbb P(Z \\geq \\psi(58)),\n\\] où \\(Z\\) est une VA qui suit une loi \\(\\mathcal N(0,1)\\) sous \\(\\mathbb P\\). On calcule \\(\\psi(58) \\asymp 1.6\\), et par lecture graphique, \\(2*P(Z \\geq \\psi(58)) \\asymp 0.11\\)\nLa \\(p_{valeur}\\) est grande, on ne donc rejette pas à un niveau de \\(5\\%\\).\n\n\n\nRedéfinissez l’hypothèse alternative \\(H_1\\)​ et calculez une p-valeur approximative si nous cherchons à déterminer si les citoyens ont une préférence pour :\n\nles sources d’énergie renouvelables.\nles sources d’énergie non renouvelables. Quelles sont vos conclusions pour ces deux autres problèmes ?\n\n\n\n\n\n\n\n\nRéponse\n\n\n\nLes problèmes de test deviennent unilatéraux.\n\n\\(H_1: p &gt; 0.5\\), \\(p_{valeur} \\asymp 0.11/2 \\asymp 0.055\\) on pourrait rejeter à un niveau \\(10\\%\\)\n\\(H_1: p &lt; 0.5\\) , \\(p_{valeur} \\asymp (1-0.11)/2 \\asymp 0.445\\), on ne rejette pas."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 2 : Surveillance environnementale de la pollution fluviale",
    "text": "Exercice 2 : Surveillance environnementale de la pollution fluviale\nUne agence environnementale surveille les niveaux de pollution d’une rivière pour déterminer si une usine à proximité provoque une augmentation de la concentration de produits chimiques nocifs. La concentration cible pour un produit chimique spécifique est de \\(15 \\, \\text{ppm}\\) (parties par million), ce qui est considéré comme sûr pour la vie aquatique. Pour un échantillon de \\(n = 20\\) prélèvements d’eau effectués en aval de l’usine, la concentration moyenne empirique est \\(\\bar{X}_n = 16,3 \\, \\text{ppm}\\), et la variance empirique est \\(S^2_n = 2,4 \\, \\text{ppm}^2\\).\nA priori, on suppose que la rivière respecte le seuil de pollution sans danger de \\(15 \\, \\text{ppm}\\).\nNous visons à tester, avec un niveau de signification \\(\\alpha = 0,05\\), si la concentration chimique en aval dépasse le seuil de sécurité, indiquant une pollution provenant de l’usine.\n\nQuestions :\n\nEn utilisant une hypothèse Gaussienne, formalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\). S’agit-il d’un test unilatéral ou bilatéral ?\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\((X_1, \\dots, X_n)\\) les concentrations des prélèvement en ppm (toujours bien de mettre les unités)\nOn suppose que les \\(X_i\\) sont iid de loi \\(\\mathcal N(\\mu, \\sigma^2)\\) où \\(\\mu\\) et \\(\\sigma^2\\) sont des paramètres inconnus\nOn souhaite tester s’il y a un problème de pollution, cad\n\\(H_0: \\mu = 15\\) ppm VS \\(H_1: \\mu &gt; 15\\) ppm\n\nC’est un problème de test unilatéral droit (\\(\\mu \\leq 15\\) ppm aussi accepté pour \\(H_0\\))\n\n\n\nDéfinissez la statistique de test. Quelle est sa distribution sous \\(H_0\\) ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn utilise la statistique de test de Student \\[\\psi(X) = \\sqrt{n}\\frac{\\overline X - 15}{\\hat \\sigma},\\]\noù \\(\\overline X=\\frac{1}{n}\\sum_{i=1}^n X_i\\) et \\(\\hat \\sigma^2 = \\tfrac{1}{n-1}\\sum_{i=1}^n (X_i-\\overline X)^2\\) est un estimateur non biaisé de \\(\\sigma^2\\) sous \\(H_0\\). Comme les \\(X_i\\) sont iid de loi normale, \\(\\psi(X)\\) suit une loi de student \\(\\mathcal T(n-1)\\) sous \\(H_0\\). Ce n’est pas une approximation ici, on peut même dire que \\(\\psi\\) est une statistique de test pivot\n\n\n\nDéterminez la zone de rejet. Vous pouvez utiliser une approximation Gaussienne et le graphe de l’exercice 1.\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn souhaite tester (à droite) au niveau \\(0.05\\). Ainsi, on rejette si \\(\\psi(X) \\geq t_{0.95}\\), où \\(t_{0.95}\\) est le quantile \\(0.95\\) de \\(\\mathcal T(n-1)\\)\nLorsque \\(n\\) tend vers \\(+\\infty\\), \\(\\mathcal T(n-1)\\) converge en loi vers \\(\\mathcal N(0,1)\\). On approxime donc \\(t_{0.95}\\) par le quantile de la loi Gaussienne (inverse de la cdf sur le graphe), d’où \\(t_{0.95} \\asymp 1.6\\)\nOn rejette si \\(\\psi(X) \\geq 1.6\\)\n\n\n\nEcrivez une ligne de code permettant de calculer le seuil de rejet exact.\n\nquantile(TDist(19), 0.95) # julia\nqt(0.95,n) # R, (1.73 légèrement plus grand que 1.6)\n\nLa rivière présente-t-elle une concentration chimique accrue qui pourrait indiquer une pollution provenant de l’usine ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nOn calcule \\(\\psi(X_{\\mathrm{obs}}) = 3.75\\), on rejette donc \\(H_0\\), la rivière est peut être polluée"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux",
    "text": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux\nUn chercheur en faune sauvage étudie le comportement d’une certaine espèce d’oiseaux qui migrent vers une réserve naturelle. Le chercheur a une hypothèse sur la façon dont les oiseaux se répartissent entre différents types d’habitats dans la réserve. La distribution attendue, basée sur des données historiques, est la suivante :\n\nPrairie : 40%\nZones humides : 30%\nForêts : 20%\nZones rocheuses : 10%\n\nPour tester cette hypothèse, le chercheur observe 200 oiseaux et enregistre leurs préférences d’habitat. Les comptages observés sont les suivants :\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé\n90\n60\n30\n20\n\n\n\n\nQuestions :\n\nFormalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nOn observe \\((X_1, X_2,X_3,X_4)\\) les effectifs d’oiseaux présent respectivement dans les Prairie,…, zones rocheuses.\nOn suppose que le vecteur \\((X_1, X_2,X_3,X_4)\\) suit une loi multinomiale de paramètre \\(n=200\\) et \\(q=(q_1,q_2,q_3,q_4)\\) inconnu\nOn veut tester si les observations correspondent à la distribution attendue, c’est à dire \\(H_0: q=(0.4,0.3,0.2,0.1)\\) VS \\(H_1: q\\neq (0.4,0.3,0.2,0.1)\\) Ce problème correspond au test d’adéquation du Chi2 (Goodness of fit).\n\n\n\n\nCalculez les effectifs attendus.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé (\\(X_i\\))\n90\n60\n30\n20\n\n\nAttendus (\\(E_i\\))\n80\n60\n40\n20\n\n\n\n\n\n\nCalculez la statistique du chi-deux.\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\psi(X) = \\sum_{i=1}^4 \\frac{(X_i - E_i)^2}{E_i} = 100/80 + 100/40 = 3.75\\)\n\n\n\nDéterminez le degré de liberté \\(df\\) de la statistique du chi-deux, et lisez la p-value sur le graphique suivant de la fonction de répartition.\n\n\n\n\n\n\n\nNote\n\n\n\nLorsque \\(n \\to +\\infty\\), \\(\\psi(X)\\) converge en loi vers une loi de \\(\\chi^2\\) de degré \\(df=3\\). On lit pvaleur = 1-cdf(Chisq(df), 3.75)=0.3\n\n\n\nQuelle est votre conclusion ?\n\n\n\n\n\n\n\nRéponse\n\n\n\nLa \\(p_{valeur}\\) est assez grande, la distribution colle avec celle attendue et on ne rejette donc pas \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-4",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#exercice-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 4",
    "text": "Exercice 4\n\nProductivité des employés entre départements\nUne entreprise souhaite évaluer si un nouveau style de management a eu un effet uniforme sur la productivité des employés dans cinq départements. Chaque département a adopté une variation spécifique du style de management pendant trois mois, et l’entreprise a enregistré le nombre moyen de tâches accomplies par employé durant cette période.\nDonnées :\n\n\n\nDépartement\n1\n2\n3\n4\n5\n\n\n\n\nNombre d’employés\n12\n10\n8\n9\n11\n\n\nMoyenne des tâches accomplies\n72,4\n68,9\n75,6\n74,3\n69,7\n\n\nVariance des tâches\n8,5\n9,2\n10,1\n7,8\n9,6\n\n\n\nL’entreprise cherche à comprendre si les niveaux de productivité varient significativement entre les départements, indiquant que les styles de management pourraient avoir des impacts différents.\nSoit \\(d=5\\) le nombre de départements et \\(N_{\\text{tot}} = 50\\) le nombre total d’employés. Pour tout département \\(j\\), nous notons \\(N_k\\) le nombre d’employés dans le département \\(k\\), et \\(P_{ik}\\) le nombre de tâches accomplies par l’employé \\(i\\) dans le département \\(k\\). Nous supposons que les \\(P_{ik}\\) sont indépendants et suivent une distribution normale de moyenne \\(\\mu_k\\) et de variance \\(\\sigma^2\\).\nNous écrivons \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDéfinissez les hypothèses du problème pour tester si les styles de management ont eu un impact uniforme sur la productivité.\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nDans le département \\(k\\), on observe \\(P_{ik}\\) le nombre de tâche accompli par l’employé \\(i\\)\nOn suppose que les \\(P_{ik}\\) sont indépendants, et que \\(P_{ik}\\) suit une \\(\\mathcal N(\\mu_k, \\sigma^2)\\) où \\((\\mu_1, \\dots, \\mu_5)\\) et \\(\\sigma^2\\) sont des paramètres inconnus.\nProblème de test: \\(H_0\\): \\(\\mu_1 = \\dots = \\mu_5\\) VS \\(H_1\\): \\(\\exists k,l\\) tels que \\(\\mu_k \\neq \\mu_l\\)\n\n\n\n\nDonnez une brève interprétation de chacune des quantités \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\n\n\n\n\n\n\n\nNote\n\n\n\nCf cours\n\n\n\nDémontrez la formule d’analyse de la variance : \\(V_T = V_W + V_B\\)\n\n\n\n\n\n\n\nNote\n\n\n\nExercice à refaire\n\n\n\nCalculez \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), et \\(V_T\\).\n\n\n\n\n\n\n\nNote\n\n\n\nOn applique les formules…\n\\(\\overline P=71.96\\)\n\\(V_W = 9.01\\)\n\\(V_B=6.15\\)\n\\(V_T=15.16\\)\n\n\n\n\nExprimez la statistique de test ANOVA en termes de \\(V_W\\) et \\(V_B\\).\nQuelles sont les distributions de \\(N_k V_k\\) et de \\(N_{\\mathrm{tot}}V_W\\) sous \\(H_0\\) ? Changent-elles sous \\(H_1\\) ?\n\n\n\n\n\n\n\nNote\n\n\n\n\\(N_kV_k\\) suit une loi \\(\\sigma^2\\chi^2(N_k-1)\\).\nAinsi, \\(N_{\\mathrm{tot}}V_W/\\sigma^2\\) suit une loi \\(\\chi^2\\) de degré \\(\\sum_{k=1}^5(N_k-1) = N_{tot}-5\\) Ces lois ne changes pas sous \\(H_1\\)! C’est en fait la loi de la variance interclasses \\(V_B\\) qui va changer sous \\(H_1\\).\n\n\n\nRappelez la définition de la statistique de test ANOVA, donnez sa distribution \\(\\mathcal D\\) sous \\(H_0\\) et effectuez le test ANOVA au niveau \\(\\alpha =0,05\\). On donne les quantiles \\(0.05\\) et \\(0.95\\) de \\(\\mathcal D\\): \\(0.18\\) and \\(2.58\\). Concluez si la productivité diffère significativement entre les départements.\n\n\n\n\n\n\n\nRéponse\n\n\n\nLa statistique ANOVA se définit comme \\(\\psi((P_{ik})) = \\frac{N_k-1}{d-1}\\frac{V_B}{V_W}\\). Ici, on calcule \\(\\psi((P_{ik}))=7.7\\). Comme \\(7.7 &gt; 2.5\\) (On regarde le quantile à droite), on rejette \\(H_0\\) au niveau \\(0.05\\). La productivité n’est pas homogène entre les départements."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/correction_2025.html#questions-de-cours",
    "href": "teaching/hypothesis_testing/annals/correction_2025.html#questions-de-cours",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions de cours",
    "text": "Questions de cours\n\nRappelez la définition d’une statistique de test \\(\\psi\\) et d’un test (ou règle de décision) \\(T\\).\n\n\n\n\n\n\n\nRéponse\n\n\n\n\nUne statistique de test \\(\\psi\\) est une fonction mesurable des donnée à valeurs réelles et qui ne dépend pas des paramètres inconnus du modèle.\nUne règle de décision \\(T\\) a la même définition, sauf qu’elle prend ses valeurs dans \\(\\{0,1\\}\\) (\\(0\\) correspond à conserver \\(H_0\\) et \\(1\\) correspond au rejet de \\(H_0\\))\n\n\n\n\nQuels sont les deux types d’erreur que nous pouvons commettre ?\n\n\n\n\n\n\n\nNote\n\n\n\n\nL’erreur de type 1: on rejette \\(H_0\\) alors qu’elle est vraie\nL’erreur de type 2: on “accepte” \\(H_0\\) alors qu’elle est fausse Ne pas écrire de probabilité, à moins de préciser le cadre! (exemple: \\(H_0\\) est simple)\n\n\\(\\mathbb P_{H_0}(X \\in A)\\) n’a aucun sens si \\(H_0\\) est multiple. C’est quoi \\(\\mathbb P_{H_0}\\)??\n\n\n\nPour une statistique de test \\(\\psi\\) donnée et un problème de test bilatéral, rappelez la définition de la p-valeur.\n\n\n\n\n\n\n\nRéponse\n\n\n\nSoit un problème de test où \\(H_0\\) est simple. Soit \\(X\\) la variable aléatoire, et \\(X_{obs}\\) une observation (c’est formellement une réalisation de \\(X\\))\nPour un problème de test bilatéral et une statistique de test \\(\\psi\\) donnée, la pvaleur s’écrit \\(2*\\min(\\mathbb P(\\psi(X) \\geq \\psi(X_{\\mathrm{obs}})), \\mathbb P(\\psi(X) \\leq \\psi(X_{\\mathrm{obs}})))\\)\n\n\n\nÉnoncez le théorème de Neyman-Pearson.\n\n\n\n\n\n\n\nRéponse\n\n\n\nDans le cas d’un problème simple VS simple, le rapport de vraissemblance est optimal pour un niveau de test \\(\\alpha\\) fixé, au sens où aucun test de niveau \\(\\alpha\\) ne peut avoir une plus grande puissance. Cf cours"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#definition-and-link-with-clt",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#definition-and-link-with-clt",
    "title": "Gaussian Populations",
    "section": "Definition and link with CLT",
    "text": "Definition and link with CLT\nWe observe \\((X_1, \\dots, X_n)\\) iid real valued random variables.\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n X_i\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when } n \\to \\infty \\]\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\nEquality when \\(X_i\\)’s are Gaussian \\(\\mathcal N(\\mu, \\sigma^2)\\), that is \\[\n\\mathbb P(X_1 \\in [x,x+dx]) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Known Variance",
    "text": "Testing Mean with Known Variance\n\n\\(X = (X_1, \\dots, X_n)\\), iid with distribution \\(\\mathcal N(\\mu, \\sigma^2)\\).\n\n\n\n\n\n\n\nTest problems\n\n\n\\[\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nTest statistic: \\[ \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\\]\n\\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nQ1, Q2\n\n\n\n\n\n\nTests\n\n\n\\[\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{(left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "Why 0.05 and 1.96 ?",
    "text": "Why 0.05 and 1.96 ?\n\n\n\n\n\nFisher’s Quote\n\n\nThe value for which \\(p=0.05\\), or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "Testing Mean with Unknown Variance",
    "text": "Testing Mean with Unknown Variance\n\nMultiple VS multiple test problem: \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n\\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\) no longer test statistic.\nIdea: replace \\(\\sigma\\) by its estimator \\[ \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\mu_0)^2} \\; .\\]\nThis gives \\[\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\\]\nIs \\(\\psi(X)\\) pivotal under \\(H_0\\) ? What is its distribution ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "Chi-Square and Student Distributions",
    "text": "Chi-Square and Student Distributions\n\n\n\n\n\n\nChi-squared distribution \\(\\chi^2(k)\\)\n\n\n\nDistrib of \\(\\sum_{i=1}^k Z_i^2\\) where the \\(Z_i\\)’s are iid \\(\\mathcal N(0,1)\\).\n\\(\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\\)\n\\(k\\): degree of freedom\n\\(\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\n\nStudent distribution \\(\\mathcal T(k)\\)\n\n\n\nDistrib of \\(\\tfrac{Z}{\\sqrt{U/k}}\\) where \\(Z\\), \\(U\\) are independent and follow resp. \\(\\mathcal N(0,1)\\) and a \\(\\chi^2(k)\\)\n\\(k\\): degree of freedom\n\\(\\mathcal T(k) \\sim \\mathcal N(0,1)\\) when \\(k \\to +\\infty\\)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\nAssume \\(X_i\\) are iid \\(\\mathcal N(\\mu_0, \\sigma^2)\\).\n\nThe test statistic \\(\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma}\\) pivotal (indep. of \\(\\sigma\\)).\nIt follows a Student Distribution \\(\\mathcal T(n-1)\\).\n\n\n\n\n\nProof idea: \\(\\overline X \\cdot (1, \\dots, 1)\\) and \\((X_1 - \\overline X, \\dots, X_n - \\overline X)\\) are orthogonal in \\(\\mathbb R^n\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "Student T-Test",
    "text": "Student T-Test\n\nMultiple VS multiple test problem \\(X=(X_1, \\dots, X_n)\\): \\[\nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\\]\n(Student) T-test statistic: \\[\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variance-unknown-mean",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variance-unknown-mean",
    "title": "Gaussian Populations",
    "section": "Testing Variance, Unknown Mean",
    "text": "Testing Variance, Unknown Mean\n\n\n\n\nWe observe \\(X=(X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu, \\sigma^2)\\). \\(\\mu\\), \\(\\sigma\\) are unknown. \\(\\sigma_0\\) is fixed and known.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(H_0\\): \\(\\sigma \\leq \\sigma_0\\), \\(H_1\\): \\(\\sigma &gt; \\sigma_0\\)\n\\(\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum_{i=1}^n (X_i - \\overline X)^2\\) Wooclap\n\\(T(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\\)\n\\(q_{1-\\alpha}\\): quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\\(H_0\\): \\(\\sigma \\geq \\sigma_0\\), \\(H_1\\): \\(\\sigma &lt; \\sigma_0\\)\n\\(\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum_{i=1}^n (X_i - \\overline X)^2\\)\n\\(T(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\\)\n\\(q_{\\alpha}\\): quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\) are known, \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nTest problem: \\(H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\\)\nIdea: normalize \\(\\overline X - \\overline Y\\): \\[\n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\\]\nTwo-tailed test for testing means: \\[\nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\\]\n\\(t_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of a Gaussian distribution"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example",
    "title": "Gaussian Populations",
    "section": "Example",
    "text": "Example\n\nObjective. Test if a new medication is efficient to lower cholesterol level\nExperiment.\n\nGroup A: \\(n_A = 45\\) patients receiving the new medication\nGroup B: \\(n_B = 50\\) patients receiving a placebo\n\nTest Problem.\n\nWe observe \\((X_1, \\dots, X_{n_A})\\) iid \\(\\mathcal N(\\mu_A,\\sigma^2)\\) and \\((Y_1, \\dots, Y_{n_B})\\) iid \\(N(\\mu_B,\\sigma^2)\\) the chol levels. \\(\\sigma = 8\\) mg/dL is known from calibration.\n\\(H_0: \\mu_A = \\mu_B\\) VS \\(H_1: \\mu_A &lt; \\mu_B\\)\n\nTest Statistic. \\(\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_2}}}\\)\nData. \\(\\overline X = 24.5\\) mg/dL and \\(\\overline Y = 21.3\\) mg/dL. Hence \\(\\psi(X,Y)= 5.5\\).\nConclusion. Do not reject, and do not use this medication!"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe \\((X_1, \\dots, X_{n})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nVariance Testing Problem: \\[\nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\\]\nF-Test Statistic of the Variances (ANOVA) \\[\n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher distribution \\(\\mathcal F(k_1,k_2)\\)\n\n\n\nDistribution of \\(\\frac{U_1/k_1}{U_2/k_2}\\), where \\(U_1\\), \\(U_2\\) are indep. and follow \\(\\chi^2(k_1)\\), \\(\\chi^2(k_2)\\). wiki Wooclap\n\\(\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right)\\) when \\(k_1,k_2 \\to +\\infty\\)\n\\((k_1, k_2)\\): degrees of freedom\nExample: \\(\\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1)\\) if \\(Z_i \\sim \\mathcal N(0,1)\\)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2}\\) is independent of \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1\\), \\(\\sigma_2\\). It is pivotal\nIt follow distribution \\(\\mathcal F(n_1-1, n_2-1)\\)\n\n\n\n\n\n\n\n\nTwo-tailed test: \\[ \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown, but we know that \\(\\sigma_1=\\sigma_2\\)\nEquality of mean testing problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}\\).\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\\(\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\\)\nNormalize \\(\\overline X - \\overline Y\\): \\[\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\\]\n\\(\\psi(X,Y)\\) is pivotal because \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\).\n\\(\\sigma_1\\), \\(\\sigma_2\\), \\(\\mu_1\\), \\(\\mu_2\\) are unknown\nEquality of Mean Testing Problem: \\[\nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\\]\nFormally, \\(H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}\\).\n\n\n\n\n\nStudent Welch test statistic\n\n\n\\[\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\\]\n\nWooclap \\(\\psi(X,Y)\\) is not pivotal\nGaussian approximation: \\(\\psi(X,Y) \\approx \\mathcal N(0,1)\\) when \\(n_1, n_2 \\to \\infty\\)\nBetter approximation: Student Welch"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\nCLT\n\n\n\nLet \\(S_n = \\sum_{i=1}^n\\) with \\((X_1, \\dots, X_n)\\) iid (\\(L^2\\)) then \\[ \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \\]\nEquality when \\(X_i\\)’s are \\(\\mathcal N(\\mu, \\sigma^2)\\)\nRule of thumb: \\(n \\geq 30\\)\n\n\n\n\n\n\n\n\n\nExample: binomials\n\n\n\nIf \\(p \\in (0,1)\\)\n\\(\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1)\\) when \\(n \\to \\infty\\)\n\\(n\\) should be \\(\\gg \\frac{1}{p}\\)\n\n\n\n\n\n\n\n\n\nGood Approx for (\\(n=100\\), \\(p=0.2\\))\n\n\n\n\nBad Approx for (\\(n=100\\), \\(p=0.01\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe \\(X \\sim Bin(n_1, p_1)\\) and \\(Y \\sim Bin(n_2, p_2)\\).\n\\(n_1\\), \\(n_2\\) are known but \\(p_1\\), \\(p_2\\) are unknown in \\((0,1)\\)\n\\(H_0\\): \\(p_1 = p_2\\) or \\(H_1\\): \\(p_1 \\neq p_2\\)\n\n\n\n\n\nTest Statistic\n\n\n\\[ \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\\]\n\n\\(\\hat p_1 = X/n_1\\), \\(\\hat p_2 = Y/n_2\\)\n\\(\\hat p = \\frac{X+Y}{n_1+n_2}\\) [Wooclap]\nIf \\(np_1, np_2 \\gg 1\\): \\(\\psi(X) \\sim \\mathcal N(0,1)\\)\nWe reject if \\(|\\psi(X,Y)| \\geq t_{1-\\alpha/2}\\) (gaussian quantile)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/slides/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion: “should we raise taxes on cigarettes to pay for a healthcare reform ?”\n\\(p_1\\), \\(p_2\\): proportion of non-smokers or smokers willing to raise taxes\n\\(H_0\\): \\(p_1=p_2\\) or \\(H_1\\): \\(p_1 &gt; p_2\\)\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\(\\hat p_1 = \\overline X= \\approx 0.58\\), \\(\\hat p_2=\\overline Y= \\approx 0.21\\).\n\\(\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\\)\n\\(\\mathbb P(\\psi(X,Y) &gt; 8.99)\\) = 1-cdf(Normal(0,1), 8.99)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#multinomials",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#multinomials",
    "title": "Goodness of Fit Tests",
    "section": "Multinomials",
    "text": "Multinomials\n\n\n\n\n\n\nBinomial distribution\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red [Wooclap]\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1), Y=n-X \\sim \\mathrm{Bin}(n,1-p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color [Wooclap]\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m)) [Wooclap]\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit Test",
    "text": "\\chi^2 Goodness of Fit Test\n\n\n\n\nWe observe (X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q). This corresponds to n counts: X_1 + \\dots + X_m = n\nq = (q_1, \\dots, q_m) corresponds to probabilities of getting color 1, \\dots, m\nLet p = (p_1, \\dots, p_m) be a known vector s.t. p_1 + \\dots + p_m = 1.\nH_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\n\n\n\n\n\n\n\n\n\n\n\\chi^2 Goodness of Fit Test (Adéquation)\n\n\n\nChi-squared test statistic:: \\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; , where n_i = np_i = \\mathbb E[X_i] is the expected number of counts for color i.\nWhen np_i = n_i are large, under H_0, we approximate the distribution of \\psi(X) as \\psi(X) \\sim \\chi^2(m-1)\nReject if \\psi(X) &gt; t_{1- \\alpha} (right-tail of \\chi^2(m-1))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "title": "Goodness of Fit Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\n\n\n\nWe observe a bag of sweets containing n=100 sweets of m=3 different colors: red, green, and yellow.\nManufacturer: p_1= 40\\% red, p_2=35\\% green, and p_3=25\\% yellow.\nH_0: q=p (manufacturer’s claim is correct)\nH_1: q\\neq p (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\n\n\n\nColor\nObserved Counts\n\n\n\n\nRed\nX_1=50\n\n\nGreen\nX_2=30\n\n\nYellow\nX_3=20\n\n\n\n\n\n\n\n\n\nExpected Counts\n\n\n\n\nn_1=40\n\n\nn_2=35\n\n\nn_3=25\n\n\n\n\n\n\n\n\n\n\n\n\n\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\n\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878 (p_{value} \\approx 0.222)\nConclusion: do not reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#histograms",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#histograms",
    "title": "Goodness of Fit Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nhistogram\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n\n\n\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\n\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\n\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\nwhere I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\n\n\n\n\n\n\n\n\n\n\n\n\nNormalization\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of large numbers, Monte Carlo (informal)\n\n\n\nAssume that (X_1, \\dots, X_n) are iid of distrib P, and that a,b, k are fixed\nThe histogram \\mathrm{hist}(a,b,k) converges to the histogram of the density P"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit to a given distribution",
    "text": "\\chi^2 Goodness of Fit to a given distribution\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n, iid with unknown distrib P\nH_0: P = P_0, where P_0 is known\nH_1: P \\neq P_0 [Wooclap]\nIdea: under H_0, the counts in disjoint intervals I_1, I_2, \\dots, I_k follow a multinomial distribution \\mathrm{Mult}(n, (p_1, \\dots, p_{k})) where p_1 = P_0(I_1), \\dots, p_{k} = P_0(I_k)\n\n\n\n\n\n\n\n\n\n\nReduction to chi-squared test statistic\n\n\n\nCount the number of data (c_1, \\dots, c_k) in I_1, \\dots, I_k\nTheoretical counts nP_0(I_1) \\dots, nP_0(I_k)\nChi-squared statistic: \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_j)}\nDecide using an 1-\\alpha-quantile of a \\chi^2(k-1) distribution\n\n\n\n\n\n\n\n\n\n\nCorrected \\chi^2 Test\n\n\n\nIf P_0 is unknown\nIn a class \\mathcal P parameterized by \\ell parameters\nThen estimate the \\ell parameters\nCompute theoretical counts with \\hat P_{\\hat \\theta}\nBut decide with \\chi^2(k - 1 - \\ell)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Goodness of Fit Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\nH_0: X_i iid \\mathcal P(2)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n0\n1\n2 \n\\geq 3\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\n\n\n\n\n\n\nTo get 9.7, we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\gtrsim \\frac{(16-4)^2}{4} = 36\n(1-cdf(Chisq(3),36)) is very small: Reject\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf H_0 is X_i iid \\mathcal P(\\lambda) with unknown \\lambda\n\\hat \\lambda = 0.8, \\sum_{i=1}^4 \\frac{(X_i - n_i)^2}{n_i} \\approx 9.4\nNot \\chi^2(3) but \\chi^2(2)\n(1-cdf(Chisq(2),9.4)) \\approx 0.009. Reject at level 1%\n\n\n\n\n\n\n\n[Wooclap]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "title": "Goodness of Fit Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)} [Wooclap]\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\n\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with y=x"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "href": "teaching/hypothesis_testing/slides/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "title": "Goodness of Fit Tests",
    "section": "Kolmogorov-Smirnov Test",
    "text": "Kolmogorov-Smirnov Test\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile:\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n} [Wooclap]\nEmpirical CDF: \\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf 1\\{X_i \\leq x\\}\nIdea: Max distance between empirical and true CDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\\psi(X) = \\sup_{x}|\\hat F(x) - F_0(x)|\nApprox: \\mathbb P_0(\\psi(X) &gt;c/\\sqrt{n}) \\to 2\\sum_{r=1}^{+\\infty}(-1)^{r-1}\\exp(-2c^2r^2) when n \\to +\\infty\nIn practice, use Julia, Python or R for KS Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html",
    "href": "teaching/hypothesis_testing/TDs/TP.html",
    "title": "TP: Hypothesis Testing",
    "section": "",
    "text": "We observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\). We assume that the vectors \\(X\\) and \\(Y\\) are independent. We want to test \\(H_0\\): \\(\\mu_1 = \\mu_2\\) VS \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\). We observe the data\nX=[-0.2657064426519085, -0.27538323622274347, 0.11419811877193782, 0.1158736466676504, 1.7071154417851981, 0.9306910454777643, 0.5834941669559498, -1.536447927372139, -1.4158768806157345, 1.0532694288697444, 1.2955133629200777, -0.4195557179577367]\nY=[-0.6452416530469819, 0.3662048411679129, -0.09943069837472361, 0.8738423322164134, 0.7163913715056272, -0.32450102319617485, 0.9159821874321818, -2.3583609849887224]\n\nCompute the student-Welch test statistic \\(\\tfrac{\\overline X - \\overline Y}{\\sqrt{\\hat\\sigma_1/n_1 + \\hat\\sigma_2/n_2}}\\)\nConclude using a Gaussian approximation (use the cdf of a N(0,1))\nConclude using an UnequalVarianceTTest (julia) or test.welch (R) or scipy.stats.ttest_ind(a, b, equal_var=False) (python)\nBonus. Conclude using a better chi-squared approximation. Compare these result to 3."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#student-test",
    "href": "teaching/hypothesis_testing/TDs/TP.html#student-test",
    "title": "TP: Hypothesis Testing",
    "section": "",
    "text": "We observe \\((X_1, \\dots, X_{n_1})\\) iid \\(\\mathcal N(\\mu_1, \\sigma_1)\\) and \\((Y_1, \\dots, Y_{n_2})\\) iid \\(\\mathcal N(\\mu_2, \\sigma_2)\\). We assume that the vectors \\(X\\) and \\(Y\\) are independent. We want to test \\(H_0\\): \\(\\mu_1 = \\mu_2\\) VS \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\). We observe the data\nX=[-0.2657064426519085, -0.27538323622274347, 0.11419811877193782, 0.1158736466676504, 1.7071154417851981, 0.9306910454777643, 0.5834941669559498, -1.536447927372139, -1.4158768806157345, 1.0532694288697444, 1.2955133629200777, -0.4195557179577367]\nY=[-0.6452416530469819, 0.3662048411679129, -0.09943069837472361, 0.8738423322164134, 0.7163913715056272, -0.32450102319617485, 0.9159821874321818, -2.3583609849887224]\n\nCompute the student-Welch test statistic \\(\\tfrac{\\overline X - \\overline Y}{\\sqrt{\\hat\\sigma_1/n_1 + \\hat\\sigma_2/n_2}}\\)\nConclude using a Gaussian approximation (use the cdf of a N(0,1))\nConclude using an UnequalVarianceTTest (julia) or test.welch (R) or scipy.stats.ttest_ind(a, b, equal_var=False) (python)\nBonus. Conclude using a better chi-squared approximation. Compare these result to 3."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#monte-carlo-and-chi-squared-tests",
    "href": "teaching/hypothesis_testing/TDs/TP.html#monte-carlo-and-chi-squared-tests",
    "title": "TP: Hypothesis Testing",
    "section": "1. Monte Carlo and Chi-squared Tests",
    "text": "1. Monte Carlo and Chi-squared Tests\nA statistician observes \\(X = (X_1, \\dots, X_n)\\) where the \\(X_i's\\) are iid of distribution \\(P\\). If the problem is to test whether \\(P\\) is Gaussian with known \\(\\mu\\) and \\(\\sigma\\), the problem is:\n\\[H_0: P=\\mathcal N(\\mu, \\sigma) \\quad \\text{VS} \\quad H_1: P\\neq \\mathcal N(\\mu, \\sigma)\\]\nIf \\(\\mu\\) and \\(\\sigma\\) are unknown, the problem is \\[H_0: P\\in \\{\\mathcal N(\\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt;0\\}\\quad \\text{VS} \\quad H_1: P\\not \\in \\{\\mathcal N(\\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt;0\\}\\]\nWe first assume that \\(\\mu\\) and \\(\\sigma\\) are known, and that:\nmu = 0\nsigma = 1\nn = 100\nm = 5\nThis practical exercise aims to empirically demonstrate how a chi-squared test statistic converges to a chi-squared distribution in both known and unknown parameter scenarios. We will:\n\nDivide the observation space into 5 disjoint intervals\nCount how many observations fall into each interval for randomly generated data\nCalculate the chi-squared test statistic for randomly generated data\nRepeat this process 1,000 times to build an empirical distribution (an histogram)\nThe resulting empirical histogram should approach a theoretical chi-squared distribution as both the sample size \\(n\\) and the number of repetitions \\(N\\) approach infinity.\n\n\nQuestions\n\nGenerate a vector \\(X\\) made of \\(n\\) iid \\(\\mathcal N(\\mu, \\sigma)\\)\nCompute the vector \\(Y = \\frac{X-\\mu}{\\sigma}\\)\nCompute the list of counts \\(C\\) of \\(Y\\) in \\((-\\infty, -3)\\), \\([\\tfrac{3i}{m}, \\frac{3(i+1)}{m})\\) for \\(i\\) in \\(\\{-m, \\dots, m-1\\}\\) and \\([3,+\\infty)\\).\n\nHow many intervals do we have here?\nWhat is the expected number of entries of \\(Y\\) falling in \\([3, +\\infty)\\)? (compute this using the cdf function). Change the value of \\(n\\) so that we have at least \\(5\\) expected counts in \\([3, +\\infty)\\).\n\n\n\n\n#Julia: use the broadcasting .&lt;\nsum(x .&lt;= Y .&lt; y) # counts in [x, y)\n\n#R: use bitwise operator &\nsum(Y &gt;= x & Y &lt; y) # counts in [x, y)\n\n\n\nUsing the cdf of \\(\\mathcal N(0,1)\\), compute the list of expected counts in the same intervals\nCompute the Chi-squared test statistic using the two preceeding questions. We recall that \\(\\psi(Y) = \\sum_{i=1}^n \\tfrac{(c_i - e_i)^2}{e_i}\\) where \\(c_i\\) and \\(e_i\\) are the counts and expected counts.\nSummarize the preceeding questions into a function trial_chisq(X, mu, sigma, m) that normalizes \\(X\\), computes counts, expected counts and the chisq test statistic:\n\n# function trial_chisq(X, mu, sigma, m)\n# n = length(X)\n# Y = (X-mu)/sigma\n# Compute counts \n# Compute expcounts\n# Compute and Return chisq\n\nUsing the previous question, write a function monte_carlo_known that computes \\(N\\) chi-squared test statistics on iid random samples \\(X\\sim \\mathcal N(\\mu, \\sigma)^{\\otimes n}\\). It returns a list trials of length \\(N\\).\n\nN = 1000\n# function monte_carlo_known(N, mu, sigma, n, m)\n# empty list trials\n\n# for i = 1 ... N\n\n# Generate X made of n iid gaussian (mu, sigma)\n# append trial_chisq(X, mu, sigma, m) to trials\n\n# endfor\n# return trials\n\nPlot a histogram of a list of trials using a builtin function. Normalize it in density (area=1), and precise the bins (0:0.5:30).\nWhat is a good distribution to approximate the histogram? Plot the distribution’s density and check that it fits the histogram. Vary the parameters \\(m\\), \\(n\\), and \\(N\\).\n\nNow, we assume that \\(\\mu\\) and \\(\\sigma\\) are unknown.\n\nGiven \\(X\\), compute to estimators hatmu and hatsigma of mu and sigma\nSimilarly to Q.7, write a function monte_carlo_unknown(N, n, m) that computes a Monte-Carlo simulation. \\(\\hat \\mu\\) and \\(\\hat \\sigma\\) must be computed for all trial \\(i=1,\\dots,N\\).\nRevisit questions 8 and 9, considering the case where \\(\\mu\\) and \\(\\sigma\\) are unknown. How does this affect the distribution of the histogram?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TP.html#application-with-bitcoin",
    "href": "teaching/hypothesis_testing/TDs/TP.html#application-with-bitcoin",
    "title": "TP: Hypothesis Testing",
    "section": "2. Application with Bitcoin",
    "text": "2. Application with Bitcoin\n\nUse your favorite AI to write the code to import the last \\(500\\) hourly close prices of bitcoin in usdt from binance. Plot the prices and compute the returns defined as \\(R_t = \\tfrac{P_t}{P_{t-1}}-1\\), where \\(P_t\\) is the price at time \\(t\\) (in hours).\n\n\n\n\n\n\n\nR (Click to See a Solution)\n\n\n\n\n\nlibrary(httr)\nlibrary(jsonlite)\n# Define the API endpoint and parameters\napi_url &lt;- \"https://api.binance.com/api/v3/klines\"\nsymbol &lt;- \"BTCUSDT\" # Bitcoin to USDT trading pair\ninterval &lt;- \"1h\" # 1-hour interval\nlimit &lt;- 500 # Limit to 500 data points\n\n# Create the query URL with parameters\nquery_params &lt;- list(\n    symbol = symbol,\n    interval = interval,\n    limit = limit\n)\n\n# Fetch the data from Binance API\nresponse &lt;- GET(api_url, query = query_params)\nresponse.body\ndata &lt;- content(response, as = \"text\", encoding = \"UTF-8\")\ndata &lt;- fromJSON(data)\ndata &lt;- data.frame(data)[2]\ndata &lt;- data.frame(lapply(data, as.numeric))\nn &lt;- length(data$X2)\nR &lt;- (data[2:n,1] / data[1:(n - 1),1]) - 1\n\n\n\n\n\n\n\n\n\nJulia (Click to See a Solution)\n\n\n\n\n\nusing HTTP\nusing JSON\nusing DataFrames\n\nfunction BTC_returns()\n    # Define the API endpoint and parameters\n    api_url = \"https://api.binance.com/api/v3/klines\"\n    symbol = \"BTCUSDT\"  # Bitcoin to USDT trading pair\n    interval = \"1h\"     # 1-hour interval\n    limit = 1000         # Limit to 500 data points\n    \n    # Construct the full query URL\n    query_url = \"$api_url?symbol=$symbol&interval=$interval&limit=$limit\"\n    \n    # Fetch the data from Binance API\n    response = HTTP.get(query_url)\n    data = JSON.parse(String(response.body))\n    P = [parse(Float64, data[i][2]) for i in 1:length(data)]\n    R = [P[t] / P[t-1] - 1 for t in 2:length(P)]\n    return R\nend\n    R=BTC_returns()\n\n\n\n\nWe first test\n\\(H_0\\): the mean of the returns is zero VS \\(H_1\\): it is nonzero.\nCompute \\(\\hat \\sigma\\) as std(R) and the Student statistic \\(\\psi(R) = \\sqrt{n}\\tfrac{\\overline R}{\\hat \\sigma}\\). Compute the p-value using the cdf function of a Student(499) (or Gaussian). Obtain the same result with a library function like OneSampleTTest in Julia, t.test in R or ttest_1samp in Python\nPlot a histogram of the returns, normalized in density. Plot on the same graph the density of a Gaussian of mean mean(R) and of std std(R).\nUsing the previous exercise with \\(m=5\\), compute a chi-squared statistic and an approximated p-value.\nDo a scatter plot of \\((R_{t-1}, R_t)\\). Do you see any correlation between \\(R_{t-1}\\) and \\(R_t\\)?\nCompute the correlation \\(r\\) between \\((R_t)\\) and \\((R_{t-1})\\).\nCompute the p-value of a two-sided Pearson’s correlation test, using the test statistic \\(\\tfrac{r}{\\sqrt{1-r^2}} \\sqrt{n-2}\\) and the cdf of a Student distribution. Compare with the function CorrelationTest in Julia or cor.test in R or pearsonr in Python."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/MoreExercises.html",
    "href": "teaching/hypothesis_testing/TDs/MoreExercises.html",
    "title": "More Exercises",
    "section": "",
    "text": "A quality control analyst at a semiconductor manufacturing plant is tracking chip failures during production. For each production batch, the analyst records the number of chips that pass inspection before the batch is terminated.\nDataset: Number of working chips produced in \\(150\\) different production batches.\n\n\n\nWorking Chips\nNumber of Batches\nPercentage\n\n\n\n\n0\n8\n5.3%\n\n\n1\n10\n6.7%\n\n\n2\n16\n10.7%\n\n\n3\n17\n11.3%\n\n\n4\n21\n14.0%\n\n\n5\n20\n13.3%\n\n\n6\n17\n11.3%\n\n\n7\n14\n9.3%\n\n\n8\n10\n6.7%\n\n\n9\n7\n4.7%\n\n\n10\n3\n2.0%\n\n\n11\n5\n3.3%\n\n\n12\n2\n1.3%\n\n\nTotal\n150\n100%\n\n\n\nWe assume that each batch terminates after \\(r= 5\\) failures.\n\nWhat distribution the number of working chips in a batch should follow?\nTest the goodness of fit to this distribution.\nWhat does it change if the number of failures after which each batch ends is unknown?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/MoreExercises.html#exercise",
    "href": "teaching/hypothesis_testing/TDs/MoreExercises.html#exercise",
    "title": "More Exercises",
    "section": "",
    "text": "A quality control analyst at a semiconductor manufacturing plant is tracking chip failures during production. For each production batch, the analyst records the number of chips that pass inspection before the batch is terminated.\nDataset: Number of working chips produced in \\(150\\) different production batches.\n\n\n\nWorking Chips\nNumber of Batches\nPercentage\n\n\n\n\n0\n8\n5.3%\n\n\n1\n10\n6.7%\n\n\n2\n16\n10.7%\n\n\n3\n17\n11.3%\n\n\n4\n21\n14.0%\n\n\n5\n20\n13.3%\n\n\n6\n17\n11.3%\n\n\n7\n14\n9.3%\n\n\n8\n10\n6.7%\n\n\n9\n7\n4.7%\n\n\n10\n3\n2.0%\n\n\n11\n5\n3.3%\n\n\n12\n2\n1.3%\n\n\nTotal\n150\n100%\n\n\n\nWe assume that each batch terminates after \\(r= 5\\) failures.\n\nWhat distribution the number of working chips in a batch should follow?\nTest the goodness of fit to this distribution.\nWhat does it change if the number of failures after which each batch ends is unknown?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html",
    "href": "teaching/hypothesis_testing/TDs/TD4.html",
    "title": "TD4: Homogeneity/Dependency",
    "section": "",
    "text": "A sociologist wants to investigate whether the choice of transportation mode (Car, Bicycle, or Public Transit) varies among residents of three different cities: City A, City B, and City C.\nThe sociologist conducted a survey, and the responses are summarized in the contingency table below:\n\n\n\nTransportation Mode\nCity A\nCity B\nCity C\nTotal\n\n\n\n\nCar\n120\n150\n100\n370\n\n\nBicycle\n80\n60\n90\n230\n\n\nPublic Transit\n100\n90\n110\n300\n\n\nTotal\n300\n300\n300\n900\n\n\n\n\nFormulate the Hypothesis Testing Problem corresponding to the initial objective of the sociologist. Introduce the notation\nAnswer to the initial question using a chi-squared test at a \\(0.05\\) significance level"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-1",
    "title": "TD4: Homogeneity/Dependency",
    "section": "",
    "text": "A sociologist wants to investigate whether the choice of transportation mode (Car, Bicycle, or Public Transit) varies among residents of three different cities: City A, City B, and City C.\nThe sociologist conducted a survey, and the responses are summarized in the contingency table below:\n\n\n\nTransportation Mode\nCity A\nCity B\nCity C\nTotal\n\n\n\n\nCar\n120\n150\n100\n370\n\n\nBicycle\n80\n60\n90\n230\n\n\nPublic Transit\n100\n90\n110\n300\n\n\nTotal\n300\n300\n300\n900\n\n\n\n\nFormulate the Hypothesis Testing Problem corresponding to the initial objective of the sociologist. Introduce the notation\nAnswer to the initial question using a chi-squared test at a \\(0.05\\) significance level"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-2",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe statistician of an insurance company is tasked with studying the impact of an advertising campaign conducted in 7 regions where the company operates. To do this, he has extracted from the database the number of new clients acquired by a certain number of agents in each region.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nNumber of agents\n9\n7\n7\n6\n7\n6\n6\n\n\nAverage number of new clients\n26.88\n22.34\n19.54\n18.95\n27.17\n25.87\n25.72\n\n\nVariance of new clients\n13.54\n12.59\n12.87\n13.42\n13.17\n12.56\n12.64\n\n\n\nThe statistician decides to perform an analysis of variance to test whether the regional factor influences the number of new clients. Let \\(X_{ik}\\) denote the number of new clients of agent \\(i\\) in region \\(k\\), \\(N_k\\) the number of agents in region \\(k\\), \\(d = 7\\) the number of regions and \\(N_{\\mathrm{tot}} = 48\\) the total number of agents. Assume that the random variables \\(X_{ik}\\) are normal with mean \\(\\mu_k\\) and variance \\(\\sigma^2\\). Define:\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{i=1}^{N_k} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\nFormulate the hypothesis testing problem to test whether the number of new clients is homogeneous accross the regions.\nWhat do \\(\\overline X_k\\), \\(\\overline X\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\) represent?\nProve the analysis of variance formula: \\[V_T = V_W + V_B \\; .\\] substract and add \\(\\overline X_k\\) in the definition of \\(V_T\\)\nCompute \\(\\overline X\\), \\(V_W\\), \\(V_B\\), and \\(V_T\\).\nWrite the definition of the ANOVA test statistic in terms of \\(V_W\\) and \\(V_B\\).\nDid the advertising campaign have the same impact in all regions?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-3",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 3",
    "text": "Exercise 3\nSome data are collected from 7 students and we want to analyze the correlation between the number of hours students spend studying before an exam and their test scores.\n\n\n\nStudent\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nStudy Hours\n2.5\n3.0\n1.5\n4.0\n3.5\n5.0\n3.0\n\n\nTest Score\n56\n64\n45\n72\n68\n80\n59\n\n\n\n\nFormulate the hypothesis testing problem for a linear correlation test\nPerform the linear correlation test at level \\(0.05\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-4",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 4",
    "text": "Exercise 4\nBelow are stress scores for \\(10\\) patients before and after a sport session:\n\n\n\n\n\n\n\n\n\n\nParticipant\nStress Score (Before)\nStress Score (After)\nDifference\nRank/Sign\n\n\n\n\n1\n40\n32\n\n\n\n\n2\n38\n35\n\n\n\n\n3\n45\n40\n\n\n\n\n4\n50\n42.5\n\n\n\n\n5\n44\n41.5\n\n\n\n\n6\n48\n48\n\n\n\n\n7\n39\n30\n\n\n\n\n8\n42\n38\n\n\n\n\n9\n47\n46\n\n\n\n\n10\n46.5\n40\n\n\n\n\n\nWe want to test if sport has an effect on the stress of the patients\n\nFormulate the hypothesis testing problem\nComplete the above table\nPerform a Wilcoxon signed rank test"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD4.html#exercise-5",
    "href": "teaching/hypothesis_testing/TDs/TD4.html#exercise-5",
    "title": "TD4: Homogeneity/Dependency",
    "section": "Exercise 5",
    "text": "Exercise 5\nLet \\(X=(X_1, \\dots, X_N)\\) be a Gaussian vectors \\(\\mathcal N(0, I_N)\\) in \\(\\mathbb R^N\\) (i.e. \\(X_i\\) are iid \\(\\mathcal N(0,1)\\)).\n\n\nWhat is the distribution of \\(QX\\), if \\(Q\\) is an orthogonal matrix ? (\\(QQ^T = I_n\\))\nWhat is the distribution of \\(\\|PX\\|^2\\) if \\(P\\) is an orthogonal projector ?\nUse the rank of \\(P\\) defined as \\(rk(P) = dim(Im(P))\\)\nDefinition of orthogonal projector: (\\(P^2=P\\) and \\(P = P^T\\))\nShow that if \\(P\\) is an orthogonal projector, then \\(PX\\) is independent of \\((I-P)X\\).\nUse the fact that two centered gaussian vectors \\(X\\),\\(Y\\) are independent iif \\(\\mathbb E[X_iY_j] = 0\\) for all \\(i,j\\). Translate this fact in a matrix form.\nWhat is the distribution of \\(\\frac{n-rk(P)}{rk(P)}\\frac{\\|PX\\|^2}{\\|(I-P)X\\|^2}\\) ?\nShow that if \\(P\\), \\(P_0\\) are two orthogonal projectors such that \\(Im(P_0) \\subset Im(P)\\), then \\(P(I-P_0)X\\) is independent of \\((I-P)(I-P_0)X\\). What is the distribution of \\(\\|P(I-P_0)X\\|^2\\) ?\nShow first that \\(PP_0=P_0P= P_0\\), and that \\(P-P_0\\) is an orthogonal projector\nWhat is the orthogonal projector \\(P_0\\) on \\(\\mathrm{Span}(1, \\dots, 1)\\) ? Deduce that \\((X_i - \\overline X)\\) is independent of \\(\\overline X\\) for all \\(i\\).\n\nWe divide \\(N\\) into \\(d\\) blocks: \\(N = N_1 + \\dots + N_d\\). We write \\((X_1, \\dots, X_n) = ((X_{11}, \\dots, X_{N_11}), (X_{12}, \\dots X_{N_2 2}), \\dots, (X_{N_d 1}, \\dots, X_{N_d d}))\\).\n\nWhat is the orthogonal projection on \\(E_k =((0, \\dots, 0), \\dots, (0, \\dots, 0),(1, \\dots, 1), (0,\\dots, 0) ,\\dots, (0, \\dots, 0))\\) ? (\\(0\\) everywhere except on block \\(k\\) where we put ones everywhere)\nGive the orthogonal projection \\(P\\) on \\(\\mathrm{Span}(E_1, \\dots, E_d)\\). Explicit \\((I-P)(I-P_0)X\\) and \\(P(I-P_0)X\\).\nDeduce the distribution of \\(\\frac{d-1}{n-d}\\frac{\\|(I-P)(I-P_0)X\\|^2}{\\|P(I-P_0)X\\|^2}\\) and of the ANOVA Test Statistic under \\(H_0\\)."
  },
  {
    "objectID": "teaching/glossary.html",
    "href": "teaching/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "General\n\n\n\nEnglish\n\nincreasing function\nnondecreasing function\ninterval \\([a,b)\\)\n\n\n\n\nFrench\n\nfonction strictement croissante\nfonction croissante\nintervalle \\([a,b[\\)\n\n\n\n\n\n\nProbability/Statistics\n\n\n\nEnglish\n\nprobability distribution\nexpectation\nlikelihood\ndistribution tail\nCDF (Cumul. Distrib. Func.)\nPDF (Proba. Density Func.)\nCLT (Central Limit Theorem)\nLLN (Law of Large Numbers)\n\n\n\n\nFrench\n\nloi de probabilité\nespérance\nvraissemblance\nqueue de distribution\nFonction de Répartition\nDensité d’une distribution\nTLC (Th. de la limite centrale)\nLoi des grands nombres\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nEnglish\n\nA sample\none-sided test\ntwo-sided test\nchi-squared goodness of fit test\nchi-squared homogeneity test\n\n\n\n\nFrench\n\nUn échantillon\ntest unilatéral\ntest bilatéral\ntest d’adéquation du Khi-deux\ntest d’homogénéité du Khi-deux\n\n\n\n\n\n\nProgramming Languages (JupytR)\n\n\n\n\nJulia\nusing Distributions\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\ncdf(Binomial(n, p), x)\nquantile(Binomial(n,p), q)\ncdf(Normal(0,1), x)\nquantile(Normal(0,1), q)\n\ncdf(Chisq(n), x) # Chi-squared\ncdf(TDist(n), x) # Student\n\nn1,n2 = 5,10\ncdf(FDist(n1, n2), x) # Fisher\n\nlmbda = 2\ncdf(Gamma(n, lmbda), x) # Gamma\ncdf(Poisson(lmbda), x) # Poisson\n\n\n\nPython\nfrom scipy.stats import *\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\nbinom.cdf(x, n, p)\nbinom.ppf(q, n, p) # Quantile\nnorm.cdf(x)\nnorm.ppf(q)\n\nchi2.cdf(x, n)\nt.cdf(x, n) # Student\n\nn1,n2 = 5,10\nf.cdf(x, n1, n2) # Fisher\n\nlmbda = 2\ngamma.cdf(x,n,lmbda) # Gamma\npoisson.cdf(x, lmbda) # Poisson\n\n\n\nR\n\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\npbinom(x, n, p)\nqbinom(q, n, p)\npnorm(x)\nqnorm(q)\n\npchisq(x, n)\npt(x, n)\n\nn1,n2 = 5,10\npf(x, n1,n2)\n\nlmbda = 2\npgamma(x,n,lmbda) \nppois(x, lmbda)"
  },
  {
    "objectID": "teaching/probability/density_likelihood.html",
    "href": "teaching/probability/density_likelihood.html",
    "title": "Density, Likelihood and Radon Nikodym",
    "section": "",
    "text": "Notation\nIn this note, \\((\\Omega,\\mathbb P)\\) denote a common probability measured space for all the random variables introduced in this note. We also write \\(\\mathbb E\\) for the expectation. Let \\(X\\) be a random variable with values in a measurable space \\((\\mathcal X, \\mathcal A)\\). Let say for simplicity that \\(\\mathcal X = \\mathbb R^n\\).\nWe say that \\(X\\) has distribution \\(P\\) if \\(\\mathbb P(X \\in A)=P(A)\\) for any measurable set \\(A\\). For clarity, we sometimes write \\(\\mathbb P_{X \\sim P}(X \\in A)\\), which means “the probability of \\(X\\) being in \\(A\\) if \\(X\\) follows distribution \\(P\\)”. Sometimes, we do the slight abuse of notation by writing that \\(P(A) = P(X \\in A)\\).\n\\(P\\) can be seen as the “pushforward” measure of the common probability measure \\(\\mathbb P\\) by random variable \\(X\\), since by definition, \\(\\mathbb P(X \\in A) = \\mathbb P(\\{\\omega \\in \\Omega, X(\\omega) \\in A\\})= \\mathbb P(X^{-1}(A))\\).\n\n\nContinuous Densities\nA measure \\(P\\) has density \\(p\\) with respect to the Lebesgue measure if for any event \\(A\\) (which is simply a measurable set of \\(\\mathbb R^n\\)), \\[P(A)= \\int_{x \\in \\mathbb R^n}\\mathbf 1\\{x \\in A\\}p(x)dx \\; .\\] \\(p(x)\\) is sometimes called the likelihood of a random variables that has distribution \\(P\\) at point \\(x\\). An equivalent condition is that for any “kind” real valued function \\(f\\) (e.g. continuous with bounded support), \\[\\mathbb E_{X \\sim P}[f(X)] \\stackrel{\\mathrm{def}}{=} \\int_{x \\in \\mathbb R^n}f(x)dP =\\int_{x \\in \\mathbb R^n}f(x)p(x)dx \\; .\\] We write that \\[dP(x) = p(x)dx ~~~ \\text{or}~~~~ \\tfrac{dP}{dx}(x) = p(x) \\; ,\\] and \\(\\tfrac{dP}{dx}\\) is called the Radon-Nikodym derivative of \\(P\\) with respect to the Lebesgue measure. The intuition of this abstract notation is the following. If \\(x \\in \\mathbb R^n\\) and \\(h\\) is a small quantity that goes to \\(0\\), \\(dP\\) represents the measure of the interval \\([x, x+h]\\), with respect to the measure \\(P\\). Then, \\(d P(x) = P([x, x+h]) = \\int_x^{x+h}p(x)dx \\sim p(x)h\\).\n\n\nThe Counting Measure for Discrete Random Variables\nRandom variables in \\(\\mathbb{R}\\) that take on a finite number of values are referred to as discrete random variables, and they do not have a density with respect to the Lebesgue measure. However, this case is much simpler and is handled within measure theory using the counting measure. As its name indicates, the counting measure \\(\\mu\\) on \\(\\mathcal X=\\mathbb R^n\\) counts the elements of a given set \\(A\\): \\[ \\mu(A) = |A| \\enspace .\\] In particular, \\(\\mu(A)\\) is infinite if \\(A\\) is an infinite set.\nLet \\(X\\) be a discrete random variable that takes values in \\(\\{x_1, \\dots, x_N\\}\\), e.g. a Bernoulli, Binomial or Poisson random variable, and let \\(P\\) be its probability distribution. Let \\(p(x_i)\\) be the probability that \\(X = x_i\\), that is \\(p(x_i) = P(\\{x_i\\}) = P(X = x_i) \\in [0,1]\\). In this discrete case, the probability \\(p(x_i)\\) represents the likelihood of the value \\(x_i\\) for the random variable \\(X\\). While \\(X\\) has not a density with respect to the Lebesgue measure, it has density \\(p\\) with respect to the counting measure \\(\\mu\\), that is \\[\\mathbb E_{X \\sim P}[f(X)] =\\int_{x \\in \\mathbb R^n}f(x)p(x)d\\mu, ~~~~~~~ \\frac{dP}{d\\mu}(x) = p(x) \\; .\\] \\[ \\] ### The General Radon Nikodym Theorem\nThe Radon Nikodym Theorem tells us that any probability \\(P\\) admits a density with respect to a given measure \\(\\nu\\) if it is absolutely continuous with respect to \\(\\nu\\), that is \\[ \\nu(A) = 0 \\implies P(A) = 0 \\; .\\] In this case, the density is the Radon Nikodym derivative \\(\\frac{dP}{d\\nu}\\) of \\(P\\) with respect to \\(\\nu\\) and satisfies\n\\[ \\mathbb E_{X \\sim P}[f(X)] =\\int_{x \\in \\mathbb R^n}f(x)\\frac{dP}{d\\nu}(x)d\\nu \\; .\\] Informally, the \\(d\\nu\\) simplify so that \\(\\frac{dP}{d\\nu}d\\nu\\) = \\(dP\\).\n\n\nGeneralized Likelihood Ratio\nIf \\(P\\) and \\(Q\\) are two probability measures such that \\(P\\) is absolutely continuous with respect to \\(Q\\), then the Radon Nikodym derivative \\(\\frac{dP}{d\\nu}\\) of \\(P\\) with respect to \\(Q\\) is a generalized likelihood ratio.\nIf \\(P\\) and \\(Q\\) are both absolutely continuous with respect to another measure \\(\\nu\\) (for example the Lebesgue measure), then the generalized likelihood ratio can be written \\[\n\\frac{dP}{dQ} = \\frac{\\frac{dP}{d\\nu}}{\\frac{dQ}{d\\nu}} \\; .\n\\]\nIn particular, if \\(P\\) and \\(Q\\) have positive densities \\(p\\) and \\(q\\) with respect to the Lebesgue measure, that is \\(\\frac{dP}{dx} = p(x)\\) and \\(\\frac{dQ}{dx} = q(x)\\), then\n\\[\n\\frac{dP}{dQ}(x) = \\frac{p(x)}{q(x)} \\; .\n\\]\nIn particular, the likelihood ratio does not depend on the reference measure (here Lebesgue).\n\n\nChange of Measure\nIf \\(\\mathbb E_{P}\\) (resp. \\(\\mathbb E_{Q}\\)) denotes the expectation when the random variable \\(X\\) follows distribution \\(P\\) (resp. \\(Q\\)), then for any measurable and bounded function \\(f\\),\n\\[\\mathbb E_{P}[f(X)] = \\mathbb E_{Q}\\left[f(X) \\frac{dP}{dQ}(X)\\right] \\; .\\] In other words, we simply replace the real random variable \\(f(X)\\) by the random variable \\(f(X) \\frac{dP}{dQ}(X)\\) when we observe \\(X\\) under \\(Q\\) instead of \\(P\\). This results directly follows from Radon-Nikodym: \\[\n\\int_{x \\in R^n} f(x) dP(x) = \\int_{x \\in R^n} f(x) \\frac{dP}{dQ}(x) dQ(x) \\; .\n\\]",
    "crumbs": [
      "Probability",
      "Density and Likelihood"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html",
    "href": "teaching/hypothesis_testing/TDs/TD1.html",
    "title": "TD1: Introduction to statistical models",
    "section": "",
    "text": "We aim to determine whether ENSAI students have any preference for cats or dogs. We assume that, a priori, they have no preference on average. We ask \\(n\\) students what their preferences are, and we let \\(X\\) be the number of “cat” answers.\n\nDefine \\(H_0\\) and \\(H_1\\). Is it a one-sided (unilatéral) or two-sided (bilatéral) test ?\nWe observe \\(10\\) students and \\(X=8\\) “cat” answers. Compute the pvalue in this specific case. Interprete the result.\nWrite the expression of the pvalue in terms of \\(n\\) and \\(X\\) and \\(F\\), the cdf of \\(\\mathrm{Bin}(n,0.5)\\).\nWrite a line of code to compute the pvalue in Julia, Python or R.\nWhat is the pvalue if \\(H_1\\) is instead\n\n“Students prefer cats” or\n“Students prefer dogs”"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-1",
    "title": "TD1: Introduction to statistical models",
    "section": "",
    "text": "We aim to determine whether ENSAI students have any preference for cats or dogs. We assume that, a priori, they have no preference on average. We ask \\(n\\) students what their preferences are, and we let \\(X\\) be the number of “cat” answers.\n\nDefine \\(H_0\\) and \\(H_1\\). Is it a one-sided (unilatéral) or two-sided (bilatéral) test ?\nWe observe \\(10\\) students and \\(X=8\\) “cat” answers. Compute the pvalue in this specific case. Interprete the result.\nWrite the expression of the pvalue in terms of \\(n\\) and \\(X\\) and \\(F\\), the cdf of \\(\\mathrm{Bin}(n,0.5)\\).\nWrite a line of code to compute the pvalue in Julia, Python or R.\nWhat is the pvalue if \\(H_1\\) is instead\n\n“Students prefer cats” or\n“Students prefer dogs”"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-2",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet \\((X_1, X_2, \\ldots, X_n)\\) be iid random variables that follow distribution \\(\\mathcal E(\\lambda)\\). We want to test: \\[\nH_0: \\lambda = \\frac{1}{2} \\quad \\text{vs.} \\quad H_1: \\lambda = 1.\n\\]\n\nShow that if \\(X \\sim E(\\lambda)\\) and \\(Y \\sim \\Gamma(k, \\lambda)\\) are independent, with \\(k \\in \\mathbb{N}^*\\), then \\(X + Y \\sim \\Gamma(k+1, \\lambda)\\). We recall that the density of \\(\\Gamma(\\lambda, k)\\) is given by \\(p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\)\nDeduce that \\(S_n=\\sum_{i=1}^n X_i\\) follows a Gamma distribution \\(\\Gamma(n, \\lambda)\\).\nFor a sample of size \\(n = 10\\), what is the rejection region of \\(S_n\\) for the simple likelihood ratio test with \\(0.05\\) significance level?\nWe admit that a Gamma distribution \\(\\Gamma(n, \\frac{1}{2})\\) is a chi-squared distribution with \\(2n\\) degrees of freedom, \\(\\chi^2(2n)\\). Bonus: Show this fact for \\(n=1\\), using a polar change of variable.\nThe empirical mean is \\(\\bar{x}_{10} = 2.5\\). What can we conclude?\nRecall what a cdf is, and read the p-value on the cdf of the \\(\\chi^2(20)\\) distribution \nCompare the p-value if we use a Gaussian approximation of \\(\\sum X_i\\) with the TCL. We recall that \\(\\mathbb V(X_1) = \\frac{1}{\\lambda^2}\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-3",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet \\(X_1, X_2, \\ldots, X_n\\) be random variables drawn from a normal distribution \\(N(\\theta, 1)\\). To test \\(H_0: \\theta = 5\\) against \\(H_1: \\theta &gt; 5\\), we propose the following test:\n\\[\nT = \\mathbf 1\\{\\bar{x} &gt; 5 + u \\},\n\\]\nwhere \\(\\bar{x}\\) is the empirical mean and \\(u\\) is to be fixed.\n\n\nDerive the function \\(g:~t \\to \\mathbb P(Z \\geq t) - e^{-t^2/2}\\), where \\(Z \\sim \\mathcal N(0,1)\\)\nDeduce that \\(\\mathbb P(Z \\geq t) \\leq e^{-t^2/2}\\) for all \\(t \\geq 0\\).\n\nDeduce a value of \\(u\\) such that the type I error of this test is smaller than a given \\(\\alpha\\). Rewrite the test \\(T\\) in function of \\(\\alpha\\).\nFix \\(\\alpha = 1/e\\) (and \\(u= \\sqrt{2/n}\\)). Compute the power function."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD1.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD1.html#exercise-4",
    "title": "TD1: Introduction to statistical models",
    "section": "Exercise 4",
    "text": "Exercise 4\nLet the family of Pareto distributions with known parameter \\(a\\) and unknown parameter \\(\\theta\\):\n\\[\nf(x) =\n\\begin{cases}\n\\frac{\\theta}{a} \\left( \\frac{a}{x} \\right)^{\\theta+1}, & \\text{if } x \\geq a, \\\\\n0, & \\text{if } x &lt; a.\n\\end{cases}\n\\]\n\nCompute the mean and variance of \\(X\\), if \\(X\\) follows a Pareto distribution of parameter \\(a\\) and \\(\\theta\\).\nRewrite the density in the form \\(f(x) = a(x)b(\\theta)e^{c(\\theta)d(x)}\\) and identify \\(a\\),\\(b\\),\\(c\\) and \\(d\\).\nDeduce the general form of the uniformly most powerful test \\(UMP_\\alpha\\) for \\(H_0: \\theta \\geq \\theta_0\\) vs. \\(H_1: \\theta &lt; \\theta_0\\).\nFor \\(a = 1\\), construct the test for the null hypothesis: the mean of the distribution is smaller than or equal to 2.\nWhat is the density of \\(d(X_1)\\) ? \\(d\\) is defined in Q.2\nWrite a line of code in Julia, Python or R to compute the rejection region at level \\(\\alpha=0.05\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html",
    "href": "teaching/hypothesis_testing/TDs/TD3.html",
    "title": "TD3: Goodness of Fit",
    "section": "",
    "text": "We want to test if a die is biased. it is rolled \\(1000\\) times, and the number of occurrences for each face is recorded. The data is as follows:\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nCounts\n159\n168\n167\n160\n175\n171\n\n\n\n\nFormulate the hypothesis testing problem\nCompute the expected counts under \\(H_0\\), give the degree of freedom \\(d\\) of the chi-squared test statistic and give the approximated p-value, using the cdf of \\(\\chi^2(d)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-1",
    "title": "TD3: Goodness of Fit",
    "section": "",
    "text": "We want to test if a die is biased. it is rolled \\(1000\\) times, and the number of occurrences for each face is recorded. The data is as follows:\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\nCounts\n159\n168\n167\n160\n175\n171\n\n\n\n\nFormulate the hypothesis testing problem\nCompute the expected counts under \\(H_0\\), give the degree of freedom \\(d\\) of the chi-squared test statistic and give the approximated p-value, using the cdf of \\(\\chi^2(d)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-2",
    "title": "TD3: Goodness of Fit",
    "section": "Exercise 2",
    "text": "Exercise 2\nIn a survey of \\(825\\) families with \\(3\\) children, the number of boys was recorded:\n\\[\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{Number of Boys} & 0 & 1 & 2 & 3 & \\text{Total} \\\\\n\\hline\n\\text{Number of Families} & 71 & 297 & 336 & 121 & 825 \\\\\n\\hline\n\\end{array}\n\\]\nWe assume under \\(H_0\\) that the genders of children in successive births within a family are independent categorical variables and that the probability \\(p\\) of having a boy remains constant.\n\nDetermine the distribution of the number of boys in a family with 3 children as a function of \\(p\\).\nEstimate \\(p\\) using a maximum likelihood estimator.\nTest the goodness of fit to the distribution obtained in question 1."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD3.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD3.html#exercise-3",
    "title": "TD3: Goodness of Fit",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe observe X = [0, 1, 0, 0, 0, 0, 0, 0.5, 1, 1, 1, 0.7, 0.9, 1, 1, 1, 1, 0, 0.1, 0, 1] We assume that the entries of \\(X\\) are iid of distribution \\(P\\). We consider the following hypothesis testing problem:\n\\(H_0\\): \\(P= \\mathcal B(0.5)\\) (Bernoulli)\\(\\quad\\) VS \\(\\quad\\) \\(H_1\\): \\(P \\neq \\mathcal B(0.5)\\).\n\nWhat can you say about the assumptions, \\(H_0\\) and \\(H_1\\)?\nDraw on the same graph the CDF of a Bernoulli \\(0.5\\) and the empirical CDF of the observed data \\(X\\).\nApply the Kolmogorov-Smirnov Test at level \\(0.1\\). To do so, use this table.\nComment on the result."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html",
    "href": "teaching/hypothesis_testing/TDs/TD2.html",
    "title": "TD2: Gaussian Populations",
    "section": "",
    "text": "A bread manufacturing factory wants to establish control procedures with the primary goal of reducing overproduction issues, which result in losses for the factory. Here, we focus on the weights of baguettes produced by the factory, with a target weight of \\(250\\) grams. For a sample of \\(n = 30\\) baguettes, the empirical mean is \\(\\bar{X}_n = 256.3\\) and the empirical variance is \\(S^2_n = 82.1\\). A priori, the factory reaches the target weight of \\(250\\) grams. We aim to test at the level of significance \\(\\alpha = 0.01\\) whether there is an overproduction issue.\n\nIs this a one-sided (unilatéral) or two-sided (bilatéral) testing problem?\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWhat is the test statistic to use, and what is its distribution under \\(H_0\\)?\nDetermine the rejection region.\nDoes the factory have an overproduction issue?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-1",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-1",
    "title": "TD2: Gaussian Populations",
    "section": "",
    "text": "A bread manufacturing factory wants to establish control procedures with the primary goal of reducing overproduction issues, which result in losses for the factory. Here, we focus on the weights of baguettes produced by the factory, with a target weight of \\(250\\) grams. For a sample of \\(n = 30\\) baguettes, the empirical mean is \\(\\bar{X}_n = 256.3\\) and the empirical variance is \\(S^2_n = 82.1\\). A priori, the factory reaches the target weight of \\(250\\) grams. We aim to test at the level of significance \\(\\alpha = 0.01\\) whether there is an overproduction issue.\n\nIs this a one-sided (unilatéral) or two-sided (bilatéral) testing problem?\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWhat is the test statistic to use, and what is its distribution under \\(H_0\\)?\nDetermine the rejection region.\nDoes the factory have an overproduction issue?"
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-2",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-2",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe want to test the precision of a method for measuring blood alcohol concentration on a blood sample. Precision is defined as twice the standard deviation of the method (assumed to follow a Gaussian distribution). The reference sample is divided into \\(6\\) test tubes, which are subjected to laboratory analysis. The following blood alcohol concentrations were obtained in g/L: \\[\n1.35, \\; 1.26, \\; 1.48, \\; 1.32, \\; 1.50, \\; 1.44.\n\\]\nWe aim to test the hypothesis that the precision is less than or equal to \\(0.1 \\, \\text{g/L}\\).\n\nFormulate the hypothesis testing problem (Define the parameters of the model, the corresponding distributions, what is known or unknown, and write \\(H_0\\) and \\(H_1\\)). Write the corresponding sets of parameters \\(\\Theta_0, \\Theta_1\\).\nWrite the test statistic and give its distribution under \\(H_0\\).\nPerform the test at a significance level of \\(\\alpha = 0.05\\).\nShow that the p-value of this test lies between \\(0.001\\) and \\(0.01\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-3",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-3",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 3",
    "text": "Exercise 3\nA candidate for the European elections wants to know if their popularity differs between men and women. A survey was conducted with \\(250\\) men, of whom \\(42\\%\\) expressed support for the candidate, and \\(250\\) women, of whom \\(51\\%\\) expressed support.\n\nFormulate the hypothesis testing problem.\nAt a significance level of \\(\\alpha = 0.05\\), can we say that these values indicate a statistically significant difference in popularity?\nGive an approximation of the p-value in terms of \\(F\\) ,the CDF of \\(\\mathcal N(0,1)\\) and read it on the graph below."
  },
  {
    "objectID": "teaching/hypothesis_testing/TDs/TD2.html#exercise-4",
    "href": "teaching/hypothesis_testing/TDs/TD2.html#exercise-4",
    "title": "TD2: Gaussian Populations",
    "section": "Exercise 4",
    "text": "Exercise 4\nWe aim to compare the average daily durations (in hours) of home-to-work commutes in two departments, labeled \\(A\\) and \\(B\\). We randomly surveyed 26 people in \\(A\\) and 22 in \\(B\\). Let \\(X_i\\) of pepople \\(i\\) be the random variable representing the commute duration in department \\(A\\), and \\(Y_j\\) that in department \\(B\\) of people \\(j\\). We assume the samples obtained are i.i.d following a Gaussian distribution: \\[\nX_i \\sim \\mathcal{N}(\\mu_A, \\sigma_A) \\quad \\text{and} \\quad Y_j \\sim \\mathcal{N}(\\mu_B, \\sigma_B).\n\\]\nHere is a summary of the data:\n\n\n\nDepartment A\nDepartment B\n\n\n\n\n\\(n_A= 26\\)\n\\(n_B=22\\)\n\n\n\\(\\sum x_i = 535\\)\n\\(\\sum y_j = 395\\)\n\n\n\\(\\sum x_i^2 = 11400\\)\n\\(\\sum y_j^2 = 7900\\)\n\n\n\n\nFormulate the hypothesis testing problem\nTest the equality of variances at a significance level of \\(\\alpha = 0.1\\)\nTest the equality of mean commute times between the two departments at a significance level of \\(\\alpha = 0.05\\), and conclude\nGive a Gaussian approximation of the test statistic using the CLT and the LLN, and approximate the p-value using the graph of the cdf of \\(\\mathcal N(0,1)\\) given in the previous exercise"
  },
  {
    "objectID": "teaching/hypothesis_testing/glossary.html",
    "href": "teaching/hypothesis_testing/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "General\n\n\n\nEnglish\n\nincreasing function\nnondecreasing function\ninterval \\([a,b)\\)\n\n\n\n\nFrench\n\nfonction strictement croissante\nfonction croissante\nintervalle \\([a,b[\\)\n\n\n\n\n\n\nProbability/Statistics\n\n\n\nEnglish\n\nprobability distribution\nexpectation\nlikelihood\ndistribution tail\nCDF (Cumul. Distrib. Func.)\nPDF (Proba. Density Func.)\nCLT (Central Limit Theorem)\nLLN (Law of Large Numbers)\n\n\n\n\nFrench\n\nloi de probabilité\nespérance\nvraissemblance\nqueue de distribution\nFonction de Répartition\nDensité d’une distribution\nTLC (Th. de la limite centrale)\nLoi des grands nombres\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nEnglish\n\nA sample\none-sided test\ntwo-sided test\nchi-squared goodness of fit test\nchi-squared homogeneity test\n\n\n\n\nFrench\n\nUn échantillon\ntest unilatéral\ntest bilatéral\ntest d’adéquation du Khi-deux\ntest d’homogénéité du Khi-deux\n\n\n\n\n\n\nProgramming Languages (JupytR)\n\n\n\n\nJulia\nusing Distributions\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\ncdf(Binomial(n, p), x)\nquantile(Binomial(n,p), q)\ncdf(Normal(0,1), x)\nquantile(Normal(0,1), q)\n\ncdf(Chisq(n), x) # Chi-squared\ncdf(TDist(n), x) # Student\n\nn1,n2 = 5,10\ncdf(FDist(n1, n2), x) # Fisher\n\nlmbda = 2\ncdf(Gamma(n, lmbda), x) # Gamma\ncdf(Poisson(lmbda), x) # Poisson\n\n\n\nPython\nfrom scipy.stats import *\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\nbinom.cdf(x, n, p)\nbinom.ppf(q, n, p) # Quantile\nnorm.cdf(x)\nnorm.ppf(q)\n\nchi2.cdf(x, n)\nt.cdf(x, n) # Student\n\nn1,n2 = 5,10\nf.cdf(x, n1, n2) # Fisher\n\nlmbda = 2\ngamma.cdf(x,n,lmbda) # Gamma\npoisson.cdf(x, lmbda) # Poisson\n\n\n\nR\n\n\nn = 100\nx = 20\np = 0.5\nq = 0.95\n\npbinom(x, n, p)\nqbinom(q, n, p)\npnorm(x)\nqnorm(q)\n\npchisq(x, n)\npt(x, n)\n\nn1,n2 = 5,10\npf(x, n1,n2)\n\nlmbda = 2\npgamma(x,n,lmbda) \nppois(x, lmbda)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Correlation Test",
    "text": "Correlation Test\n\n\n\n\n\n\nPearson’s Correlation Test\n\n\n\n\\(r =  \\frac{\\sum_{i=1}^n (X_i - \\overline X)(Y_i - \\overline Y)}{\\sqrt{\\sum_{i=1}^n (X_i - \\overline X)^2\\sum_{i=1}^n (Y_i - \\overline Y)^2}}\\)\n\\(\\psi(X,Y) = \\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\)\nUnder \\(H_0\\), \\(\\psi(X,Y) \\approx \\mathcal T(n-2)\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#correlation-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Correlation Test",
    "text": "Correlation Test\nMonte Carlo Simulation with \\(n=4\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\\(d\\) independent groups (bags), each containing \\(N_1, \\dots, N_d\\) individuals. \\(N_{\\mathrm{tot}} = \\sum_{k=1}^d N_k\\)\nFor each group \\(k\\) (eg a region), we observe \\((X_{1,k}, \\dots, X_{N_k,k}) \\in \\mathbb R^{N_k}\\) (eg salaries).\nWe assume that the \\(X_{ik}\\) are iid \\(\\mathcal N(\\mu_k, \\sigma)\\).\nEx: \\(X_{ik} =\\) sallary of \\(i\\) living in region \\(k\\) [Wooclap]\n\\(H_0: \\mu_1 = \\dots = \\mu_d\\) vs \\(H_1: \\mu_l \\neq \\mu_k\\) for some \\((l,k)\\) (unknown \\(\\sigma\\))"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\n\nNote\n\n\n\n\n\nEmpirical mean of group \\(k\\): \\(\\overline X_k = \\tfrac{1}{N_k}\\sum_{i=1}^{N_k} X_{ik}\\) [Wooclap]\nSum of Squares in group \\(k\\):\n\\(SS_k = \\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\\)\n\\(SS_k \\sim \\sigma^2\\chi^2(N_k-1)\\) under \\(H_0\\)\nEmpirical var of group \\(k\\): \\(V_k = \\tfrac{1}{N_k}SS_k\\)\n\n\n\nTotal empirical mean: \\(\\overline X = \\tfrac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} X_{ik}\\)\nSum of Squares btween Groups: \\(SSB = \\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\)\n\\(SSB \\sim \\sigma^2\\chi^2(d-1)\\) under \\(H_0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#anova-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#anova-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\n\n\n\nANOVA Test Statistic\n\n\n\n\\(\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}\\)\n\\(\\psi(X) \\sim \\mathcal F(d-1, N_{\\mathrm{tot}}-d)\\) (Fisher under \\(H_0\\))\nright-tailed test: \\(p_{value} = 1-\\mathrm{cdf}(\\mathcal F(d-1, N_{\\mathrm{tot}}-d)), \\psi(x_{obs})\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#interpretation-of-variances-in-anova",
    "href": "teaching/hypothesis_testing/slides/dependency.html#interpretation-of-variances-in-anova",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Interpretation of variances in ANOVA",
    "text": "Interpretation of variances in ANOVA\n\n\\[\n\\left.\\begin{array}{cl}\n\\overline X_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} X_{ik}\\\\\n\\overline{X} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline X_{k},&\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (X_{ik} - \\overline X_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline X_k - \\overline X)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (X_{ik} - \\overline X)^2\n\\end{array}\n\\right.\n\\]\n\n\n\n\\(V_k\\): Empirical variance of group \\(k\\)\n\\(V_W\\): Average empirical variance within groups (unexplained variance)\n\\(V_B\\): Empirical variance between groups (explained variance)\n\\(V_T\\): Total variance of the sample\n\n\n\\[\\psi(X) = \\frac{\\tfrac{1}{d-1}SSB}{\\tfrac{1}{N_{\\mathrm{tot}} - d}\\sum_{k=1}^d SS_k}=\\frac{\\tfrac{1}{d-1}V_B}{\\tfrac{1}{N_{tot}-d}V_W} = \\frac{N_{tot}-d}{d-1} \\frac{V_B}{V_W} \\sim \\mathcal F(d-1, N_{tot}-d) \\; .\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#chi2-homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#chi2-homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Homogeneity Test",
    "text": "\\(\\chi^2\\) Homogeneity Test\n\n\\(d\\) different bags (or groups), each containing balls of \\(m\\) potential colors.\nIf \\(d = 3\\) and \\(m=2\\), we observe the following \\(2\\times 3\\) matrix of counts:\n\n\n\n\n\n\nbag 1\nbag 2\nbag 3\nTotal\n\n\n\n\ncolor 1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(R_1\\)\n\n\ncolor 2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(R_2\\)\n\n\nTotal\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#homogeneity-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#homogeneity-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Homogeneity Test",
    "text": "Homogeneity Test\n\n\n\n\nBag \\(j\\): \\((X_{1j}, X_{2j}) \\sim \\mathrm{Mult}(N_j, (p_{1j}, p_{2j}))\\)\nThe parameters \\(p_{ij}\\) are unknown [Wooclap]\n\\(H_0\\): \\(p_{i1} = p_{i2}=p_{i3}\\) for all color \\(i\\) (bags are homogeneous)\n\\(H_1\\): bags are heterogeneous\n\\(\\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_jp_{ij})^2}{N_jp_{ij}}\\) not a test statistic\n\n\n\n\n\n\n\n\nChi-Squared Homogeneity Test Statistic\n\n\n\n\\(\\hat p_{i} = \\tfrac{1}{N}\\sum_{j=1}^{d}X_{ij} = \\frac{R_i}{N}\\)\n\\(\\psi(X) = \\sum_{i=1}^m\\sum_{j=1}^d \\frac{(X_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\)\nApproximation: \\(\\psi(X) \\sim \\chi^2((m-1)(d-1))\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#example-soft-drink-preferences",
    "href": "teaching/hypothesis_testing/slides/dependency.html#example-soft-drink-preferences",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Example: Soft drink preferences",
    "text": "Example: Soft drink preferences\n\n\nSplit population into \\(3\\) categories: Young Adults (18-30), Middle-Aged Adults (31-50), and Seniors (51 and above).\n\\(H_0\\): The groups are homogeneous in terms of soft drink preferences\n\n\n\n\n\nAge Group\nYoung Adults\nMiddle-Aged\nSeniors\nTotal\n\n\n\n\nCoke\n60\n40\n30\n130\n\n\nPepsi\n50\n55\n25\n130\n\n\nSprite\n30\n45\n55\n130\n\n\nTotal\n140\n140\n110\n390\n\n\n\n\n\n\\(N_1 \\hat p_1 = 140*\\frac{130}{390} \\approx 46.7\\) (proportion of Coke lovers)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#computation-of-chi2-stat",
    "href": "teaching/hypothesis_testing/slides/dependency.html#computation-of-chi2-stat",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Computation of Chi2 stat",
    "text": "Computation of Chi2 stat\n\\[\n\\begin{aligned}\n\\psi(X) &= \\frac{(60-46.7)^2}{46.7}&+ \\frac{(40-46.7)^2}{46.7}&+\\frac{(30-36.7)^2}{36.7} \\\\\n&+\\frac{(50-46.7)^2}{46.7}&+ \\frac{(55-46.7)^2}{46.7}&+\\frac{(25-36.7)^2}{36.7}\\\\\n&+\\frac{(30-46.7)^2}{46.7}&+ \\frac{(45-46.7)^2}{46.7}&+\\frac{(55-36.7)^2}{36.7}\\\\\n    &\\approx 26.57\n\\end{aligned}\n\\]\n\n1-cdf(Chisq(4), 26.57) # 2.4e-5, reject H_0"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#chi2-independence-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#chi2-independence-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "\\(\\chi^2\\) Independence Test",
    "text": "\\(\\chi^2\\) Independence Test\n\n\n\n\nObservations: Paired Categorical Variables \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\ne.g. \\(X_i \\in \\{\\mathrm{male}, \\mathrm{female}\\}\\), \\(Y_i \\in \\{\\mathrm{coffee}, \\mathrm{tea}\\}\\) [Wooclap]\nIdea: regroup the data into bags, e.g. \\(\\{\\mathrm{male}, \\mathrm{female}\\}\\)\nBuild the contingency table\nPerform a chi-square homogeneity test\n\n\n\n\n\n\n\nExample of contingency table:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n30\n20\n50\n\n\nTea\n28\n22\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\nExpected counts:\n\n\n\nGender\nMale\nFemale\nTotal\n\n\n\n\nCoffee\n29\n21\n50\n\n\nTea\n29\n21\n50\n\n\nTotal\n58\n42\n100\n\n\n\n\n\n\n\\(N_1 \\hat p_1 = 58 \\cdot 50/100  = 29\\), Degree of freedom \\(= 1\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable",
    "href": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\n\nA median of \\(X\\) is a \\(0.5\\)-quantile of its distribution\nIf \\(X\\) has density \\(p\\), the median \\(m\\) is such that \\[\n\\int_{-\\infty}^m p(x)dx = \\int_{m}^{+\\infty} p(x)dx =  0.5 \\; .\n\\]\nA symmetric random variable \\(X\\) is such that the distribution of \\(X\\) is the same as the distribution of \\(-X\\).\nIn particular, its median is \\(0\\).\nIf \\(X\\) is a symmetric random variable, then it has the same distribution as \\(\\varepsilon |X|\\) where \\(\\varepsilon\\) is independent of \\(X\\) and uniformly distributed in \\(\\{-1,1\\}\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#symetric-random-variable-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Symetric Random Variable",
    "text": "Symetric Random Variable\n\n\n\nSymetrization\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent variables with same density \\(p\\), then \\(X-Y\\) is symetric.\nIndeed: \\[\n\\begin{aligned}\n\\mathbb P(X - Y \\leq t) &=\\int_{-\\infty}^t \\mathbb P(X \\leq t+y)p(y)dy \\\\\n&=\\int_{-\\infty}^t \\mathbb P(Y \\leq t+x)p(x)dy = \\mathbb P(Y-X \\leq t) \\; .\n\\end{aligned}\n\\]\n\\(X\\) indep of \\(Y\\) \\(\\implies\\) \\(X-Y\\) symmetric \\(\\implies\\) \\(\\mathrm{median}(X-Y) = 0\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#dependency-problem-for-paired-data",
    "href": "teaching/hypothesis_testing/slides/dependency.html#dependency-problem-for-paired-data",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Dependency Problem for Paired Data",
    "text": "Dependency Problem for Paired Data\n\n\n\n\nWe observe iid pairs of real numbers \\((X_1, Y_1), \\dots, (X_n, Y_n)\\). The density of each pair \\((X_i, Y_i)\\) is unknown \\(p_{XY}(x,y)\\).\nThe marginal distribution of \\(X_i\\) and \\(Y_i\\) are, respectively, \\[p_X(x) = \\int_{y \\in \\mathbb R} p_{XY}(x,y)~~~~ \\text{ and }~~~~ p_{Y}(y) = \\int_{x \\in \\mathbb R} p_{XY}(x,y) \\; .\\]\n\\(H_0:\\) The median of \\((X_i - Y_i)\\) is \\(0\\) for all \\(i\\)\n\\(H_1:\\) The median of \\((X_i - Y_i)\\) is not \\(0\\) for some \\(i\\) [Wooclap]\n\n\n\n\n\n\n\n\nGenerality of \\(H_0\\)\n\n\n\nIf \\(X_i-Y_i\\) is symmetric \\(i\\), then we are under \\(H_0\\).\nIf \\(X_i\\) is independent of \\(Y_i\\) for all \\(i\\), then we are under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#section",
    "href": "teaching/hypothesis_testing/slides/dependency.html#section",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "Warning\n\n\n\nThe pairs are assume to be independent, but within each pair, \\(Y_i\\) can depend on \\(X_i\\) (that is, we don’t necessarily have \\(p(x,y) =p_X(x)p_Y(y)\\))."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\\(D_i = X_i - Y_i\\)\nWe define the sign of pair \\(i\\) as the sign of \\(D_i=X_i - Y_i\\). It is in \\(\\{-1, 1\\}\\).\nWe define the rank of pair \\(i\\) as the permutation \\(R_i\\) that satisfies \\(|D_{R_i}| = |D_{(i)}|\\), where [Wooclap] \\[\n|D_{(1)}| \\leq  \\dots \\leq |D_{(n)}|\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#properties-on-the-signed-ranks",
    "href": "teaching/hypothesis_testing/slides/dependency.html#properties-on-the-signed-ranks",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Properties on the Signed Ranks",
    "text": "Properties on the Signed Ranks\nUnder \\(H_0\\),\n\nThe signs \\((D_i)\\)’s are independent and uniformly distributed in \\(\\{-1, 1\\}\\).\nIn particular, the number of signs equal to \\(+1\\) follows a Binomial distribution \\(\\mathcal B(n,0.5)\\).\nThe ranks \\(R_i\\) of the \\((|D_i|)\\)’s does not depend on the unknown density \\(p_{X-Y}\\). \\((R_1, \\dots, R_n)\\) is a random permutation under \\(H_0\\).\nHence, any deterministic function of the ranks and of the differences is a pivotal test statistic: it does not depend on the distribution of the data under \\(H_0\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-2",
    "href": "teaching/hypothesis_testing/slides/dependency.html#wilcoxons-signed-rank-test-2",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Wilcoxon’s Signed Rank Test",
    "text": "Wilcoxon’s Signed Rank Test\n\n\n\n\nProperties on the Signed Ranks\n\n\n\nWilcoxon’s test statistic: \\(W_- = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &lt; 0\\}\\)\nSometimes, also \\(W_+ = \\sum_{i=1}^n R_i \\mathbf 1\\{D_i &gt; 0\\}\\) or \\(\\min(W_-, W_+)\\).\nGaussian approximation: \\(W_- \\asymp n(n+1)/4 + \\sqrt{n(n+1)(2n+1)/24} \\mathcal N(0,1)\\)\nif \\(H_1\\): \\(\\mathrm{median}(D_i) &gt; 0\\): left-tailed test on \\(W_-\\).\n\n\n\n\n\n\nTo generate a \\(W_-\\) under \\(H_0\\):\nk = rand(Binomial(n, 0.5))\nw = sum(randperm(n)[1:k])"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#section-1",
    "href": "teaching/hypothesis_testing/slides/dependency.html#section-1",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "",
    "text": "This approximation fits well the exact distribution. Monte-Carlo simulation:"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#effect-of-drug-on-blood-pressure",
    "href": "teaching/hypothesis_testing/slides/dependency.html#effect-of-drug-on-blood-pressure",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Effect of Drug on Blood Pressure",
    "text": "Effect of Drug on Blood Pressure\n\n\\(H_0\\): the drug has no effect. \\(H_1\\): it lowers the blood pressure\n\n\n\n\n\n\nPatient\n\\(X_i\\) (Before)\n\\(Y_i\\)​ (After)\n\\(D_i = X_i-Y_i\\)​\n\\(R_i\\)\n\n\n\n\n1\n150\n140\n10\n6 (+)\n\n\n2\n135\n130\n5\n5 (+)\n\n\n3\n160\n162\n-2\n2 (-)\n\n\n4\n145\n146\n-1\n1 (-)\n\n\n5\n154\n150\n4\n4 (+)\n\n\n6\n171\n160\n11\n7 (+)\n\n\n7\n141\n138\n3\n3 (+)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/dependency.html#numerical-appli",
    "href": "teaching/hypothesis_testing/slides/dependency.html#numerical-appli",
    "title": "Correlation, Homogeneity and Dependency",
    "section": "Numerical Appli",
    "text": "Numerical Appli\n\n\\(W_- = 1+2 = 3\\).\nFrom a simulation, we approx \\(\\mathbb P(W_-=i)\\), for \\(i \\in \\{0, 1, 2, 3, 4, 5,6\\}\\) under \\(H_0\\) by\n[0.00784066, 0.00781442, 0.00781534, 0.01563892, 0.01562184, 0.02343478]\nFrom a simulation, \\(p_{value}=\\mathbb P(W_- \\leq 3) \\approx 0.039 &lt; 0.05\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#organization",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#organization",
    "title": "Hypothesis Testing",
    "section": "Organization",
    "text": "Organization\n\n15h of lectures, 18h of TD\n\\(12^{th}\\) may: Exam (2h)\nLecture notes and slides on the website\nAbout english and programming languages\nWooclap sessions [Test]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#objective",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#objective",
    "title": "Hypothesis Testing",
    "section": "Objective",
    "text": "Objective\n\nGiven a general decision problem\n\nIntroduce precise notations to describe the pb\nformulate mathematically hypotheses \\(H_0\\) (a priori) and \\(H_1\\) (alternative)\n\nChoose a statistic adapted to the problem\nCompute this statistic and its pvalue (or an approx.)\nConclude and make a decision"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#general-principles",
    "title": "Hypothesis Testing",
    "section": "General Principles",
    "text": "General Principles\n\nFix an objective: test whether Bob has diabetes\nDesign an experiment: measure of glucose level\nDefine hypotheses\n\nNull hypothesis: \\(H_0\\): a priori, Bob has no diabetes\nAlternative hypothesis: \\(H_1\\): Bob has diabete\n\nDefine a decision dule: function of the glucose level\nCollect Data: do the measure of glucose level\nApply the decision rule: reject \\(H_0\\) or not\nDraw a conclusion: should Bob follow a treatment or make other tests ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#good-and-bad-decisions",
    "title": "Hypothesis Testing",
    "section": "Good and Bad Decisions",
    "text": "Good and Bad Decisions\n\n\n\n\nDecision\n\n\n\\(H_0\\) True\n\n\n\\(H_1\\) True\n\n\n\n\n\n\n\\(T=0\\)\n\n\nTrue Negative (TN)\n\n\nFalse Negative (FN)\n\n\nSecond Type Error\n\n\nMissed Detection\n\n\n\n\n\n\n\\(T=1\\)\n\n\nFalse Positive (FP)\n\n\nFirst Type Error\n\n\nFalse Alarm\n\n\n\n\nTrue Positive (TP)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#dice-biased-toward-6",
    "title": "Hypothesis Testing",
    "section": "Dice Biased Toward \\(6\\)",
    "text": "Dice Biased Toward \\(6\\)\n\nObjective: test if Bob is cheating with a dice.\n\nExperiment: Bob rolls the dice \\(10\\) times.\nHypotheses:\n\n\\(H_0\\): the probability of getting \\(6\\) is \\(1/6\\)\n\\(H_1\\): the probability of getting \\(6\\) is larger than \\(1/6\\)\n\nDecision rule: the probability of getting a number of \\(6\\) at least equal to the number of observed \\(6\\) is \\(&lt; 0.05\\)\nData: the dice falls \\(10\\) times on \\(6\\)\nDecision: the probability is \\(1/6^{10} &lt; 0.05\\)\nConclusion?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-dice",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#fairness-of-dice",
    "title": "Hypothesis Testing",
    "section": "Fairness of Dice",
    "text": "Fairness of Dice\n\nWe observe \\((X_1, \\dots, X_n)\\) iid where \\(X_i \\in \\{1, \\dots, 6\\}\\) where each \\(\\mathbb P(X_i = k) = p_k\\)\n\n\n\n\n\n\n\n\n\n\n\nSame data, two conclusions\n\n\n\n\n\n\\(H_0: p_6 = 1/6\\) VS \\(H_1: p_6 &gt; 1/6\\)\nDo not reject \\(H_0\\) (\\(H_0\\) is “likely”)\n\n\n\n\\(H_0: (p_1=\\dots=p_6 = 1/6)\\) (dice is fair) VS \\(H_1: \\exists k: p_k &gt; 1/6\\)\nReject \\(H_1\\) (\\(H_1\\) is “unlikely”)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#medical-test",
    "title": "Hypothesis Testing",
    "section": "Medical Test",
    "text": "Medical Test\n\nObjective: test if a fetus has Down syndrome\nExperiment: measure the Nucal translucency\nHypotheses:\n\n\\(H_0\\): nucal translucency is normal \\(\\sim \\mathcal N(1.5,0.8)\\)\n\\(H_1\\): nucal translucency is large\n\n\n\n\n\nDecision rule: reject if \\(P_0(X \\geq x_{obs}) \\leq 0.05\\)\nCollect data: \\(x_{obs}=3.02\\)\nMake a decision: calculate the value of \\(P_0(X \\geq 3.02)=0.029\\)\nConclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#recall-of-proba",
    "title": "Hypothesis Testing",
    "section": "Recall of Proba",
    "text": "Recall of Proba\nConsider a probability measure \\(P\\) on \\(\\mathbb R\\).\n\nCDF (Cumulative Distribution Function): \\[x \\to P(~(-\\infty,x]~) = \\mathbb P(X \\leq x) ~~~~\\text{(if $X \\sim P$ under $\\mathbb P$)}\\]\n\n\n\nContinous Measures\n\ndensity wrp to Lebesgue: \\(\\mathbb P(X\\in[x,x+dx])=dP(x) = p(x)dx\\)\nPDF (Proba Density Function): \\(x \\to p(x)\\)\nCDF: \\(x \\to \\int_{\\infty}^x p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\(\\int_{\\infty}^{q_{\\alpha}} p(x)dx = \\alpha\\)\nor \\(\\mathbb P(X \\leq q_{\\alpha}) = \\alpha\\)\n\n\n\nDiscrete Measures\n\ndensity wrp to counting measure: \\(\\mathbb P(X=x) = P(\\{x\\})=p(x)\\)\nCDF: \\(x \\to \\sum_{x' \\leq x} p(x')dx'\\)\n\\(\\alpha\\)-quantile \\(q_{\\alpha}\\): \\[\\inf_{q \\in \\mathbb R}\\{q:~\\sum_{x_i \\leq q}p(x_i) &gt; \\alpha\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nGaussian \\(\\mathcal N(\\mu,\\sigma)\\): \\[p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\n\nApproximation of sum of iid RV (TCL)\n\n\n\nBinomial \\(\\mathrm{Bin}(n,q)\\): \\[p(x)= \\binom{n}{x}q^x (1-q)^{n-x}\\]\n\n\n\n\n\nNumber of success among \\(n\\) Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-1",
    "title": "Hypothesis Testing",
    "section": "Examples",
    "text": "Examples\n\n\n\nExponential \\(\\mathcal E(\\lambda)\\): \\[p(x) = \\lambda e^{-\\lambda x}\\]\n\n\n\n\n\nWaiting time for an atomic clock of rate \\(\\lambda\\)\n\n\n\nGeometric \\(\\mathcal{G}(q)\\): \\[p(x)= q(1-q)^{x-1}\\]\n\n\n\n\n\nIndex of first success for iid Bernoulli \\(q\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#examples-gammapoisson",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#examples-gammapoisson",
    "title": "Hypothesis Testing",
    "section": "Examples Gamma/Poisson",
    "text": "Examples Gamma/Poisson\n\n\n\nGamma \\(\\Gamma(k, \\lambda)\\): \\[p(x) = \\frac{\\lambda^k x^{k-1}e^{-\\lambda x}}{(k-1)!}\\]\n\n\n\n\n\nWaiting time for \\(k\\) atomic clocks of rate \\(\\lambda\\)\n\n\n\nPoisson \\(\\mathcal{P}(\\lambda)\\): \\[p(x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\]\n\n\n\n\n\nNumb. of tics before time \\(1\\) of clock \\(\\lambda\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#estimation-vs-test",
    "title": "Hypothesis Testing",
    "section": "Estimation VS Test",
    "text": "Estimation VS Test\n\nWe observe some data \\(X\\) in a measurable space \\((\\mathcal X, \\mathcal A)\\).\nExample \\(\\mathcal X = \\mathbb R^n\\): \\(X= (X_1, \\dots, X_n)\\).\n\n\n\nEstimation\n\nOne set of distributions \\(\\mathcal P\\)\nParameterized by \\(\\Theta\\) \\[\n\\mathcal P = \\{P_{\\theta},~ \\theta \\in \\Theta\\} \\; .\n\\]\n\\(\\exists \\theta \\in \\Theta\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: estimate a given function of \\(P_{\\theta}\\), e.g.:\n\n\\(F(P_{\\theta}) = \\int x dP_{\\theta}\\)\n\\(F(P_{\\theta}) = \\int x^2 dP_{\\theta}\\)\n\n\n\nTest\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_0\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]\nQuestion Tests"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Estimation\n\n\n\n\nTest"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Problems\n\nSimple VS Simple:\\[\\Theta_0 = \\{\\theta_0\\} \\text{ and } \\Theta_1 = \\{\\theta_1\\}\\]\nSimple VS Multiple:\\[\\Theta_0 = \\{\\theta_0\\}\\]\nElse: Multiple VS Multiple\n\n\nTest Model\n\nTwo sets of distributions \\(\\mathcal P_0\\), \\(\\mathcal P_1\\)\nParameterized by disjoints \\(\\Theta_0\\), \\(\\Theta_1\\) \\[\n\\begin{aligned}\n\\mathcal P_0 = \\{P_{\\theta} : \\theta \\in \\Theta_0\\}, ~~~~  \\mathcal P_1 = \\{P_{\\theta} : \\theta \\in \\Theta_1\\}\\; .\n\\end{aligned}\n\\]\n\\(\\exists \\theta \\in \\Theta_0 \\cup \\Theta_1\\) such that \\(X \\sim P_{\\theta}\\)\n\nGoal: decide between \\[H_0: \\theta \\in \\Theta_0 \\text{ or } H_1: \\theta \\in \\Theta_1\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#section-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Simple VS Simple\n\n\n\n\nMultiple VS Multiple"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#parametric-vs-non-parametric",
    "title": "Hypothesis Testing",
    "section": "Parametric VS Non-Parametric",
    "text": "Parametric VS Non-Parametric\n\nparametric: \\(\\Theta_0\\) and \\(\\Theta_1\\) included in subspaces of finite dimension.\nnon-parametric: Otherwise\n\n\nExample of Multiple VS Multiple Parametric Problem:\n\n\\(H_0: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &lt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_0 \\subset \\mathbb R^2\\)\n\\(H_1: X \\sim \\mathcal N(\\theta,\\sigma)\\), unknown \\(\\theta &gt; 0\\) and unknown \\(\\sigma &gt; 0\\): \\(\\Theta_1 \\subset \\mathbb R^2\\)\n\n\n\nSimple VS Multiple Non-Parametric Problems"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#decision-rule-and-test-statistic",
    "title": "Hypothesis Testing",
    "section": "Decision Rule and Test Statistic",
    "text": "Decision Rule and Test Statistic\n\n\n\n\n\n\n\nDecision Rule\n\n\n\nA Decision Rule or Test \\(T\\) is a measurable function from \\(\\mathcal X\\) to \\(\\{0,1\\}\\): \\[ T : \\mathcal X \\to \\{0,1\\}\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter.\n\\(T(x) = 0\\) (or \\(1\\)) for all \\(x\\) is the trivial decision rule. Question: Decision Rule\n\n\n\n\n\n\n\n\n\n\n\n\nTest Statistic\n\n\n\na Test Statistic \\(\\psi\\) is a measurable function from \\(\\mathcal X\\) to \\(\\mathbb R\\): \\[ \\psi : \\mathcal X \\to \\mathbb R\\; .\\]\nIt can depend on the sets \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\)\nbut not on any unknown parameter. Question: Test Statistic"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#rejection-region",
    "title": "Hypothesis Testing",
    "section": "Rejection Region",
    "text": "Rejection Region\n\nFor a given test \\(T\\), the rejection region \\(\\mathcal R \\subset \\mathbb R\\) is the set \\[\\{\\psi(x) \\in \\mathbb R:~ T(x)=1\\} \\; .\\]\nExample of \\(\\mathcal R\\), for a given threshold \\(t&gt;0\\), \\[\n\\begin{aligned}\nT(x) &= \\mathbf{1}\\{\\psi(x) &gt; t\\}:~~~~~~\\mathcal R = (t,+\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) &lt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\\\\nT(x) &= \\mathbf{1}\\{|\\psi(x)| &gt; t\\}:~~~~~~\\mathcal R = (-\\infty,t)\\cup (t, +\\infty)\\\\\nT(x) &= \\mathbf{1}\\{\\psi(x) \\not \\in [t_1, t_2]\\}:~~~~~~\\mathcal R = (-\\infty,t_1)\\cup (t_2, +\\infty)\\;\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#simple-vs-simple-problem",
    "title": "Hypothesis Testing",
    "section": "Simple VS Simple Problem",
    "text": "Simple VS Simple Problem\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\nFor a given test \\(T\\) we define:\n\nlevel of \\(T\\): \\(\\alpha = P(T(X)=1)\\) (also: type-1 error)\npower of \\(T\\): \\(\\beta = Q(T(X)=1) = 1-Q(T(X)=0)\\) (1-\\(\\beta\\) is the type-2 error)\n\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta \\geq \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) ! But \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#likelihood-ratio-test",
    "title": "Hypothesis Testing",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\n\n\n\nTest \\(T\\) that maximizes \\(\\beta\\) at fixed \\(\\alpha\\) ?\nIdea: Consider the likelihood ratio test statistic \\[\\psi(x)=\\frac{dQ}{dP}(x) = \\frac{q(x)}{p(x)}\\]\nWe consider the likelihood ratio test \\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\n\\(t_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the distrib \\(\\frac{q(X)}{p(X)}\\) if \\(X\\sim P\\) \\[ \\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\n\n\nWe observe \\(X \\in \\mathcal X=\\mathbb R^n\\).\n\\(H_0: X \\sim P\\) or \\(H_1: X \\sim Q\\).\nWe know \\(P\\) and \\(Q\\) but we do not know whether \\(X \\sim P\\) or \\(X \\sim Q\\)\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\nunbiased: \\(\\beta &gt; \\alpha\\)\n\\(\\alpha = 0\\) for trivial test \\(T(x)=0\\) !\nBut \\(\\beta =0\\) too…"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#neyman-pearsons-theorem",
    "title": "Hypothesis Testing",
    "section": "Neyman Pearson’s Theorem",
    "text": "Neyman Pearson’s Theorem\n\n\n\n\n\n\n\n\n\n\nNeyman Pearson’s Theorem\n\n\nThe likelihood Ratio Test of level \\(\\alpha\\) maximizes the power among all tests of level \\(\\alpha\\).\n\n\n\n\n\nExample: \\(X \\sim \\mathcal N(\\theta, 1)\\).\n\\(H_0: \\theta=\\theta_0\\), \\(H_1: \\theta=\\theta_1\\).\nCheck that the log-likelihood ratio is \\[ (\\theta_1 - \\theta_0)x + \\frac{\\theta_0^2 -\\theta_1^2}{2} \\; .\\]\n\n\n\nIf \\(\\theta_1 &gt; \\theta_0\\), an optimal test if of the form \\[ T(x) = \\mathbf 1\\{ x &gt; t \\} \\; .\\]\n\n\n\n\n\n\nDecision\n\n\n\\(H_0: X \\sim P\\)\n\n\n\\(H_1: X \\sim Q\\)\n\n\n\n\n\n\n\\(T=0\\)\n\n\n\\(1-\\alpha\\)\n\n\n\\(1-\\beta\\)\n\n\n\n\n\\(T=1\\)\n\n\n\\(\\alpha\\)\n\n\n\\(\\beta\\)\n\n\n\n\n\\[ T^*(x)=\\mathbf 1\\left\\{\\frac{q(x)}{p(x)} &gt; t_{\\alpha}\\right\\} \\;\\]\nWhere, if \\(X\\sim P\\), \\[ P(T^*(X)=1)=\\mathbb P_{X \\sim P}\\left(\\frac{q(X)}{p(X)} &gt; t_{\\alpha}\\right) = \\alpha\\]\n\nEquivalent to Log-Likelihood Ratio Test: \\[T^*(x)=\\mathbf 1\\left\\{\\log\\left(\\frac{q(x)}{p(x)}\\right) &gt; \\log(t_{\\alpha})\\right\\}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-with-gaussians",
    "title": "Hypothesis Testing",
    "section": "Example with Gaussians",
    "text": "Example with Gaussians\n\n\n\nLet \\(P_{\\theta}\\) be the distribution \\(\\mathcal N(\\theta,1)\\).\nObserve \\(n\\) iid data \\(X = (X_1, \\dots, X_n)\\)\n\\(H_0: X \\sim P^{\\otimes n}_{\\theta_0}\\) or \\(H_1: X \\sim P^{\\otimes n}_{\\theta_1}\\)\nRemark: \\(P^{\\otimes n}_{\\theta}= \\mathcal N((\\theta,\\dots, \\theta), I_n)\\)\nDensity of \\(P^{\\otimes n}_{\\theta}\\):\n\n\n\n\\[\n\\begin{aligned}\n\\frac{d P^{\\otimes n}_{\\theta}}{dx} &= \\frac{d P_{\\theta}}{dx_1}\\dots\\frac{d P_{\\theta}}{dx_n} \\\\\n&= \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left({-\\sum_{i=1}^n\\frac{(x_i - \\theta)^2}{2}}\\right) \\\\\n&=  \\frac{1}{\\sqrt{2\\pi}^n}\\exp\\left(-\\frac{\\|x\\|^2}{2} + n\\theta \\overline x - \\frac{\\theta^2}{2}\\right)\\; .\n\\end{aligned}\n\\]\n\n\n\n\nLog-Likelihood Ratio Test:\n\\(T(x) = \\mathbf 1\\{\\overline x &gt; t_{\\alpha}\\}\\) if \\(\\theta_1 &gt; \\theta_0\\)\n\\(T(x) = \\mathbf 1\\{\\overline x &lt; t_{\\alpha}\\}\\) otherwise\nDistrib of \\(\\overline X\\) ?\n\\(t_{\\alpha}\\) ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#generalization-exponential-families",
    "title": "Hypothesis Testing",
    "section": "Generalization: Exponential Families",
    "text": "Generalization: Exponential Families\n\nA set of distributions \\(\\{P_{\\theta}\\}\\) is an exponential family if each density \\(p_{\\theta}(x)\\) is of the form \\[ p_{\\theta}(x) = a(\\theta)b(x) \\exp(c(\\theta)d(x)) \\; , \\]\nWe observe \\(X = (X_1, \\dots, X_n)\\). Consider the following testing problem: \\[H_0: X \\sim P_{\\theta_0}^{\\otimes n}~~~~ \\text{or}~~~~ H_1:X \\sim P_{\\theta_1}^{\\otimes n} \\; .\\]\nLikelihood Ratio: \\[\n\\frac{dP^{\\otimes n}_{\\theta_1}}{dP^{\\otimes n}_{\\theta_0}} = \\left(\\frac{a(\\theta_1)}{a(\\theta_0)}\\right)^n\\exp\\left((c(\\theta_1)-c(\\theta_0))\\sum_{i=1}^n d(x_i)\\right) \\; .\n\\]\nLikelihood Ratio Test: (Q: Select Exp. Families) \\[\nT(X) = \\mathbf 1\\left\\{\\frac{1}{n}\\sum_{i=1}^n d(X_i) &gt; t\\right\\} \\;. ~~~~\\text{(calibrate $t$)}\\]"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#example-radioactive-source",
    "title": "Hypothesis Testing",
    "section": "Example: Radioactive Source",
    "text": "Example: Radioactive Source\n\nThe number of particle emitted in \\(1\\) unit of time is follows distribution \\(P \\sim \\mathcal P(\\lambda)\\).\nWe observe \\(20\\) time units, that is \\(N \\sim \\mathcal P(20\\lambda)\\).\nType A sources emit an average of \\(\\lambda_0 = 0.6\\) particles/time unit\nType B sources emit an average of \\(\\lambda_1 = 0.8\\) particles/time unit\n\\(H_0\\): \\(N \\sim \\mathcal P(20\\lambda_0)\\) or \\(H_1\\): \\(N\\sim \\mathcal P(20\\lambda_1)\\)\nLikelihood Ratio Test: \\[T(X)=\\mathbf 1\\left\\{\\sum_{i=1}^{20}X_i &gt; t_{\\alpha}\\right\\} \\; .\\]\n\\(t_{0.95}\\):   quantile(Poisson(20*0.6), 0.95) gives \\(18\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) &gt; 17)\\): 1-cdf(Poisson(20*0.6), 17) gives \\(0.063\\)\n\n\\(\\mathbb P(\\mathcal P(20*0.6) &gt; 18)\\): 1-cdf(Poisson(20*0.6), 18) gives \\(0.038\\): Reject if \\(N \\geq 19\\)"
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#definitions",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#definitions",
    "title": "Hypothesis Testing",
    "section": "Definitions",
    "text": "Definitions\n\n\n\n\\(H_0 = \\mathcal P_0=\\{P_{\\theta}, \\theta \\in \\Theta_0 \\}\\) is not a singleton\nNo meaning of \\(\\mathbb P_{H_0}(X \\in A)\\)\nlevel of \\(T\\): \\[\n\\alpha = \\sup_{\\theta \\in \\Theta_0}P_{\\theta}(T(X)=1) \\; .\n\\]\npower function \\(\\beta: \\Theta_1 \\to [0,1]\\) \\[\n\\beta(\\theta) = P_{\\theta}(T(X)=1)\n\\]\n\\(T\\) is unbiased if \\(\\beta (\\theta) \\geq \\alpha\\) for all \\(\\theta \\in \\Theta_1\\).\n\n\n\nIf \\(T_1\\), \\(T_2\\) are two tests of level \\(\\alpha_1\\), \\(\\alpha_2\\)\n\\(T_2\\) is uniformly more powerfull (\\(UMP\\)) than \\(T_1\\) if\n\n\\(\\alpha_2 \\leq \\alpha_1\\)\n\\(\\beta_2(\\theta) \\geq \\beta_1(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\)\n\n\\(T^*\\) is \\(UMP_{\\alpha}\\) if it is \\(UMP\\) than any other test \\(T\\) of level \\(\\alpha\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#multiple-multiple-tests-in-mathbb-r",
    "title": "Hypothesis Testing",
    "section": "Multiple-Multiple Tests in \\(\\mathbb R\\)",
    "text": "Multiple-Multiple Tests in \\(\\mathbb R\\)\nAssumption: \\(\\Theta_0 \\cup \\Theta_1 \\subset \\mathbb R\\).\n\nOne-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta \\leq \\theta_0 ~~~~ &\\text{ or } ~~~ H_1: \\theta &gt; \\theta_0 ~~~ \\text{(right-tailed: unilatéral droit)}\\\\\nH_0: \\theta \\geq \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta &lt; \\theta_0 ~~~ \\text{(left-tailed: unilatéral gauche)}\n\\end{aligned}\n\\]\nTwo-tailed tests: \\[\n\\begin{aligned}\nH_0: \\theta = \\theta_0 ~~~ &\\text{ or } ~~~ H_1: \\theta \\neq \\theta_0 ~~~ \\text{(simple/multiple)}\\\\\nH_0: \\theta \\in [\\theta_1, \\theta_2] ~~~ &\\text{ or } ~~~ H_1: \\theta \\not \\in [\\theta_1, \\theta_2] ~~~ \\text{( multiple/multiple)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume that \\(p_{\\theta}(x) = a(\\theta)b(x)\\exp(c(\\theta)d(x))\\) and that \\(c\\) is a non-decreasing (croissante) function, and consider a one-tailed test problem.\nThere exists a UMP\\(\\alpha\\) test. It is \\(\\mathbf 1\\{\\sum d(X_i) &gt; t \\}\\) if \\(H_1: \\theta &gt; \\theta_0\\) (right-tailed test)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\nHere, \\(\\Theta_0\\) is not necessarily a singleton. \\(\\mathbb P_{H_0}(X \\in A)\\) has no meaning.\n\n\n\n\n\n\n\n\n\nPivotal Test Statistic\n\n\n\\(\\psi: \\mathcal X \\to \\mathbb R\\) is pivotal if the distribution of \\(\\psi(X)\\) under \\(H_0\\) does not depend on \\(\\theta \\in \\Theta_0\\):\n\nfor any \\(\\theta, \\theta' \\in \\Theta_0\\), and any event \\(A\\), \\[ \\mathbb P_{\\theta}(\\psi(X) \\in A) = \\mathbb P_{\\theta'}(\\psi(X) \\in A) \\; .\\]\n\n\n\n\n\n\nExample: If \\(X=(X_1, \\dots, X_n)\\) are iid \\(\\mathcal N(0, \\sigma)\\), the distrib of \\[ \\psi(X) = \\frac{\\sum_{i=1}^n \\overline X}{\\sqrt{\\sum_{i=1}^n X_i^2}}\\] does not depend on \\(\\sigma\\)."
  },
  {
    "objectID": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "href": "teaching/hypothesis_testing/slides/testing_models.html#pivotal-test-statistic-and-p-value-1",
    "title": "Hypothesis Testing",
    "section": "Pivotal Test Statistic and P-value",
    "text": "Pivotal Test Statistic and P-value\n\n\n\n\n\n\n\nP-value: definition\n\n\nWe define \\(p_{value}(x_{\\mathrm{obs}}) =\\mathbb P(\\psi(X) \\geq x_{\\mathrm{obs}})\\) for a right-tailed test.\nFor a two-tailed test, \\(p_{value}(x_{\\mathrm{obs}}) =2\\min(\\mathbb P(\\psi(X) \\geq x_{\\mathrm{obs}}),\\mathbb P(\\psi(X) \\leq x_{\\mathrm{obs}}))\\)\n\n\n\n\n\n\n\n\n\n\n\nP-value under \\(H_0\\)\n\n\nUnder \\(H_0\\), for a pivotal test statistic \\(\\psi\\), \\(p_{value}(X)\\) has a uniform distribution \\(\\mathcal U([0,1])\\).\n\n\n\n\n\n\n\nIn practice: reject if \\(p_{value}(x_{\\mathrm{obs}}) \\leq \\alpha = 0.05\\)\n\\(\\alpha\\) is the level or type-1-error of the test\n\n\n\nIllustration if \\(\\psi(X) \\sim \\mathcal N(0,1)\\):"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html",
    "href": "teaching/hypothesis_testing/annals/test_2025.html",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "",
    "text": "Duration: 2 hours, no document allowed. Special attention will be given to clarity of writing."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-1-testing-a-preference-for-renewable-or-non-renewable-energy-sources",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-1-testing-a-preference-for-renewable-or-non-renewable-energy-sources",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 1: Testing a Preference for Renewable or Non-Renewable Energy Sources",
    "text": "Exercise 1: Testing a Preference for Renewable or Non-Renewable Energy Sources\nWe aim to determine whether citizens in a region have any preference for renewable energy sources (e.g., solar, wind) or non-renewable energy sources (e.g., coal, natural gas). We assume that, a priori, there is no preference on average. We survey \\(n\\) individuals, and let \\(X\\) be the number of respondents who prefer renewable energy.\n\nQuestions:\n\nFormalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\). Indicate whether this test is one-tailed or two-tailed.\nWe survey \\(n = 100\\) individuals, and \\(X = 58\\) prefer renewable energy sources. Write the \\(p_{value}\\) in function of \\(F\\), the cdf of \\(\\text{Bin}(100, 0.5)\\) (binomial distribution with parameter \\(p = 0.5\\)).\nWrite a line of code that would compute the exact p-value in Julia, Python, or R.\nGive an approximation of the p-value using a Gaussian approximation and the graph of the cdf of \\(\\mathcal N(0,1)\\) given bellow. What do you conclude?\nRedefine the alternative hypothesis \\(H_1\\) and compute an approximated p-value if we aim to determine whether citizens have a preference for:\n\nrenewable energy sources.\nCitizens prefer non-renewable energy sources.\n\nWhat do you conclude for these two other problems?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-2-environmental-monitoring-of-river-pollution",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-2-environmental-monitoring-of-river-pollution",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 2: Environmental Monitoring of River Pollution",
    "text": "Exercise 2: Environmental Monitoring of River Pollution\nAn environmental agency is monitoring the pollution levels of a river to determine whether a nearby factory is causing an increase in harmful chemical concentration. The target concentration for a specific chemical is \\(15 \\, \\text{ppm}\\) (parts per million), which is considered safe for aquatic life. For a sample of \\(n = 20\\) water samples taken downstream from the factory, the empirical mean concentration is \\(\\bar{X}_n = 16.3 \\, \\text{ppm}\\), and the empirical variance is \\(S^2_n = 2.4 \\, \\text{ppm}^2\\).\nA priori, the river is assumed to meet the safe pollution threshold of \\(15 \\, \\text{ppm}\\).\nWe aim to test at the level of significance \\(\\alpha = 0.05\\) whether the chemical concentration downstream exceeds the safe threshold, indicating pollution from the factory.\n\nQuestions:\n\nUsing a Gaussian assumption, formalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\). Is this a one-tailed or two-tailed testing problem? Precise what the unknown parameters are.\nDefine the test statistic. What is its distribution under \\(H_0\\)?\nDetermine the rejection region. You can use a Gaussian approximation and the cdf of Exercise 1.\nWrite a line of code do compute the exact rejection threshold.\nDoes the river exhibit an increased chemical concentration that could indicate pollution from the factory?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-3-bird-migration-habitat-distribution-analysis",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-3-bird-migration-habitat-distribution-analysis",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 3: Bird Migration Habitat Distribution Analysis",
    "text": "Exercise 3: Bird Migration Habitat Distribution Analysis\nA wildlife researcher is studying the behavior of a certain species of birds that migrate to a nature reserve. The researcher has a hypothesis about how the birds distribute themselves across different types of habitats in the reserve. The expected distribution, based on historical data, is as follows:\n\nGrassland: 40%\nWetlands: 30%\nForests: 20%\nRocky Areas: 10%\n\nTo test this hypothesis, the researcher surveys 200 birds and records their habitat preferences. The observed counts are as follows:\n\n\n\nHabitat\nGrassland\nWetlands\nForests\nRocky Areas\n\n\n\n\nObserved\n90\n60\n30\n20\n\n\n\n\nQuestions:\n\nFormalize the hypothesis testing problem, and define \\(H_0\\) and \\(H_1\\).\nCompute the expected counts.\nCompute the chi-square statistic.\nDetermine the degree of freedom \\(df\\) of the chi-square statistic, and read the p-value on the following graph of the cdf.\nWhat do you conclude?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercise-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercise-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nEmployee Productivity Across Departments\nA company wants to evaluate whether a new management style has had a consistent effect on employee productivity across five departments. Each department has adopted a specific variation of the management style for three months, and the company has recorded the average number of tasks completed per employee during that period.\nData:\n\n\n\nDepartment\n1\n2\n3\n4\n5\n\n\n\n\nNumber of employees\n12\n10\n8\n9\n11\n\n\nAverage tasks completed\n72.4\n68.9\n75.6\n74.3\n69.7\n\n\nVariance of tasks\n8.5\n9.2\n10.1\n7.8\n9.6\n\n\n\nThe company seeks to understand whether productivity levels vary significantly across departments, indicating that the management styles might have different impacts.\nLet \\(d=5\\) be the number of departments and \\(N_{\\text{tot}} = 50\\) the total number of employees. For any department \\(j\\), we denote \\(N_k\\) the number of employees in department \\(k\\), and \\(P_{ik}\\) the number of tasks completed by employee \\(i\\) in department \\(k\\). We assume that the \\(P_{ik}\\)’s are independent and normally distributed with mean \\(\\mu_k\\) and variance \\(\\sigma^2\\).\nWe write \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDefine the hypotheses of the problem to test whether the management styles had a uniform impact on productivity.\nGive a brief interpretation of each one of the quantities \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\n\nProve the analysis of variance formula \\(V_T = V_W + V_B\\)\nCalculate \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), and \\(V_T\\).\n\nExpress the ANOVA test statistic in terms of \\(V_W\\) and \\(V_B\\).\n\nWhat are the distributions of \\(N_k V_k\\) and of \\(N_{\\mathrm{tot}}V_W\\) under \\(H_0\\)? Do they change under \\(H_1\\)?\nRecall the definition of ANOVA test statistic, and perform the ANOVA test at significance level \\(\\alpha =0.05\\). We give the \\(0.05\\) and \\(0.95\\)-quantiles of \\(\\mathcal D\\) which are approximately \\(0.18\\) and \\(2.58\\).\nConclude whether productivity differs significantly between departments."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#course-questions",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#course-questions",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Course Questions",
    "text": "Course Questions\n\nRecall the definition of a test statistic \\(\\psi\\) and a test (or decision rule) \\(T\\).\nWhat are the two types of errors that we can commit?\nFor a given test statistic \\(\\psi\\), recall the definition of the p-value in the context of a two-sided test.\nState the Neyman-Pearson theorem."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-1-test-dune-préférence-pour-les-sources-dénergie-renouvelables-ou-non-renouvelables",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables",
    "text": "Exercice 1 : Test d’une préférence pour les sources d’énergie renouvelables ou non renouvelables\nNous cherchons à déterminer si les citoyens d’une région ont une quelconque préférence pour les sources d’énergie renouvelables (par exemple, solaire, éolienne) ou les sources d’énergie non renouvelables (par exemple, charbon, gaz naturel). Nous supposons que, a priori, il n’y a aucune préférence en moyenne. Nous interrogeons \\(n\\) individus, et nous notons \\(X\\) le nombre de répondants qui préfèrent les énergies renouvelables."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#questions-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#questions-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions :",
    "text": "Questions :\n\nFormalisez le problème de test d’hypothèse, et définissez \\(H_0\\) et \\(H_1\\)​. Indiquez si ce test est unilatéral ou bilatéral.\nNous interrogeons \\(n=100\\) individus, et \\(X=58\\) préfèrent les sources d’énergie renouvelables. Écrivez la p-valeur en fonction de \\(F\\), la fonction de répartition d’une distribution Binomiale Bin\\((100,0.5)\\).\nÉcrivez une ligne de code qui calculerait la p-valeur exacte en Julia, Python, ou R.\nDonnez une approximation de la p-valeur en utilisant une approximation gaussienne et le graphique de la fonction de répartition de \\(\\mathcal N(0,1)\\) fourni dans le sujet. Quelle est votre conclusion ?\nRedéfinissez l’hypothèse alternative \\(H_1\\)​ et calculez une p-valeur approximative si nous cherchons à déterminer si les citoyens ont une préférence pour :\n\nles sources d’énergie renouvelables.\nles sources d’énergie non renouvelables. Quelles sont vos conclusions pour ces deux autres problèmes ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-2-surveillance-environnementale-de-la-pollution-fluviale",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 2 : Surveillance environnementale de la pollution fluviale",
    "text": "Exercice 2 : Surveillance environnementale de la pollution fluviale\nUne agence environnementale surveille les niveaux de pollution d’une rivière pour déterminer si une usine à proximité provoque une augmentation de la concentration de produits chimiques nocifs. La concentration cible pour un produit chimique spécifique est de \\(15 \\, \\text{ppm}\\) (parties par million), ce qui est considéré comme sûr pour la vie aquatique. Pour un échantillon de \\(n = 20\\) prélèvements d’eau effectués en aval de l’usine, la concentration moyenne empirique est \\(\\bar{X}_n = 16,3 \\, \\text{ppm}\\), et la variance empirique est \\(S^2_n = 2,4 \\, \\text{ppm}^2\\).\nA priori, on suppose que la rivière respecte le seuil de pollution sans danger de \\(15 \\, \\text{ppm}\\).\nNous visons à tester, avec un niveau de signification \\(\\alpha = 0,05\\), si la concentration chimique en aval dépasse le seuil de sécurité, indiquant une pollution provenant de l’usine.\n\nQuestions :\n\nEn utilisant une hypothèse Gaussienne, formalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\). S’agit-il d’un test unilatéral ou bilatéral ?\nDéfinissez la statistique de test. Quelle est sa distribution sous \\(H_0\\) ?\nDéterminez la zone de rejet. Vous pouvez utiliser une approximation Gaussienne et le graphe de l’exercice 1.\nEcrivez une ligne de code permettant de calculer le seuil de rejet exact.\nLa rivière présente-t-elle une concentration chimique accrue qui pourrait indiquer une pollution provenant de l’usine ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-3-analyse-de-la-distribution-des-habitats-lors-de-la-migration-des-oiseaux",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux",
    "text": "Exercice 3 : Analyse de la distribution des habitats lors de la migration des oiseaux\nUn chercheur en faune sauvage étudie le comportement d’une certaine espèce d’oiseaux qui migrent vers une réserve naturelle. Le chercheur a une hypothèse sur la façon dont les oiseaux se répartissent entre différents types d’habitats dans la réserve. La distribution attendue, basée sur des données historiques, est la suivante :\n\nPrairie : 40%\nZones humides : 30%\nForêts : 20%\nZones rocheuses : 10%\n\nPour tester cette hypothèse, le chercheur observe 200 oiseaux et enregistre leurs préférences d’habitat. Les comptages observés sont les suivants :\n\n\n\nHabitat\nPrairie\nZones humides\nForêts\nZones rocheuses\n\n\n\n\nObservé\n90\n60\n30\n20\n\n\n\n\nQuestions :\n\nFormalisez le problème de test d’hypothèse et définissez \\(H_0\\) et \\(H_1\\).\nCalculez les effectifs attendus.\nCalculez la statistique du chi-deux.\nDéterminez le degré de liberté \\(df\\) de la statistique du chi-deux, et lisez la p-value sur le graphique suivant de la fonction de répartition.\nQuelle est votre conclusion ?"
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#exercice-4",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#exercice-4",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Exercice 4",
    "text": "Exercice 4\n\nProductivité des employés entre départements\nUne entreprise souhaite évaluer si un nouveau style de management a eu un effet uniforme sur la productivité des employés dans cinq départements. Chaque département a adopté une variation spécifique du style de management pendant trois mois, et l’entreprise a enregistré le nombre moyen de tâches accomplies par employé durant cette période.\nDonnées :\n\n\n\nDépartement\n1\n2\n3\n4\n5\n\n\n\n\nNombre d’employés\n12\n10\n8\n9\n11\n\n\nMoyenne des tâches accomplies\n72,4\n68,9\n75,6\n74,3\n69,7\n\n\nVariance des tâches\n8,5\n9,2\n10,1\n7,8\n9,6\n\n\n\nL’entreprise cherche à comprendre si les niveaux de productivité varient significativement entre les départements, indiquant que les styles de management pourraient avoir des impacts différents.\nSoit \\(d=5\\) le nombre de départements et \\(N_{\\text{tot}} = 50\\) le nombre total d’employés. Pour tout département \\(j\\), nous notons \\(N_k\\) le nombre d’employés dans le département \\(k\\), et \\(P_{ik}\\) le nombre de tâches accomplies par l’employé \\(i\\) dans le département \\(k\\). Nous supposons que les \\(P_{ik}\\) sont indépendants et suivent une distribution normale de moyenne \\(\\mu_k\\) et de variance \\(\\sigma^2\\).\nNous écrivons \\[\n\\left.\\begin{array}{cl}\n\\overline P_k &= \\frac{1}{N_k} \\sum_{i=1}^{N_k} P_{ik}\\\\\n\\overline{P} &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_k\\overline P_{k}\\end{array}\\right.\n\\left.\\begin{array}{cl}\nV_k &= \\frac{1}{N_k}\\sum_{i=1}^{N_k} (P_{ik} - \\overline P_k)^2\n\\\\\nV_W &= \\frac{1}{N_{\\mathrm{tot}}} \\sum_{k=1}^d N_kV_k\\\\\nV_B &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^{d} N_k(\\overline P_k - \\overline P)^2\\\\\nV_{T} &= \\frac{1}{N_{\\mathrm{tot}}}\\sum_{k=1}^d\\sum_{i=1}^{N_k} (P_{ik} - \\overline P)^2\n\\end{array}\n\\right.\n\\]\n\n\nQuestions\n\nDéfinissez les hypothèses du problème pour tester si les styles de management ont eu un impact uniforme sur la productivité.\nDonnez une brève interprétation de chacune des quantités \\(\\overline{P}_k\\), \\(\\overline{P}\\), \\(V_k\\), \\(V_W\\), \\(V_B\\), \\(V_T\\).\nDémontrez la formule d’analyse de la variance : \\(V_T = V_W + V_B\\)\nCalculez \\(\\overline{P}\\), \\(V_W\\), \\(V_B\\), et \\(V_T\\).\nExprimez la statistique de test ANOVA en termes de \\(V_W\\) et \\(V_B\\).\nQuelles sont les distributions de \\(N_k V_k\\) et de \\(N_{\\mathrm{tot}}V_W\\) sous \\(H_0\\) ? Changent-elles sous \\(H_1\\) ?\nRappelez la définition de la statistique de test ANOVA, donnez sa distribution \\(\\mathcal D\\) sous \\(H_0\\) et effectuez le test ANOVA au niveau \\(\\alpha =0,05\\). On donne les quantiles \\(0.05\\) et \\(0.95\\) de \\(\\mathcal D\\): \\(0.18\\) and \\(2.58\\). Concluez si la productivité diffère significativement entre les départements."
  },
  {
    "objectID": "teaching/hypothesis_testing/annals/test_2025.html#questions-de-cours",
    "href": "teaching/hypothesis_testing/annals/test_2025.html#questions-de-cours",
    "title": "Introduction to Hypothesis Testing, Exam 2025",
    "section": "Questions de cours",
    "text": "Questions de cours\n\nRappelez la définition d’une statistique de test \\(\\psi\\) et d’un test (ou règle de décision) \\(T\\).\nQuels sont les deux types d’erreur que nous pouvons commettre ?\nPour une statistique de test \\(\\psi\\) donnée et un problème de test bilatéral, rappelez la définition de la p-valeur.\nÉnoncez le théorème de Neyman-Pearson."
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html",
    "title": "Goodness of Fit Tests",
    "section": "",
    "text": "Binomial distribution\n\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting.",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#multinomials",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#multinomials",
    "title": "Goodness of Fit Tests",
    "section": "",
    "text": "Binomial distribution\n\n\n\n\nDraw n balls, blue or red, with resampling\np_1, (1-p_1): proportion of blues/red\nX, Y: counts of blues/red\nX \\sim \\mathrm{Bin}(n,p_1)\n\\mathbb P((X,Y) = (k_1,k_2)) = \\binom{n}{k_1}p_1^k(1-p_1)^{k_2}\nif k_1 +k_2 = n\n\n\n\n\n\n\n\n\n\nMultinomial distribution\n\n\n\n\nDraw n balls, m potential colors, with resampling\n(p_1, \\dots, p_m): proportions of each color: \\sum_{i=1}^m p_i = 1\nX_1, \\dots, X_m: count of each color\n(X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n,(p_1, \\dots, p_m))\n\\mathbb P((X_1, \\dots, X_m)=(k_1, \\dots, k_m)) = \\frac{n!}{k_1!\\dots k_m!}p_1^{k_1} \\dots p_m^{k_m}\nif k_1 + \\dots + k_m = n\n\n\n\n\n\\frac{n!}{k_1!\\dots k_m!} = \\binom{n}{k_1,\\dots, k_m} is a multinomial coefficient\nIn this course: n \\gg m.\nm \\gg n corresponds to a high-dimensional setting.",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-test",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit Test",
    "text": "\\chi^2 Goodness of Fit Test\n\nWe observe (X_1, \\dots, X_m) \\sim \\mathrm{Mult}(n, q). This corresponds to n counts: X_1 + \\dots + X_m = n\nq = (q_1, \\dots, q_m) corresponds to probabilities of getting color 1, \\dots, m\nLet p = (p_1, \\dots, p_m) be a known vector s.t. p_1 + \\dots + p_m = 1.\nH_0:~ q = p ~~~\\text{or}~~~ H_1: q \\neq p \\; .\n\n\n\n\n\n\n\n\\chi^2 Goodness of Fit Test (Adéquation)\n\n\n\n\nChi-squared test statistic: \\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\; , where n_i = np_i = \\mathbb E[X_i] is the expected number of counts for color i.\nWhen np_i = n_i are large, under H_0, we approximate the distribution of \\psi(X) as \\psi(X) \\sim \\chi^2(m-1)\nReject if \\psi(X) &gt; t_{1- \\alpha} (right-tail of \\chi^2(m-1))",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-bag-of-sweets",
    "title": "Goodness of Fit Tests",
    "section": "Example: Bag of Sweets",
    "text": "Example: Bag of Sweets\n\n\n\n\n\n\n\nWe observe a bag of sweets containing n=100 sweets of m=3 different colors: red, green, and yellow.\nManufacturer: p_1= 40\\% red, p_2=35\\% green, and p_3=25\\% yellow.\nH_0: q=p (manufacturer’s claim is correct)\nH_1: q\\neq p (manufacturer’s claim is incorrect)\n\n\n\n\n\n\n\nColor\nObserved Counts\nExpected Counts\n\n\n\n\nRed\nX_1=50\nn_1=40\n\n\nGreen\nX_2=30\nn_2=35\n\n\nYellow\nX_3=20\nn_3=25\n\n\n\n\n\\psi(X) = \\sum_{i=1}^m\\frac{(X_i-n_i)^2}{n_i} \\approx 2.5 +0.71+1 \\approx 4.21\n\\mathrm{cdf}(\\chi^2(2), 4.21) \\approx 0.878 (p_{value} \\approx 0.222)\nConclusion: do not reject H_0",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#histograms",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#histograms",
    "title": "Goodness of Fit Tests",
    "section": "Histograms",
    "text": "Histograms\n\n\n\n\n\n\nHistogram\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n\n\n\\mathrm{counts}(I) = \\sum \\mathbf 1\\{X_i \\in I\\} \\in \\{1, \\dots, n\\}\\; .\n\\mathrm{freq}(I) = \\mathrm{counts}(I)/n\n\\mathrm{hist}(a,b,k) = (\\mathrm{counts}(I_1), \\dots,\\mathrm{counts}(I_k))\nwhere I_l = \\big[a + (l-1)\\tfrac{b-a}{k},a + l\\tfrac{b-a}{k}\\big)\n\n\n\n\n\n\n\n\n\nNormalization\n\n\n\nCan be normalized in counts (default), frequency, or density (area under the curve = 1)\n\n\n\n\n\n\n\n\n\nLaw of large numbers, Monte Carlo (informal)\n\n\n\n\nAssume that (X_1, \\dots, X_n) are iid of distrib P, and that a,b, k are fixed\nThe histogram \\mathrm{hist}(a,b,k) converges to the histogram of the density P",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#chi2-goodness-of-fit-to-a-given-distribution",
    "title": "Goodness of Fit Tests",
    "section": "\\chi^2 Goodness of Fit to a given distribution",
    "text": "\\chi^2 Goodness of Fit to a given distribution\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) \\in \\mathbb R^n, iid with unknown distrib P\nH_0: P = P_0, where P_0 is known\nH_1: P \\neq P_0\nIdea: under H_0, the counts in disjoint intervals I_1, I_2, \\dots, I_k follow a multinomial distribution \\mathrm{Mult}(n, (p_1, \\dots, p_{k})) where p_1 = P_0(I_1), \\dots, p_{k} = P_0(I_k)\n\n\n\n\n\n\n\n\n\n\nReduction to chi-squared test statistic\n\n\n\n\nCount the number of data (c_1, \\dots, c_k) in I_1, \\dots, I_k\nTheoretical counts nP_0(I_1) \\dots, nP_0(I_k)\nChi-squared statistic: \\sum_{j=1}^k \\frac{(c_j - nP_0(I_j))^2}{nP_0(I_j)}\nDecide using an \\alpha-quantile of a \\chi^2(k-1) distribution\n\n\n\n\n\n\n\n\n\nCorrected \\chi^2 Test\n\n\n\n\nIf P_0 is unknown\nIn a class \\mathcal P parameterized by \\ell parameters\nThen estimate the \\ell parameters\nCompute theoretical counts with \\hat P_{\\hat \\theta}\nBut decide with \\chi^2(k - 1 - \\ell)",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#example-goodness-of-fit-to-a-poisson-distribution",
    "title": "Goodness of Fit Tests",
    "section": "Example: Goodness of Fit to a Poisson distribution",
    "text": "Example: Goodness of Fit to a Poisson distribution\nH_0: X_i iid \\mathcal P(2)\nX = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 3, 0, 1, 1, 2, 3, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0]\n\n\n\n\n\n0\n1\n2 \n\\geq 3\nTotal\n\n\n\n\nCounts\n16\n8\n3\n3\n30\n\n\nTheoretical Counts\n4.06\n8.1\n8.1\n9.7\n\n\n\n\n\n\n\n\n\n\n\n\nTo get 9.7, we compute (1-cdf(Poisson(2),2))*30\nchi square stat \\gtrsim \\frac{(16-4)^2}{4} = 36\n(1-cdf(Chisq(3),36)) is very small: Reject\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nIf H_0 is X_i iid \\mathcal P(\\lambda) with unknown \\lambda\nNot \\chi^2(3) but \\chi^2(2)\n(1-cdf(Chisq(2),36)) is even smaller: (Still Reject)",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#comparison-with-qq-plots",
    "title": "Goodness of Fit Tests",
    "section": "Comparison with QQ-Plots",
    "text": "Comparison with QQ-Plots\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)}\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\n\n\n\n\n\n\n\n\n\n\nQQ-Plot\n\n\n\n\nRepresent the empirical quantiles in function of the theoretical quantiles.\nCompare the scatter plot with y=x",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "href": "teaching/hypothesis_testing/lectures/goodness_of_fit_chi2.html#kolmogorov-smirnov-test",
    "title": "Goodness of Fit Tests",
    "section": "Kolmogorov-Smirnov Test",
    "text": "Kolmogorov-Smirnov Test\n\n\n\n\n\n\n\nWe observe (X_1, \\dots, X_n) of unknown CDF F\nH_0: F = F_0 where F_0 is known\nH_1: F \\neq F_0\nWe write X_{(1)} \\leq \\dots \\leq X_{(n)} for the ordered data\nempirical \\frac{k}{n}-quantile: X_{(k)}\n\\frac{k}{n}-quantile: x such that F_0(x) = \\frac{k}{n}\nEmpirical CDF: \\hat F(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf 1\\{X_i \\leq x\\}\nIdea: Max distance between empirical and true CDF\n\n\n\n\n\n\n\n\n\n\n\nKolmogorov-Smirnov test\n\n\n\n\n\\psi(X) = \\sup_{x}|\\hat F(x) - F_0(x)|\nApprox: \\mathbb P_0(\\psi(X) &gt;c/\\sqrt{n}) \\to 2\\sum_{r=1}^{+\\infty}(-1)^{r-1}\\exp(-2c^2r^2) when n \\to +\\infty\nIn practice, use Julia, Python or R for KS Tests",
    "crumbs": [
      "Hypothesis Testing",
      "Goodness of Fit, Homogeneity"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X = (X_1, \\dots, X_n), iid with distribution \\mathcal N(\\mu, \\sigma). We assume that \\mu is unknown but that \\sigma is known.\n\n\n\n\n\n\nTest problems\n\n\n\n\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\nTest Statistic:  \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\n\\psi(X) \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\nTests\n\n\n\n\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFisher’s Quote\n\n\n\nThe value for which p=0.05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.\n\n\n\n\n\n\nMultiple VS Multiple Test Problem: \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} no longer test statistic.\nIdea: replace \\sigma by its estimator  \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\nThis gives \n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\nIs \\psi(X) pivotal under H_0 ? What is its distribution ?\n\n\n\n\n\n\n\n\n\n\nChi-Squared Distribution \\chi^2(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\sum_{i=1}^k Z_i^2\nwhere the Z_i’s are iid \\mathcal N(0,1).\n\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\n\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1) when k \\to +\\infty\n\n\n\n\n\n\n\n\n\nStudent distribution \\mathcal T(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\tfrac{Z}{\\sqrt{U/k}}\nZ, U are independent and follow resp. \\mathcal N(0,1) and a \\chi^2(k)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume X_i are iid \\mathcal N(\\mu_0, \\sigma).\n\nThe test statistic \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} pivotal (indep. of \\sigma).\nIt follows a Student distribution \\mathcal T(n-1).\n\n\n\nSketch of Proof.\nRemark that, the orthogonal projection of (X_1, \\dots, X_n) on (1, \\dots, 1) is equal to \\overline X \\cdot (1, \\dots, 1). This is precisely because the empirical mean minimizes the quantity \\tfrac{1}{n}\\sum (X_i - \\overline X)^2. Hence, the two vectors \\overline X \\cdot (1, \\dots, 1) and (X_1 - \\overline X, \\dots, X_n - \\overline X) are orthogonal. From the Cochran’s theorem, Orthogonality is equivalent to independence for Gaussian random variables, and we deduce that \\overline X is independent of \\hat \\sigma^2. \n\\tag*{$\\blacksquare$}\n\nThe student tests are the same as Gaussian tests with known variance, except that we replace the quantiles of the Gaussian the quantiles of the Student distribution.\n\nThe quantiles of the Student distribution are close to the quantile of the Standard Gaussian when the degree of freedom k is large:\nThe quantiles are slightly larger when k is small. The Student Distribution has a slightly heavier tail.\n\n\n\n\n\nMultiple VS multiple test problem X=(X_1, \\dots, X_n): \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n(Student) T-test statistic: \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe X=(X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu, \\sigma). \\mu, \\sigma are unknown. \\sigma_0 is fixed.\n\n\n\n\n\n\n\n\n\n\nRight-tailed test\n\n\n\n\nH_0: \\sigma \\leq \\sigma_0, H_1: \\sigma &gt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\nq_{1-\\alpha}: quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\n\n\n\n\n\nLeft-tailed test\n\n\n\n\nH_0: \\sigma \\geq \\sigma_0, H_1: \\sigma &lt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\nq_{\\alpha}: quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-known-variance",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-known-variance",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X = (X_1, \\dots, X_n), iid with distribution \\mathcal N(\\mu, \\sigma). We assume that \\mu is unknown but that \\sigma is known.\n\n\n\n\n\n\nTest problems\n\n\n\n\n\\begin{aligned}\nH_0: \\mu = \\mu_0 ~~~~ &\\text{ or } ~~~ H_1: \\mu &gt; \\mu_0 ~~~ \\text{(right-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu &lt; \\mu_0 ~~~ \\text{(left-tailed)}\\\\\nH_0: \\mu = \\mu_0 ~~~ &\\text{ or } ~~~ H_1: \\mu \\neq \\mu_0 ~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}\n\n\n\n\nTest Statistic:  \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} \\; .\n\\psi(X) \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\nTests\n\n\n\n\n\\begin{aligned}\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &gt; t_{1-\\alpha} ~~~ \\text{(right-tailed)}\\\\\n\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} &lt; t_{\\alpha} ~~~ \\text{( left-tailed)}\\\\\n\\left|\\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma}\\right| &gt; t_{1-\\tfrac{\\alpha}{2}}~~~ \\text{(two-tailed)}\\\\\n\\end{aligned}",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#why-0.05-and-1.96",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#why-0.05-and-1.96",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Fisher’s Quote\n\n\n\nThe value for which p=0.05, or 1 in 20, is 1.96 or nearly 2 ; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-unknown-variance",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-mean-with-unknown-variance",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Multiple VS Multiple Test Problem: \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\sigma} no longer test statistic.\nIdea: replace \\sigma by its estimator  \\hat \\sigma(X) = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\overline X)^2} \\; .\nThis gives \n\\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} \\; .\n\nIs \\psi(X) pivotal under H_0 ? What is its distribution ?",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#chi-square-and-student-distributions",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#chi-square-and-student-distributions",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Chi-Squared Distribution \\chi^2(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\sum_{i=1}^k Z_i^2\nwhere the Z_i’s are iid \\mathcal N(0,1).\n\\mathbb E[Z_i^4] - \\mathbb E[Z_i^2]=2\n\\chi^2(k) \\sim k + \\sqrt{2k}\\mathcal N(0,1) when k \\to +\\infty\n\n\n\n\n\n\n\n\n\nStudent distribution \\mathcal T(k)\n\n\n\n\nk: degree of freedom\nDistrib of \\tfrac{Z}{\\sqrt{U/k}}\nZ, U are independent and follow resp. \\mathcal N(0,1) and a \\chi^2(k)\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\nAssume X_i are iid \\mathcal N(\\mu_0, \\sigma).\n\nThe test statistic \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma} pivotal (indep. of \\sigma).\nIt follows a Student distribution \\mathcal T(n-1).\n\n\n\nSketch of Proof.\nRemark that, the orthogonal projection of (X_1, \\dots, X_n) on (1, \\dots, 1) is equal to \\overline X \\cdot (1, \\dots, 1). This is precisely because the empirical mean minimizes the quantity \\tfrac{1}{n}\\sum (X_i - \\overline X)^2. Hence, the two vectors \\overline X \\cdot (1, \\dots, 1) and (X_1 - \\overline X, \\dots, X_n - \\overline X) are orthogonal. From the Cochran’s theorem, Orthogonality is equivalent to independence for Gaussian random variables, and we deduce that \\overline X is independent of \\hat \\sigma^2. \n\\tag*{$\\blacksquare$}\n\nThe student tests are the same as Gaussian tests with known variance, except that we replace the quantiles of the Gaussian the quantiles of the Student distribution.\n\nThe quantiles of the Student distribution are close to the quantile of the Standard Gaussian when the degree of freedom k is large:\nThe quantiles are slightly larger when k is small. The Student Distribution has a slightly heavier tail.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#student-t-test",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#student-t-test",
    "title": "Gaussian Populations",
    "section": "",
    "text": "Multiple VS multiple test problem X=(X_1, \\dots, X_n): \nH_0: \\{\\mu_0,\\sigma &gt; 0\\} \\text{ or } H_1: \\{\\mu \\neq \\mu_0,\\sigma &gt; 0\\} \\;.\n\n(Student) T-test statistic: \\psi(X) = \\frac{\\sqrt{n}(\\overline X-\\mu_0)}{\\hat \\sigma(X)} \\sim \\mathcal T(n-1)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variance-unknown-mean",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variance-unknown-mean",
    "title": "Gaussian Populations",
    "section": "",
    "text": "We observe X=(X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu, \\sigma). \\mu, \\sigma are unknown. \\sigma_0 is fixed.\n\n\n\n\n\n\n\n\n\n\nRight-tailed test\n\n\n\n\nH_0: \\sigma \\leq \\sigma_0, H_1: \\sigma &gt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &gt; q_{1-\\alpha}\\}\nq_{1-\\alpha}: quantile(Chisq(n-1), 1-alpha)\npvalue: 1-cdf(Chisq(n-1), xobs)\n\n\n\n\n\n\n\n\n\n\nLeft-tailed test\n\n\n\n\nH_0: \\sigma \\geq \\sigma_0, H_1: \\sigma &lt; \\sigma_0\n\\psi(X) = \\frac{1}{\\sigma_0^2}\\sum (X_i - \\overline X)^2\nT(X) = \\mathbf{1}\\{\\psi(X) &lt; q_{\\alpha}\\}\nq_{\\alpha}: quantile(Chisq(n-1), alpha)\npvalue: cdf(Chisq(n-1), xobs)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-known-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-known-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Known Variances",
    "text": "Testing Means, Known Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1^2) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_1, \\sigma_1^2).\n\\sigma_1, \\sigma_2 are known, \\mu_1, \\mu_2 are unknown\nTest Problem: H_0: \\mu_1 = \\mu_2 ~~~\\text{or} ~~~H_1: \\mu_1 \\neq \\mu_2\nIdea: Normalize \\overline X - \\overline Y: \n\\psi(X,Y)=\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\nTwo-Tailed Test for Testing Variance: \nT(X,Y)=\\left|\\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right| \\geq t_{1-\\alpha/2} \\;  ,\n\nt_{1-\\alpha/2} is the (1-\\alpha/2)-quantile of a Gaussian distribution",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variances-unknown-means",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-variances-unknown-means",
    "title": "Gaussian Populations",
    "section": "Testing Variances, Unknown Means",
    "text": "Testing Variances, Unknown Means\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown\nVariance Testing Problem: \nH_0: \\sigma_1 = \\sigma_2 ~~~~ \\text{ or } ~~~~ H_1: \\sigma_1 \\neq \\sigma_2\n\nF-Test Statistic of the Variances (ANOVA) \n\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} = \\frac{\\tfrac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\overline X)^2}{\\tfrac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\overline Y)^2}\\; .",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#fisher-distribution",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#fisher-distribution",
    "title": "Gaussian Populations",
    "section": "Fisher Distribution",
    "text": "Fisher Distribution\n\n\n\n\n\n\nFisher Distribution \\mathcal F(k_1,k_2)\n\n\n\n\n(k_1, k_2): degrees of freedom\nDistribution of \\frac{U_1/k_1}{U_2/k_2}\nWhere U_1, U_2 are indep. and follow \\chi^2(k_1), \\chi^2(k_2). wiki\n\\mathcal F(k_1,k_2) \\approx 1 + \\sqrt{\\frac{2}{k_1} + \\frac{2}{k_2}}\\mathcal N\\left(0, 1\\right) when k_1,k_2 \\to +\\infty\nExample: \\frac{Z_1^2+Z_2^2}{2Z_3^2} \\sim \\mathcal F(2,1) if Z_i \\sim \\mathcal N(0,1)\n\n\n\n\n\n\n\n\n\nProposition\n\n\n\n\n\\psi(X,Y)=\\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} is independent of \\mu_1, \\mu_2, \\sigma_1, \\sigma_2. It is pivotal\nIt follow distribution \\mathcal F(n_1-1, n_2-1)\n\n\n\n\n\nTwo-tailed test:  \\frac{\\hat \\sigma^2_1}{\\hat \\sigma_2^2} \\not \\in [t_{\\alpha/2}, t_{1-\\alpha/2}] ~~~\\text{(quantile of Fisher)}",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-equal-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#testing-means-equal-variances",
    "title": "Gaussian Populations",
    "section": "Testing Means, Equal Variances",
    "text": "Testing Means, Equal Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown, but we know that \\sigma_1=\\sigma_2\nEquality of Mean Testing Problem: \nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\nFormally, H_0 = \\{(\\mu,\\sigma, \\mu, \\sigma), \\mu \\in \\mathbb R, \\sigma &gt; 0\\}.\n\n\n\n\n\n\n\nStudent T-Test for two populations with equal variance\n\n\n\n\n\\hat \\sigma^2 = \\frac{1}{n_1 + n_2 - 2}\\left(\\sum_{i=1}^{n_1}(X_i - \\overline X)^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline Y)^2 \\right)\nNormalize \\overline X - \\overline Y:\n\n\\psi(X,Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\hat \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\sim \\mathcal T(n_1+n_2 - 2) \\; .\n\n\\psi(X,Y) is pivotal because \\sigma_1 = \\sigma_2.",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#equality-means-unequal-variances",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#equality-means-unequal-variances",
    "title": "Gaussian Populations",
    "section": "Equality Means, Unequal Variances",
    "text": "Equality Means, Unequal Variances\n\nWe observe (X_1, \\dots, X_{n_1}) iid \\mathcal N(\\mu_1, \\sigma_1) and (Y_1, \\dots, Y_{n_2}) iid \\mathcal N(\\mu_2, \\sigma_2).\n\\sigma_1, \\sigma_2, \\mu_1, \\mu_2 are unknown\nEquality of Mean Testing Problem: \nH_0: \\mu_1 = \\mu_2 ~~~~ \\text{ or } ~~~~ H_1: \\mu_1 \\neq \\mu_2\n\nFormally, H_0 = \\{(\\mu,\\sigma_1, \\mu, \\sigma_2), \\mu \\in \\mathbb R, \\sigma_1, \\sigma_2 &gt; 0\\}.\n\n\n\n\n\n\n\nStudent Welch Test Statistic\n\n\n\n\\psi(X, Y) = \\frac{\\overline X - \\overline Y}{\\sqrt{\\frac{\\hat \\sigma_1^2}{n_1} + \\frac{\\hat \\sigma_2^2}{n_2}}}\n\n\\psi(X,Y) is not pivotal\nGaussian approximation: \\psi(X,Y) \\approx \\mathcal N(0,1) when n_1, n_2 \\to \\infty\nBetter approximation: Student Welch",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#central-limit-theorem",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#central-limit-theorem",
    "title": "Gaussian Populations",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\n\nCLT\n\n\n\n\nLet S_n = \\sum_{i=1}^n X_i with (X_1, \\dots, X_n) iid (L^2) then  \\frac{S_n - \\mathbb E[S_n]}{\\sqrt{\\mathrm{Var}(S_n)}} \\approx \\mathcal N(0,1) \\text{ when $n \\to \\infty$} \nEquality when X_i’s are \\mathcal N(\\mu, \\sigma)\nRule of thumb: n \\geq 30\n\n\n\n\n\n\n\n\n\nExample: Binomials\n\n\n\n\nIf p \\in (0,1)\n\\frac{\\mathrm{Bin}(n,p) - np}{\\sqrt{np(1-p)}} \\approx \\mathcal N(0,1) when n \\to \\infty\nn should be \\gg \\frac{1}{p}\n\n\n\nGood Approx for (n=100, p=0.2)\n\nBad Approx for (n=100, p=0.01)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#proportion-test",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#proportion-test",
    "title": "Gaussian Populations",
    "section": "Proportion Test",
    "text": "Proportion Test\n\nWe observe X \\sim Bin(n_1, p_1) and Y \\sim Bin(n_2, p_2).\nn_1, n_2 are known but p_1, p_2 are unknown in (0,1)\nH_0: p_1 = p_2 or H_1: p_1 \\neq p_2\n\n\n\n\n\n\n\nTest Statistic\n\n\n\n \\psi(X,Y) = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\; .\n\n\\hat p_1 = X/n_1, \\hat p_2 = X/n_2\n\\hat p = \\frac{X+Y}{n_1+n_2}\nIf np_1, np_2 \\gg 1: \\psi(X) \\sim \\mathcal N(0,1)\nWe reject if |\\psi(X,Y)| \\geq t_{1-\\alpha/2} (gaussian quantile)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/hypothesis_testing/lectures/gaussian_populations.html#example-reference",
    "href": "teaching/hypothesis_testing/lectures/gaussian_populations.html#example-reference",
    "title": "Gaussian Populations",
    "section": "Example (reference)",
    "text": "Example (reference)\n\nQuestion: “should we raise taxes on cigarettes to pay for a healthcare reform ?”\np_1, p_2: proportion of non-smokers or smokers willing to raise taxes\nH_0: p_1=p_2 or H_1: p_1 &gt; p_2\n\n\n\n\n\n\nNon-Smokers\nSmokers\nTotal\n\n\n\n\nYES\n351\n41\n392\n\n\nNO\n254\n195\n449\n\n\nTotal\n605\n154\n800\n\n\n\n\n\n\\hat p_1 \\approx 0.58, \\hat p_2 \\approx 0.21.\n\\psi(X,Y)= \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p ( 1-\\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\approx 8.99\n\\mathbb P(\\psi(X,Y) &gt; 8.99) = 1-cdf(Normal(0,1), 8.99)",
    "crumbs": [
      "Hypothesis Testing",
      "Gaussian Populations"
    ]
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#practical-question",
    "href": "teaching/linear_model/slides/selection.html#practical-question",
    "title": "Selection",
    "section": "Practical Question",
    "text": "Practical Question\n\nIn practice, we often hesitate between several models:\n\nWhich variables to include in the model?\nHow to choose between one model and another?\nIdeally: How to select the “best” model among all possible sub-models of a large linear regression model?"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#selection-criteria",
    "href": "teaching/linear_model/slides/selection.html#selection-criteria",
    "title": "Selection",
    "section": "Selection Criteria",
    "text": "Selection Criteria\n\nSeveral criteria exist. The main ones:\n\n\\(R_a^2\\): Adjusted \\(R^2\\) (already seen)\nFisher test for nested models (already seen)\n\nMallows’ \\(C_p\\)\nAIC criterion\nBIC criterion"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#setup",
    "href": "teaching/linear_model/slides/selection.html#setup",
    "title": "Selection",
    "section": "Setup",
    "text": "Setup\n\nSuppose we have \\(p_{\\max}\\) explanatory variables, forming the “maximal” design matrix \\(X_{\\max}\\).\n\n\nTrue model (unknown):\n\n\\[Y = X^*\\beta^* + \\varepsilon\\]\n\nwhere \\(X^*\\) is a sub-matrix of \\(X_{\\max}\\) formed by \\(p^* \\leq p_{\\max}\\) columns.\n\n\nWe don’t know \\(p^*\\) nor which variables are involved.\nGoal: Select the correct matrix \\(X^*\\) and estimate \\(\\beta^*\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#practice",
    "href": "teaching/linear_model/slides/selection.html#practice",
    "title": "Selection",
    "section": "Practice",
    "text": "Practice\n\nWe regress \\(Y\\) on \\(p \\leq p_{\\max}\\) variables, assuming: \\[Y = X\\beta + \\varepsilon\\] where \\(X\\): sub-matrix of \\(X_{\\max}\\) containing the \\(p\\) chosen columns (yielding \\(\\hat{\\beta}\\)).\n\n\nThis model is potentially wrong (bad choice of variables).\n\n\nObjective: Calculate a quality score for this submodel."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#adjusted-r2---reminder",
    "href": "teaching/linear_model/slides/selection.html#adjusted-r2---reminder",
    "title": "Selection",
    "section": "Adjusted \\(R^2\\) - Reminder",
    "text": "Adjusted \\(R^2\\) - Reminder\n\nFor a model with constant:\n\n\\[R_a^2 = 1 - \\frac{n-1}{n-p} \\cdot \\frac{SSR}{SST}\\]\n\n\n\\(SST = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\) (independent of chosen model)\n\\(SSR = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) (specific to considered model)\n\n\n\nSelection rule: Between two models, prefer highest \\(R_a^2\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#fisher-test-for-nested-models",
    "href": "teaching/linear_model/slides/selection.html#fisher-test-for-nested-models",
    "title": "Selection",
    "section": "Fisher Test for Nested Models",
    "text": "Fisher Test for Nested Models\n\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR}\\]\n\n\n\\(SSR\\): residual sum of squares of the larger model\n\\(SSR_c\\): SSR of the sub-model (fewer variables)\n\\(p\\): number of variables in the larger model\n\\(q\\): number of constraints (\\(p-q\\) variables in sub-model)\n\n\nIf \\(F &lt; f_{q,n-p}(1-\\alpha)\\): prefer sub-model (\\(H_0\\) at level \\(\\alpha\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#mallows-c_p",
    "href": "teaching/linear_model/slides/selection.html#mallows-c_p",
    "title": "Selection",
    "section": "Mallows’ \\(C_p\\)",
    "text": "Mallows’ \\(C_p\\)\n\nTrue model (unknown): \\(Y = X^*\\beta^* + \\varepsilon\\)\n\nTested model (possibly wrong): \\(Y = X\\beta + \\varepsilon\\) with OLS estimate \\(\\hat{\\beta}\\)\n\n\nMallows’ \\(C_p\\) aims to estimate the prediction risk: \\[\\E(\\|\\tilde{Y} - X\\hat{\\beta}\\|^2)\\]\nwhere \\(\\tilde{Y}\\) follows the same distribution as \\(Y\\) but is independent."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#formula-for-mallows-c_p",
    "href": "teaching/linear_model/slides/selection.html#formula-for-mallows-c_p",
    "title": "Selection",
    "section": "Formula for Mallows’ \\(C_p\\)",
    "text": "Formula for Mallows’ \\(C_p\\)\n\n\n\\[C_p = \\frac{SSR}{\\hat{\\sigma}^2} - n + 2p\\]\n\n\n\\(p\\): number of variables in the considered model\n\\(SSR\\): residual sum of squares of the considered model\n\n\\(\\hat{\\sigma}^2\\): estimation of \\(\\sigma^2\\) in the largest model\nSame for all tested models\n\n\n\nSelection rule: Among all tested models, choose the one with lowest \\(C_p\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#aic-criterion",
    "href": "teaching/linear_model/slides/selection.html#aic-criterion",
    "title": "Selection",
    "section": "AIC Criterion",
    "text": "AIC Criterion\n\nAIC (Akaike Information Criterion) is motivated like \\(C_p\\).\nIt also focuses on prediction error \\(\\tilde{Y} - X\\hat{\\beta}\\), but Kullback distance instead of Quadratic distance.\n\n\n\n\\[AIC = n \\ln\\left(\\frac{SSR}{n}\\right) + 2(p+1)\\]\n\n\n\nSelection rule: choose model with lowest AIC.\nIn practice, AIC and \\(C_p\\) are very close (choose same model)"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#bic-criterion",
    "href": "teaching/linear_model/slides/selection.html#bic-criterion",
    "title": "Selection",
    "section": "BIC Criterion",
    "text": "BIC Criterion\n\nBIC (Bayesian Information Criterion) seeks the “most probable” model in a Bayesian formalism.\n\n\\[BIC = n \\ln\\left(\\frac{SSR}{n}\\right) + (p+1) \\ln n\\]\n\n\n\nSelection rule: choose the one with lowest BIC.\n\nKey difference: The “2” in front of \\((p+1)\\) is replaced by \\(\\ln n\\)\nThis difference frequently leads to a different model choice between AIC and BIC"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#relationship-between-criteria",
    "href": "teaching/linear_model/slides/selection.html#relationship-between-criteria",
    "title": "Selection",
    "section": "Relationship Between Criteria",
    "text": "Relationship Between Criteria\n\n“Large” model: low \\(SSR\\), but high number of variables \\(p\\)\n(if too large: overfitting)\n“Small” model: high \\(SSR\\), but low number of variables \\(p\\)   (if two small: underfitting)\n\n\nAll previous criteria try to find a compromise between:\n\nGood fit to data (low \\(SSR\\))\nSmall model size (low \\(p\\))\n\n\n\nThis is a permanent trade-off in statistics (not just in regression)."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#general-form",
    "href": "teaching/linear_model/slides/selection.html#general-form",
    "title": "Selection",
    "section": "General Form",
    "text": "General Form\n\n\\(C_p\\), AIC, and BIC consist of minimizing an expression of the form:\n\n\\[f(SSR) + c(n) \\cdot p\\]\n\n\n\nBIC: \\(c(n) = \\ln n\\) \\(\\quad\\quad\\) AIC: \\(c(n) = 2\\)\n\n\\(f\\) is an increasing function of \\(SSR\\)\n\\(c(n) \\cdot p\\) is a term penalizing models with many variables\n\n\n\n(Other criteria exist built on the same principle.)"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#relationship-between-criteria-1",
    "href": "teaching/linear_model/slides/selection.html#relationship-between-criteria-1",
    "title": "Selection",
    "section": "Relationship Between Criteria",
    "text": "Relationship Between Criteria\n\n\\[f(SSR) + c(n) \\cdot p\\]\n\n\nWhen \\(\\ln n &gt; 2\\), BIC penalizes large models more than AIC.\n\n\nOrdering criteria by their propensity to select the most sparse model:\n\\[BIC \\leq F\\text{ test} \\leq C_p \\approx AIC \\leq R_a^2\\]\n\nBIC will favor a smaller model than \\(C_p\\) or AIC\n\\(R_a^2\\) will tend to favor an even larger model"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#theoretical-aspects",
    "href": "teaching/linear_model/slides/selection.html#theoretical-aspects",
    "title": "Selection",
    "section": "Theoretical Aspects",
    "text": "Theoretical Aspects\n\n\n\n\n\n\n\n\nProbability as \\(n \\to \\infty\\)                                                             \nBIC\n\\(C_p\\), AIC, \\(R_a^2\\)\n\n\n\n\n\\(\\mathbb{P}\\)(selects model smaller than true)\n\\(\\to 0\\)\n\\(\\to 0\\)\n\n\n\\(\\mathbb{P}\\)(selects model larger than true)\n\\(\\to 0\\)\n\\(\\not\\to 0\\)\n\n\n\\(\\mathbb{P}\\)(selects correct model)\n\\(\\to 1\\)\n\\(\\not\\to 1\\)\n\n\n\n\nBIC is asymptotically consistent, while other criteria tend to overfit."
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#exhaustive-search",
    "href": "teaching/linear_model/slides/selection.html#exhaustive-search",
    "title": "Selection",
    "section": "Exhaustive Search",
    "text": "Exhaustive Search\n\nGiven \\(p_{\\max}\\) available explanatory variables:\n\nTempting approach: Test all possible sub-models\nSelection: Keep the one with lowest BIC (or other criterion)\nComputational cost: \\(2^{p_{\\max}}\\) models to test (that’s a lot!)\n\n\n\nIf \\(p_{\\max}\\) is not too large, this remains feasible.\nR function: regsubsets from leaps library"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#exhaustive-search-1",
    "href": "teaching/linear_model/slides/selection.html#exhaustive-search-1",
    "title": "Selection",
    "section": "Exhaustive Search",
    "text": "Exhaustive Search\n\n\n\n\nImportant Warning\n\n\nAutomatic selection does not guarantee that the selected model is good.\nIt’s simply the best model according to the chosen criterion.\nThe selected model may be bad in terms of:\n\nExplanatory power\nMulticollinearity problems\n\nHeteroscedasticity issues\nAuto-correlation problems"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#stepwise-procedures",
    "href": "teaching/linear_model/slides/selection.html#stepwise-procedures",
    "title": "Selection",
    "section": "Stepwise Procedures",
    "text": "Stepwise Procedures\n\nIf \\(p_{\\max}\\) is too large for exhaustive search:\n\n\nStepwise Backward (according to chosen criterion, e.g., BIC):\n\nStart with largest model (\\(p_{\\max}\\) variables)\nRemove least significant variable\nRepeat: remove remaining least significant variable\nStop when no removal improves the model"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#stepwise-procedures-1",
    "href": "teaching/linear_model/slides/selection.html#stepwise-procedures-1",
    "title": "Selection",
    "section": "Stepwise Procedures",
    "text": "Stepwise Procedures\nIf \\(p_{\\max}\\) is too large for exhaustive search:\n\nStepwise Forward:\n\nStart with smallest model (constant only)\nAdd most significant variable at each step\n\n\n\nStepwise Backward (or Forward) Hybrid:\n\nLike backward (or forward), but also try adding (or removing) a variable at each step"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#limitations-and-characteristics",
    "href": "teaching/linear_model/slides/selection.html#limitations-and-characteristics",
    "title": "Selection",
    "section": "Limitations and Characteristics",
    "text": "Limitations and Characteristics\n\nStepwise procedures do not explore all possible sub-models:\n\nMay miss the best model\n\n\n\nSpeed comparison:\n\nForward: fastest (small models are quicker to estimate)\nHybrid procedures: slower, but explore more possible models"
  },
  {
    "objectID": "teaching/linear_model/slides/selection.html#r-implementation",
    "href": "teaching/linear_model/slides/selection.html#r-implementation",
    "title": "Selection",
    "section": "R Implementation",
    "text": "R Implementation\n\nFunction: step with option direction:\n\n\"backward\" or \"forward\" or \"both\"\nDefault criterion: AIC (k = 2)\nFor BIC: use k = ln(n)\n\n\n\nThe option k corresponds to the penalty \\(c(n)\\) introduced earlier."
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#general-setting",
    "href": "teaching/linear_model/slides/linear_model.html#general-setting",
    "title": "Definition of the Linear Model",
    "section": "General Setting",
    "text": "General Setting\n\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\n\nIn other words, we observe\n\\(Y= (Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\n\n\\(X^{(1)} = (X^{(1)}_1, \\dots, X^{(1)}_n) \\in \\mathbb R^n\\)\n\\(X^{(2)} = (X^{(2)}_1, \\dots, X^{(2)}_n) \\in \\mathbb R^n\\)\n…\n\\(X^{(p)} = (X^{(p)}_1, \\dots, X^{(p)}_n)\\in \\mathbb R^n\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#non-parametric-model",
    "href": "teaching/linear_model/slides/linear_model.html#non-parametric-model",
    "title": "Definition of the Linear Model",
    "section": "Non Parametric Model",
    "text": "Non Parametric Model\n\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\n\nWe assume that\n\n\\[Y_i = F(X^{(1)}_i, X^{(2)}_i, \\dots, X^{(p)}_i, \\varepsilon_i)\\]\n\n\n\nwhere \\(\\varepsilon = (\\varepsilon_1, \\dots, \\varepsilon_n) \\in \\mathbb R^n\\) are iid random noise\n\n\n\n\\(\\varepsilon\\) is not observed\n\n\n\\(F\\) is unknown\n–&gt; Too ambitious, risk of overfitting"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-1",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-1",
    "title": "Definition of the Linear Model",
    "section": "Linear Model",
    "text": "Linear Model\nWe observe \\(n\\) individuals, and variables \\(Y \\in \\mathbb R^n\\) and \\((X^{(1)}, \\dots, X^{(p)}) \\in \\mathbb R^{n \\times p}\\).\n\nWe assume that\n\n\\[Y = \\beta_1 X^{(1)}+ \\beta_2 X^{(2)}+ \\dots+ \\beta_p X^{(p)}+ \\varepsilon\\]\n\n\n\nThat is, we know that \\(F\\) is of the form \\(F(x_1, \\dots, x_p, \\varepsilon) = \\beta_1 x_1+ \\beta_2 x_2+ \\dots+ \\beta_p x_p+ \\varepsilon\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-2",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-2",
    "title": "Definition of the Linear Model",
    "section": "Linear Model",
    "text": "Linear Model\n\\(Y\\) and the \\(X^{(k)}\\)’s are vectors in \\(\\mathbb R^n\\).\n\nFor all \\(i\\),\n\n\\[Y_i = \\beta_1 X^{(1)}_i+ \\beta_2 X^{(2)}_i+ \\dots+ \\beta_p X^{(p)}_i+ \\varepsilon_i\\]\n\nWe assume that\n\n\\(X^{(k)}\\) are known and deterministic (otherwise we condition on \\(X^{(k)}\\)’s)\n\\(\\mathbb E[\\varepsilon_i] = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-with-intercept",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-with-intercept",
    "title": "Definition of the Linear Model",
    "section": "Linear Model with Intercept",
    "text": "Linear Model with Intercept\n\\(Y\\) and the \\(X^{(k)}\\)’s are vectors in \\(\\mathbb R^n\\).\n\nIf we set \\(X^{(1)}= (1, \\dots, 1)\\), then the model rewrites\n\n\\[Y_i = \\beta_1+ \\beta_2 X^{(2)}_i+ \\dots+ \\beta_p X^{(p)}_i+ \\varepsilon_i\\]\n\n\n\n\nThe model is linear in \\(X^{(2)}, \\dots, X^{(p)}\\)\n\\(\\beta_1\\) is then called the intercept"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation",
    "href": "teaching/linear_model/slides/linear_model.html#notation",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\nWe write \\(Y = (Y_1, \\dots, Y_n)\\) and \\(X^{(k)} = (X^{(k)}_1, \\dots, X^{(k)}_n)\\) as columns:\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\) \\(\\newcommand{\\E}{\\mathbb E}\\) \\(\\newcommand{\\P}{\\mathbb P}\\) \\(\\newcommand{\\Var}{\\mathbb V}\\)\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X^{(k)}=\\begin{pmatrix}\nX^{(k)}_1 \\\\\n\\vdots \\\\\nX^{(k)}_n\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation-1",
    "href": "teaching/linear_model/slides/linear_model.html#notation-1",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\n\nTo get a matrix form, we write \\(X_{ik} = X^{(k)}_i\\). Then:\n\n\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X^{(k)}=\\begin{pmatrix}\nX_{1k} \\\\\n\\vdots \\\\\nX_{n,k}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#notation-2",
    "href": "teaching/linear_model/slides/linear_model.html#notation-2",
    "title": "Definition of the Linear Model",
    "section": "Notation",
    "text": "Notation\n\nTo get a matrix form, we write \\(X\\) for the matrix \\((X_{ik}) \\in \\mathbb R^{n \\times p}\\)\n\n\nThat is, \\(X = (X^{(1)}, \\dots, X^{(p)})\\)\n\n\nAnd:\n\n\\[Y = \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X=\\begin{pmatrix}\n&X_{1,1} &\\dots &X_{1,p} \\\\\n&\\vdots &~ &\\vdots \\\\\n&X_{n,1} &\\dots &X_{n,p}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#model-matrix-form",
    "href": "teaching/linear_model/slides/linear_model.html#model-matrix-form",
    "title": "Definition of the Linear Model",
    "section": "Model, Matrix Form",
    "text": "Model, Matrix Form\n\nLet \\(\\beta = (\\beta_1, \\dots, \\beta_p) \\in \\mathbb R^p\\) be unknown parameters, and \\(\\varepsilon = (\\varepsilon_1, \\dots, \\varepsilon_n)\\) be iid noise.\n\n\nIn column notation:\n\n\\[\\beta = \\begin{pmatrix}\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{pmatrix} \\and \\varepsilon=\\begin{pmatrix}\n\\varepsilon_{1} \\\\\n\\vdots \\\\\n\\varepsilon_{n}\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#linear-model-matrix-form",
    "href": "teaching/linear_model/slides/linear_model.html#linear-model-matrix-form",
    "title": "Definition of the Linear Model",
    "section": "Linear Model, Matrix Form",
    "text": "Linear Model, Matrix Form\n\nWe observe \\(Y = (Y_1, \\dots, Y_n) \\in \\mathbb R^n\\) and \\(X \\in \\mathbb R^{n \\times p}\\)\n\n\nWe assume that\n\n\\[Y = X \\beta + \\varepsilon\\]\n\nwhere\n\n\\(X\\) is known,\n\\(\\beta \\in \\mathbb R^p\\) is unknown\n\\(\\varepsilon \\in \\mathbb R^n\\) is a vector of iid random noise with \\(\\mathbb E[\\varepsilon_i] = 0\\) and \\(\\mathbb V(\\varepsilon_i) = \\sigma^2\\)(unknown)"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#remarks",
    "href": "teaching/linear_model/slides/linear_model.html#remarks",
    "title": "Definition of the Linear Model",
    "section": "Remarks",
    "text": "Remarks\n\n\\((\\varepsilon_1, \\dots, \\varepsilon_n)\\) independent implies no correlation between individuals\n\\(\\mathbb V(\\varepsilon_i)= \\sigma^2\\) does not depend on \\(i\\): this is called homoscedasticity assumption\nWe can write \\(\\mathbb V(\\varepsilon) = \\sigma^2 I_n\\) (covariance matrix of \\(\\varepsilon\\))"
  },
  {
    "objectID": "teaching/linear_model/slides/linear_model.html#identifiability-condition",
    "href": "teaching/linear_model/slides/linear_model.html#identifiability-condition",
    "title": "Definition of the Linear Model",
    "section": "Identifiability Condition",
    "text": "Identifiability Condition\n\nRecall that \\(X \\in \\mathbb R^{n \\times p}\\)\n\n\nWe assume that \\(rk(X)=p\\).\n\n\nThis implies \\(p \\leq n\\)\n\n\nIf this condition is not satisfied:\n\n\nIt means that there is a linear relation between the \\(X^{(k)}\\)!\n\n\nIt means that \\(X\\alpha=\\alpha_1X^{(1)} + \\dots + \\alpha_p X^{(p)}=0\\) for some \\(\\alpha \\in \\mathbb R^p\\setminus \\{0\\}\\)\n\n\nWe can take infinitely many possible \\(\\beta\\), since for \\(t \\in \\mathbb R\\),\n\\[\nX(\\beta + t\\alpha) = X\\beta\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#course-structure",
    "href": "teaching/linear_model/slides/introduction.html#course-structure",
    "title": "Introduction",
    "section": "Course Structure",
    "text": "Course Structure\n\n8 lecture sessions\nCourse materials and slides (both evolving) available on Moodle\n8 TD/TP sessions (tutorial/practical work)\nContinuous assessment: November 6 (date to be confirmed)\nFinal exam: December 18 (date to be confirmed)\nAttention: Some practical sessions may take place in the tutorial room with your personal computer (not the first session)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#objectives-of-a-regression-model",
    "href": "teaching/linear_model/slides/introduction.html#objectives-of-a-regression-model",
    "title": "Introduction",
    "section": "Objectives of a Regression Model",
    "text": "Objectives of a Regression Model\n\nExplain a quantity \\(Y\\) based on \\(p\\) quantities \\(X^{(1)}, ..., X^{(p)}\\) (explanatory variables, or regressors).\n\n\nFor this purpose, we have \\(n\\) observations of each quantity from \\(n\\) individuals."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#examples",
    "href": "teaching/linear_model/slides/introduction.html#examples",
    "title": "Introduction",
    "section": "Examples:",
    "text": "Examples:\n\n\\(Y\\): daily electricity consumption in France\n\n\\(X= X^{(1)}\\): average daily temperature\n\n\n\nThe data consists of a history of \\((Y_1, \\dots, Y_n)\\) and \\((X_1, \\dots, X_n)\\) over \\(n\\) days\n\n\nQuestion: Do we have \\(Y \\approx f(X)\\) for a certain function f?\nSimplifying: Do we have \\(Y ≈ aX + b\\) for certain values \\(a\\) and \\(b\\)?\nIf yes, what is \\(a\\)? What is \\(b\\)? Is the relationship “reliable”?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#examples-1",
    "href": "teaching/linear_model/slides/introduction.html#examples-1",
    "title": "Introduction",
    "section": "Examples",
    "text": "Examples\n\n\\(Y \\in \\{0,1\\}\\): customer quality (\\(1\\): good; \\(0\\): not good)\n\n\\(X^{(1)}\\): customer income\n\n\\(X^{(2)}\\): socio-professional category (6-7 possibilities)\n\n\\(X^{(3)}\\): age\n\n\n\nData: n customers.\nIn this case, we model \\(p = P(Y = 1)\\).\nDo we have \\(p \\approx f(X^{(1)}, X^{(2)}, X^{(3)})\\) for a function f with values in \\([0, 1]\\)?"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#predictivedescriptive-model",
    "href": "teaching/linear_model/slides/introduction.html#predictivedescriptive-model",
    "title": "Introduction",
    "section": "Predictive/Descriptive Model",
    "text": "Predictive/Descriptive Model\n\nThe “approximate” relationship we’re trying to establish between \\(Y\\) and \\(X^{(1)}\\), …, \\(X^{(p)}\\) is a model.\n\n\nWhy seek to establish such a model? Two main reasons:\n\n\nDescriptive objective: quantify the marginal effect of each variable. For example, if \\(X^{(1)}\\) increases by 10%, how does \\(Y\\) change?\n\n\nPredictive objective: given new values for \\(X^{(1)}\\), …, \\(X^{(p)}\\), we can deduce the (approximate) associated \\(Y\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#course-outline",
    "href": "teaching/linear_model/slides/introduction.html#course-outline",
    "title": "Introduction",
    "section": "Course Outline",
    "text": "Course Outline\n\nIntroduction → Bivariate analysis (review): relationship between 2 variables → General aspects of modeling\nLinear Regression → Quantitative \\(Y\\) as a function of quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)\nAnalysis of Variance and Covariance → Quantitative \\(Y\\) as a function of qualitative and/or quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)\nGeneralized Linear Regression → Qualitative or quantitative \\(Y\\) as a function of qualitative and/or quantitative \\(X^{(1)}\\), …, \\(X^{(p)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#two-types-of-variables",
    "href": "teaching/linear_model/slides/introduction.html#two-types-of-variables",
    "title": "Introduction",
    "section": "Two Types of Variables",
    "text": "Two Types of Variables\nWe are interested in the relationship between \\(2\\) variables \\(X\\) and \\(Y\\). We distinguish two main categories, each divided into two types."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#quantitative-variables",
    "href": "teaching/linear_model/slides/introduction.html#quantitative-variables",
    "title": "Introduction",
    "section": "Quantitative Variables",
    "text": "Quantitative Variables\n\nA variable whose observation is a measured quantity. Examples: age, salary, number of infractions, etc.\nWe distinguish between:\n\nDiscrete quantitative variables whose possible values are finite or countable (Examples: number of children, number of infractions, etc.)\nContinuous quantitative variables which can take any value within an interval (Examples: height, salary, etc.)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#qualitative-variables-or-factors",
    "href": "teaching/linear_model/slides/introduction.html#qualitative-variables-or-factors",
    "title": "Introduction",
    "section": "Qualitative Variables (or Factors)",
    "text": "Qualitative Variables (or Factors)\n\nA variable whose observation results in a category or code. The possible observations are called the modalities of the qualitative variable. Examples: gender, socio-professional category, nationality, high school honors, etc.\nWe distinguish between:\n\nordinal qualitative variable: a natural order appears in the modalities (Examples: high school honors, etc.).\nnominal qualitative variable otherwise (Examples: gender, socio-professional category, etc.)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-of-the-pottery-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-of-the-pottery-dataset",
    "title": "Introduction",
    "section": "Example of the “Pottery” Dataset",
    "text": "Example of the “Pottery” Dataset\n\nData: chemical composition of pottery found at different archaeological sites in the United Kingdom\n\n\n\n\n\nSite\nAl\nFe\nMg\nCa\nNa\n\n\n\n\n1\nLlanedyrn\n14.4\n7.00\n4.30\n0.15\n0.51\n\n\n2\nLlanedyrn\n13.8\n7.08\n3.43\n0.12\n0.17\n\n\n3\nLlanedyrn\n14.6\n7.09\n3.88\n0.13\n0.20\n\n\n4\nLlanedyrn\n10.9\n6.26\n3.47\n0.17\n0.22\n\n\n5\nCaldicot\n11.8\n5.44\n3.94\n0.30\n0.04\n\n\n6\nCaldicot\n11.6\n5.39\n3.77\n0.29\n0.06\n\n\n7\nIsleThorns\n18.3\n1.28\n0.67\n0.03\n0.03\n\n\n8\nIsleThorns\n15.8\n2.39\n0.63\n0.01\n0.04\n\n\n9\nIsleThorns\n18.0\n1.88\n0.68\n0.01\n0.04\n\n\n10\nIsleThorns\n20.8\n1.51\n0.72\n0.07\n0.10\n\n\n11\nAshleyRails\n17.7\n1.12\n0.56\n0.06\n0.06\n\n\n12\nAshleyRails\n18.3\n1.14\n0.67\n0.06\n0.05\n\n\n13\nAshleyRails\n16.7\n0.92\n0.53\n0.01\n0.05\n\n\n\n\n\nIndividuals: pottery numbered from 1 to 13\n\nVariables: the archaeological site (factor with 4 modalities) and different chemical compounds (quantitative)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-of-the-no2traffic",
    "href": "teaching/linear_model/slides/introduction.html#example-of-the-no2traffic",
    "title": "Introduction",
    "section": "Example of the “NO2traffic”",
    "text": "Example of the “NO2traffic”\n\nData: NO2 concentration inside cars in Paris, type of road, (P, T, A, V or U) and traffic fluidity (A to D).\n\n\n\n\n\nNO2\nType\nFluidity\n\n\n\n\n1\n378.94\nP\nA\n\n\n2\n806.67\nT\nD\n\n\n3\n634.58\nA\nD\n\n\n4\n673.35\nT\nC\n\n\n5\n589.75\nP\nA\n\n\n…\n…\n…\n…\n\n\n283\n184.16\nP\nB\n\n\n284\n121.88\nV\nD\n\n\n285\n152.39\nU\nA\n\n\n286\n129.12\nU\nC\n\n\n\n\n\nIndividuals: vehicles numbered from 1 to 286\n\nVariables: NO2 (quantitative), type (factor with 5 modalities) and fluidity (ordinal factor with 4 modalities)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#pairwise-scatter-plots",
    "href": "teaching/linear_model/slides/introduction.html#pairwise-scatter-plots",
    "title": "Introduction",
    "section": "Pairwise Scatter Plots",
    "text": "Pairwise Scatter Plots\n\nWe observe\n\\(X=(X_1, \\ldots, X_n) \\in \\mathbb R^n\\) and \\(Y=(Y_1, \\ldots, Y_n) \\in \\mathbb R^n\\), (quantitative variables)\n\n\nRelationship between \\(X\\) and \\(Y\\): scatter plot of points \\((X_i, Y_i)\\)."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-pottery-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-pottery-dataset",
    "title": "Introduction",
    "section": "Example: Pottery Dataset",
    "text": "Example: Pottery Dataset"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-plot",
    "href": "teaching/linear_model/slides/introduction.html#correlation-plot",
    "title": "Introduction",
    "section": "Correlation Plot",
    "text": "Correlation Plot\n\nplot_cor=@df pottery_num corrplot(cols(1:4),grid=false, compact=true) #Julia"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-empirical-correlation",
    "href": "teaching/linear_model/slides/introduction.html#linear-empirical-correlation",
    "title": "Introduction",
    "section": "Linear Empirical Correlation",
    "text": "Linear Empirical Correlation\n\nThe linear relationship is quantified by Pearson’s linear correlation: \\(\\DeclareMathOperator{\\cov}{cov}\\) \\(\\DeclareMathOperator{\\var}{var}\\)\n\n\\[\\hat \\rho = \\frac{\\hat\\cov(X,Y)}{\\sqrt{\\hat \\var(X)\\hat \\var(Y)}}\\]\n\n\n\nwhere \\(\\hat \\var\\) and \\(\\hat \\cov\\) denote the empirical variance and covariance:\n\n\\(\\hat \\cov(X,Y)= \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\overline X)(Y_i - \\overline Y)\\)\n\\(\\hat \\var(X) = \\hat \\cov(X,X)\\), \\(\\hat \\var(Y)=\\hat\\cov(Y,Y)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#properties-of-empirical-correlation",
    "href": "teaching/linear_model/slides/introduction.html#properties-of-empirical-correlation",
    "title": "Introduction",
    "section": "Properties of Empirical Correlation",
    "text": "Properties of Empirical Correlation\n\nFrom the Cauchy-Schwarz inequality, we deduce that:\n\n\nThe correlation \\(\\hat \\rho\\) is always between \\(-1\\) and \\(1\\):\n\nIf \\(\\hat \\rho = 1\\): for all \\(i\\), \\(Y_i = aX_i + b\\) for some \\(a &gt; 0\\)\nIf \\(\\hat \\rho = -1\\): for all \\(i\\), \\(Y_i = aX_i + b\\) for some \\(a &lt; 0\\)\nIf \\(\\hat \\rho = 0\\): no linear relationship. notebook"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-test",
    "href": "teaching/linear_model/slides/introduction.html#correlation-test",
    "title": "Introduction",
    "section": "Correlation test",
    "text": "Correlation test\n\n\\(\\hat \\rho(X, Y)\\) is an estimator of the unknown theoretical correlation \\(\\rho\\) between \\(X\\) and \\(Y\\) defined by \\[\\rho = \\frac{\\mathbb E[(X - \\mathbb E(X))(Y - \\mathbb E(Y))]}{\\sqrt{\\mathbb V(X)\\mathbb V(Y)}}\\]\n\n\nCorrelation test problem:\n\\[H_0: \\rho = 0 \\quad \\text{VS}\\quad  H_1: \\rho \\neq 0\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#correlation-test-1",
    "href": "teaching/linear_model/slides/introduction.html#correlation-test-1",
    "title": "Introduction",
    "section": "Correlation test",
    "text": "Correlation test\n\nTest statistic (here we use \\(\\psi\\) for test statistics and \\(T\\) for tests) \\[\\psi(X,Y) = \\frac{\\hat \\rho\\sqrt{n-2}}{\\sqrt{1-\\hat \\rho^2}}\\]\n\n\nTest\nUnder \\(H_0\\), if \\((X,Y)\\) is Gaussian, \\(\\psi(X,Y) \\sim \\mathcal T(n-2)\\) (Student distribution of degree of freedom \\(n-2\\))\n\n\n\\[T(X,Y) = \\mathbf{1}\\{|\\psi(X,Y)| &gt; t_{1-\\alpha/2}\\}\\]\nIn R: cor.test"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#least-square-p1",
    "href": "teaching/linear_model/slides/introduction.html#least-square-p1",
    "title": "Introduction",
    "section": "Least Square \\((p=1)\\)",
    "text": "Least Square \\((p=1)\\)\n\nGiven observations \\((X_i, Y_i)\\), we consider \\(\\hat \\alpha\\), \\(\\hat \\mu\\) that minimize, over all \\((\\alpha, \\mu) \\in \\mathbb R^2\\):\n\n\\[\nL(\\alpha, \\mu) = \\sum_{i=1}^n (Y_i - \\alpha X_i - \\mu)^2\n\\]\n\n\n\nSolution: (check homogeneity!)\n\n\\[\\hat \\alpha = \\hat \\cov(X,Y)  \\quad \\text{and} \\quad \\hat \\mu = \\overline Y - \\hat a \\overline X\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#contingency-table-and-notation",
    "href": "teaching/linear_model/slides/introduction.html#contingency-table-and-notation",
    "title": "Introduction",
    "section": "Contingency Table and Notation",
    "text": "Contingency Table and Notation\n\nWe observe \\(X=(X_1, \\dots, X_n)\\) and \\(Y=(Y_1, \\dots, Y_n)\\), where\n\n\\(X_k \\in \\{1, \\dots, I\\}\\) (factor with \\(I\\) categories, “colors”)\n\\(Y_k \\in \\{1, \\dots, J\\}\\) (factor with \\(J\\) categories, “bags”)\n\n\n\n\n\n\nCategory X/Y\nBag 1\nBag 2\nBag 3\nTotals\n\n\n\n\nCol 1\n\\(n_{11}\\)\n\\(n_{12}\\)\n\\(n_{13}\\)\n\\(R_1\\)\n\n\nCol 2\n\\(n_{21}\\)\n\\(n_{22}\\)\n\\(n_{23}\\)\n\\(R_2\\)\n\n\nTotals\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n\\(N\\)\n\n\n\n\\(n_{ij}\\): number of individuals having category \\(i\\) for \\(X\\) and \\(j\\) for \\(Y\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#example-no2trafic-dataset",
    "href": "teaching/linear_model/slides/introduction.html#example-no2trafic-dataset",
    "title": "Introduction",
    "section": "Example: NO2trafic dataset",
    "text": "Example: NO2trafic dataset\ncontingency table of variable “Type” and “Fluidity”\n\n\n\nFluidity/Type\nP\nU\nA\nT\nV\n\n\n\n\nA\n21\n21\n19\n9\n9\n\n\nB\n20\n17\n16\n8\n7\n\n\nC\n17\n17\n16\n8\n7\n\n\nD\n20\n20\n18\n8\n8\n\n\n\nIn R: table(X,Y)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#in-julia",
    "href": "teaching/linear_model/slides/introduction.html#in-julia",
    "title": "Introduction",
    "section": "In Julia",
    "text": "In Julia\n\nfluidity_types = [\"A\", \"B\", \"C\", \"D\"]\ntype_p = [21, 20, 17, 20]\ntype_u = [21, 17, 17, 20]\ntype_a = [19, 16, 16, 18]\ntype_t = [9, 8, 8, 8]\ntype_v = [9, 7, 7, 8]\n\n# Create a matrix for the grouped bar plot\n# Each row represents a fluidity type, each column represents a measurement type\ndata_matrix = hcat(type_p, type_u, type_a, type_t, type_v)\n\n# Create a grouped bar plot\np1 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Dodge (Beside)\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:dodge,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\np2 = groupedbar(\n    fluidity_types,\n    data_matrix,\n    title=\"Stack\",\n    xlabel=\"Fluidity\",\n    ylabel=\"Value\",\n    label=[\"Type P\" \"Type U\" \"Type A\" \"Type T\" \"Type V\"],\n    legend=:topleft,\n    bar_position=:stack,\n    color=[:steelblue :orange :green :purple :red],\n    alpha=0.7,\n    size=(800, 500)\n)\nplot(p1,p2)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-problem",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test-problem",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test Problem",
    "text": "\\(\\chi^2\\) Dependency Test Problem\n\\(\\newcommand{\\VS}{\\quad \\mathrm{VS} \\quad}\\) \\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)\n\nWe observe\n\\(X=(X_1, \\dots, X_n) \\in \\{1, \\dots, I\\}^n\\) and \\(Y=(Y_1, \\dots, Y_n) \\in \\{1, \\dots, J\\}^n\\)\n\n\nAssumptions: \\((X_k,Y_k)\\) are independent, each pair has unknown distribution \\(P_{XY}\\)\n\n\ndependency test problem:\n\n\\[H_0: P_{XY}=P_{X}P_Y \\VS H_1: P_{XY} \\neq P_{X}P_{Y}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#definitions",
    "href": "teaching/linear_model/slides/introduction.html#definitions",
    "title": "Introduction",
    "section": "Definitions",
    "text": "Definitions\n\nEntries of the table:\n\n\\[n_{ij} = \\sum_{k=1}^n \\mathbf 1\\{X_{k} = i\\}\\mathbf 1\\{Y_k=j\\}\\]\n\n\n\nTotal proportion of individuals \\(k\\) of color \\(X_k =i\\):\n\n\n\\(\\hat p_{i}=\\frac{R_i}{N}\\) \\(= \\tfrac{1}{N}\\sum_{j=1}^{J}n_{ij}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#chi2-dependency-test",
    "href": "teaching/linear_model/slides/introduction.html#chi2-dependency-test",
    "title": "Introduction",
    "section": "\\(\\chi^2\\) Dependency Test",
    "text": "\\(\\chi^2\\) Dependency Test\n\nChi-squared statistic, or chi-squared distance:\n\n\\[\\psi(X,Y) = \\sum_{i=1}^I\\sum_{j=1}^J \\frac{(n_{ij}- N_j\\hat p_{i})^2}{N_j\\hat p_{i}}\\]\n\n\nApproximation: \\(\\psi(X,Y) \\sim \\chi^2((I-1)(J-1))\\) when \\(n \\to \\infty\\)\nTest: \\(T=\\mathbf 1\\{\\psi(X,Y) \\geq t_{1-\\alpha/2}\\}\\), where\n\\(t_{0.975}\\) = quantile(Chisq(I-1,J-1), 0.975)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#setting",
    "href": "teaching/linear_model/slides/introduction.html#setting",
    "title": "Introduction",
    "section": "Setting",
    "text": "Setting\n\nWe observe \\(X=(X_1, \\dots, X_n)\\) and \\(Y=(Y_1, \\dots, Y_n)\\), where\n\n\\(X_k \\in \\{1, \\dots, I\\}\\) (Quali)\n\\(Y_k \\in \\mathbb R\\) (Quanti)\n\n\n\nBoxplot: represents \\(0, 25, 75\\) and \\(100\\) percentiles."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#singer-dataset-julia-statsplots",
    "href": "teaching/linear_model/slides/introduction.html#singer-dataset-julia-statsplots",
    "title": "Introduction",
    "section": "Singer Dataset (Julia StatsPlots)",
    "text": "Singer Dataset (Julia StatsPlots)\n\n\\(X\\): Height (in inches), \\(Y\\): Type of singer"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#definitions-1",
    "href": "teaching/linear_model/slides/introduction.html#definitions-1",
    "title": "Introduction",
    "section": "Definitions",
    "text": "Definitions\n\n\\(X=(X_1, \\dots, X_n) \\in \\{1, \\dots, I\\}^n\\), \\(Y=(Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\nif \\(i \\in \\{1, \\dots, I\\}\\), we define partial means as\n\n\n\\[N_i = \\sum_{k=1}^n \\mathbf 1\\{X_k=i\\} \\and \\overline Y_i = \\frac{1}{N_i}\\sum_{k=1}^n Y_k \\mathbf 1\\{X_k=i\\}\\]\n\n\n\n\nTotal mean:\n\\(Y_i = \\frac{1}{N}\\sum_{k=1}^n Y_k = \\frac{1}{N}\\sum_{i=1}^I\\sum N_i \\overline Y_i\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#variance-decomposition",
    "href": "teaching/linear_model/slides/introduction.html#variance-decomposition",
    "title": "Introduction",
    "section": "Variance Decomposition",
    "text": "Variance Decomposition\n\n\n\n\\[\\frac{1}{n}\\underbrace{\\sum_{k=1}^n(Y_k - \\overline Y)^2}_{SST} =\n\\frac{1}{n}\\underbrace{\\sum_{i=1}^IN_i(\\overline Y_i - \\overline Y)^2}_{SSB}\n+ \\frac{1}{n}\\underbrace{\\sum_{k=1}^n\\mathbf 1\\{X_k=i\\}(Y_k - \\overline Y_i)^2}_{SSW}\\]\n\n\n\n\ncorrelation ratio:\n\n\\[ \\hat \\eta^2 = \\frac{SSB}{SST}  \\in [0,1]\\]\n\n\n\nThis is an estimator of unknown \\(\\eta = \\frac{\\mathbb V(\\mathbb E[Y|X])}{\\mathbb V(Y)}\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#anova-test",
    "href": "teaching/linear_model/slides/introduction.html#anova-test",
    "title": "Introduction",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\n\\((X_1, \\dots, X_n) \\in \\{1, \\dots I\\}^n\\)\n\\((Y_1, \\dots, Y_n) \\in \\mathbb R^n\\)\n\n\nAssumption: \\(Y_k\\) are independent, Gaussian of same variance. \\(\\mu_i = \\mathbb E[Y|X=i]=\\frac{\\mathbb E[Y\\mathbf 1\\{X=i\\}]}{\\mathbb P(X=i)}\\) (unknown)\n\n\nProblem:\n\n\\[H_0: \\mu_1=\\dots \\mu_I \\VS H_1: \\mu_i \\neq \\mu_j \\text{ for some $i,j$}\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#anova-test-1",
    "href": "teaching/linear_model/slides/introduction.html#anova-test-1",
    "title": "Introduction",
    "section": "ANOVA Test",
    "text": "ANOVA Test\n\nTest Statistic\n\n\\[\\psi(X,Y) = \\frac{SSB/(I-1)}{SSW/(N-I)}\\]\n\n\n\n\\(\\psi(X,Y) \\sim \\mathcal F(I-1, N-I)\\) under \\(H_0\\)\n\n\npvalue:\n1-cdf(FDist(I-1, N-1), psiobs)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#context",
    "href": "teaching/linear_model/slides/introduction.html#context",
    "title": "Introduction",
    "section": "Context",
    "text": "Context\n\n\\(n\\) individuals, \\(p\\) explanatory variables \\(X=(X^{(1)}, \\dots, X^{(p)})\\).\nGoal: Explain/Predict \\(Y\\) in function of \\(X\\)\n\n\nWe observe\n\\[Y=\\begin{pmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\and X=\\begin{pmatrix}\nX^{(1)}_1 & \\cdots & X^{(p)}_1 \\\\\nX^{(1)}_2 & \\cdots & X^{(p)}_1 \\\\\n\\vdots & & \\vdots \\\\\nX^{(1)}_n & \\cdots & X^{(p)}_1\n\\end{pmatrix}\\]\nEach individual \\(k\\) correspond to \\(Y_k\\) a row \\((X^{(1)}_k, \\dots, X^{(p)}_k)\\)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#where-is-randomness",
    "href": "teaching/linear_model/slides/introduction.html#where-is-randomness",
    "title": "Introduction",
    "section": "Where is Randomness?",
    "text": "Where is Randomness?\nGenerally, we don’t know any values a priori. Example:\n\nindividual characteristics of a customer\n\n\n\\(Y\\) and \\(X^{(1)}, \\ldots, X^{(p)}\\) are random variables.\n\n\nWe observe realizations the \\(Y\\)’s and \\(X\\)’s\n\n\nSometimes \\(X = (X^{(1)}, \\ldots, X^{(p)})\\) is chosen a priori. Example:\n\n\\(X\\): medication dosages (and \\(Y\\): a physiological response)\n\n\n\nIn this context, \\(Y\\) is random, but \\(X^{(1)}, \\ldots, X^{(p)}\\) are not."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#summary",
    "href": "teaching/linear_model/slides/introduction.html#summary",
    "title": "Introduction",
    "section": "Summary:",
    "text": "Summary:\n\n\\(Y\\) is always viewed as a random variable\n\n\n\\(X^{(1)}, \\ldots, X^{(p)}\\) are viewed as random variables or deterministic variables, depending on the context"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#general-model",
    "href": "teaching/linear_model/slides/introduction.html#general-model",
    "title": "Introduction",
    "section": "General Model",
    "text": "General Model\n\n\\(Y=(Y_1, \\dots, Y_n)\\)\n\\(X^{(k)} = (X^{(k)}_1, \\dots, X^{(k)}_n)\\), \\(k= 1, \\dots, p\\) (row notation)\n\n\nGeneral model:\n\n\\[Y_i = F(X_i^{(1)}, \\dots, X_i^{(p)}, \\varepsilon_i)\\]\n\n\n\\(F\\) is an unknown and deterministic function of \\(p\\) variables.\n\\(\\varepsilon_i\\) are iid random representing external independent noise\n\n\n\nNonparametric problem: space of all \\(F\\) is of infinite dimension!"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#linear-model",
    "href": "teaching/linear_model/slides/introduction.html#linear-model",
    "title": "Introduction",
    "section": "Linear Model",
    "text": "Linear Model\n\nIdea: reduce to a smaller class of function \\(F \\in \\mathcal F\\).\n\n\nLinear Model:\n\n\\[\nY_i = \\mu + \\beta_1 X^{(1)}_i + \\beta_2 X^{(2)}_i + \\dots + \\beta_p X^{(p)}_i + \\sigma \\varepsilon_i\n\\]\n\n\n\nSpace of affine function:\n\\[\n\\mathcal F = \\{F:~ F(x, \\varepsilon) = \\mu + \\beta^T x + \\sigma \\varepsilon, (\\mu, \\beta, \\sigma) \\in \\mathbb R^{p+2}\\}\n\\]\n\n\n\\(\\dim(\\mathcal F) = p+2\\) (number of unknown parameters)\n\n\nmuch easier to estimate \\(f\\) (and perhaps less overfitting)"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#wait-what-is",
    "href": "teaching/linear_model/slides/introduction.html#wait-what-is",
    "title": "Introduction",
    "section": "Wait, what is + ?",
    "text": "Wait, what is + ?\n\nIf the \\(X^{(k)}\\) are qualitative factors,\n\n\nWhat is the meaning of\n\\[\nY_i = \\mu + \\beta_1 X^{(1)}_i + \\beta_2 X^{(2)}_i + \\dots + \\beta_p X^{(p)}_i + \\sigma \\varepsilon_i\n\\]"
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#case-of-categorical-variables",
    "href": "teaching/linear_model/slides/introduction.html#case-of-categorical-variables",
    "title": "Introduction",
    "section": "Case of categorical variables",
    "text": "Case of categorical variables\n\nEncode each category\n\n\nIf \\(Y \\in \\{A, B\\}\\) has \\(2\\) categories, we encode \\[\\widetilde Y = \\mathbf 1\\{Y = A\\}\\]\n\n\nIf \\(Y\\in \\{A_1, \\dots, A_k\\}\\), we use one hot encoding:\n\\[\\widetilde{Y}_k = \\mathbf 1\\{Y=A_k\\}\\]\n\nAlso encode \\(X^{(1)}, \\ldots, X^{(p)}\\) if needed.\nsee also the chapter on ANOVA and ANCOVA."
  },
  {
    "objectID": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x",
    "href": "teaching/linear_model/slides/introduction.html#from-random-to-deterministic-x",
    "title": "Introduction",
    "section": "From random to deterministic \\(X\\)",
    "text": "From random to deterministic \\(X\\)\n\nIf \\(X^{(1)}, \\ldots, X^{(p)}\\) are random,\n\n\nThen for all deterministic \\(x^{(1)}, \\dots, x^{(p)}\\)\n\n\nConditionnally to \\((X^{(1)}=x^{(1)}, \\dots, X^{(p)} =x^{(p)})\\), we have the general model\n\\[Y = F(x^{(1)},\\dots, x^{(p)}, \\varepsilon)\\]\n\nBecause \\(\\varepsilon\\) is independent of \\(X\\)\nReplace \\(X^{(k)}\\) by their observations \\(x^{(k)}\\).\nThe only randomness is now in \\(\\varepsilon\\)!"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html",
    "href": "teaching/linear_model/notes/cramer-rao.html",
    "title": "Cramér-Rao Bound",
    "section": "",
    "text": "AI was used to assist with the formatting and writing of the proofs on this page."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#setup",
    "href": "teaching/linear_model/notes/cramer-rao.html#setup",
    "title": "Cramér-Rao Bound",
    "section": "Setup",
    "text": "Setup\nLet:\n\n\\(\\beta \\in \\mathbb R^p\\) be a vector of parameters\n\\(X=(X_1, \\dots, X_n) \\in \\mathbb R^n\\) be observations with joint pdf \\(f\\)\n\\(\\tilde \\beta\\) be an unbiased estimator of \\(\\beta\\), so \\(\\mathbb E[\\tilde\\beta]= \\mathbb E_{X\\sim f}[\\tilde\\beta] = \\beta\\)\n\\(s(x; \\beta) = \\nabla_{\\beta} \\log f(x; \\beta)\\) be the derivative of the log-likelihood"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#key-definitions",
    "href": "teaching/linear_model/notes/cramer-rao.html#key-definitions",
    "title": "Cramér-Rao Bound",
    "section": "Key Definitions",
    "text": "Key Definitions\nThe Fisher Information Matrix is: \\[I(\\beta) = E[s(x; \\beta)s(x; \\beta)^T]\\]\nUnder regularity conditions, this equals: \\[I(\\beta) = -E\\left[\\frac{\\partial^2 \\log f(x; \\beta)}{\\partial \\beta \\partial \\beta^T}\\right]\\]\n\n\n\n\n\n\nCramér-Rao (vector version)\n\n\n\nIn this context, it holds that \\[[I(\\beta)]^{-1} \\preceq \\mathbb V(\\tilde \\beta) \\; .\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\\(I(\\beta)\\) and \\(\\mathbb V(\\tilde \\beta)\\) are matrices\n\\(I(\\beta)\\) does not depend on the estimator, unlike \\(\\mathbb V(\\tilde \\beta)\\)."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#matrix-cauchy-schwarz-inequality",
    "href": "teaching/linear_model/notes/cramer-rao.html#matrix-cauchy-schwarz-inequality",
    "title": "Cramér-Rao Bound",
    "section": "Matrix Cauchy-Schwarz Inequality",
    "text": "Matrix Cauchy-Schwarz Inequality\nFor random vectors \\(U \\in \\mathbb R^p\\) and \\(V \\in \\mathbb R^q\\), the covariance satisfies: \\[\\text{Cov}(U, V)^T [\\text{Var}(V)]^{-1} \\text{Cov}(U, V) \\preceq \\text{Var}(U)\\]\nwhere \\(A\\preceq B\\) means \\(B-A\\) is positive semidefinite."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#proof-of-cramér-rao-bound",
    "href": "teaching/linear_model/notes/cramer-rao.html#proof-of-cramér-rao-bound",
    "title": "Cramér-Rao Bound",
    "section": "Proof of Cramér-Rao Bound",
    "text": "Proof of Cramér-Rao Bound\nSince \\(\\tilde \\beta\\) is unbiased, \\(\\mathbb E[\\tilde\\beta]\\) = \\(\\beta\\). Differentiating both sides with respect to \\(\\beta\\): \\[\\frac{\\partial}{\\partial \\beta} \\int \\tilde \\beta(x) f(x; \\beta) dx = I_p\\]\nwhere \\(I_p\\) is the p×p identity matrix.\nBy interchanging differentiation and integration (under regularity conditions): \\[\\int \\tilde \\beta(x) \\big(\\nabla_{\\beta}f(x; \\beta)\\big)^T dx = I_p\\]\nUsing the identity \\(\\nabla_{\\beta} f(x;\\beta)= f(x;\\beta)\\cdot\\nabla_{\\beta} \\log(f(x;\\beta))\\): \\[\\int \\tilde \\beta(x) f(x; \\beta) s(x; \\beta)^T dx = I_p\\]\nThis gives us: \\[E[\\tilde \\beta s^T] = I_p\\]\nSince \\(\\mathbb E[s]=0\\) (under regularity conditions), we have: \\[\\text{Cov}(\\tilde \\beta, s) = E[\\tilde \\beta s^T] - \\mathbb E[\\tilde\\beta]E[s]^T = I_p\\]\nApply the matrix Cauchy-Schwarz inequality with \\(U=\\tilde \\beta\\) and \\(V = s\\): \\[\\text{Cov}(\\tilde \\beta, s)^T [\\text{Var}(s)]^{-1} \\text{Cov}(\\tilde \\beta, s) \\preceq \\text{Var}(\\tilde \\beta)\\]\nSubstituting our results:\n\n\\(\\mathrm{Cov}(\\tilde \\beta, s) = I_p\\)\n\\(\\mathbb V(s) = I(\\beta)\\) (the Fisher Information Matrix)\n\nWe get: \\[I_p^T [I(\\beta)]^{-1} I_p \\preceq \\mathbb V(\\tilde \\beta)\\]\nSimplifying: \\[[I(\\beta)]^{-1} \\preceq \\text{Var}(\\tilde \\beta)\\]\nThis is the Cramér-Rao Lower Bound for vector parameters."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#interpretation",
    "href": "teaching/linear_model/notes/cramer-rao.html#interpretation",
    "title": "Cramér-Rao Bound",
    "section": "Interpretation",
    "text": "Interpretation\n\nFor any unbiased estimator \\(\\tilde \\beta\\) of \\(\\beta\\), its covariance matrix is bounded below by the inverse of the Fisher Information Matrix.\nFor a scalar function \\(c^T \\beta\\), we have: \\(\\mathbb V(c^T \\tilde \\beta) \\succeq c^T[I(\\beta)]^{-1}c\\)\n\nThis generalizes the scalar Cramér-Rao bound to the multivariate case using matrix inequalities."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#theorem",
    "href": "teaching/linear_model/notes/cramer-rao.html#theorem",
    "title": "Cramér-Rao Bound",
    "section": "Theorem",
    "text": "Theorem\nFor random vectors \\(U\\in \\mathbb R^p\\) and \\(V \\in \\mathbb R^q\\) with finite second moments, if \\(\\mathbb V(V)\\) is invertible, then: \\[\\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\text{Cov}(V, U) \\preceq \\text{Var}(U)\\]"
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#proof",
    "href": "teaching/linear_model/notes/cramer-rao.html#proof",
    "title": "Cramér-Rao Bound",
    "section": "Proof",
    "text": "Proof\nFor any matrix \\(A \\in \\mathbb R^{p \\times q}\\), consider: \\[\\text{Var}(U - AV) = \\text{Var}(U) - A\\text{Cov}(V, U) - \\text{Cov}(U, V)A^T + A\\text{Var}(V)A^T\\]\nTo minimize this quadratic form in A, take the derivative and set to zero: \\[\\frac{\\partial}{\\partial A} = -2\\text{Cov}(U, V) + 2A\\text{Var}(V) = 0\\]\nSolving gives: \\[A^* = \\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\]\nAt this minimum: \\[\\text{Var}(U - A^*V) = \\text{Var}(U) - \\text{Cov}(U, V)[\\text{Var}(V)]^{-1}\\text{Cov}(V, U) \\succeq 0\\]\nThis gives the generalized CS inequality."
  },
  {
    "objectID": "teaching/linear_model/notes/cramer-rao.html#connection-to-scalar-case",
    "href": "teaching/linear_model/notes/cramer-rao.html#connection-to-scalar-case",
    "title": "Cramér-Rao Bound",
    "section": "Connection to Scalar Case",
    "text": "Connection to Scalar Case\nWhen U and V are scalars, this reduces to: \\[\\frac{[\\text{Cov}(U,V)]^2}{\\text{Var}(V)} \\leq \\text{Var}(U)\\]\nWhich is equivalent to the familiar form: \\[[\\text{Cov}(U,V)]^2 \\leq \\text{Var}(U)\\text{Var}(V)\\]\nThe matrix version generalizes this to higher dimensions using positive semidefiniteness instead of simple inequality."
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html",
    "href": "teaching/linear_model/lectures/validation.html",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "If \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R) = q\\), and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR} \\sim F(q, n-p)\\]\nwhere:\n\n\\(SSR\\): sum of squares of residuals in the unconstrained model\n\\(SSR_c\\): sum of squares of residuals in the constrained model satisfying \\(R\\beta = 0\\)\n\n\n\n\n\n\nDefine the following spaces:\n\n\\(V = [X] \\subseteq \\mathbb{R}^n\\) (column space of \\(X\\))\n\\(V_0 = \\{X\\beta : R\\beta = 0\\} = X(\\text{Ker}(R))\\) (constrained subspace)\n\nSince \\(\\text{rank}(X) = p\\) and \\(\\text{rank}(R) = q\\):\n\nBy rank-nullity theorem: \\(\\dim(\\text{Ker}(R)) = p - q\\)\nSince \\(X\\) has full column rank, the map \\(\\beta \\mapsto X\\beta\\) is injective\nTherefore: \\(\\dim(V_0) = \\dim(X(\\text{Ker}(R))) = \\dim(\\text{Ker}(R)) = p - q\\)\nAlso: \\(\\dim(V) = p\\)\n\n\n\n\nSince \\(V_0 \\subseteq V\\), we can decompose \\(V\\) as: \\[V = V_0 \\oplus V_0^{\\perp_V}\\]\nwhere \\(V_0^{\\perp_V}\\) is the orthogonal complement of \\(V_0\\) within \\(V\\), with: \\[\\dim(V_0^{\\perp_V}) = \\dim(V) - \\dim(V_0) = p - (p-q) = q\\]\n\n\n\n\n\\(\\hat{y} = P_V y\\) is the projection of \\(y\\) onto \\(V\\) (unconstrained fit)\n\\(\\hat{y}_c = P_{V_0} y\\) is the projection of \\(y\\) onto \\(V_0\\) (constrained fit)\n\n\n\n\nSince \\(V_0 \\subseteq V\\), we have \\(\\hat{y}_c \\in V\\), and by the Pythagorean theorem: \\[\\|y - \\hat{y}_c\\|^2 = \\|y - \\hat{y}\\|^2 + \\|\\hat{y} - \\hat{y}_c\\|^2\\]\nThis gives us: \\[SSR_c = SSR + \\|\\hat{y} - \\hat{y}_c\\|^2\\]\nSince \\(\\hat{y} - \\hat{y}_c = P_V y - P_{V_0} y = P_{V_0^{\\perp_V}} y\\): \\[SSR_c - SSR = \\|P_{V_0^{\\perp_V}} y\\|^2\\]\n\n\n\nUnder \\(H_0: R\\beta = 0\\), we have \\(\\mathbb{E}[y] = X\\beta \\in V_0\\), which implies: - \\(P_{V_0} \\mathbb{E}[y] = X\\beta\\) - \\(P_{V_0^{\\perp_V}} \\mathbb{E}[y] = 0\\)\nSince \\(y = X\\beta + \\varepsilon\\) with \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\):\nFor the numerator: \\[P_{V_0^{\\perp_V}} y = P_{V_0^{\\perp_V}} \\varepsilon \\sim N(0, \\sigma^2 P_{V_0^{\\perp_V}})\\]\nSince \\(P_{V_0^{\\perp_V}}\\) is a projection onto a \\(q\\)-dimensional space: \\[\\frac{1}{\\sigma^2}\\|P_{V_0^{\\perp_V}} y\\|^2 = \\frac{SSR_c - SSR}{\\sigma^2} \\sim \\chi^2_q\\]\nFor the denominator: \\[P_{V^{\\perp}} y = P_{V^{\\perp}} \\varepsilon \\sim N(0, \\sigma^2 P_{V^{\\perp}})\\]\nSince \\(P_{V^{\\perp}}\\) is a projection onto an \\((n-p)\\)-dimensional space: \\[\\frac{1}{\\sigma^2}\\|P_{V^{\\perp}} y\\|^2 = \\frac{SSR}{\\sigma^2} \\sim \\chi^2_{n-p}\\]\n\n\n\nThe projections \\(P_{V_0^{\\perp_V}}\\) and \\(P_{V^{\\perp}}\\) are orthogonal because:\n\n\\(V_0^{\\perp_V} \\subseteq V\\)\n\\(V \\perp V^{\\perp}\\)\nTherefore \\(V_0^{\\perp_V} \\perp V^{\\perp}\\)\n\nThis implies \\(P_{V_0^{\\perp_V}} \\varepsilon\\) and \\(P_{V^{\\perp}} \\varepsilon\\) are independent.\n\n\n\nThe F-statistic is: \\[F = \\frac{(SSR_c - SSR)/q}{SSR/(n-p)} = \\frac{\\chi^2_q/q}{\\chi^2_{n-p}/(n-p)}\\]\nSince this is the ratio of two independent chi-squared random variables divided by their respective degrees of freedom, we have: \\[F \\sim F(q, n-p)\\]\n\n\n\n\nThe F-statistic measures the relative magnitude of:\n\nThe projection onto \\(V_0^{\\perp_V}\\) (the constraint violation space within the model)\nThe projection onto \\(V^{\\perp}\\) (the residual space)\n\nUnder \\(H_0\\), both projections capture only noise, leading to the F-distribution. Large values of \\(F\\) suggest the constraint \\(R\\beta = 0\\) is violated."
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html#theorem",
    "href": "teaching/linear_model/lectures/validation.html#theorem",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "If \\(\\text{rank}(X) = p\\), \\(\\text{rank}(R) = q\\), and \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), then under \\(H_0: R\\beta = 0\\):\n\\[F = \\frac{n-p}{q} \\cdot \\frac{SSR_c - SSR}{SSR} \\sim F(q, n-p)\\]\nwhere:\n\n\\(SSR\\): sum of squares of residuals in the unconstrained model\n\\(SSR_c\\): sum of squares of residuals in the constrained model satisfying \\(R\\beta = 0\\)"
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html#proof",
    "href": "teaching/linear_model/lectures/validation.html#proof",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "Define the following spaces:\n\n\\(V = [X] \\subseteq \\mathbb{R}^n\\) (column space of \\(X\\))\n\\(V_0 = \\{X\\beta : R\\beta = 0\\} = X(\\text{Ker}(R))\\) (constrained subspace)\n\nSince \\(\\text{rank}(X) = p\\) and \\(\\text{rank}(R) = q\\):\n\nBy rank-nullity theorem: \\(\\dim(\\text{Ker}(R)) = p - q\\)\nSince \\(X\\) has full column rank, the map \\(\\beta \\mapsto X\\beta\\) is injective\nTherefore: \\(\\dim(V_0) = \\dim(X(\\text{Ker}(R))) = \\dim(\\text{Ker}(R)) = p - q\\)\nAlso: \\(\\dim(V) = p\\)\n\n\n\n\nSince \\(V_0 \\subseteq V\\), we can decompose \\(V\\) as: \\[V = V_0 \\oplus V_0^{\\perp_V}\\]\nwhere \\(V_0^{\\perp_V}\\) is the orthogonal complement of \\(V_0\\) within \\(V\\), with: \\[\\dim(V_0^{\\perp_V}) = \\dim(V) - \\dim(V_0) = p - (p-q) = q\\]\n\n\n\n\n\\(\\hat{y} = P_V y\\) is the projection of \\(y\\) onto \\(V\\) (unconstrained fit)\n\\(\\hat{y}_c = P_{V_0} y\\) is the projection of \\(y\\) onto \\(V_0\\) (constrained fit)\n\n\n\n\nSince \\(V_0 \\subseteq V\\), we have \\(\\hat{y}_c \\in V\\), and by the Pythagorean theorem: \\[\\|y - \\hat{y}_c\\|^2 = \\|y - \\hat{y}\\|^2 + \\|\\hat{y} - \\hat{y}_c\\|^2\\]\nThis gives us: \\[SSR_c = SSR + \\|\\hat{y} - \\hat{y}_c\\|^2\\]\nSince \\(\\hat{y} - \\hat{y}_c = P_V y - P_{V_0} y = P_{V_0^{\\perp_V}} y\\): \\[SSR_c - SSR = \\|P_{V_0^{\\perp_V}} y\\|^2\\]\n\n\n\nUnder \\(H_0: R\\beta = 0\\), we have \\(\\mathbb{E}[y] = X\\beta \\in V_0\\), which implies: - \\(P_{V_0} \\mathbb{E}[y] = X\\beta\\) - \\(P_{V_0^{\\perp_V}} \\mathbb{E}[y] = 0\\)\nSince \\(y = X\\beta + \\varepsilon\\) with \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\):\nFor the numerator: \\[P_{V_0^{\\perp_V}} y = P_{V_0^{\\perp_V}} \\varepsilon \\sim N(0, \\sigma^2 P_{V_0^{\\perp_V}})\\]\nSince \\(P_{V_0^{\\perp_V}}\\) is a projection onto a \\(q\\)-dimensional space: \\[\\frac{1}{\\sigma^2}\\|P_{V_0^{\\perp_V}} y\\|^2 = \\frac{SSR_c - SSR}{\\sigma^2} \\sim \\chi^2_q\\]\nFor the denominator: \\[P_{V^{\\perp}} y = P_{V^{\\perp}} \\varepsilon \\sim N(0, \\sigma^2 P_{V^{\\perp}})\\]\nSince \\(P_{V^{\\perp}}\\) is a projection onto an \\((n-p)\\)-dimensional space: \\[\\frac{1}{\\sigma^2}\\|P_{V^{\\perp}} y\\|^2 = \\frac{SSR}{\\sigma^2} \\sim \\chi^2_{n-p}\\]\n\n\n\nThe projections \\(P_{V_0^{\\perp_V}}\\) and \\(P_{V^{\\perp}}\\) are orthogonal because:\n\n\\(V_0^{\\perp_V} \\subseteq V\\)\n\\(V \\perp V^{\\perp}\\)\nTherefore \\(V_0^{\\perp_V} \\perp V^{\\perp}\\)\n\nThis implies \\(P_{V_0^{\\perp_V}} \\varepsilon\\) and \\(P_{V^{\\perp}} \\varepsilon\\) are independent.\n\n\n\nThe F-statistic is: \\[F = \\frac{(SSR_c - SSR)/q}{SSR/(n-p)} = \\frac{\\chi^2_q/q}{\\chi^2_{n-p}/(n-p)}\\]\nSince this is the ratio of two independent chi-squared random variables divided by their respective degrees of freedom, we have: \\[F \\sim F(q, n-p)\\]"
  },
  {
    "objectID": "teaching/linear_model/lectures/validation.html#geometric-interpretation",
    "href": "teaching/linear_model/lectures/validation.html#geometric-interpretation",
    "title": "F-Test for Linear Constraints",
    "section": "",
    "text": "The F-statistic measures the relative magnitude of:\n\nThe projection onto \\(V_0^{\\perp_V}\\) (the constraint violation space within the model)\nThe projection onto \\(V^{\\perp}\\) (the residual space)\n\nUnder \\(H_0\\), both projections capture only noise, leading to the F-distribution. Large values of \\(F\\) suggest the constraint \\(R\\beta = 0\\) is violated."
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problem-with-binary-tasks",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problem-with-binary-tasks",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problem with Binary Tasks",
    "text": "Crowdsourcing Problem with Binary Tasks\n\nImage Classification: “Does this image contain a dog?”\nText Moderation: “Is this comment toxic or offensive?”\nSentiment Analysis: “Does this review express positive sentiment?”\nData Verification: “Is this information factually correct?”"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#main-question",
    "href": "research/crowdsourcing/presentation.html#main-question",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Question",
    "text": "Main Question\n\nGiven \\(n\\) workers and \\(d\\) binary tasks\n\n\nA proportion \\(\\lambda\\) of observations\n\n\nHow can we accurately recover the labels?"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#illustration",
    "href": "research/crowdsourcing/presentation.html#illustration",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration",
    "text": "Illustration\n\nTwo binary tasks with labels in \\(\\{\\color{green}{-1},\\color{blue}{+1}\\}\\)\nUnknown vector \\(x^*\\) of labels with \\(d=9\\) tasks \\(x^*= (\\color{blue}{+1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{green}{-1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1},\\color{blue}{+1})\\)\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{blue}{+1}\n\\end{array}\\right)\n\\]\n\n\n\n\n\\[\nY=\\left(\\begin{array}{ccccccccc}\n\\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{red}{0} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{red}{0} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} \\\\\n\\color{blue}{+1} & \\color{green}{-1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{blue}{+1} & \\color{blue}{+1} \\\\\n\\color{green}{-1} & \\color{blue}{+1} & \\color{blue}{+1} & \\color{red}{0} & \\color{green}{-1} & \\color{blue}{+1} & \\color{red}{0} & \\color{red}{0} & \\color{red}{0}\n\\end{array}\\right)\n\\]\n\n\n\n\nRate of observations: \\(\\color{red}{\\lambda= 0.72}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#observation-model",
    "href": "research/crowdsourcing/presentation.html#observation-model",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nGiven workers \\(i\\in\\{1, \\dots, n\\}\\) and tasks \\(k\\in\\{1, \\dots, d\\}\\)\n\n\nWe observe\n\n\\[\nY_{ik} = B_{ik}(M_{ik}x_k^* + E_{ik}) \\enspace\n\\]\n\n\nWhere:\n\n\\(M_{ik} \\in [0,1]\\)\n\\(E_{ik}\\) are independent and \\(1\\)-subGaussian\n\\(B_{ik}\\) are iid Bernoulli \\(\\lambda \\in [0,1]\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#observation-model-1",
    "href": "research/crowdsourcing/presentation.html#observation-model-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Observation Model",
    "text": "Observation Model\n\nWe observe\n\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\nwhere \\(\\odot\\) is the Hadamard product.\n\n\\(M \\in [0,1]^{n \\times d}\\) is the ability matrix\n\\(x^*\\) is the vector of unknown labels\n\\(E\\) is a matrix of independent noise\n\\(B\\) is a Bernoulli “mask” matrix, modelling partial observations"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#bernoulli-observation-submodel",
    "href": "research/crowdsourcing/presentation.html#bernoulli-observation-submodel",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Bernoulli Observation SubModel",
    "text": "Bernoulli Observation SubModel\n\\[Y = B\\odot(M\\mathrm{diag}(x^*) + E) \\in \\mathbb R^{n \\times d}\\]\n\n\n\n\nBernoulli submodel (Shah et al., 2020)\n\n\n\\[\n\\begin{aligned}\\label{eq:bernoulli_model}\n    Y_{ik} = \\begin{cases}\n        x^*_k \\text{ with proba } \\lambda\\left(\\frac{1+M_{ik}}{2}\\right)\n        \\\\\n        -x^*_k \\text{ with proba } \\lambda\\left(\\frac{1-M_{ik}}{2}\\right)\\\\\n        0 \\text{ with proba } 1-\\lambda\n        \\end{cases}\n\\end{aligned}\n\\]\n\n\n\n\n\n\\(\\frac{1+M_{ik}}{2}\\) is the proba that \\(i\\) answers correctly to task \\(k\\).\n\n\n\\(\\lambda\\) is the probability of observing worker/task pair \\((i,k)\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#shape-constraints",
    "href": "research/crowdsourcing/presentation.html#shape-constraints",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\n\\(M \\in [0,1]^{n \\times d}\\) represents the ability matrix of worker/task pairs\nWe assume that \\(M\\) is isotonic up to a permutation \\(\\pi^*\\) , that is\n\n\n\n\\[\nM_{\\pi^*}=\\left(\\begin{array}{ccccccccc}\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8} \\\\\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6}\n\\end{array}\\right)\n\\]\n\n\n\\[\nM_{\\phantom{{\\pi^*}}}=\\left(\\begin{array}{ccccccccc}\n\\color{#990066}{0.6} & \\color{#990066}{0.7} & \\color{#990066}{0.7} & \\color{#990066}{0.6} \\\\\n\\color{#CC0000}{0.5} & \\color{#CC0000}{0.7} & \\color{#CC0000}{0.5} & \\color{#CC0000}{0.6} \\\\\n\\color{#000099}{0.9} & \\color{#000099}{0.8} & \\color{#000099}{0.9} & \\color{#000099}{1\\phantom{.0}} \\\\\n\\color{#4B0082}{0.8} & \\color{#4B0082}{0.7} & \\color{#4B0082}{0.9} & \\color{#4B0082}{0.8}\n\\end{array}\\right)\n\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#shape-constraints-1",
    "href": "research/crowdsourcing/presentation.html#shape-constraints-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Shape Constraints",
    "text": "Shape Constraints\n\nFor all \\(k = 1, \\dots, d\\), \\[\nM_{\\pi^*(1), k}\\geq \\dots \\geq M_{\\pi^*(n),k}\n\\]\nIt means that the rows are uniformly increasing\nA worker \\(i\\) is better on average than \\(j\\), if it is better on all tasks on average"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#illustration-when-x-is-known",
    "href": "research/crowdsourcing/presentation.html#illustration-when-x-is-known",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Illustration when \\(x^*\\) is Known",
    "text": "Illustration when \\(x^*\\) is Known"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problems",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problems",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\n\nObjectives\n\n\nRecovering the labels\n\n\nRanking the workers\n\n\nEstimating their abilities\n\n\n\nLosses\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\)\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2\\)\n\n\n\n\\(\\phantom{\\color{purple}{\\mathbb E}}\\|M - \\hat M\\|_F^2\\)\n\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "href": "research/crowdsourcing/presentation.html#crowdsourcing-problems-1",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Crowdsourcing Problems",
    "text": "Crowdsourcing Problems\n\n\n\n\nSetting\n\n\n\\[ Y = B\\odot(M \\mathrm{diag}(x^*) + E)\\] Shape constraint: \\(\\exists \\pi^*\\) s.t. \\(M_{\\pi^*} \\in [0,1]^{n \\times d}\\) is isotonic\n\n\n\n\n\n\nObjectives\nRecovering the labels\n\nRanking the workers\n\nEstimating their abilities\n\nRisks\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)\n\n\nThese three objectives are closely intertwined!"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "href": "research/crowdsourcing/presentation.html#square-norm-loss-vs-hamming-loss",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Square Norm Loss VS Hamming Loss",
    "text": "Square Norm Loss VS Hamming Loss\n\n\n\n\nHamming Loss: \\[ \\sum_{k=1}^d \\mathbf 1\\{\\hat x_k \\neq x_k^*\\}\\]\n\n\n\n\n\nSquare Norm Loss: \\[ \\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2\\]\n\n\n\n\nIf workers are bad, \\(M\\) is small but Hamming Loss is large\nIf \\(M \\sim 0\\), Hamming loss \\(\\sim d\\)\nSquare norm loss evaluates the quality of \\(\\hat x\\) rather than workers"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#other-parametric-models",
    "href": "research/crowdsourcing/presentation.html#other-parametric-models",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Other Parametric Models",
    "text": "Other Parametric Models\n\nDS: \\(M_{ik} = q_i\\) (Dawid & Skene, 1979), (Shah et al., 2020)\n\nThe abilities are independent of the tasks\n\nBTL: \\(M_{ik}=\\phi(a_i-b_k)\\) (Bradley & Terry, 1952)\n\n\\(a_i\\): abilities of the workers\n\\(b_i\\): difficulties of the tasks\n\n\n\nThese parametric models often fail to fit data well"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#non-parametric-models",
    "href": "research/crowdsourcing/presentation.html#non-parametric-models",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Non-Parametric Models",
    "text": "Non-Parametric Models\n\nknown labels \\(x^*\\), \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic (Mao et al., 2020) (Liu & Moitra, 2020)\nknown labels \\(x^*\\), \\(M_{\\pi^*}\\) is isotonic (Flammarion et al., 2019) (Pilliat et al., 2024)\nunknown labels \\(x^*\\), \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic (Shah et al., 2020)\nunknown labels \\(x^*\\), \\(M_{\\pi^*}\\) is isotonic (this paper)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#risks-recap",
    "href": "research/crowdsourcing/presentation.html#risks-recap",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Risks Recap",
    "text": "Risks Recap\n\n\n\n\nObjectives\n\n\nRecovering the labels\n\nRanking the workers\n\nEstimating their abilities\n\nRisks\n\n\n\\(\\color{purple}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\)\n\n\\(\\color{purple}{\\mathbb E}[\\|M - \\hat M\\|_F^2]\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#minimax-recovering-risk",
    "href": "research/crowdsourcing/presentation.html#minimax-recovering-risk",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "MiniMax Recovering Risk",
    "text": "MiniMax Recovering Risk\n\n\n\n\nMax risk for recovering labels\n\n\n\\[\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\nmaximize on all \\(M\\), \\(\\pi^*\\), \\(x^*\\) such that \\(M_{\\pi^*}\\) is isotonic\n\n\n\n\n\n\n\n\n\nMiniMax risk for recovering labels\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\nminimize on all estimator \\(\\hat x\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#minimax-risks",
    "href": "research/crowdsourcing/presentation.html#minimax-risks",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Minimax Risks",
    "text": "Minimax Risks\n\n\n\n\nRecovering labels\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)=\\min_{\\hat x}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M \\mathrm{diag}(x^* \\neq \\hat x)\\|_F^2]\\]\n\n\n\n\n\n\n\n\nRanking workers\n\n\n\\[\\mathcal R^*_{\\mathrm{rk}}(n,d,\\lambda)=\\min_{\\hat \\pi}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M_{\\pi^*} - M_{\\hat \\pi}\\|_F^2]\\]\n\n\n\n\n\n\n\n\nEstimating abilities\n\n\n\\[\\mathcal R^*_{\\mathrm{est}}(n,d,\\lambda)=\\min_{\\hat M}\\max_{M,\\pi^*, x^*}{\\mathbb E}[\\|M- \\hat M\\|_F^2]\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#short-story",
    "href": "research/crowdsourcing/presentation.html#short-story",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Short Story",
    "text": "Short Story\n\n(Shah et al., 2020): recovering \\(x^*\\) optimally using a least square method, conjectured NP hard (\\(x^*\\) unknown, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic).\n\n\n(Mao et al., 2020): estimating abilities \\(M\\) of workers optimally with least square method (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Liu & Moitra, 2020): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): improve state of the art poly. time (\\(x^*\\) known, \\(M_{\\pi^*\\eta^*}\\) bi-isotonic)\n\n\n(Pilliat et al., 2024): ranking \\(\\pi^*\\) and estimating abilities \\(M\\): achieves rates of Liu & Moitra (2020) without bi-isotonic assumption (\\(x^*\\) known, \\(M_{\\pi^*}\\) isotonic)\n\n\nThis paper: recovering \\(x^*\\), ranking \\(\\pi^*\\) and estimating abilities \\(M\\) in poly. time when \\(n=d\\) (\\(x^*\\) unknown, \\(M_{\\pi^*}\\) isotonic)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#main-results",
    "href": "research/crowdsourcing/presentation.html#main-results",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Main Results",
    "text": "Main Results\n\n\n\n\nOptimal poly. time method\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), there exists a poly. time method \\(\\hat x\\) achieving \\(\\mathcal R^*_{\\mathrm{reco}}\\) up to polylog factors, i.e. \\[\n\\mathcal R_{\\mathrm{reco}}(n,d,\\lambda, \\hat x) \\lesssim \\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda)\n\\]\n\n\n\n\n\n\n\n\nMinimax Risk\n\n\nIf \\(\\tfrac{1}{\\lambda} \\leq n \\leq d\\), up to polylogs, \\[\n\\mathcal R^*_{\\mathrm{reco}}(n,d,\\lambda) \\asymp \\frac{d}{\\lambda}\n\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#majority-vote",
    "href": "research/crowdsourcing/presentation.html#majority-vote",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Majority Vote",
    "text": "Majority Vote\n\n\\[ \\hat x^{(maj)}_k = \\mathrm{sign} \\left( \\sum_{i=1}^n Y_{ik} \\right) \\enspace .\\]\n\n\n\n\n\nMax risk of majority vote\n\n\n\\[\\mathcal R^*_{\\mathrm{reco}}(n, d, \\lambda, \\hat x^{(maj)}) \\asymp \\tfrac{d \\sqrt{n}}{\\lambda}\\]\n\n\n\n\n\nWorst case (\\(\\lambda=1\\)): \\(M \\asymp \\frac{1}{\\sqrt{n}}(\\mathbf 1_{n\\times d})\\)\n\n\nIn this case, \\(\\hat x^{(maj)}\\) is no better than random labelling and \\(\\|M\\mathrm{diag}(\\hat x \\neq x^*)\\|_F^2 \\asymp d\\sqrt{n}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#least-square-conjectured-np-hard",
    "href": "research/crowdsourcing/presentation.html#least-square-conjectured-np-hard",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Least square (conjectured NP Hard)",
    "text": "Least square (conjectured NP Hard)\n\nMinimize\n\n\\[\\|Y- \\lambda M' \\mathrm{diag}(x)\\|_F^2\\]\n\n\nover all \\(x \\in \\{-1, 1\\}^d\\)\nand \\(M'\\) isotonic, up to a permutation \\(\\pi^*\\)\n\n\n\nThe set of isotonic matrices is convex…\n\n\n\nBut not isotonic matrices up to a permutation \\(\\pi^*\\)\n\n\nIt is minimax optimal (Shah et al., 2020)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "href": "research/crowdsourcing/presentation.html#obi-wan-shah2020permutation",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "OBI-WAN (Shah et al., 2020)",
    "text": "OBI-WAN (Shah et al., 2020)\n\nIdea: Population term \\((M\\mathrm{diag}(x^*))(M\\mathrm{diag}(x^*))^T\\) is independent of \\(x^*\\)\n\n\nPCA Step:\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y\\|^2\\]\n\n\n\n(Shah et al., 2020) sort \\(|\\hat v|\\) to get a partial ranking\n\n\nAggregation: Majority vote on \\(k\\) top experts according to \\(|\\hat v|\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "href": "research/crowdsourcing/presentation.html#result-on-obi-wan-method",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Result on Obi-Wan Method",
    "text": "Result on Obi-Wan Method\n\n\n\n\n\nTheorem 1 (Shah et al., 2020)\n\n\nIn submodel where \\(\\mathrm{rk}(M)= 1\\), \\(\\hat x^{(\\mathrm{Obi-Wan})}\\) achieves minimax risk up to polylogs: \\[\\mathcal R(n,d, \\lambda, \\hat x^{(\\mathrm{Obi-Wan})}) \\lesssim \\frac{d}{\\lambda} \\quad \\textbf{(minimax)}\\]\n\n\n\n\n\n\n\n\n\n\nTheorem 2 (Shah et al., 2020)\n\n\nIn the model where \\(M_{\\pi^*\\eta^*}\\) is bi-isotonic up to polylogs: \\[\\mathcal R(n,d, \\lambda, \\hat x^{(\\mathrm{Obi-Wan})}) \\lesssim \\frac{\\sqrt{n}d}{\\lambda} \\quad \\textbf{(not minimax)}\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#subsampling",
    "href": "research/crowdsourcing/presentation.html#subsampling",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Subsampling",
    "text": "Subsampling\n\nLet \\(T \\geq 1\\). We generate \\(T\\) samples \\((Y^{(1)}, \\dots, Y^{(T)})\\) from \\(Y\\).\n\n\nPut \\(Y_{ik}\\) uniformly at random into one of the \\(Y^{(s)}\\).\n\n\\(Y_{ik}^{(s)}= 0\\) for all \\(s\\) except one, which is \\(Y_{ik}\\)\nThe \\((Y^{(s)})\\)’s are not independent!\nTechnical trick: condition on the sampling scheme"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#pca-step",
    "href": "research/crowdsourcing/presentation.html#pca-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "PCA Step",
    "text": "PCA Step\n\n\n\\[\\hat v = \\underset{\\|v\\|=1}{\\mathrm{argmax}}\\|v^T Y^{(1)}\\|^2 \\quad \\text{and} \\quad \\tilde v = \\hat v \\land \\sqrt{\\lambda / T}\\]\n\n\n\nMain idea: if \\(M\\) is isotonic, then up to a polylog\n\n\\[\\|MM^T\\|_{\\mathrm{op}} \\gtrsim \\|M\\|_F^2\\]\n\n\n\nIdea for the proof: if \\(\\|M\\|_F^2 \\gg \\frac{d}{\\lambda}\\), then \\(\\|\\tilde v^T M\\| \\gtrsim \\|M\\|_F^2\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#voting-step",
    "href": "research/crowdsourcing/presentation.html#voting-step",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Voting Step",
    "text": "Voting Step\n\nDefine the weighted vote vector\n\n\\[\\hat w = \\tilde v^T Y^{(2)}\\]\n\n\n\nDefine the estimated label as\n\n\\[\\hat x_k^{(1)} = \\mathrm{sign}(\\hat w_k)\\mathbf{1}\\bigg\\{|\\hat w_k| \\gg \\sqrt{\\sum_{i=1}^n \\tilde v_i B_{ik}^{(2)}}\\bigg\\}\\]"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#iterate",
    "href": "research/crowdsourcing/presentation.html#iterate",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Iterate",
    "text": "Iterate\n\nRepeat PCA Step + Voting Step on \\(Y \\mathrm{diag}(\\hat x \\neq 0)\\) a polylogarithmic number of times.\n\n\nWe get \\(\\hat x^{(1)}, \\hat x^{(2)}, \\dots, \\hat x^{(T)}\\)\n\n\nOutput \\(\\hat x^{(T)}\\)\n\\(\\newcommand{\\and}{\\quad \\mathrm{and} \\quad}\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#proof-idea",
    "href": "research/crowdsourcing/presentation.html#proof-idea",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Proof Idea",
    "text": "Proof Idea\n\nLet \\(M(t) = M\\mathrm{diag}(x^{(t-1)} = 0)\\)\n\n\nWhile \\(M(t) \\gg d/\\lambda\\), we prove that\n\n\\[\\|\\tilde v^TM(t)\\|_2^2 \\gtrsim \\|M\\|_F^2 \\and \\|\\tilde v^TM(t+1)\\|_2^2 \\lesssim d/\\lambda\\]\n\n\n\nBy Pythagoeran Theorem, we have\n\n\\[\\|M(t)\\|_F^2 - \\|M(t+1)\\|_F^2 \\geq \\|\\tilde v M(t)\\|_2^2 - \\|\\tilde v^TM(t+1)\\|_2^2\\]\n\n\n\nThis leads to exponential decay of \\(\\|M(t)\\|_F^2\\) until \\(M(t) \\leq d/\\lambda\\)"
  },
  {
    "objectID": "research/crowdsourcing/presentation.html#synthetic-data",
    "href": "research/crowdsourcing/presentation.html#synthetic-data",
    "title": "Recovering Labels from Crowdsourced Data",
    "section": "Synthetic Data",
    "text": "Synthetic Data\n\n\n\n\n\n\n\n\n\n \n\nBlack: \\(M_{ik}=0\\)\nBlue: \\(M_{ik} = h\\)"
  }
]